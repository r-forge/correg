\documentclass[12pt]{article}
%
%
% Retirez le caractere "%" au debut de la ligne ci--dessous si votre
% editeur de texte utilise des caracteres accentues
\usepackage[utf8]{inputenc}
%
% Retirez le caractere "%" au debut des lignes ci--dessous si vous
% utiisez les symboles et macros de l'AMS
\usepackage{amsmath}
\usepackage{amsfonts}
%
%
\setlength{\textwidth}{16cm}
\setlength{\textheight}{21cm}
\setlength{\hoffset}{-1.4cm}
%
%
\usepackage{graphicx}

 \graphicspath{{figures/}}

\begin{document}

     \def\Var{{\rm Var}\,}

%--------------------------------------------------------------------------

\begin{center}
{\Large
	{\sc  CorReg : Régression sur variables Corrélées\\ et Application à l'industrie Sidérurgique}
}
\bigskip

  Clément Théry$^{1}$ \& Christophe Biernacki$^{2}$ \& Gaétan Loridant$^{3}$
\bigskip

{\it
$^{1}$ ArcelorMittal, Université Lille 1, Inria, CNRS, clement.thery@inria.fr
 
$^{2}$ Université Lille 1, Inria, CNRS, christophe.biernacki@math.univ-lille1.fr

$^{3}$ Etudes Industrielles ArcelorMittal Dunkerque, gaetan.loridant@arcelormittal.com\textbf{}
}
\end{center}
\bigskip

%--------------------------------------------------------------------------

{\bf Résumé.} La régression linéaire suppose en général l'usage de variables explicatives décorrélées, hypothèse souvent irréaliste pour les bases de données d'origine industrielle où de nombreuses corrélations sont dues au process, à des lois physiques, {\it etc}. Le modèle  proposé explicite les corrélations présentes sous la forme d'une famille de régressions linéaires entre covariables, permettant d'obtenir par marginalisation un modèle de régression parcimonieux libéré des corrélations, facilement interprétable et compatible avec les méthodes de sélection de variables. La structure de corrélations est estimée à l'aide d'un algorithme de type MCMC. Un package R (\textsc{CorReg}) permet la mise en oeuvre de cette méthode qui sera illustrée sur données simulées et sur données réelles issues de l'industrie sidérurgique.
\smallskip

{\bf Mots-clés.} Régression, corrélations, industrie, sélection de variables, modèles génératifs
\bigskip\bigskip

{\bf Abstract.} Linear regression generally suppose to have decorrelated covariates. This hypothesis is often irrealist with industrial datasets that contains many highly correlated covariates due to the process, physcial laws,  {\it etc}. The proposed generative model consists in explicit modeling of the correlations with a family of linear regressions between the covariates permitting to obtain by marginalization a parsimonious correlation-free regression model, easily understandable and compatible with variable selection methods. The structure of correlations is found with an MCMC algorithm. An R package (\textsc{CorReg}) implements this new method which will be illustrated on both simulated datasets and real-life datasets from steel industry.
\smallskip

{\bf Keywords.} Regression, correlations, industry, variable selection, generative models

%--------------------------------------------------------------------------

\section{Introduction}
	La régression linéaire classique suppose la décorrélation des covariables, source de problèmes en termes de variance des estimateurs. En effet, pour une variable réponse $Y$ et un ensemble de covariables $X \in \mathcal{R}^{n \times p}$, la régression $Y=XA+\varepsilon$ avec $ \varepsilon\sim \mathcal{N}(0,\sigma^2I_n)$ et $A$ vecteur des $p$ coefficients donne un estimateur de variance $ \Var(\hat{A}|X)= \sigma^2(X'X)^{-1}$ dégénéré si les colonnes de $X$ sont linéairement corrélées. Les méthodes de sélection comme le LASSO [4] muni du LAR [1] sont elles-mêmes touchées par ce problème de corrélation [5].
		\\
		
		Notre idée est de modéliser explicitement les corrélations présentes entre covariables sous la forme d'une famille de régression entre celles-ci. Nous présenterons donc en première partie le modèle génératif associé puis l'algorithme MCMC permettant d'estimer la famille de régression à utiliser avant d'illustrer dans les parties 3 et 4 l'efficacité de la méthode sur des données simulées puis sur des données réelles avant de conclure.
		
\section{Modèle supprimant les covariables corrélées}		
	On suppose le modèle génératif suivant :
	
	\begin{itemize}
		\item Régression principale entre $Y$ et $X$:
			 \begin{equation}
			 	Y_{|X,S}=XA+\varepsilon_Y= X_1A_{1}+X_2A_{2}+\varepsilon_Y \textrm{ avec } \varepsilon_Y \sim \mathcal{N}(0,\sigma_Y^2I_n) \label{MainR}
			 \end{equation}
		\item Famille de $p_2$ régressions entre covariables de $X$ corrélées :
			\begin{equation}
			\forall j \in I_2 : \  X^j_{|X_1,S}=X_1B_{1}^j + \varepsilon_{j} \textrm{ avec } \varepsilon_j \sim \mathcal{N}(0,\sigma_j^2I_n) \label{SR}
			\end{equation}
		\item Mélanges gaussiens indépendants pour les covariables non corrélées :
			\begin{equation}
				\forall j \notin I_2 : \ X^j \sim \sum_{k=1}^{k_j}\pi_k\mathcal{N}(\mu_{k_j},\sigma_{k_j}^2I_n)
			\end{equation}
	\end{itemize}
	
où %$B_{I_1^j}^j$ est le vecteur de taille $p_1^j$ des coefficients de la sous-régression en $X^j$,\\ 
$I_1=\{I_1^1,\dots,I_1^{p}\}$ est le vecteur des indices des variables à droite dans (\ref{SR}) , $I_2=\{j |\sharp I_1^j>0 \}$ est l'ensemble des indices des variables corrélées à gauche dans (\ref{SR}). Les $B_1^j$ sont les coefficients des régressions entre covariables. On a donc une partition des données $X=(X_1,X_2)$ où $X_2=X^{I_2}$ et $X_1=X\setminus X_2$.\\
On suppose $I_1\cap I_2=\emptyset$, $i.e.$ les variables dépendantes dans $X$ n'en expliquent pas d'autres.
On note $p_2= \sharp I_2$ le nombre de régressions entre covariables et $p_1=(p_1^1,\dots,p_1^{p_2})$ qui est le vecteur des longueurs des régressions au sein de $X$.
	
On a ainsi rendu explicites les corrélations au sein de $X$ sous la forme d'une structure de sous-régressions linéaires $S=(I_1,I_2,p_1,p_2)$.	
Ce modèle génératif est identifiable sous certaines conditions simples (sur les $k_j$) non détaillées ici.

On remarque que alors (\ref{MainR}) et (\ref{SR}) impliquent par simple intégration sur $X_2$, un modèle de régression en $Y$ s'exprimant {\it uniquement en fonction des variables non corrélées $X_1$} :
\begin{eqnarray}
	Y_{|X_1,S}&=&X_1 (A_{1}+ \sum_{j \in I_2}B^{j}_{I_1}A_{j})+  \sum_{j \in I_2}\varepsilon_{j}A_{j}+\varepsilon_Y 
					= X_1\alpha_{1}+ \varepsilon_{\alpha}. \label{Trueexpl} 			
\end{eqnarray}		
L'estimateur classique du Maximum de Vraisemblance de $\alpha$ est sans biais et s'exprime par	
\begin{eqnarray}
	\hat{\alpha}_{1}&=& (X'_{1} X_1)^{-1}X'_{1}Y  \\
\textrm{et sa matrice de variance } && \nonumber \\
	\Var[\hat{\alpha}_{1}|X,S]&=& (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}A_{j}^2 )(X'_{1} X_{1})^{-1}
\end{eqnarray}
		peut être notablement mieux conditionnée que celle de $\hat A$ initial.						
	En outre, ce nouveau modèle réduit consiste en une régression linéaire classique qui peut donc bénéficier des outils de sélection de variables au même titre que le modèle complet.			
		 \\		
		 Notons enfin que la structure explicite entre les variables permet de mieux comprendre les phénomènes en jeu et la parcimonie du modèle facilite son interprétation. 
\\ 

		\textbf{\underline{Remarque}} : En ajoutant une étape de sélection de variables on obtient ainsi deux  ``types de $0$"  : ceux issus de l'étape de décorrélation et ceux issus de cette éventuelle étape de sélection de type LASSO.
		
\section{Estimation de la structure de corrélation}
	 	
	%	\subsection{Pénalisation de la vraisemblance}
	
		Le choix de structure s'appuie sur un critère noté $BIC^*$ et qui correspond à la vraisemblance pénalisée de la structure à la manière du critère BIC [3], mais en prenant comme loi {\it a priori} sur $S$ une loi uniforme hiérarchique $P(S)=P(I_1 | p_1,I_2,p_2)P(p_1|I_2,p_2)P(I_2|p_2)P(p_2)$ plutôt qu'une loi uniforme simple. On a donc :
		\begin{eqnarray}
		%P(S|X)&\propto &P(X|S)P(S) \\
%		\ln(P(S|X))&=&\ln(P(X|S))+\ln(P(S))+cste \\
		%&=&BIC +\ln(P(S))+cste \\
		BIC^*&=&BIC +\ln(P(S)) \label{Bicstar} .
	\end{eqnarray}	
	L'équiprobabilité ainsi supposée des $p_2$ et $p_1^j$ vient pénaliser davantage la compléxité sous l'hypothèse $p_2<\frac{p}{2}$ , hypothèse réaliste sur le nombre de régressions entre covariables. La recherche du meilleur $S$ selon $BIC^*$ n'est pas un problème simple et on va s'appuyer sur un algorithme MCMC.

	A chaque étape de l'algorithme, pour $S \in \mathcal{S}$ (ensemble des structures réalisables) on définit un voisinage $\mathcal{V}_{S}$ %de $p$ candidats (le package \textsc{CorReg} permet à l'utilisateur de choisir parmi plusieurs types de voisinage).
		et ensuite la fonction de transition est guidée par $BIC^*$ de la façon suivante :	
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=& \mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S}\} }\frac{\exp(\frac{-1}{2} BIC^*(\tilde{S}))}{\sum_{S_l\in \mathcal{V}_{S}}\exp(\frac{-1}{2} BIC^*(S_l))} .
	\end{eqnarray}
La chaîne de Markov ainsi constituée est ergodique dans un espace d'états finis et possède une unique loi stationnaire dont le mode correspond à la structure de plus grande valeur de $BIC^*$.
 
L'intialisation peut se faire en utilisant la matrice des corrélations et/ou la méthode du Graphical Lasso [2].		
La grande dimension de l'espace parcouru rend préférable  (pour un temps de calcul égal) l'utilisation de multiples chaînes courtes plutôt qu'une seule très longue (permettant aussi la parallélisation).
	
	En pratique, on commence par estimer pour chaque variable de $X$ sa densité sous l'hypothèse d'un mélange gaussien (avec le package Rmixmod de Mixmod [6]). On peut alors calculer la loi jointe de $X$ pour chaque structure réalisable rencontrée durant l'algorithme MCMC. Sans cette hypothèse générative supplémentaire sur $X_1$, l'utilisation de $BIC^*$ serait compromise. Notons cependant la souplesse de cette hypothèse due à la grande flexibilité des mélanges gaussiens [7].
\section{Résultats sur données simulées}	
L'ensemble de la méthode à été programmé dans un package R dénommé \textsc{CorReg}. Pour les simulations présentées dans les tableaux \ref{tableMSEsimdroite}, \ref{tableMSEsimtout} et \ref{tableMSEsimgauche}, chacune des configurations à été simulée $100$ fois. Les tableaux affichent le nombre de variables dépendantes trouvées (``bon gauche"), le nombre de variables jugées dépendantes à tort (``faux gauche") et les erreurs moyennes en prédiction (MSE) sur $Y$ à partir d'échantillons de validation de 1 000 individus. Pour l'ensemble des simulations $p=40$, $\sigma_Y=10$, $\sigma=0.001$, les $X$ indépendants suivent des mélanges gaussiens à $\lambda=5$ classes de moyenne selon une loi de Poisson de paramètre $\lambda$ et d'écart-type $\lambda$. Les $B_{1}^j$ suivent la même loi de Poisson mais avec un signe aléatoire. On cherche ici à se comparer à la méthode LASSO dans les cas où celle-ci est en difficulté (fortes corrélations 2 à 2) donc les $p_1^j$ non nuls valent tous 1 dans le vrai modèle. \textsc{CorReg}  a  travaillé avec $p_2$ et $p_1$ libres et a utilisé Mixmod pour estimer les densités dans $X_1$. 

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$ & bon gauche & faux gauche    & LAR  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} vrai $S$\\ 
\hline 
30 & 16 &  8 & 5.39 & 2 466 225.35 & 13 796.03 & 588.9\\ 
\hline 
30 & 32 & 17.05 & 2.7 & 979.16 & 196.33 & 141.2\\ 
\hline 
\hline 
50 & 0 & 0 & 0 & 499.18 & 499.18 & 499.18 \\ 
\hline 
50 & 16 & 9.18 & 4.94 & 315.34 & 202.64 & 193.38 \\ 
\hline 
50 & 32 & 19.13 & 2.24 & 179.89 & 138.96 & 120.21 \\ 
\hline \hline
400 & 32 & 23.66 & 1.13 & 105.38 & 103.88 & 102.81\\ 
\hline 
\end{tabular} 
\caption{$Y$ ne dépend pas de $X_2$.} \label{tableMSEsimdroite}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$ & bon gauche & faux gauche    & LAR  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} vrai $S$\\ 
\hline 
30 & 16 & 8.48 & 4.88 & 3 511 185.23 & 10 686.62 & 738.89 \\ 
\hline 
30 & 32 & 16.89 & 2.78 & 565.51 & 189.54 & 139.24\\ 
\hline 
\hline 
50 & 0 & 0 & 0 & 529.94 & 529.94 & 529.94 \\ 
\hline 
50 & 16 & 8.89 & 5.4 & 347.59 & 233.99 & 197.95\\ 
\hline 
50 & 32 & 18.95 & 2.44 & 163.7 & 139.39 & 121.56 \\ 
\hline \hline
400 & 32 & 23.49 & 1.06 & 104.52 & 103.6 & 102.67\\ 
\hline 
\end{tabular} 
\caption{$Y$  dépend  de $X$ entier.} \label{tableMSEsimtout}
\end{table}


\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$ & bon gauche & faux gauche    & LAR  &    \textsc{CorReg} $\hat S$ & \textsc{CorReg} vrai $S$\\ 
\hline 
30 & 16 & 8.29 & 5 & 5 851.45 & 559.58 & 340.29\\ 
\hline 
30 & 32 & 17 & 2.59 & 893 & 196.01 & 135.78 \\ 
\hline 
\hline 
50 & 16 &  8.98 & 5.19 & 201.56 & 164.58 & 162.49\\ 
\hline 
50 & 32 & 19.05 & 2.32 & 172.93 & 136.77 & 121.19\\ 
\hline \hline
400 & 32 & 23.51 & 1.09 & 104.49 & 103.02 & 102.26 \\ 
\hline 
\end{tabular} 
\caption{$Y$  dépend  de $X_2$ uniquement (cas normalement défavorable à \textsc{CorReg} ).} \label{tableMSEsimgauche}
\end{table}


Les résultats (tableaux 1 à 3) montrent que \textsc{CorReg} est équivalent au LASSO en l'absence de corrélations et le bat quand les corrélations sont fortes. On retrouve le phénomène attendu du LASSO moins impacté par les corrélations quand $n$ grandit. On constate enfin la convergence asymptotique de \textsc{CorReg} vers le vrai modèle de régression en $Y$, comme pour le LASSO.

On remarque que quand $p_2$ augmente le LASSO commence à se ressaisir car il y a de plus en plus de faux modèles proches du vrai en termes de prédiction. Le LASSO trouve donc par moment des modèles inconsistants en interprétation mais relativement corrects en prédiction. ATTENTION : dans le cadre industriel, l'interprétation du modèle mène à des actions sur le process et donc un modèle inconsistant en interprétation peut mener à des actions contre-productives (d'où l'importance de l'interprétabilité de \textsc{CorReg}). 
\section{Résultats sur données réelles}
Les données industrielles sont fortement corrélées de manière naturelle : largeur et poids d'une brame ($\rho=0.905$), température avant et après un outil ($\rho=0.983$), rugosité des deux faces du produit ($\rho=0.919$), Moyenne et maximum d'une courbe ($\rho=0.911$).
Exemples de Sous-régressions obtenues par \textsc{CorReg} ayant une interprétation physique :
\begin{itemize}
	\item Moyenne = f (Min , Max , Sigma ) pour des données courbes
	\item Largeur du produit = f (débit de fonte , vitesse de la coulée continue)	\\
Vrai modèle physique (non linéaire) :

	 Largeur = $\frac{\textrm{débit}}{\textrm{vitesse } \times \textrm{ épaisseur}}$ (Mais dans ce cas précis l'épaisseur est constante)
			\end{itemize}
			
			D'autres sous-régressions traduisent des modèles physiques qui régulent le process industriel sur la base d'expertises métallurgiques.
\\

Exemple de régression sur une variable réponse dans le cadre de données réelles :
\begin{figure}[!h]
	\begin{minipage}[c]{.40\linewidth}
			\includegraphics[width=200px]{figures/histcor_auchan.JPG} 
	\end{minipage} \hfill
   \begin{minipage}[c]{.52\linewidth}
		\begin{tabular}{|c|c|c|}
		\hline 
		  & MSE  & Variables retenues  \\ 
		\hline
		LASSO (lars) & 0.80 & 54 \\ 
		\hline 
		CorReg (et lars) & 0.53 & 24  \\ 
		\hline 
		\end{tabular} 
   \end{minipage}
   \caption{Résultats obtenus sur données réelles : $n=117$ et $p=168$. L'erreur est réduite d'un tiers alors que la complexité du modèle est divisée par $2.5$.   }
\end{figure}   
	

\section{Conclusion et perspectives}
	\textsc{CorReg} est fonctionnel et disponible sur R-forge. L'outil a d'ores et déjà montré son efficacité sur de vraies problématiques de régression en entreprise.
	La force de \textsc{CorReg} est la grande interprétabilité du modèle proposé, qui est constitué de plusieurs modèles de régression simples et donc facilement accessibles aux non statisticiens (régressions linéaires) tout en luttant efficacement contre les problématiques de corrélations, omniprésentes dans l'industrie.
	On note néanmoins le besoin d'élargir le champ d'application à la gestion des valeurs manquantes, très présentes dans l'industrie. Cet aspect est envisagé sérieusement pour la prochaine version de \textsc{CorReg}. En effet, le modèle génératif actuel permettrait cette nouvelle fonctionnalité sans hypothèse supplémentaire.
	

\section*{Bibliographie}
%\bibliography{biblio}{}
%\bibliographystyle{plain}
%à recopier dans le bon ordre comme demandé ci-dessous.

\noindent [1] Efron, B., Hastie, T., Johnstone,I. et Tibshirani, R. (2004), Least angle regression. {\it The
Annals of statistics}, 32(2):407-499.

\noindent [2] Friedman, J., Hastie, T. et Tibshirani, R. (2008), Sparse inverse covariance estimation with
the graphical lasso.  {\it Biostatistics}, 9(3):432-441 .

\noindent [3] Lebarbier, E. et Mary-Huard,T. (2006), Une introduction au critère bic: fondements
théoriques et interprétation.  {\it Journal de la SFdS }, 147(1):39-57.

\noindent[4] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso,  {\it Journal of the Royal
Statistical Society}. Series B (Methodological), pages 267-288.

\noindent [5] Zhao,P. et Yu,B. (2006), On model selection consistency of lasso, {\it J. Mach. Learn.
Res.} 7:2541-2563.

\noindent [6] Biernacki, C., Celeux, G., Govaert, G., \& Langrognet, F. (2006), Model-based cluster and discriminant analysis with the MIXMOD software, Computational Statistics \& Data Analysis, 51(2), 587-600.

\noindent [7] McLachlan, G. J., et Basford, K. E. (1988), Mixture models : Inference and applications to clustering. {\it Statistics: Textbooks and Monographs, New York: Dekker }, 1988, 1.
%
%\noindent [1] Auteurs (année), Titre, revue, localisation.
%
%\noindent [2] Achin, M. et Quidont, C. (2000), {\it Théorie des
%Catalogues}, Editions du Soleil, Montpellier.
%
%\noindent [3] Noteur, U. N. (2003), Sur l'intér\^et des
%résumés, {\it Revue des Organisateurs de Congrès}, 34, 67--89.
\end{document}

