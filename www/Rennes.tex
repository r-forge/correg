\documentclass[12pt]{article}
%
%
% Retirez le caractere "%" au debut de la ligne ci--dessous si votre
% editeur de texte utilise des caracteres accentues
\usepackage[utf8]{inputenc}
%
% Retirez le caractere "%" au debut des lignes ci--dessous si vous
% utiisez les symboles et macros de l'AMS
\usepackage{amsmath}
\usepackage{amsfonts}
%
%
\setlength{\textwidth}{16cm}
\setlength{\textheight}{21cm}
\setlength{\hoffset}{-1.4cm}
%
%
\usepackage{graphicx}

 \graphicspath{{figures/}}

\begin{document}

     \def\Var{{\rm Var}\,}

%--------------------------------------------------------------------------

\begin{center}
{\Large
	{\sc  CorReg : Régression sur variables Corrélées\\ et Application à l'industrie Sidérurgique}
}
\bigskip

 Clément Théry $^{1}$
\bigskip

{\it
$^{1}$ ArcelorMittal Dunkerque, Inria Lille, Université de Lille 1, clement.thery@arcelormittal.com
 
}
\end{center}
\bigskip

%--------------------------------------------------------------------------

{\bf Résumé.} La régression linéaire suppose en général l'usage de variables explicatives indépendantes. Les variables présentes dans les bases de données d'origine industrielle sont souvent très fortement corrélées (de par le process, diverses lois physiques, etc). Le modèle génératif proposé consiste à expliciter les corrélations présentes sous la forme d'une structure de sous-régressions linéaires. La structure est ensuite utilisée pour obtenir un modèle libéré des corrélations, facilement interprétable et compatible avec les méthodes de sélection de variables. La structure de corrélations est déterminée à l'aide d'un algorithme de type MCMC. Un package R (CorReg) permet la mise en oeuvre de cette méthode. 
\smallskip

{\bf Mots-clés.} Régression, corrélations, industrie, sélection de variables, modèles génératifs, SEM (Structural Equation Model), \ldots
\bigskip\bigskip

{\bf Abstract.} Linear regression generally suppose independence between the covariates. Datasets found in industrial context often contains many highly correlated covariates (due to the process, physcial laws, etc). The proposed generative model consists in explicit modeling of the correlations with a structure of sub-regressions between the covariates. This structure is then used to obtain a model with independent covariates, easily interpreted, and compatible with any variable selection method. The structure of correlations is found with an MCMC algorithm. An R package (CorReg) implements this new method.  
\smallskip

{\bf Keywords.} Regression, correlations, industry, variable selection, generative models, Structural Equation Model, \ldots

%--------------------------------------------------------------------------

\section{Le contexte}
	La régression linéaire classique suppose l'indépendance des covariables. Les corrélations posent en effet des problèmes, tant au niveau de l'interprétation qu'en termes de variance des estimateurs.
	\begin{eqnarray}
		Y&=&XA+\varepsilon \ \ \textrm{ avec } \ \varepsilon\sim \mathcal{N}(0,\sigma^2) \\
		\Var(\hat{A}|X)&=& \sigma^2(X'X)^{-1} \textrm{ explose si les colonnes de x sont linéairement corrélées}
	\end{eqnarray}
			
	
	
\section{Le modèle génératif}
On dispose de $X=(X^1,\dots,X^p)$ variables fortement corrélées pour expliquer une variable réponse $Y$.
On rend explicite les corrélations au sein de $X$ sous la forme d'une structure de sous-régressions linéaires $S=(p_2,I_2,p_1,I_1)$ définie ainsi :
	\begin{eqnarray}
		I_1&=&(I_1^1,\dots,I_1^{p_2}) \textrm{ avec}		\\
		I_1^j &=& \{i |Z_{i,j}=1 \} \textrm{ indices des covariables qui expliquent $X^j$} \\
		I_2&=&\{j |\sharp I_1^j>0 \}  \textrm{ indices des variables dépendantes} \\
		p_2&=& \sharp I_2 \\
		p_1&=&(p_1^1,\dots,p_1^{p_2}) \textrm{ avec }p_1^j=\sharp I_1^j 
	\end{eqnarray}
	On suppose $I_1\cap I_2=\emptyset$, $i.e.$ Les variables dépendantes dans $X$ n'en expliquent pas d'autres. 
	
	On note $I_2^c=\{1,\dots,p\}\setminus I_2$
Le modèle génératif s'écrit alors :
\begin{eqnarray}
	Y_{|X,S}&=&XA+\varepsilon_Y= X^{I_2^c}A_{I_2^c}+X^{I_2}A_{I_2}+\varepsilon_Y \textrm{ avec } \varepsilon_Y \sim \mathcal{N}(0,\sigma_Y^2) \label{MainR}\\
	\forall j \in I_2 : \  X^j_{|X^{I_1^j},S}&=&X^{I_1^j}B_{I_1^j}^j + \varepsilon_{j} \textrm{ avec } \varepsilon_j \sim \mathcal{N}(0,\sigma_j^2) \label{SR}\\
    \forall j \notin I_2 : \ X^j &=& f(\theta_j) \textrm{ loi quelconque}	
\end{eqnarray}
Où $B_{I_1^j}^j$ est le vecteur de taille $p_1^j$ des coefficients de la sous-régression de $X^j$.

On remarque que (\ref{MainR}) et (\ref{SR}) impliquent :
\begin{eqnarray}
	Y&=&X^{I_2^c} (A_{I_2^c}+ \sum_{j \in I_2}B^{j}_{I_1}A_{j})+  \sum_{j \in I_2}\varepsilon_{j}A_{j}+\varepsilon_Y \\
					&=& X^{I_2^c}\tilde{A}_{I_2^c}+ \tilde{\varepsilon}=X\tilde{A}+ \tilde{\varepsilon}\label{Trueexpl} 			
\end{eqnarray}
\section{Estimateur}
	Les Moindres Carrés Ordinaires (MCO) donnent (maximum de vraisemblance): 
		\begin{equation}
			\hat A = (X'X)^{-1}X'Y \textrm{ (matrice à inverser mal conditionnée)}
		\end{equation}
		avec les propriétés suivantes :
		\begin{eqnarray}
			\rm{E}[\hat{A}|X]&=&A \\
			\Var[\hat{A}|X]&=& \sigma_Y^2(X'X)^{-1}
		\end{eqnarray}				
		La variance de l'estimateur explose quand les corrélations sont fortes (matrice quasi-singulière).
 			
		CorReg réduit la variance de l'estimateur en estimant $Y$ seulement avec $X^{I_1}$, sachant (\ref{SR}) et (\ref{Trueexpl}).
			\begin{equation}
				Y= X^{I_2^c}\tilde{A}_{I_2^c}+ \tilde{\varepsilon}\label{explicatif}
			\end{equation}							
		Ainsi, l'estimateur devient : 
		\begin{eqnarray}
			\hat{\tilde{A}}_{I_2^c} &=& (X'_{I_2^c} X^{I_2^c})^{-1}X'_{I_2^c}Y \\
			\hat{\tilde{A}}_{I_2} &=& 0
		\end{eqnarray}
		avec les propriétés suivantes :
		\begin{eqnarray}
			\rm{E}[\hat{\tilde{A}}|X]&=&\tilde{A} \\
			\Var[\hat{\tilde{A}}_{I_2^c}|X]&=& (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}A_{j}^2 )(X'_{I_2^c} X^{I_2^c})^{-1} \\
			\Var[\hat{\tilde{A}}_{I_2}|X]&=& 0 
		\end{eqnarray}
		La variance est réduite (retrait des corrélations et réduction de la dimension améliorent drastiquement le conditionnement) pour les faibles valeurs de $\sigma_j$ $i.e.$ les fortes corrélations.					
		
		Le modèle complet et le nôtre prédisent tous les deux $Y$ sans biais (vrai modèle)\cite{saporta2006probabilites}.
	\\	
	Ce nouveau modèle est de dimension réduite et consiste en une régression linéaire classique qui peut donc bénéficier des outils de sélection de variables au même titre que le modèle complet.			
		 \\		
		 La structure explicite entre les variables permet de mieux comprendre les phénomènes en jeu et la parsimonie du modèle facilite son interprétation.		

		En ajoutant une étape de sélection de variable on obtient deux types de $0$ :
			\begin{enumerate}
			\item La structure implique $\hat{\tilde{A}}^{I_2} = 0 $. Ce type de 0 est à interpréter comme une redondance d'information, mais la variable associée n'est pas pour autant jugée indépendante de $Y$. On obtient un modèle parcimonieux sans effet groupe, mais sans erreur d'interprétation, contrairement au LASSO ~\cite{Zhao2006MSC} car on connaît la signification de ce type de $0$.
			\item Une phase de sélection de variables peut également produire des coefficients nuls dans $\hat{\tilde{A}}^{I_1}$. Ces $0$ sont alors des $0$ d'indépendance vis-à-vis de la réponse qui s'interprètent donc classiquement.Et comme les variables de $X^{I_1}$ sont orthogonales, on peut avoir confiance dans cette interprétation.
			\end{enumerate}
		Le modèle obtenu est donc sans biais de prédiction, parcimonieux, et consistant en interprétation.	
\section{Recherche de structure}
	 	
	%	\subsection{Pénalisation de la vraisemblance}
		On va s'appuier sur la vraisemblance pénalisée de la structure à la manière du critère BIC~\cite{BIChuard}. 
		\begin{eqnarray}
		P(S|X)&\propto &P(X|S)P(S) \\
		\ln(P(S|X))&=&\ln(P(X|S))+\ln(P(S))+cste \\
		%&=&BIC +\ln(P(S))+cste \\
		BIC^*&=&BIC +\ln(P(S)) \label{Bicstar}
	\end{eqnarray}	
	Pour éviter une surcomplexité de la structure trouvée, on peut alors faire des hypothèses a priori sur $P(S)$. Par exemple, au lieu de supposer l'équiprobabilité pour tous les $S$, on peut supposer l'équiprobabilité des $p_2$ et $p_1^j$, ce qui vient pénaliser davantage la compléxité sous l'hypothèse $p_2<\frac{p}{2}$ (qui devient alors une contrainte supplémentaire dans l'algorithme de recherche). 
	On a
	\begin{eqnarray}
		P(S)&=&P(I_1 | p_1,I_2,p_2)P(p_1|I_2,p_2)P(I_2|p_2)P(p_2) % \\
%		P(I_1 | p_1,I_2,p_2)&=&\prod_{j =1}^{p_2}P(I_1^j|p_1^j,I_2,p_2) \\
%		P(I_1^j|p_1^j,I_2,p_2)&=&\left(\begin{array}{c}
%			p-p_2 \\ 
%			p_1^j
%			\end{array}  \right)^{-1} =\frac{p_1^j ! (p-p_2-p_1^j)!}{(p-p_2)!}\\
%		P(p_1|I_2,p_2)&=&\prod_{j =1}^{p_2}P(p_1^j|I_2,p_2)		\\
%		P(p_1^j|I_2,p_2)&=&\frac{1}{p-p_2}  \\
%		P(I_2|p_2)&=&\left(\begin{array}{c}
%			p \\ 
%			p_2
%			\end{array}  \right)^{-1}=\frac{p_2!(p-p_2)!}{p!}\\
%		P(p_2) &=&\frac{1}{p_2} \\
%		P(S)&=&\left(\prod_{j =1}^{p_2}\left(\begin{array}{c}
%			p-p_2 \\ 
%			p_1^j
%			\end{array}  \right)^{-1}\right) \left(\frac{1}{p-p_2}\right)^{p_2}\frac{p_2!(p-p_2)!}{p!}\frac{1}{p_2} \\
%			\ln P(S) &=& -\sum_{j=1}^{p_2}	\ln\left(\begin{array}{c}
%			p-p_2 \\ 
%			p_1^j
%			\end{array}  \right)
%			-p_2\ln (p-p_2)
%			-\ln\left(\begin{array}{c}
%			p \\ 
%			p_2
%			\end{array}  \right)
%			-\ln( p_2	)
	\end{eqnarray}		
%	It increases penalty on complexity for $p_2<\frac{p}{2}$ thus in the following we will use $BIC*$ under this hypothesis (that becomes a constraint in the MCMC).			
%	\subsection{Parcours Markovien de saut}
	$S$ est entièrement défini à partir de $I_1$ donc on se contente ici de modifier $I_1$. %(les autres éléments de $S$ seront ensuite recalculés selon leur définition).
	A chaque étape, pour $S \in \mathcal{S}$ on définit un voisinage $\mathcal{V}_{S,j}$ avec $j \sim \mathcal{U}(\{1,\dots,p\}) $ :% de la manière suivante  :	
	\begin{eqnarray}
		\mathcal{V}_{S,j}&=&\{ S^{(i,j)} | 1\leq i\leq p \} \cup\{S \}
	\end{eqnarray}	
	avec $S^{(i,j)}$ obtenu selon l'algorithme :
	\begin{itemize}
		\item Si $i \notin I_i^j$ (ajout): 
			\begin{itemize}
				\item $I_1^j :=I_1^j\cup \{i\}$, et pour garder $I_1\cap I_2=\emptyset$ :
				\item $I_1^i :=\emptyset$ et $I_1:=I_1 \setminus \{j\}$
			\end{itemize}			 
		\item Sinon ( $i \in I_1^j$ (suppression)): $I_1^j=I_1^j\setminus \{i\}$
	\end{itemize}
	
	On a donc $p$ candidats à chaque étape. Mais le package CorReg permet à l'utilisateur de modifier ce voisinage.
	
		
	On fait l'approximation suivante  (\ref{Bicstar}) : 
	\begin{equation}
		P(S|X)\approx exp(BIC^*(S))
	\end{equation}
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=& \frac{1}{p} \sum_{j=1}^p q(\tilde{S},\mathcal{V}_{S,j}) \\
	\textrm{où }	q(\tilde{S},\mathcal{V}_{S,j})&=&\mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S,j}\} }\frac{exp(\frac{-1}{2}\Delta BIC(\tilde{S},\mathcal{V}_{S,j}))}{\sum_{S_l\in \mathcal{V}_{S,j}}exp(\frac{-1}{2}\Delta BIC(S_l,\mathcal{V}_{S,j}))} \\
	\textrm{ avec} \Delta BIC(S,\mathcal{V}_{S,j})&=&BIC(S)-\min\{BIC(\tilde{S})| \tilde{S} \in \mathcal{V}_{S,j} \}
	\end{eqnarray}
	
		La chaîne de Markov ainsi constituée est ergodique dans un espace d'états finis et possède une unique loi stationnaire.
		Le résultat obtenu est la meilleur structure rencontrée en termes de $BIC^*$ (vraisemblance pénalisée). 
 
L'intialisation peut se faire en utilisant la matrice des corrélations et/ou la méthode du Graphical Lasso\cite{friedman2008sparse}.		
La grande dimension de l'espace parcouru rend préférable  (pour un temps de calcul égal) l'utilisation de multiples chaînes courtes plutôt qu'une seule très longue (permet aussi la parallélisation).

\section{Résultats}	
Les données industrielles sont fortement corrélées de manière naturelle : Largeur et poids d'une brame ($\rho=0.905$), Température avant et après un outil (($\rho=0.983$), rugosité des deux faces du produit ($\rho=0.919$), Moyenne et maximum d'une courbe ($\rho=0.911$).
			
Certaines des sous-régressions obtenues par CorReg ont une interprétation physique:
\begin{itemize}
	\item Moyenne = f (Min , Max , Sigma ) pour des données courbes
	\item Largeur du produit= f (débit de fonte , vitesse de la coulée continue)	
Vrai modèle physique (non linéaire) :

	 Largeur = $\frac{\textrm{débit}}{\textrm{vitesse } \times \textrm{ épaisseur}}$ (Mais dans ce cas précis l'épaisseur est constante)
			\end{itemize}
			
			D'autres sous-régressions traduisent des modèles physiques qui régulent le process.


%Exemple de régression finale :
\begin{figure}[!h]
	\begin{minipage}[c]{.40\linewidth}
			\includegraphics[width=200px]{figures/histcor_auchan.JPG} 
	\end{minipage} \hfill
   \begin{minipage}[c]{.52\linewidth}
		\begin{tabular}{|c|c|c|}
		\hline 
		  & MSE  & nombre de régresseurs  \\ 
		\hline
		LASSO (lars) & 0.80 & 54 \\ 
		\hline 
		CorReg (et lars) & 0.53 & 24  \\ 
		\hline 
		\end{tabular} 
   \end{minipage}
   \caption{résultats obtenus sur données réelles : $n=117$ et $p=168$.    }
\end{figure}   
	

\section{Conclusion et perspectives}
	CorReg est fonctionnel et disponible. L'outil a d'ores et déjà montré son efficacité sur de vraies problématiques de régression en entreprise.
	La force de CorReg est la grande interprétabilité du modèle proposé, qui est constitué de plusieurs modèles simples (parsimonieux) et facilement accessibles aux non statisticiens (régressions linéaires) tout en luttant efficacement contre les problématiques de corrélations, omniprésentes dans l'industrie.
	On note néanmoins le besoin d'élargir le champ d'application à la gestion des valeurs manquantes, très présentes dans l'industrie. Cet aspect est envisagé sérieusement pour la prochaine version de CorReg.
	

\section*{Bibliographie}
\bibliography{biblio}{}
\bibliographystyle{plain}
à recopier dans le bon ordre comme demandé ci-dessous.

\noindent [1] Auteurs (année), Titre, revue, localisation.

\noindent [2] Achin, M. et Quidont, C. (2000), {\it Théorie des
Catalogues}, Editions du Soleil, Montpellier.

\noindent [3] Noteur, U. N. (2003), Sur l'intér\^et des
résumés, {\it Revue des Organisateurs de Congrès}, 34, 67--89.
\end{document}

