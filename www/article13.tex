\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[table]{xcolor}
 \graphicspath{{figures/}}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Clément THERY, Christophe BIERNACKI, Gaétan LORIDANT}
\title{CorReg: model-based pretreatment for regression with correlated variables. Application in steel industry}

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%


\definecolor{darkgreen}{rgb}{0,0.4,0}
	 \definecolor{darkred}{rgb}{0.75,0,0}
	 \definecolor{darkblue}{rgb}{0,0,0.4}

\begin{document}
\maketitle
\bigskip
{\bf Abstract.} Linear regression outcomes are known to be damaged by correlated covariates. However many modern datasets are expected to convey more and more highly correlated covariates. We propose to explicitly model the correlations by a family of linear regressions between the covariates. Marginalization allows then to obtain a parsimonious correlation-free regression model, easily understandable and from which it is then possible to perform standard linear estimation methods including variables selection procedures for instance. The structure of correlations is found with an MCMC algorithm. An R package (\textsc{CorReg}) available on the CRAN implements this new method which will be illustrated on both simulated datasets and real-life datasets from steel industry where correlated variables are frequent.
\smallskip

{\bf Keywords.} Regression, correlations, industry, variable selection, generative models

\section{Introduction}
%la régression et ses problèmes


When one wants to explain a phenomenon based on some covariates, the first statistical method tried frequently is the linear regression. It provides a predictive model with a good interpretability even  for non-statistician and is simple to learn. Therefore, linear regression is used in nearly all the fields where statistics are made, from industry (ballistic models to calibrate the process) to sociology (predicting some numerical properties of a population).
Linear regression is a very classic situation but sometimes it has to face an also classical problem: the variance of the estimators, even whithout bias from modelisation. 
We note the linear regression model:
\begin{equation}
		Y_{|X}=X\beta + \varepsilon
	\end{equation}
	where $X$ is the $n\times p$ matrix of the explicative variables, $Y$ the  $n\times 1$ response vector and $\varepsilon \sim \mathcal{N}(0,\sigma_Y^2I_n)$ the noise of the regression, with $I_n$ the $n$-sized identity matrix and $\sigma_Y \in \mathcal{R}$. The $p\times 1$ vector $\beta$ is the vector of the coefficients of the regression, estimated with Ordinary Least Squares (\textsc{OLS}). As shown in section \ref{sectionOLS}, estimation of the parameters requires the inversion of a matrix which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
Variance of $\hat{\beta}$ increases based on two aspects :
\begin{itemize}
	\item The dimension $p$ (number of covariates) of the model  : the more covariates you have the greater variance you get.
	\item The correlations within the covariates : strongly correlated covariates give bad-conditioning and increase variance of the estimators .
\end{itemize}

	With the rise of informatic, datasets contains more and more covariates and thus more and more useless covariates. So dimension reduction becomes a necessity. Moreover, when you use more covariates, you increase the chance to have correlated ones. It will be illustrated with examples from steel industry in the last section of this paper (many parameters of the whole process without any a priori) highly correlated (physical laws, process rules, etc). 
	 In such a context, variance of the estimators can lead to arbitrary results or even no results at all. Prediction and interpretation are both strongly needed, depending on the context. For instance, in an industrial context interpretation could be favored to improve the process instead of only predict defects.
		~\\	~\\
		
%bibliographie	
	Because \textsc{OLS} is the minimum-variance unbiased estimator, penalized methods try to reduce the variance introducing some bias to improve the bias-variance trade-off and get better prediction.
Moreover, real datasets imply many irrelevant variables so we have to use variable selection methods.

In the following we note classical norms: $\parallel\beta\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$ and $\parallel\beta \parallel_1=\sum_{i=1}^p|\beta_i| $.

	Ridge regression\cite{marquardt1975ridge} proposes a biased estimator that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \hat{\beta}=\operatorname{argmin} \left\lbrace \parallel Y-X\beta\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\beta\parallel_2^2\leq k
	\end{equation}
	But Ridge regression do not aim at selecting covariates because coefficients tends to 0 but don't reach 0.
	So it gives difficult interpretations for large values of $p$.	
	
	
	The Least Absolute Shrinkage and Selection Operator (\textsc{LASSO} \cite{tibshirani1996regression}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty.
		\begin{equation}
		 \hat{\beta}=\operatorname{argmin} \left\lbrace \parallel Y-X\beta\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\beta\parallel_1\leq \lambda
		\end{equation}	
	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) Algorithm offers a very efficient way to obtain the whole LASSO path and is very attractive. It requires only the same order of magnitude of computational effort as \textsc{OLS} applied to the full set of covariates. And it  really selects covariates with coefficients set exactly to 0.
	 But \textsc{LASSO} also faces consistency problems \cite{Zhao2006MSC} when confronted  with correlated covariates. Another limitation of the \textsc{LASSO} is that it preserves at most $n$ predictors (troublesome when in high dimension). Some recent variants of the \textsc{LASSO} does exist for the choice of the penalization coefficient like the adaptative \textsc{LASSO} \cite{zou2006adaptive} or the random \textsc{LASSO} \cite{wang2011random}.
	 \\
	 
	Elastic net\cite{zou2005regularization}	is a method developed to be a compromise between Ridge regression and the \textsc{LASSO}. 
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	Elastic net can be written:
	\begin{equation}
		\hat{\beta}=(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel Y-X\beta \parallel_2^2 \right\rbrace, \textrm{ subject to } (1-\alpha)\parallel\beta\parallel_1+\alpha\parallel\beta\parallel_2^2\leq t \textrm{ for some } t
	\end{equation}
	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	%It seems to give good predictions. 
	But it is based on the grouping effect and if the dataset contains two identical variables they will obtain the same coefficient whereas LASSO will choose between one of them and will then obtain same predictions with a more parsimonious model. 
	\\
	
	Another way of reducing the dimension is to consider clusters of variables with the same coefficients, like the Octogonal Shrinkage and Clustering Algorithm for Regression (\textsc{OSCAR} \cite{bondell2008simultaneous}).
	The CLusterwise Effect REgression (\textsc{CLERE} \cite{yengo2012variable}) describes the $\beta_j$ no longer as fixed effect parameters but as unobserved independant random variables whith grouped $\beta_j$ following a Gaussian Mixture distribution. 

The idea is to hope that the model have a small number of groups of covariates and that the mixture will have few enough components to have a number of parameters to estimate significantly lower than $p$. In such a case, it improves interpretability and ability to yeld reliable prediction with a smaller variance on $\hat{\beta}$. But it requires to suppose having many covariates with the same level of effect on the response variable and seems to stay less efficient in prediction than elastic net. Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the $\beta_j$ and gives a subset of covariates (not grouped) on which to compute \textsc{OLS} but has no specific protection against correlations issues.
	~\\	~\\
% Principe de la méthode

 Where some try to reduce the dimension and then have to suppose having no correlations between the remaining covariates,  we propose to focus on the correlations, giving a model with orthogonal covariates and an explicit structure between covariates. In fact we search the greatest set of orthogonal covariates to keep the maximum information but with an orthogonality constraint. This can be viewed as a pretreatment on the dataset allowing to use then other dimension reduction tools without suffering from correlations. %We only consider strong correlations (i.e. : problematic ones) thus we keep most of the information contained in the dataset. 
 %We will in a second time be able to use the remaining part of the information (sequential approach).	
 
	Our work is based on the assumption that if we know explicitly the source of correlations, we could use this knowledge to avoid the problem.
	The structure obtained gives a system of linear regression that can be viewed as a recursive Simultaneous Equation Model (\textsc{SEM})\cite{davidson1993estimation}. Such a system is easy to interpret but estimation don't take advantage of the explicit structure \cite{TIMM} when the structure is straight forward (recursive \textsc{SEM}).
  	Other methods take into account correlations but they only consider covariances between residuals \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) or  endogenous variables like \textsc{SPRING} (Structured selection of Primordial Relationships IN the General linear model \cite{chiquetconf}).

 	
	  We make the choice to distinguish the response variable from the other endogenous variables (that are on the left of a regression). Thus we have one regression on our response variable and a system of sub-regressions (without the response variable). Then we consider correlations between the explicative covariates of the main regression, not between the residuals.
	  The structure is supposed to be the source of the correlations and allows us to define a marginal regression model based on a reduced set of independent covariates without significant information loss. So we may obtain two kinds of zeros in the marginal model : coerced zeros due to correlations (redundant information) and estimated ones with classical variable selection methods applied on remaining variables. This two kinds of zero won't be interpreted in the same way and thus consistency issues don't mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.
 	 	
\textsc{SEM} are often used in social sciences and economy where a structure is supposed "by hand" but here we want to find it automatically. Graphical LASSO \cite{friedman2008sparse} offers a method to obtain a structure based on the precision matrix (inverse of the variance-covariance matrix). It consists in a selection in the precision matrix, setting some covariances to zero. But the resulting matrix is symmetric and we need an oriented structure for our \textsc{SEM}. So we developped an MCMC algorithm to find the structure (R package \textsc{CorReg} on CRAN). The structure is based on a full generative model on $X$ to be able to compare likelihood-based criterion in the \textsc{MCMC}.
 	\\
 	
 	%plan
 	This paper will first present the marginal regression model and its properties before describing in Section 3 the random walk used to find the structure.
 	We will then look at some numerical results on simulated (Section \ref{sectionsimul}) and real industrial datasets (Section \ref{sectionrealcase}) before concluding and giving some perspectives in the last part.
	
\section{Model to select decorrelated covariates}
\subsection{The decorrelation problem in regression}\label{sectionOLS}
For a model defined by 
	\begin{equation}
		Y_{|X}=X\beta + \varepsilon
	\end{equation}
	where $X$ is the $n\times p$ matrix of the explicative variables, $Y$ the response vector and $\varepsilon \sim \mathcal{N}(0,\sigma_Y^2I_n)$ with $I_n$ the $n$-sized identity matrix,
	we have the following Ordinary Least Squares (\textsc{OLS}) estimator:
	\begin{equation}
		\hat{\beta}=\left(X'X \right) ^{-1}X'Y
	\end{equation}
	With variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\beta})=\sigma_Y^2(X'X)^{-1} \label{eqOLS}
	\end{equation}
	 The Mean Squared Error (\textsc{MSE}) on $\hat{\beta}$ is:
	\begin{eqnarray}
		\textsc{MSE}(\hat{\beta}|\beta,X)&=&\operatorname{Bias}^2+\operatorname{Tr}(\operatorname{Var}(\hat{\beta})) \\
			&=&0+\sigma_Y^2 \operatorname{Tr}((X'X)^{-1})
	\end{eqnarray}	 
	and when correlations between covariates are strong, the matrix to invert is ill-conditioned, the variance explodes and so does the MSE.
	Another problem is that matricial inversion requires $n\geq p$. When it is not the case, a frequently used alternative is the Moore-Penrose generalized inverse \cite{katsikis2008fast}. In the followings, matrices inverses when $n<p$ will refer to this generalized inverse. Thus OLS can obtain some results even with $n<p$ (see section \ref{sectionsimul} ).
\subsection{Modelisation of the correlations}
Let now $X \in \mathcal{R}^{n\times p}$ be a set of $p$ correlated covariates.
We propose to explicitly define a family of $p_2$ internal regressions between covariates whith $I_2$ the set of indices of endogenous variables in $X$ (explained ones) and $I_1=\{I_1^1,\dots,I_1^p \}$ the set of the sets of indices of exogenous covariates (explaining ones) with $\forall j \notin I_2, I_1^j=\emptyset$. Then we have an explicit structure $S=(I_1,I_2,p_1,p_2)$ where $p_1=(p_1^1,\dots,p_1^{p_2})$ is the vector of the number of covariates in each internal regression. Thus we have $p_2=| I_2|$ and $p_1^j=|I_1^j|$ where $|.|$ represents the cardinal of an ensemble.

In the following, we note $X^j$ the $j^{th}$ column of a matrix $X$.
For lighter notation we define $X_2=X^{I_2}$ the matrix of the endogenous covariates and $X_1=X\setminus X_2$ the matrix of the remaining exogenous covariates. 

The family of $p_2$ regressions within correlated covariates in $X$ is noted:
	\begin{equation}
		\forall j \in I_2: X^j_{|X_1,S}=X_1\delta_j+\varepsilon_j \textrm{ with } \varepsilon_j \sim(0,\sigma^2_jI_n) \label{SR}
	\end{equation}
		where $\delta_j \in \mathcal{R}^{(p-p_2)}$ are the vectors of the regression coefficients between the covariates (containing some zeros according to $I_1^j$).
It remains $p-p_2$ independent exogenous variables $X_1$: $\forall j \notin I_2 : X^j \sim f(\theta_j)$

We make the hypothesis of the uncrossing rule $I_1\cap I_2=\emptyset$ {\it i.e.} endogenous variables don't explain other covariates, thus we have a partition $X=[X_1,X_2]$.

This generative model is conditional to $S$, the discrete structure model that is identifiable because we can't permute some regressions in (\ref{SR}) and obtain the same joint distribution $P(X,Y)$, the residuals ($\varepsilon_j$) would not stay Gaussian (see the appendices). These residuals are in the following supposed independent but one can suppose dependencies between them and then use appropriate tools to estimate them and the $\delta_j$ like SUR with Feasible Generalized Least Squares (FGLS) by Zellner \cite{SURzellner} or SPRING \cite{chiquetconf} as mentioned in the introduction.
\\

	
\subsection{Marginal regression model with decorrelated covariates}
Let $Y \in \mathcal{R}^n$ be a response variable we want to explain with $X$.
We note that (\ref{MainR}) and (\ref{SR}) also give by simple integration on $X_2$ a regression model on $Y$ {\it depending only on uncorrelated covariates $X_1$}:
		\begin{equation}
			Y_{|S,X}=X\beta+\varepsilon_Y =X_1\beta_1+X_2\beta_2+\varepsilon_Y \textrm{ whith } \varepsilon_Y \sim \mathcal{N}(0,\sigma^2_YI_n);	\label{MainR}
		\end{equation}
		where $\beta=(\beta_1,\beta_2) \in  \mathcal{R}^p$ is the vector of the regression coefficients and $I_n$ the identity matrix. 
Knowing (\ref{SR}) we then obtain a marginal regression model:
\begin{eqnarray}
	Y_{|X_1}&=&X_1 (\beta_{1}+ \sum_{j \in I_2}B^{j}_{1}\beta_{j})+  \sum_{j \in I_2}\varepsilon_{j}\beta_{j}+\varepsilon_Y \label{Trueexpl} \\
		&=&X_1 \alpha_1 + X_2\alpha_2 + \varepsilon_{\alpha} \textrm{ where} \alpha_2  = 0.
\end{eqnarray}
We note that it simply is a linear regression on some of the original covariates so we only made a pretreatment on the dataset by setting some coefficients to $0$ because of correlations. These $0$ won't be interpreted as independence with the response variable but as redundant information due to correlations between the covariates. It isa variable pre-selection independent of the response $Y$. As a pretreatment, it allows usage of any method in a seconde time to estimate $\alpha$.
\subsection{Estimator and properties}
	The marginal regression model simply is
	\begin{equation}
		Y_{|X_1}= X_1\alpha_1+ \varepsilon_{\alpha}
	\end{equation}			
		So we have the unbiased estimator of $\alpha$: 
		\begin{equation}
			\hat{\alpha}_{1} = (X'_{1} X_{1})^{-1}X'_{1}Y  \textrm{ and }\hat{\alpha}_{2} = 0
		\end{equation}
		We see in (\ref{Trueexpl}) that it gives an unbiased estimation of $Y$ and $\alpha$
		but in terms of $\beta$ this estimator is biased:
		\begin{equation}
			\operatorname{E}[\hat{\alpha_1}|X_1]=\beta_1+\sum_{j \in I_2}B^{j}_{1}\beta_{j}\textrm{ and }\operatorname{E}[\hat{\alpha_2}|X_1]=0
		\end{equation}
		with variance:
		\begin{equation}
			\operatorname{Var}[\hat{\alpha}_{1}|X_1]= (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}\beta_{j}^2 )(X'_{1} X_{1})^{-1}  \textrm{ and }\operatorname{Var}[\hat{\alpha}_{2}|X_1]= 0 
		\end{equation}
		We see that the variance is reduced compared to OLS described in equation (\ref{eqOLS})(no correlations and smaller matrix give better conditioning ) for small values of $\sigma_j$ $i.e.$ strong correlations.					
			%There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypothesis. Therefore we made some simulations to compare both methods (see the end of this paper).
			Moreover, the explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset. Expert knowledge can even be added to the structure.
			 This new model is reduced even without variable selection and is just a linear regression so every method for variable selection in linear regression can be used then,like LASSO or even stepwise \cite{seber2012linear}. Hence we hope to obtain a parsimonious model with { \it two kinds of zeros}: those from decorrelating step and those from selection step, with different meanings.
			 \\
			  A last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}).
		 
	To better illustrate the bias-variance compromise, we look at a simple example with $p=3$ variables. $X_1$ is composed by two independent scaled Gaussian $\mathcal{N}(0,1)$, $X_3=\frac{1}{2}x_1+\frac{1}{2}x_2+\varepsilon_3$ where $\varepsilon_3\sim{\mathcal{N}(0,\sigma_3^2)}$. We also have $\beta=(\frac{1}{3},\frac{1}{3},\frac{1}{3})$ and $\sigma_Y=10$. Then we observe the theoretical Mean Squared Error (MSE) of the estimator of both OLS and \textsc{CorReg} model: $MSE(\hat{\beta}|\beta,X)=Bias^2+\operatorname{Tr}(\operatorname{Var}(\hat{\beta}))$ for several values of $\sigma_3$ (strength of the sub-regression) and $n$. Figures \ref{MQE1} to \ref{MQE3} show the theoretical MSE evolution with the strength of the sub-regression:
	\begin{equation}
		1-\mathcal{R}^2=\frac{\operatorname{Var}(\varepsilon_3)}{\operatorname{Var}(x_3)}=\frac{\sigma_3^2}{\sigma_3^2+\frac{1}{2}}
	\end{equation}
	
\begin{figure}[h!]
	\begin{minipage}[l]{.32\linewidth}
			\includegraphics[width=170px]{figures/MQEn15sigmaY10.png} 
			\caption{For $n=15$. Dotted: \textsc{Correg}, plain: OLS}\label{MQE1}
	\end{minipage} \hfill
	\begin{minipage}[c]{.32\linewidth}
			\includegraphics[ width=170px]{figures/MQEn100sigmaY10.png} 
			\caption{For $n=100$. Dotted: \textsc{Correg}, plain: OLS}
	\end{minipage} \hfill
   \begin{minipage}[r]{.32\linewidth}
			\includegraphics[width=170px]{figures/MQEn1000sigmaY10.png} 
			\caption{For $n=1000$. Dotted: \textsc{Correg}, plain: OLS.} \label{MQE3}
   \end{minipage} 
\end{figure} 
It is clear in Figures \ref{MQE1} to \ref{MQE3} that the reduced model is more robust than OLS. And even when sub-regression are week (small value of $1-\mathcal{R}^2$) it does not explode. We also see that the error implied by strong correlations shrinks whit the rise of $n$.  $MSE_{OLS}=0+\operatorname{Tr}(\operatorname{Var}(\hat{\beta}))=\operatorname{Tr}(\operatorname{Var}(\hat{\beta}_{1}))+\operatorname{Tr}(\operatorname{Var}(\hat{\beta}_{2}))$. both terms are multiplied by $\sigma_Y^2$ and so is $\operatorname{Tr}(\operatorname{Var}(\hat{\alpha}_{1}))$. Thus, as $\operatorname{Var}(\hat{\alpha}_{1})=0$ when $\sigma_Y^2$ rises it increases the advantage of \textsc{CorReg} versus OLS. It illustrates the importance of dimension reduction when the model has a strong noise (very usual case on real datasets where true model is not even exactly linear).

	\subsection{Advantages for interpretation}
	
		Grouping effect is the fact that correlated covariates get similar coefficient and are selected together. It's the case with elasticnet \cite{zou2005regularization} for example.	
			If $X_2=X_1+e$ and we have the grouping effect, we will obtain a model like $\hat{Y}=\hat{\beta}X_1+\hat{\beta}X_2$. 
			Then, if you try to modify the response value, you will modify one of the covariates and both will change so you won't get expected results. 
			
		%At the opposite, methods like the LASSO with LAR will only keep one covariate for each group of correlated covariates  so we get $\hat{Y}=2aX_1$ and think that $X_2 \perp Y$ but $Y$ depends on $X_2$ so it is worse than grouping effect for interpretation.
		
		
			Nothing constrains us to give only one equation. %It is clearly better to give the user another equation (or system for more complex models) describing the correlations. 
			If you have the following model : $Y=\beta X_1+\beta X_2$ and $X_1=X_2+e$. Then you have more information and are able to decide better actions. 
			With such a model, grouping effect is no more useful because when saying $Y=2\beta X_1$ and $X_2=X_1+e$ with the information that $X_2$ has been removed locally because of its correlation with $X_1$, you don't get misleading interpretations anymore. In fact, we have two kinds of zeros (those from selection and those from correlations) and we know for each zero which kind is it.
			So it is possible to combine the advantages of grouping effect and selection just giving several equations. 
			Each equation here is very simple (only linear regressions) so you don't really increase complexity of the model. 
			Moreover, the uncrossing constraint ($I_1\cap I_2=\emptyset$) guarantee to keep a simple structure easily interpretable (no cycles and no chain-effect) and straightforward readable.

	
	\section{Estimating structure of sub-regressions with a Markov chain}	
\subsection{Detailed generative model}
	To be able to compare structures with probabilistic criterions, we need a full generative model. We suppose that variables in $X_1$ follow Gaussian mixtures. 
	\begin{equation}
			\forall j \notin I_2 : X^j_{|S} \sim f(\theta_j)=\mathcal{GM}(\pi_j;\mu_j;\sigma^2_j) \textrm{ with } \pi_j,\mu_j,\sigma^2_j \textrm{ vectors of size } k_j; \label{mixtureX1}
		\end{equation}
		The great flexibility \cite{mclachlan2004finite} of such models makes our model more robust but one can use other laws if needed. Gaussian case is just a special case ($k_j=1$) of Gaussian mixture so it is included in our hypothesis.

		Variables in $X_1$ are supposed to be independent.
	Thus if one have some hypothesis on the distribution of some variables (exponentially distributed for example) it is possible to use it without impacting the model in other ways. %compute corresponding $\psi$ according to it. %and then improve the walk (will keep a structure only if it is really relevant).%and give it as an input of \textsc{CorReg} and then improve the efficiency of the algorithm (it will find a structure only if it is really relevant).
	We now have a full generative model.
	\subsection{Structure comparison} \label{compstruct}
  To obtain the marginal regression model we need $S$, the linear structure between the covariates.
	Our full generative generative model allows us to compare structures with criterions like the Bayesian Information Criterion ($BIC$) which penalize the log-likelihood according to the complexity of the structure~\cite{BIChuard}. We will prefer this kind of comparison criterion instead of cross-validation that is very time-consuming and thus not friendly with combinatory problematics.
	We note $\Theta$ the set of the parameters of the generative model
	\begin{eqnarray}
		P(S|X)&\propto& P(X|S)P(S) \\
		-2\log P(X|S)&\approx & BIC=-2\mathcal{L}(X,S,\Theta)+|\Theta|\log(n)\\  
	\end{eqnarray}
	But $BIC$ tends to give too complex structures because we test a great range of models. 
	Thus we choose to penalise the complexity a bit more with a hierarchical uniform {\it a priori} law $P(S)=P(I_1 | p_1,I_2,p_2)P(p_1|I_2,p_2)P(I_2|p_2)P(p_2)$  instead of a simple uniform law on $S$.
%	 Thus we have :
%		\begin{eqnarray}
%		BIC^*(X|S)&=&BIC(X|S) +\ln(P(S)) \label{Bicstar}
%	\end{eqnarray}		
	It increases penalty on complexity for $p_2<\frac{p}{2}$ and $p_1^j<\frac{p}{2}$ . Hence %when using $BIC*$ 
	this constraint on $\hat{p}_2$ and $\hat{p}_1^j$ is made in the MCMC in the followings.
	But we can imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
\\	
		We will now use the following notation: $\psi(S)=\psi(X|S)$	where $\psi(X|S)$ is the chosen criterion, that will be $BIC$ with hierarchical uniform hypothesis in our numerical results. We do not change $BIC$ but only $P(S)$ so the properties are the same as classical $BIC$ but we will obtain better results when our hypothesis is verified.
	
	\subsection{The neighbourhood}
	Let's define $\mathcal{S}$ the ensemble of feasible structures (those with $I_1\cap I_2=\emptyset$).
	\\
	For each step, starting from $S \in \mathcal{S}$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}_{S,j}&=& \{S \}\cup \{ S^{(i,j)} |1\leq i \leq p, i\neq j  \} \\
		\textrm{where }\ \ j &\sim & \mathcal{U}(\{1,\dots,p\}) 
	\end{eqnarray}	
	With $S^{(i,j)}$ defined by the following algorithm :
	\begin{itemize}
		\item if $i \notin I_i^j$ (add): 
			\begin{itemize}
				\item $I_1^j=I_1^j\cup \{i\}$
				\item $I_1^i=\emptyset$ (explicative variables can't depend on others : column-wise relaxation)
				\item $I_1=I_1 \setminus \{j\}$ (dependent variables can't explain others : row-wise relaxation) 
			\end{itemize}			 
		\item else (remove): $I_1^j=I_1^j\setminus \{i\}$
	\end{itemize}
	
	\smallskip
	At every moment, coherence between $I_1$ and others parts of $S$ can be done by $\forall 1\leq j\leq p :  p_1^j=|I_1^j|$, $I_2=\{j |p_1^j>0 \}$, $p_2= |I_2|$, .
		
	\subsection{The walk}
	\subsubsection{Transition probabilities}
	We first make the approximation
	\begin{equation}
		P(S|X)\approx exp(\psi(S)).
	\end{equation}
	The algorithm follows a time-homogeneous markov chain whose transition matrix $\mathcal{P}$ has $|\mathcal{S}|$ rows and columns (combinatory so we'll just compute the probabilities when we need them).
	At each step the markov chain moves with probabiliy:
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=&\sum_{j=1}^p \mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S,j}\} }\frac{\exp(-\frac{1}{2} \psi(\tilde{S}))}{\sum_{S_l\in \mathcal{V}_{S,j}}\exp(-\frac{1}{2}\psi(S_l))} \\
	\end{eqnarray}
	And $\mathcal{S}$ is a finite state space.%la relaxation rend P non symétrique mais ne remets  pas en cause l'homogénéité	
	 
Because the walk follows a regular and thus ergodic markov chain with a finite state space, it has exactly one stationary distribution \cite{grinstead1997introduction} %: $\pi$ and every rows of $\operatorname{lim}_{k\rightarrow \infty}\mathcal{P}^k=W$ equals $\pi$.
%	
%	
%	With $\forall S \in  \mathcal{S}$ :
%	\begin{eqnarray}		
%		0 \leq &\pi (S)& \leq 1 \nonumber \\
%		\sum_{S \in \mathcal{S}}\pi(S) &=&1 \nonumber \\
%		\pi (S) &=&\sum_{\tilde{S}\in \mathcal{S}} \pi(\tilde{S})\mathcal{P}(\tilde{S},S) \\%définition de la lois stationnaire
%	\end{eqnarray}
%		
and the output will be the best structure in terms of $\psi$ which weights each candidate. Practically speaking, \textsc{CorReg} returns the best structure seen during the walk.
Numerical results (Section 4) illustrates the efficiency of the walk when the true model really contains a linear structure or no structure at all (Table (\ref{compZvrai})) and when the structure is not linear (Table \ref{compZnonlin})).

 \subsubsection{Initialisation(s)}

 If we have some knowledge about some sub-regressions (physical models for example) we can add them in the found and/or initial structure. So the model is really expert-friendly.
The initial structure can be based on a first warming algorithm taking the correlations into account. coefficients are randomly placed into $I_1$, weighted by the absolute value of the correlations. We do so in the followings. Then this structure could be for example reduced by the hadamard product with the binary matrix obtained by Graphical Lasso\cite{friedman2008sparse} that makes selection in the precision matrix but it is time consuming.

	One would rather test multiple short chains than lose time in initialisation or long chains \cite{gilks1996markov}. It also helps to face local extrema. In the followings, the chain was launched with twenty initialisations.
	
\section{Numerical results on simulated datasets} \label{sectionsimul}
	\subsection{The datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if \textsc{CorReg} 	keeps its promises.
	The \textsc{CorReg} package has been tested on simulated datasets. 
Section \ref{compZ} shows the results obtained in terms of $\hat{S}$. Sections \ref{tableMSEsimtout} and \ref{tableMSEsimgauche} show the results obtained using only \textsc{CorReg}, or \textsc{CorReg} combined with other methods. Tables give both mean and standard deviation of the observed Mean Squared Errors (MSE) on a validation sample of $1 000$ individuals. For each simulation,  $p=40$, $\sigma_Y=10$, $\sigma=0.001$, variables $X_1$ follow Gaussian mixture models of $\lambda=5$ classes which means follow Poisson's law of parameter $\lambda$ and which standard deviation also is $\lambda$. The $B_1^j$ and $\beta_j$ are generated according to the same Poisson law but with a random sign. $S$ only contains binary relationships but \textsc{CorReg} was only constrained to $\max (\hat{p}_1^j)=5$.  
	We used \textsc{Rmixmod} to estimate the densities of each covariate. For each configuration, the walk was launched on $20$ initial structures with a maximum of 9 000 steps each time.
	
		\subsection{Finding the structure}
		\subsubsection{How to evaluate found structure?}
			The first criterion is $\psi$ which is minimised in the MCMC. But in our case, $\psi$ is based on the likelihood whose value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $S$ we define some indicators to compare the true model $S$ and the found one $\hat{S}$.
			Global indicators :
			\begin{itemize}
				\item $TL$ (True left) : the number of found dependent variables that really are dependent $TL=|I_2\cap \hat{I}_2|$ 
				\item $WL$ (Wrong left) : the number of found dependent variables that are not dependent $WL=|\hat{I}_2|-TL$
				\item $ML$ (Missing left) : the number of really dependent variables not found $ML=|I_2|-TL$
				\item $\Delta p_2$ : the gap between the number of sub-regression in both model : $\Delta p_2=|I_2|-|\hat{I}_2|$. The sign defines if $\hat{S}$ is too complex or too simple
				\item $\Delta compl$ : the difference in complexity between both model : $\Delta compl=\sum_{j \in p_2}p_1^j-\sum_{j \in \hat{p}_2}\hat{p}_1^j$
			\end{itemize}
		\subsubsection{Results on $S$}	\label{compZ}
In table \ref{compZvrai} we compare found structures in different contexts with both Uniform (U) and Hierarchical Uniform (HU) a priori law on $P(S)$. 
We see that (HU) hypothesis gives sparser models even if $\max (\hat{p}_1^j)=5$. $p=40$ so the maximum value for $\hat{p_2}$ is $20$ in (HU) case and we see that this max is not reached. The stronger penalty implied by (HU) really is efficient. The datasets used for (U) and (HU) are the same to keep the comparison  meaningful. These datasets and $\hat{S}$ are those used for tables 
 \ref{YXlinOLS} to \ref{YX2linstep}.
 
 It is also notable that (HU) has a greater computational cost than (U). But next versions of the package may optimize a bit more the penalization step and reduce this gap.
	We also notice that the MCMC is faster when there are numerous correlations (rejecting more candidates). 
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
$n$ & $p_2$ & $\psi$ &  Time Mixmod  & Time MCMC  & $TL$ & $WL$ & $ML$ & $\Delta p_2$ & $\Delta compl$ \\
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&0.4104 & 3.2428 & 0 & 5.43 & 0 & -5.43 & 22.55  \\
& & & (0.0275) & (0.3711) & (0) & (1.9346) & (0) & (1.9346) & (8.0884) \\
 &  &HU &0.4104 & 8.2338 & 0 & 0.53 & 0 & -0.53 & 2.27  \\
& & & (0.0275) & (0.9045) & (0) & (0.7844) & (0) & (0.7844) & (3.3024) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&0.4182 & 2.7735 & 10.96 & 5.93 & 4.98 & -0.95 & 38.16  \\
& & & (0.0329) & (0.1498) & (1.9844) & (2.0313) & (1.9948) & (0.9987) & (6.499) \\
 &  &HU &0.4182 & 4.1876 & 11.61 & 4.57 & 4.33 & -0.24 & 16.48  \\
& & & (0.0329) & (0.2178) & (1.8743) & (1.9502) & (1.8752) & (0.4948) & (5.4892) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 0.4456 & 2.9154 & 25.23 & 1.92 & 6.5 & 4.58 & 28  \\
& & & (0.0429) & (0.1331) & (1.4761) & (1.0888) & (1.4668) & (0.9866) & (5.0831) \\
 &  & HU & 0.4456 & 4.0233 & 16.96 & 3.04 & 14.77 & 11.73 & 4.35  \\
& & & (0.0429) & (0.1091) & (1.3993) & (1.3993) & (1.4692) & (0.5478) & (5.8833) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&0.5229 & 4.7068 & 0 & 4.2 & 0 & -4.2 & 13.35  \\
& & & (0.0519) & (0.5865) & (0) & (1.7233) & (0) & (1.7233) & (5.6468) \\
 &  & HU &0.5229 & 10.1198 & 0 & 0.13 & 0 & -0.13 & 0.32  \\
& & & (0.0519) & (0.5541) & (0) & (0.3667) & (0) & (0.3667) & (0.9732) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&0.5205 & 3.3681 & 11.15 & 5.42 & 4.72 & -0.7 & 22.85  \\	
& & & (0.0451) & (0.3123) & (1.93) & (1.9132) & (1.886) & (0.7317) & (5.7742) \\
 &  &HU &0.5205 & 4.909 & 11.42 & 4.59 & 4.45 & -0.14 & 7.55  \\
& & & (0.0451) & (0.4556) & (1.9079) & (1.8968) & (1.8333) & (0.3487) & (4.0611) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U& 0.5833 & 3.2683 & 28.17 & 1.38 & 3.7 & 2.32 & 12.74  \\
& & & (0.0628) & (0.316) & (1.3711) & (0.9077) & (1.3143) & (0.8394) & (4.3359) \\
 &  &HU &0.5833 & 4.3599 & 17.27 & 2.73 & 14.6 & 11.87 & -2.61  \\
& & & (0.0628) & (0.3729) & (1.1708) & (1.1708) & (1.2792) & (0.338) & (4.4854) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U& 0.9623 & 12.9373 & 0 & 2.83 & 0 & -2.83 & 6.23  \\
& & & (0.077) & (1.7778) & (0) & (1.2953) & (0) & (1.2953) & (3.1999) \\
 &  &HU & 0.9623 & 20.9817 & 0 & 0.01 & 0 & -0.01 & 0.02  \\
& & & (0.077) & (1.9421) & (0) & (0.1) & (0) & (0.1) & (0.2) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& 1.1223 & 6.9647 & 11.67 & 4.8 & 4.25 & -0.55 & 12.58  \\
& & & (0.1122) & (0.5473) & (2.0003) & (2.0646) & (1.956) & (0.7833) & (3.9471) \\
 &  &HU & 1.1223 & 8.8486 & 12.04 & 3.95 & 3.88 & -0.07 & 3.75  \\
& & & (0.1122) & (0.7174) & (1.9223) & (1.9404) & (1.9137) & (0.2564) & (2.2625) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& 1.4343 & 5.9626 & 30.14 & 0.84 & 1.61 & 0.77 & 6.96  \\
& & & (0.2528) & (0.3136) & (1.3928) & (0.8495) & (1.2941) & (0.7086) & (3.0975) \\
 &  &HU & 1.4343 & 7.3741 & 17.49 & 2.51 & 14.26 & 11.75 & -3.76  \\
& & & (0.2528) & (0.2748) & (1.1849) & (1.1849) & (1.2441) & (0.4794) & (4.4859) \\
\hline
\end{tabular} 
\caption{Results of the Markov chain with  constraint $\hat{p}_1\leq 5$. Mean observed and standard deviation (sd). } \label{compZvrai}
\end{table}


\clearpage
\subsection{Results on prediction}
	\subsubsection{$Y$ depends on all variables in $X$}	 \label{tableMSEsimtout}	
We first try the method with a response depending on all covariates (\textsc{CorReg} reduces the dimension and can't give the true model if there is a structure). The datasets used here were those from \ref{compZvrai}.

We observe that \textsc{CorReg} is better than classical methods especially whith the Hierarchical Uniform law. When the complexity of the true model is higher than $\frac{p}{2}$ Uniform hypothesis logically is better but (HU) still beats classical methods. We also observe in table \ref{YXlinstep} that simple methods like stepwise (here from the package \textsc{lars}) can give good results in prediction.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$& $\psi$ & indicator &OLS  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U& MSE (sd) & 262627.57 (732019) & 5928332.4 (49005690.2) & 262627.57 (732019) \\
& & & cpl (sd) & 30 (0) & 29.99 (0.1) & 30 (0) \\
 &  &HU &MSE (sd) & 262627.57 (732019) & 10381962.64 (90962496.9) & 262627.57 (732019) \\
& & & cpl (sd) & 30 (0) & 30 (0) & 30 (0) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 510747.53 (2287539.8) & 635.42 (335.2) & 610.32 (424.9) \\
& & & cpl (sd) & 30 (0) & 24.11 (1) & 25 (0) \\
 &  &HU &MSE (sd) & 510747.53 (2287539.8) & 603.24 (415.6) & 610.32 (424.9) \\
& & & cpl (sd) & 30 (0) & 24.82 (0.6) & 25 (0) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 178323.95 (1426610.2) & 180.02 (44.9) & 141.03 (27.8) \\
& & & cpl (sd) & 30 (0) & 13.85 (0.9) & 9 (0) \\
 &  & HU & MSE (sd) & 178323.95 (1426610.2) & 330.31 (134.8) & 141.03 (27.8) \\
& & & cpl (sd) & 30 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&MSE (sd) & 528.08 (228.6) & 886.54 (364.3) & 528.08 (228.6) \\
& & & cpl (sd) & 41 (0) & 36.8 (1.7) & 41 (0) \\
 &  & HU &MSE (sd) & 528.08 (228.6) & 542.29 (235) & 528.08 (228.6) \\
& & & cpl (sd) & 41 (0) & 40.87 (0.4) & 41 (0) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&MSE (sd) & 612.72 (291.5) & 239.17 (89.4) & 200.32 (42.6) \\	
& & & cpl (sd) & 41 (0) & 24.43 (0.7) & 25 (0) \\
 &  &HU &MSE (sd) & 612.72 (291.5) & 207.6 (51.3) & 200.32 (42.6) \\
& & & cpl (sd) & 41 (0) & 24.99 (0.5) & 25 (0) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 555.44 (262.5) & 128.98 (18.1) & 121.08 (11.9) \\
& & & cpl (sd) & 41 (0) & 11.45 (0.9) & 9 (0) \\
 &  &HU &MSE (sd) & 555.44 (262.5) & 171.9 (31.1) & 121.08 (11.9) \\
& & &cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U& MSE (sd) & 167.71 (20.6) & 323.44 (124.1) & 167.71 (20.6) \\
& & & cpl (sd) & 41 (0) & 38.17 (1.3) & 41 (0) \\
 &  &HU &MSE (sd) & 167.71 (20.6) & 167.98 (20.8) & 167.71 (20.6) \\  
& & & cpl (sd) & 41 (0) & 40.99 (0.1) & 41 (0) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 168.68 (22.4) & 158.51 (51.6) & 133.49 (12.2) \\
& & & cpl (sd) & 41 (0) & 24.53 (0.7) & 25 (0) \\
 &  &HU &  MSE (sd) & 168.68 (22.4) & 137.03 (20.3) & 133.49 (12.2) \\
& & & cpl (sd) & 41 (0) & 25.01 (0.4) & 25 (0) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  MSE (sd) & 173.25 (22.9) & 112.8 (10.7) & 110.63 (6.9) \\
& & & cpl (sd) & 41 (0) & 10.02 (0.8) & 9 (0) \\
 &  &HU & MSE (sd) & 173.25 (22.9) & 127.4 (11.9) & 110.63 (6.9) \\
& & & cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\end{tabular} 
\caption{OLS and OLS combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  $\psi$ &indicator &LASSO  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&MSE (sd) & 1246.35 (350.5) & 1433.98 (526.7) & 1246.35 (350.5) \\
& & & cpl (sd) & 17.84 (5.5) & 15.79 (5.6) & 17.84 (5.5) \\
 &  &HU &MSE (sd) & 1246.35 (350.5) & 1248.84 (341.4) & 1246.35 (350.5) \\
& & & cpl (sd) & 17.84 (5.5) & 17.63 (5.6) & 17.84 (5.5) \\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 &16& U & MSE (sd) & 712.79 (405.2) & 566.32 (226.4) & 554.01 (257.1) \\
& & & cpl (sd) & 16.68 (4.4) & 15.35 (4.3) & 16.14 (4.5) \\
 &  & HU & MSE (sd) & 712.79 (405.2) & 551.92 (242.4) & 554.01 (257.1) \\
& & & cpl (sd) & 16.68 (4.4) & 15.96 (4.4) & 16.14 (4.5) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 216.12 (128.3) & 157.78 (39.4) & 147.04 (28.4) \\
& & & cpl (sd) & 11.23 (4.4) & 8.56 (1.8) & 7.91 (1.3) \\
 &  & HU & MSE (sd) & 216.12 (128.3) & 178.88 (68.9) & 147.04 (28.4) \\
& & & cpl (sd) & 11.23 (4.4) & 9.86 (3) & 7.91 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&MSE (sd) & 658.38 (221.4) & 872.32 (239.4) & 658.38 (221.4) \\
& & & cpl (sd) & 28.2 (6) & 22.95 (5.8) & 28.2 (6) \\
 &  & HU &MSE (sd) & 658.38 (221.4) & 652.72 (211.9) & 658.38 (221.4) \\
& & & cpl (sd) & 28.2 (6) & 28.36 (5.9) & 28.2 (6) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&MSE (sd) & 274.34 (101.7) & 268.76 (106.3) & 225.64 (67.5) \\	
& & & cpl (sd) & 22.14 (4.1) & 19.52 (2.9) & 20.47 (2.8) \\
 &  &HU &MSE (sd) & 274.34 (101.7) & 233.8 (78.9) & 225.64 (67.5) \\
& & & cpl (sd) & 22.14 (4.1) & 20.19 (2.8) & 20.47 (2.8) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 165.6 (73.7) & 128.34 (18.8) & 123.73 (12.8) \\
& & & cpl (sd) & 12.93 (6.4) & 8.51 (1.5) & 8.09 (1.1) \\
 &  &HU &MSE (sd) & 165.6 (73.7) & 135.84 (24.1) & 123.73 (12.8) \\
& & &cpl (sd) & 12.93 (6.4) & 10.17 (3.1) & 8.09 (1.1) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U&  MSE (sd) & 183.33 (31.1) & 357.15 (133.4) & 183.33 (31.1) \\
& & & cpl (sd) & 37.78 (2.4) & 32.93 (3.7) & 37.78 (2.4) \\
 &  &HU &  MSE (sd) & 183.33 (31.1) & 183.78 (31.6) & 183.33 (31.1) \\
& & &  cpl (sd) & 37.78 (2.4) & 37.75 (2.4) & 37.78 (2.4) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& MSE (sd) & 148.83 (18.6) & 164.44 (54.3) & 139.57 (16.1) \\
& & & cpl (sd) & 25.22 (4) & 21.72 (2) & 22.3 (1.7) \\
 &  &HU &  MSE (sd) & 148.83 (18.6) & 142.97 (22.1) & 139.57 (16.1) \\
& & & cpl (sd) & 25.22 (4) & 22.24 (1.8) & 22.3 (1.7) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& MSE (sd) & 124.29 (19.9) & 113.36 (11) & 111.54 (7.2) \\
& & & cpl (sd) & 13.18 (5.9) & 8.68 (1.2) & 8.53 (1) \\
 &  &HU & MSE (sd) & 124.29 (19.9) & 118.33 (12) & 111.54 (7.2) \\
& & & cpl (sd) & 13.18 (5.9) & 11.02 (2.9) & 8.53 (1) \\
\hline
\end{tabular} 
\caption{LASSO (with LAR) combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X$. \textsc{CorReg} logically wins. LASSO is better than OLS but is improved by \textsc{CorReg} even for large values of $n$.}\label{YXlinLASSO}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  $\psi$ &indicator &Elasticnet  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&MSE (sd) & 1 326.54 (388.4) & 1 356.98 (331.4) & 1 326.54 (388.4) \\
& & & cpl (sd) & 12.14 (5.1) & 12.5 (5.2) & 12.14 (5.1) \\
 &  &HU & MSE (sd) & 1326.54 (388.4) & 1307.96 (356.6) & 1326.54 (388.4) \\
& & & cpl (sd) & 12.14 (5.1) & 12.27 (5.1) & 12.14 (5.1) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 1 400.56 (1598.2) & 668.57 (274.6) & 653.59 (283.7) \\
& & & cpl (sd) & 13.86 (6.8) & 14.31 (5.7) & 14.83 (5.3) \\
 &  &HU &MSE (sd) & 1 400.56 (1598.2) & 643.25 (277.9) & 653.59 (283.7) \\
& & & cpl (sd) & 13.86 (6.8) & 14.83 (5.6) & 14.83 (5.3) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 855.74 (582.6) & 181.08 (51.9) & 146.79 (32.7) \\
& & & cpl (sd) & 15.57 (6.3) & 11.19 (2.4) & 8.37 (1.3) \\
 &  & HU & MSE (sd) & 855.74 (582.6) & 311.57 (163.9) & 146.79 (32.7) \\
& & & cpl (sd) & 15.57 (6.3) & 14.87 (3.9) & 8.37 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&MSE (sd) & 738.94 (254.5) & 914.56 (283.7) & 738.94 (254.5) \\
& & & cpl (sd) & 25.85 (8.7) & 20.97 (7.8) & 25.85 (8.7) \\
 &  & HU &MSE (sd) & 738.94 (254.5) & 751.06 (262.2) & 738.94 (254.5) \\
& & & cpl (sd) & 25.85 (8.7) & 25.43 (8.8) & 25.85 (8.7) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 516.36 (226.6) & 276.6 (133.6) & 228.85 (89.3) \\
& & & cpl (sd) & 26.72 (7.4) & 20.49 (3.8) & 21.8 (3.3) \\
 &  &HU &MSE (sd) & 516.36 (226.6) & 239.77 (103.7) & 228.85 (89.3) \\
& & & cpl (sd) & 26.72 (7.4) & 21.57 (3.4) & 21.8 (3.3) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 328.7 (146.3) & 130.61 (20.6) & 124.01 (15.4) \\
& & & cpl (sd) & 24.7 (7.1) & 9.95 (1.5) & 8.22 (1.2) \\
 &  &HU &MSE (sd) & 328.7 (146.3) & 169.13 (40.2) & 124.01 (15.4) \\
& & &cpl (sd) & 24.7 (7.1) & 16.04 (3.8) & 8.22 (1.2) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U& MSE (sd) & 175.93 (25.9) & 355.79 (143.8) & 175.93 (25.9) \\
& & & cpl (sd) & 39.71 (1.7) & 34.11 (4.6) & 39.71 (1.7) \\
 &  &HU &  MSE (sd) & 175.93 (25.9) & 176.2 (26.3) & 175.93 (25.9) \\
& & &  cpl (sd) & 39.71 (1.7) & 39.69 (1.7) & 39.71 (1.7) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 173.65 (25.1) & 164.96 (59) & 139.51 (17.6) \\
& & &  cpl (sd) & 35.54 (4.2) & 22.58 (2.2) & 23.26 (1.9) \\
 &  &HU &  MSE (sd) & 173.65 (25.1) & 143.04 (23.2) & 139.51 (17.6) \\
& & & cpl (sd) & 35.54 (4.2) & 23.16 (1.9) & 23.26 (1.9) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  MSE (sd) & 161.92 (25.2) & 113.61 (10.9) & 111.46 (7.2) \\
& & &  cpl (sd) & 30.78 (6.4) & 9.25 (1.2) & 8.67 (1) \\
 &  &HU &  MSE (sd) & 161.92 (25.2) & 127.44 (13.3) & 111.46 (7.2) \\
& & &cpl (sd) & 30.78 (6.4) & 17.85 (2.7) & 8.67 (1) \\
\hline
\end{tabular} 
\caption{Elasticnet combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X$. \textsc{CorReg} logically wins. LASSO was better.}\label{YXlinenet}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  $\psi$ &indicator &Stepwise  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U& MSE (sd) & 1 919.43 (861.8) & 2 014.13 (683.6) & 1 919.43 (861.8) \\
& & &cpl (sd) & 22.8 (3.3) & 19.34 (4.9) & 22.8 (3.3) \\
 &  &HU &MSE (sd) & 1 919.43 (861.8) & 1 885.39 (814) & 1 919.43 (861.8) \\
& & & cpl (sd) & 22.8 (3.3) & 22.73 (3.1) & 22.8 (3.3) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 696.46 (492.9) & 662.07 (285.9) & 660.93 (354) \\
& & & cpl (sd) & 15.9 (3.8) & 15.1 (3.7) & 15.76 (3.7) \\
 &  &HU &MSE (sd) & 696.46 (492.9) & 655.94 (325.7) & 660.93 (354) \\
& & & cpl (sd) & 15.9 (3.8) & 15.72 (3.6) & 15.76 (3.7) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 394.93 (356) & 155.18 (36.4) & 147.81 (30.3) \\
& & &cpl (sd) & 16.1 (6.3) & 8.26 (1.7) & 7.77 (1.3) \\
 &  & HU &  MSE (sd) & 394.93 (356) & 189.49 (79) & 147.81 (30.3) \\
& & & cpl (sd) & 16.1 (6.3) & 10.23 (3) & 7.77 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&MSE (sd) & 745.88 (241.7) & 962.12 (319.3) & 745.88 (241.7) \\
& & & cpl (sd) & 27.19 (4.6) & 22.9 (4.5) & 27.19 (4.6) \\
 &  & HU &MSE (sd) & 745.88 (241.7) & 749.08 (244.2) & 745.88 (241.7) \\
& & & cpl (sd) & 27.19 (4.6) & 27.21 (4.5) & 27.19 (4.6) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 279.82 (151.1) & 290.13 (117.3) & 239.61 (69.5) \\
& & & cpl (sd) & 21.24 (5.2) & 18.13 (2.8) & 19.23 (2.7) \\
 &  &HU &MSE (sd) & 279.82 (151.1) & 252.35 (108.5) & 239.61 (69.5) \\
& & & cpl (sd) & 21.24 (5.2) & 18.97 (2.7) & 19.23 (2.7) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 199.3 (158.7) & 126.81 (17.4) & 124.19 (13) \\
& & & cpl (sd) & 14.31 (6.2) & 8.16 (1.2) & 8.04 (1) \\
 &  &HU &MSE (sd) & 199.3 (158.7) & 136.32 (25.2) & 124.19 (13) \\
& & &cpl (sd) & 14.31 (6.2) & 9.59 (2.6) & 8.04 (1) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U&   MSE (sd) & 193.26 (34.9) & 387.18 (151.6) & 193.26 (34.9) \\
& & & cpl (sd) & 36.3 (2.5) & 30.94 (4.2) & 36.3 (2.5) \\
 &  &HU &   MSE (sd) & 193.26 (34.9) & 194.43 (37.3) & 193.26 (34.9) \\
& & &  cpl (sd) & 36.3 (2.5) & 36.22 (2.7) & 36.3 (2.5) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 145.46 (20.1) & 165.74 (56.1) & 140.27 (17.5) \\
& & & cpl (sd) & 23.44 (3.8) & 21.23 (2.1) & 21.93 (1.7) \\
 &  &HU &  MSE (sd) & 145.46 (20.1) & 143.74 (23.2) & 140.27 (17.5) \\
& & & cpl (sd) & 23.44 (3.8) & 21.86 (1.7) & 21.93 (1.7) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& MSE (sd) & 128.44 (18.4) & 113.27 (10.7) & 111.77 (7.2) \\
& & &  cpl (sd) & 13.58 (5.2) & 8.59 (1.1) & 8.48 (1) \\
 &  &HU &  MSE (sd) & 128.44 (18.4) & 118.61 (11.5) & 111.77 (7.2) \\
& & & cpl (sd) & 13.58 (5.2) & 10.75 (2.8) & 8.48 (1) \\
\hline
\end{tabular} 
\caption{Stepwise  combined with constrained \textsc{CorReg}. $Y$ depends on all variables in $X$. \textsc{CorReg} logically wins but stepwise is quite good compared to elasticnet.}\label{YXlinstep}
\end{table}



\clearpage
	\subsubsection{$Y$ depends only on covariates in $X_2$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $X_2$. The datasets used here were still those from \ref{compZvrai}.
Depending only on $X_2$ imply sparsity and impossibility to obtain the true model when using the true structure. \textsc{CorReg} is still better than classical methods. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  $\psi$ &indicator &OLS  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 81781.54 (247281.7) & 507.1 (269.2) & 593.03 (428.3) \\
& & & cpl (sd) & 30 (0) & 24.11 (1) & 25 (0) \\
 &  &HU &MSE (sd) & 81781.54 (247281.7) & 587.07 (425.6) & 593.03 (428.3) \\
& & & cpl (sd) & 30 (0) & 24.82 (0.6) & 25 (0) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 74136.81 (297716.1) & 189.45 (48.1) & 144.51 (31.4) \\
& & & cpl (sd) & 30 (0) & 13.85 (0.9) & 9 (0) \\
 &  & HU & MSE (sd) & 74136.81 (297716.1) & 340.53 (169.4) & 144.51 (31.4) \\
& & & cpl (sd) & 30 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 684.04 (438.1) & 190.97 (37.7) & 197.32 (40) \\
& & & cpl (sd) & 41 (0) & 24.43 (0.7) & 25 (0) \\
 &  &HU &MSE (sd) & 684.04 (438.1) & 196.36 (40.5) & 197.32 (40) \\
& & & cpl (sd) & 41 (0) & 24.99 (0.5) & 25 (0) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 596.32 (323) & 126.94 (15.5) & 119.39 (12.4) \\
& & & cpl (sd) & 41 (0) & 11.45 (0.9) & 9 (0) \\
 &  &HU &MSE (sd) & 596.32 (323) & 168.03 (27.7) & 119.39 (12.4) \\
& & &cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 168.35 (20.1) & 133.75 (12.4) & 135.08 (13.1) \\
& & & cpl (sd) & 41 (0) & 24.53 (0.7) & 25 (0) \\
 &  &HU &  MSE (sd) & 168.35 (20.1) & 135.04 (13.1) & 135.08 (13.1) \\
& & & cpl (sd) & 41 (0) & 25.01 (0.4) & 25 (0) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  MSE (sd) & 168.07 (21.4) & 109.61 (7.4) & 108.64 (7.2) \\
& & &  cpl (sd) & 41 (0) & 10.02 (0.8) & 9 (0) \\
 &  &HU & MSE (sd) & 168.07 (21.4) & 124.23 (11.1) & 108.64 (7.2) \\
& & & cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\end{tabular} 
\caption{OLS and OLS combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X_2$. Sometimes $\hat{S}$ gives better results than $S$ because $S$ is penalized by the fact that it relies only on covariates not in the true model. } \label{YX2linOLS}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  $\psi$ &indicator &LAR  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 367.45 (198.4) & 298.32 (122.4) & 292.53 (102.2) \\
& & & cpl (sd) & 14.37 (5.1) & 12.9 (3.8) & 12.84 (3.8) \\
 &  &HU &MSE (sd) & 367.45 (198.4) & 298.1 (119.1) & 292.53 (102.2) \\
& & & cpl (sd) & 14.37 (5.1) & 12.81 (3.8) & 12.84 (3.8) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 273.12 (267.2) & 166.93 (49) & 152.45 (39.5) \\
& & & cpl (sd) & 11.7 (5.8) & 8.7 (2.5) & 7.45 (1.4) \\
 &  & HU & MSE (sd) & 273.12 (267.2) & 175.68 (57.5) & 152.45 (39.5) \\
& & & cpl (sd) & 11.7 (5.8) & 9.19 (2.9) & 7.45 (1.4) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 189.64 (52.5) & 171.97 (45.9) & 176.58 (46.8) \\
& & & cpl (sd) & 14.15 (3.9) & 13.4 (2.8) & 13.5 (3.1) \\
 &  &HU &MSE (sd) & 189.64 (52.5) & 174.36 (45.3) & 176.58 (46.8) \\
& & & cpl (sd) & 14.15 (3.9) & 13.57 (3) & 13.5 (3.1) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 163.88 (67.4) & 124.53 (17) & 121.99 (16.6) \\
& & & cpl (sd) & 13.06 (7.2) & 8.29 (1.6) & 7.73 (1.2) \\
 &  &HU &MSE (sd) & 163.88 (67.4) & 133.7 (26.4) & 121.99 (16.6) \\
& & &cpl (sd) & 13.06 (7.2) & 9.65 (3.3) & 7.73 (1.2) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& MSE (sd) & 129.65 (14.8) & 127.11 (12.1) & 127.46 (12.2) \\
& & &  cpl (sd) & 14.86 (3.1) & 13.97 (2.5) & 14.04 (2.5) \\
 &  &HU &  MSE (sd) & 129.65 (14.8) & 127.46 (12.2) & 127.46 (12.2) \\
& & & cpl (sd) & 14.86 (3.1) & 14.02 (2.5) & 14.04 (2.5) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  MSE (sd) & 121.21 (20.1) & 109.66 (7.9) & 109.04 (7.8) \\
& & & cpl (sd) & 12.84 (5.2) & 8.42 (1.1) & 8.11 (0.8) \\
 &  &HU &  MSE (sd) & 121.21 (20.1) & 112.56 (11.2) & 109.04 (7.8) \\
& & & cpl (sd) & 12.84 (5.2) & 9.77 (2.3) & 8.11 (0.8) \\
\hline
\end{tabular} 
\caption{LASSO (with LAR) combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X_2$. }\label{YX2linLASSO}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  $\psi$ &indicator & Elasticnet  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 499.32 (218.8) & 311.41 (137.4) & 305.09 (123.5) \\
& & & cpl (sd) & 14.08 (6.1) & 11.75 (4.8) & 11.49 (4.7) \\
 &  &HU &MSE (sd) & 499.32 (218.8) & 307.6 (128.7) & 305.09 (123.5) \\
& & & cpl (sd) & 14.08 (6.1) & 11.63 (4.7) & 11.49 (4.7) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 691.55 (503.2) & 193.45 (54.2) & 150.17 (35) \\
& & & cpl (sd) & 12.54 (7.2) & 10.26 (2.7) & 7.81 (1.4) \\
 &  & HU & MSE (sd) & 691.55 (503.2) & 279.21 (117.3) & 150.17 (35) \\
& & & cpl (sd) & 12.54 (7.2) & 13 (4.3) & 7.81 (1.4) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 282.05 (134.8) & 180.64 (47.6) & 181.49 (48.6) \\
& & & cpl (sd) & 20.37 (6.3) & 13.68 (4) & 14.05 (4) \\
 &  &HU &MSE (sd) & 282.05 (134.8) & 180.84 (47.5) & 181.49 (48.6) \\
& & & cpl (sd) & 20.37 (6.3) & 14.03 (4) & 14.05 (4) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 308.13 (142.9) & 129.39 (19.6) & 122.71 (17.8) \\
& & & cpl (sd) & 22.94 (7.7) & 9.37 (2.1) & 7.91 (1.4) \\
 &  &HU &MSE (sd) & 308.13 (142.9) & 159.14 (30.4) & 122.71 (17.8) \\
& & &cpl (sd) & 22.94 (7.7) & 15.42 (4) & 7.91 (1.4) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& MSE (sd) & 150.14 (19.7) & 127.48 (14) & 128.21 (15) \\
& & &  cpl (sd) & 25.6 (5.4) & 14.73 (3.8) & 14.73 (3.9) \\
 &  &HU &  MSE (sd) & 150.14 (19.7) & 128.21 (14.9) & 128.21 (15) \\
& & & cpl (sd) & 25.6 (5.4) & 14.7 (3.9) & 14.73 (3.9) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& MSE (sd) & 157.18 (23.5) & 110.24 (8) & 109.24 (7.8) \\
& & &  cpl (sd) & 30.41 (6.7) & 8.96 (1.3) & 8.3 (1) \\
 &  &HU &  MSE (sd) & 157.18 (23.5) & 123.8 (12.5) & 109.24 (7.8) \\
& & & cpl (sd) & 30.41 (6.7) & 17.18 (3.3) & 8.3 (1) \\
\hline
\end{tabular} 
\caption{Elasticnet (with Elasticnet) combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X_2$.}\label{YX2linenet}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  $\psi$ &indicator &Stepwise  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 394.59 (454.6) & 341.47 (153.2) & 351.14 (148) \\
& & & cpl (sd) & 13.09 (3.2) & 12.9 (2.9) & 12.92 (3) \\
 &  &HU &MSE (sd) & 394.59 (454.6) & 353.99 (157) & 351.14 (148) \\
& & & cpl (sd) & 13.09 (3.2) & 12.83 (2.9) & 12.92 (3) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 410.11 (343.5) & 163.05 (45.4) & 152.08 (38.1) \\
& & & cpl (sd) & 15.8 (6.5) & 8.29 (2.2) & 7.34 (1.3) \\
 &  & HU & MSE (sd) & 410.11 (343.5) & 178.94 (69.7) & 152.08 (38.1) \\
& & & cpl (sd) & 15.8 (6.5) & 8.89 (3) & 7.34 (1.3) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 180.67 (52.7) & 173.02 (38.3) & 174.84 (36.8) \\
& & & cpl (sd) & 13.56 (3.7) & 12.78 (2.4) & 13.07 (2.4) \\
 &  &HU &MSE (sd) & 180.67 (52.7) & 175.3 (38.1) & 174.84 (36.8) \\
& & & cpl (sd) & 13.56 (3.7) & 13.01 (2.4) & 13.07 (2.4) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 188.91 (88.5) & 124.38 (17.4) & 122.38 (16.7) \\
& & & cpl (sd) & 14.74 (6.6) & 8.07 (1.6) & 7.65 (1.2) \\
 &  &HU &MSE (sd) & 188.91 (88.5) & 134.22 (23.6) & 122.38 (16.7) \\
& & &cpl (sd) & 14.74 (6.6) & 9.5 (2.8) & 7.65 (1.2) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 128.18 (13.7) & 127.43 (13.7) & 127.71 (13.3) \\
& & &  cpl (sd) & 13.78 (2.8) & 13.51 (2) & 13.59 (2.1) \\
 &  &HU & MSE (sd) & 128.18 (13.7) & 127.71 (13.3) & 127.71 (13.3) \\
& & & cpl (sd) & 13.78 (2.8) & 13.59 (2.1) & 13.59 (2.1) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& MSE (sd) & 126.15 (20.6) & 109.47 (8) & 109.04 (7.8) \\
& & & cpl (sd) & 13.25 (4.4) & 8.3 (1) & 8.12 (0.8) \\
 &  &HU &  MSE (sd) & 126.15 (20.6) & 113.96 (11.4) & 109.04 (7.8) \\
& & & cpl (sd) & 13.25 (4.4) & 10.08 (2.6) & 8.12 (0.8) \\
\hline
\end{tabular} 
\caption{Stepwise  combined with constrained \textsc{CorReg}. $Y$ depends on all variables in $X_2$.}\label{YX2linstep}
\end{table}

\clearpage
\subsection{Robustess of the model}
 We have generated non linear structures (Tables \ref{compZnonlin} and \ref{resYnonlin}). Variables in $X_2$ depends on the $\log$ or the square (randomly choosen with equiprobability) of a variable in $X_1$ . Dependencies are still real but non linear. \textsc{CorReg} still found dependencies. One example of non-linear structure found with \textsc{CorReg} on real datasets is given in section \ref{sectionexfos}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
$n$ & $p_2$ & $\psi$ &Time Mixmod  & Time MCMC  & $TL$ & $WL$ & $ML$ & $\Delta p_2$ & $\Delta compl$ \\
\hline
30 & 16 & U& 0.4313 & 2.7769 & 9.96 & 2.45 & 5.95 & 3.5 & 29.42  \\
& & &(0.0391) & (0.0883) & (1.3175) & (1.3808) & (1.3056) & (1.453) & (6.2639) \\
 &  & HU & 0.4313 & 4.9079 & 7.28 & 1.25 & 8.63 & 7.38 & 5.77  \\ 
& & &(0.0391) & (0.4353) & (1.5769) & (0.9987) & (1.5548) & (1.6316) & (6.1297) \\ 
\hline
\end{tabular} 
\caption{Results of the Markov chain for non linear structure ($\log$ and square). Mean observed and standard deviation (sd). } \label{compZnonlin}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  $\psi$ &indicator &LASSO  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %nonlin0.5global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_55_33.csv
30 & 0 & U&MSE (sd) & 996.06 (323.7) & 1100.17 (391.4) & 1501.25 (905.9) \\
& & &cpl (sd) & 17.66 (5.4) & 14.7 (4.7) & 11.97 (4.9) \\
 &  &HU &MSE (sd) & 996.06 (323.7) & 1094.5 (621.8) & 1501.25 (905.9) \\
& & & cpl (sd) & 17.66 (5.4) & 16.98 (4.8) & 11.97 (4.9) \\ 
\hline
\end{tabular} 
\caption{ $Y$ depends on all variables in $X$. \textsc{CorReg} is not too far from LASSO even if it stays behind. } \label{resYnonlin}
\end{table}


	\clearpage	
\section{Numerical results on real datasets} \label{sectionrealcase}
\subsection{Quality case study} \label{sectionexfos}
This work takes place in steel industry context, with quality oriented objective : to understand and prevent quality problems on finished product, knowing the whole process. 
		
We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item 205 variables from the whole process to explain it.
			\item The stakes : a hundred euros per ton (for information: Dunkerque's  site aims to produce up to 7.5 millions tons a year)
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/mixmod.png} 
			\caption{Example of non-Gaussian real variable easily modeled by a Gaussian mixture}\label{graphMixmod}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/histR2exfos.png} 
			\caption{$R^2_{adj}$ of the 76 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/correlexfoshist.png} 
			\caption{Histogram of correlations in $X$.} \label{compareMSEexfos}
   \end{minipage}
\end{figure}   			
	We get a training set of $n=3 000$ products described by $p=205$ variables from the industrial process and a validation sample of $847$ products.
	Let's note $\rho$ the absolute value of correlations between two covariates. Industrial variables are naturally highly correlated as the width and the weight of a steel slab ($\rho=0.905$), the temperature before and after some tool ($\rho=0.983$), the  roughness of both faces of the product ($\rho= 0.919$), a mean and a max ($\rho=0.911$). \textsc{CorReg} also found more complex structures describing physical models, like   Width = f (Mean.flow , Mean.speed.CC) even if the true Physcial model is not linear : Width = flow / (speed * thickness) (here thickness is constant). Non linear regulation models used to optimize the process were also found (but are confidential). These first results are easily understandable and meet metallurgists expertise.  
			The algorithm gives a structure of $p_2=76$ subregressions with a mean of $\bar{p_1}=5.17$ regressors. In $X_1$ the number of $\rho>0.7$ is $\textbf{79.33\%}$ smaller than in $X$.		
	
			It is now time to look at the predictive results (Figure \ref{compareMSEexfos}).
				The best model found when not using \textsc{CorReg} is given by the LASSO. But when using \textsc{CorReg} elasticnet produces a better model in terms of prediction. LASSO gives a model with 21 non-zero coefficients and elasticnet with \textsc{CorReg} gives a model with 40 non-zero parameters but $6.40\%$ better in prediction on the validation sample (847 products). $14$ non-zero coefficients are common between the two models.
				Elasticnet alone get a model with 78 parameters that is improved by $9.75\%$ in prediction when used with \textsc{CorReg}. When using LASSO with \textsc{CorReg} we obtain a model with 24 non-zero coefficients that is $4.11\%$ better than LASSO alone. We also computed the OLS model (without selection) and the naive one (estimating the response by the mean of the learning set). All the MSE were modified here to obtain a value of 100 for the best (to preserve confidentiality). Elasticnet with \textsc{CorReg} is $13.51\%$ better than OLS.
		\begin{figure}[h]
			\centering
				\label{compareMSEexfos}
				\includegraphics[width=400px]{figures/MSEfinal.png}
			\caption{MSE comparison on industrial dataset. Learning set : 3 000 products, validation set : 847 products}
		\end{figure}		
		\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	OLS & 115.63 & 206 \\ 
	\hline 
	\textsc{CorReg} + OLS & 109.59&130 \\ 
	\hline  
	LASSO & 106.84 & 21 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 102.45 &24 \\ 
	\hline 
	elasticnet & 110.81 & 78\\ 
	\hline 
	\textsc{CorReg} + elasticnet & 100 &40 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained on a validation sample.}	
\end{table}

		In terms of interpretation, the main regression comes with the family of regression so it gives a better understanding of the consequences of corrective actions on the whole process. It typically permits to determine the \textit{tuning parameters} whereas LASSO would point variables we can't directly act on.	So it becomes easier to take corrective actions on the process to reach the goal. The stakes are so important that even a little improvement leads to consequent benefits, and we don't even talk of the impact on the market shares that is even more important.
		\clearpage
		
		\subsection{Production case study}
This second example is about a phenomenon that impacts the productivity of a steel plan.
We have :
		\begin{itemize}
			\item a (confidential)  response variable,
			\item $p=145$ variables from the whole process to explain it but only $n=100$ individuals.
			\item The stakes : $20\%$ of productivity to gain
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[width=150px]{figures/correlbeforeafter.png} 
			\caption{Correlations between the covariates in $X$ (upper) and $\hat{X}_1$ (lower).}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/R2corregBVBI.png} 
			\caption{$R^2_{adj}$ of the 67 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/histcorrelBVBI.png} 
			\caption{Histogram of correlations in $X$.} 
   \end{minipage}
\end{figure}   			
	Here $n<p$ so we only compare the leave-one-out cross-validation MSE.
	\textsc{CorReg} improves LASSO by $5.24\%$ and elasticnet by $8.60\%$. \textsc{CorReg} combined with LASSO gives the best result but it is only a leave-on-out MSE.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	LASSO & 105.54 & 34 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 100 & 18 \\ 
	\hline 
	elasticnet & 129.94 & 13 \\ 
	\hline 
	\textsc{CorReg} + elasticnet & 118.76 & 21 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained with leave-one out cross-validation. $n=100, p=145$.}	
\end{table}
In this precise case, \textsc{CorReg} found a structure that helped to decorrelate covariates in interpretation and to find the relevant part of the process to optimize.


\section{Conclusion and perspectives}
	We have seen that correlations can lead to serious estimation and variable selection problems in linear regression and that in such a context, it can be useful to explicitly model the structure between the covariates and to use this structure (even sequentially) to avoid correlations issues. We also show that real industrial context faces this kind of situations so our model can help to interprete and predict physical phenomenon efficiently and to help to manage missing values. But for now we still need a full dataset to learn the structure between the covariates and even if correlations are strong, some information is lost. Further work is needed to face these two challenges.
	
	\textsc{CorReg} is accessible on CRAN and has already proved its efficiency on real regression problematics in industry. \textsc{CorReg}'s strength is its great interpretability of the model, composed of several short linear regression easily managed by non-statisticians while strongly reducing correlations issues that are everywhere in industry.
	Nevertheless, we need to enlarge its application field to missing values, also very commons in industry. The actual generative model allows such a functionality without supplementary hypothesis and this also is a strength of \textsc{CorReg}. 
	
	Another perspective would be to take back lost information (the residual of each sub-regression) to improve predictive efficiency when needed. It would only consists in a second step of linear regression between the residuals and would thus still be able to use any selection method.
	
	This paper only treats linear regression but such a pretreatment could be used for logistic regression, {\it etc.}
	So the subject is still wide opened.	
	
\bibliography{biblio}{}
\bibliographystyle{plain}
\section{Appendices}
	\subsection{Identifiability of the structure}
	The model presented above relies on a discrete structure $S$ between the covariates. But to find it we need identifiability property to insure the MCMC will asymptotically find the true model. Identifiability of the discrete structure is asked in following terms: Is it possible to find another structure $\tilde{S}$ of linear regression between the covariates leading to the same joint distribution and marginal distributions? The answer is no. Thus $S$ is identifiable.
	
	
	If there are exact regressions ($\sigma^2_j=0$) in (\ref{SR}), the structure won't be identifiable. But it only means that several structure will have the same likelihood and they will have the same interpretation. So it's not really a problem. Moreover, when an exact sub-regression is found, we can delete one of the implied variables without any loss of information and the structure will define a list of variable from which to delete. %\textsc{CorReg} (Our R package) prints a warning to point out exact regressions when found.
	In the followings we suppose $\sigma^2_j\neq 0$.
	
	
\subsubsection{Alternative neighbourhoods for the MCMC}
	We have here at each step $|\mathcal{V}_{S,j}|=p$ candidates but some other constraints can be added on the definition of $\mathcal{S}$ and will consequently modify the size of the neighbourhood (for example a maximum complexity for the internal regressions or the whole structure, a maximum number of internal regressions, {\it etc.}). \textsc{CorReg} allows to modify this neighbourhood to better fit users constraints. Relaxation (column-wise and row-wise) is optional but gives more stability to the number of feasible candidates at each step and allows to modify several parts of $I_1$ in only one step when needed. Hence it improves efficiency by a significant reinforcement of the irreductibility of the Markov chain. Rejecting candidates instead of doing the relaxation steps will  however reduce the number of evaluated candidates and thus accelerate the walk. So it can be used for a warming phase when $n$ is great and time is missing.
	
	The hierchical uniform hypothesis made above for $P(S)$ implies $p_2<\frac{p}{2}$ and $p_1^j<\frac{p}{2}$ so candidates may be rejected to satisfy this hypothesis. Stronger constraints on $p_2$ and/or $p_1$ can be given in \textsc{CorReg} if relevant.
	
If the algorithm did not have time to converge (stationnarity), it can be continued with a few step for which the neighboorhood would only contain smaller candidates (in terms of complexity). It is equivalent to ask for each element in $I_1$ if the criterion $\psi$ would be better without it. Thus it can be seen as a final cleaning step. But in fact, it's just continuing the MCMC with a reduced neighbourhood.	
	
	
	
\end{document}
