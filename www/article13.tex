\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[table]{xcolor}
 \graphicspath{{figures/}}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%%\author{Clément THERY, Christophe BIERNACKI, Gaétan LORIDANT}
%\title{Model-based variable selection for regression with highly correlated variables.}

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%


\definecolor{darkgreen}{rgb}{0,0.4,0}
	 \definecolor{darkred}{rgb}{0.75,0,0}
	 \definecolor{darkblue}{rgb}{0,0,0.4}

\begin{document}
\begin{center}
{\Large
	{\sc Model-based variable selection for regression \\ with highly correlated variables.}
}
\bigskip

  Clément Théry$^{1}$ \& Christophe Biernacki$^{2}$ \& Gaétan Loridant$^{3}$
\bigskip

{\it
$^{1}$ ArcelorMittal, Université Lille 1, Inria, CNRS, clement.thery@arcelormittal.com
 
$^{2}$ Université Lille 1, Inria, CNRS, christophe.biernacki@math.univ-lille1.fr

$^{3}$ Etudes Industrielles ArcelorMittal Dunkerque, gaetan.loridant@arcelormittal.com\textbf{}
}
\end{center}
\bigskip

{\bf Abstract.} Linear regression outcomes are known to be damaged by highly correlated covariates. However many modern datasets are expected to convey more and more highly correlated covariates due to the global increase of the amount of variables in datasets. We propose to explicitly model the correlations by a family of linear regressions between the covariates, some covariates explaining others. It allows then to obtain by marginalisation on the explained covariates a parsimonious correlation-free regression model, easily understandable. 
It corresponds to a kind of variable selection preliminary step which has then to be followed by standard linear estimation methods including classical variables selection procedures for instance. The structure of correlations is found with an MCMC algorithm aiming at optimizing a specific BIC criterion.
 An R package (\textsc{CorReg}) available on the CRAN implements this new method which will be illustrated on both simulated datasets and real-life datasets from steel industry where correlated variables are frequent.
\smallskip

{\bf Keywords.} Regression, correlations, industry, variable selection, generative models

\section{Introduction}
%la régression et ses problèmes
Linear regression is a very standard and efficient method providing a predictive model with a good interpretability even  for non-statisticians. Therefore, it is used in nearly all the fields where statistics are made \cite{montgomery2012introduction},  industry (illustrated in the present paper), astronomy \cite{isobe1990linear}, sociology \cite{longford2012revision} \dots
%However, linear regression has to face the problem of the variance of the estimators.
%Linear regression is a very classic situation but sometimes it has to face an also classical problem: the variance of the estimators. 
%We note the linear regression model:
%\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \label{regressionsimple}
%	\end{equation}
%	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables, $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $p\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{OLS}). %As shown in section \ref{sectionOLS}, 
%	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
%Conditionning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects : the dimension $p$ (number of covariates) of the model (the more covariates you have the greater variance you get)
%	 and the correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .

	With the rise of informatics, datasets contain more and more covariates, % and thus dimension reduction becomes a necessity. Moreover, 
	increasing the chance to have correlated ones.  
	 In such a context, variance of the estimators can lead to arbitrary results whereas interpretation and prediction are both strongly needed, depending on the context. For instance, interpretation could be favoured to improve a situation instead of only predict problems, so robust estimation is a real stake.
		~\\	~\\
		
%bibliographie	
	The Ordinary Least Squares (\textsc{OLS}) estimator is the minimum-variance linear unbiased estimator but it suffers from ill-conditioned matrices inversion when the covariates are highly correlated (section \ref{sectionOLS}). Therefore, some other methods try to improve the robustess of the estimator by introducing some bias, selecting only some covariates, grouping some covariates, {\it etc.}.


%In the following we note classical norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$ and $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^p|\beta_i| $.

	 Penalized methods try to reduce the variance introducing some bias to improve the bias-variance tradeoff and get better prediction.
	Ridge regression \cite{marquardt1975ridge} proposes a biased estimator that can be written in terms of a parametric $L_2$ penalty.
	But this penalty is not guided by the correlations. It is the same for each covariate and will be too large for independent covariates and/or too small for correlated ones. So the efficiency of such a method is limited. 
	Moreover, coefficients tend to 0 but don't reach 0 so it gives difficult interpretations for large number of covariates. The absence of variable selection is not compatible with the need to find small set of relevant covariates to explain the response variable. 
	
	Real datasets often imply many irrelevant variables so variable selection would be favoured when possible to reduce the dimension and obtain models small enough to be understood. Moreover, selection may keep only uncorrelated covariates and thus resolve the correlations problem.	
	The Least Absolute Shrinkage and Selection Operator (\textsc{LASSO} \cite{tibshirani1996regression}) consists in a shrinkage of the regression coefficients based on a parametric $L_1$ penalty to shrink some coefficients exactly to zero.
%	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) algorithm offers a very efficient way to obtain the whole LASSO path and is very attractive. It requires only the same order of magnitude of computational effort as \textsc{OLS} applied to the full set of covariates. 
But like the ridge regression, the penalty does not distinguish correlated and independent covariates so there is no guarantee to have less correlated covariates. It only produces a parsimonious model, that is a gain for interpretation but only half the way.
%	 Another limitation of the \textsc{LASSO} is that it preserves at most $n$ predictors (troublesome when in high dimension). 
	% Some recent variants of the \textsc{LASSO} do exist for the choice of the penalization coefficient like the adaptative \textsc{LASSO} \cite{zou2006adaptive} or the random \textsc{LASSO} \cite{wang2011random}.  But
	 Indeed, \textsc{LASSO} is also known to face consistency problems \cite{Zhao2006MSC} when confronted with correlated covariates. So the quality of interpretation is compromised.
	 \\
	 
	Elastic net \cite{zou2005regularization} is a method developed to be a compromise between Ridge regression and the \textsc{LASSO} by a linear combination of $L_1$ and $L_2$ penalties.
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	%Elastic net can be written:
%	\begin{equation}
%		\boldsymbol{\hat{\beta}}=(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta} \parallel_2^2 \right\rbrace, \textrm{ subject to } (1-\alpha)\parallel\boldsymbol{\beta}\parallel_1+\alpha\parallel\boldsymbol{\beta}\parallel_2^2\leq t \textrm{ for some } t
%	\end{equation}
%	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	%It seems to give good predictions. 
	But it is based on the grouping effect so correlated covariates get similar coefficients and are selected together whereas LASSO will choose between one of them and will then obtain similar predictions with a more parsimonious model. Once again, nothing specifically aims to reduce the correlations. Interpretation will be misleading in both LASSO and Elastic net cases because nothing differentiates correlated and uncorrelated covariates.
	%Hence, when comparing the two models, interpretations are not the same and nothing explicitly explains why. So it can be very confusing. 
	\\
	
	Another way of improving the conditioning and the understandability is to consider clusters of variables with the same coefficients, like the Octogonal Shrinkage and Clustering Algorithm for Regression (\textsc{OSCAR} \cite{bondell2008simultaneous}) to reduce the dimension and also the correlations if correlated covariates are in the same clusters.
	The CLusterwise Effect REgression (\textsc{CLERE} \cite{yengo2012variable}) describes the regression coefficients no longer as fixed effect parameters but as unobserved independent random variables with grouped coefficients following a Gaussian Mixture distribution. The idea is to hope that the model have a small number of groups of covariates and that the mixture will have few enough components to have a number of parameters to estimate significantly lower than the number of covariates. In such a case, it improves interpretability and ability to yeld reliable prediction with a smaller variance on the coefficients estimator. 
	
	But it requires to suppose having many covariates with the same level of effect on the response variable and seems to stay less efficient in prediction than elastic net.
	 Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the regression coefficients and gives a subset of covariates (not grouped) on which to compute \textsc{OLS} but has no specific protection against correlations issues.
	We see that none of the above methods takes explicitly the correlations into account, even if the clustering methods introduce a kind of relationship between the covariates.



	~\\	~\\
% Principe de la méthode

 Instead of reducing the dimension without looking at correlations, Selvarclust \cite{maugis2009variable} proposes to focus on the correlations, giving an explicit structure between covariates by linear sub-regressions between them. But it is made in a clustering context, with only irrelevant covariates being dependent from relevant ones (no sub-regressions between relevant or between irrelevant covariates) and the algorithm used to find the structure is a stepwise-like algorithm without protection against correlations \cite{raftery2006variable} even it is known to be often unstable \cite{miller2002subset}. We propose to develop this method for linear regression with a specifically adapted algorithm to find the structure of sub-regression.
 
  The idea is that if we know explicitly the correlations, we could use this knowledge to avoid the problem. More precisely, we model the correlations with a system of linear sub-regressions for the joint distribution of the covariates to model the structure of the correlations. It helps to define the greatest set of orthogonal covariates to keep the maximum information but with an orthogonality constraint.
Then we can define a marginal model with independent covariates. 
Thus we have an explained variable selection guided by the correlations and it improves interpretation, not only prediction.
 This can be viewed as a pretreatment on the dataset that will be followed then by any other tools for estimation and dimension reduction without suffering from correlations. This pretreatment is specifically done to decorrelate the covariates, unlike methods described above. 
The linear structure is obtained by a MCMC algorithm optimizing the penalized likelihood of the joint distribution on the covariates, independently from the response variable. This algorithm is part of the R package \textsc{CorReg} accessible on \textsc{CRAN}. %We only consider strong correlations (i.e. : problematic ones) thus we keep most of the information contained in the dataset. 
 %We will in a second time be able to use the remaining part of the information (sequential approach).	
 
	
 	
 	%plan
 	This paper will first present the linear modelisation of the correlations and the by-product marginal regression model before describing in Section 3 the random walk used to find the structure.
 	We will then look at some numerical results on simulated (Section \ref{sectionsimul}) and real industrial datasets (Section \ref{sectionrealcase}) before concluding and giving some perspectives in Section \ref{conclusion}.
	
\section{Model to select decorrelated covariates}
\subsection{A classical problem: correlations in regression}\label{sectionOLS}
We note the linear regression model:
\begin{equation}
		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \label{regressionsimple}
	\end{equation}
	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables, $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $p\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{OLS}): %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1} \label{eqOLS}
	\end{equation}
	and without any bias.
	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
Conditionning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects: the dimension $p$ (number of covariates) of the model (the more covariates you have the greater variance you get)
	 and the correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .
	When correlations between covariates are strong, the matrix to invert is ill-conditioned and the variance increases, giving unstable and unusable estimator \cite{hoerl1970ridge}.
	Another problem is that matrix inversion requires $n\geq p$. 	
As a running example, we look at a simple case with $p=5$ variables defined by four independent scaled Gaussian $\mathcal{N}(0,1)$ named $\boldsymbol{x}_1,\boldsymbol{x}_2$ and $\boldsymbol{x}_3=\boldsymbol{x}_1+\boldsymbol{x}_2+\boldsymbol{\varepsilon}_3$ where $\boldsymbol{\varepsilon}_3\sim{\mathcal{N}(\boldsymbol{0},\sigma_3^2\boldsymbol{I}_n)}$. We also define another couple $\boldsymbol{x}_4,\boldsymbol{x}_5$ of covariates that are {\it i.i.d. } with $(\boldsymbol{x}_1,\boldsymbol{x}_2)$ and two {\it scenarii} for $\boldsymbol{Y}$ with $\boldsymbol{\beta}=(1,1,1,1,1)$ and $\sigma_Y \in \{10,20\}$ .
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_3$ gets smaller.
	
	
\subsection{Our proposal: modelisation of the correlations}
We make the hypothesis that $\boldsymbol{X}$ can be described by a partition $\boldsymbol{X}=(\boldsymbol{X}_f,\boldsymbol{X}_r) $ given by an explicit structure $S$ where variables in $\boldsymbol{X}_r$ are endogenous covariates resulting from linear sub-regressions based on $\boldsymbol{X}_f$, the submatrix of mutually independent exogenous covariates.
So we model the correlations by $P(\boldsymbol{X}_r|\boldsymbol{X}_f) $ with $\boldsymbol{X}_f$ orthogonals.
 Then $\boldsymbol{X}_r$ is the $n\times p_r$ submatrix of $0\leq p_r <p$ redundent covariates and $\boldsymbol{X}_f$ the $n\times (p-p_r)$ submatrix of the free (independent) covariates.
 
In our running example, $\boldsymbol{X}_r=\boldsymbol{x}_3$ and $\boldsymbol{X}_f=\{\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_4,\boldsymbol{x}_5 \}$ 
 
In the following, we note $\boldsymbol{X}^j$ the $j^{th}$ column of $\boldsymbol{X}$.
The structure $S$ of $p_r$ regressions within correlated covariates in $\boldsymbol{X}$ is described by:
	\begin{equation}
		\boldsymbol{X}_{r|\boldsymbol{X}_f,S} \textrm{ defined by }\forall \boldsymbol{X}^j \subset \boldsymbol{X}_r: \boldsymbol{X}^j_{|\boldsymbol{X}_f,S}=\boldsymbol{X}_f\boldsymbol{\alpha}_j+\boldsymbol{\varepsilon}_j \textrm{ with } \boldsymbol{\varepsilon}_j \sim\mathcal{N}(\boldsymbol{0},\sigma^2_j\boldsymbol{I}_n) \label{SR}
	\end{equation}
		where $\boldsymbol{\alpha}_j \in \mathcal{R}^{(p-p_r)}$ are the sparse vectors of the regression coefficients between the covariates (each sub-regression freely implies different covariates). In our running example, $p_r=1$ and $\boldsymbol{\alpha}_3=(1,1,0,0)'$
\\
\\


The partition of $\boldsymbol{X}$ implies the uncrossing rule  $\boldsymbol{X}_r \cap \boldsymbol{X}_f$ 
{\it i.e.} endogenous variables don't explain other covariates. This hypothesis ensures that $S$ contains no cycle and is straightforward readable (no need to order the sub-regressions). It is not so restrictive because cyclic structures have no sense and any non-cyclic structure can be associated with a structure that verifies the uncrossing constraint by just successively replacing endogenous covariates by their sub-regression when they are also exogenous in some other sub-regressions.

	
	  We make the choice to distinguish the response variable from the other endogenous variables (that are on the left of a sub-regression). Thus we have one regression on the response variable ($P(\boldsymbol{Y}|\boldsymbol{X}))$ and a system of sub-regressions (without the response variable: $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$). Then we consider correlations between the explicative covariates of the main regression, not between the residuals. We see that the $S$ does not depend on $\boldsymbol{Y}$ so it can be learnt independently, even with a larger dataset (if missing values in $\boldsymbol{Y}$).
	 
%ancienne intro


	
\subsection{A by-product model: marginal regression with decorrelated covariates}
Now we know $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$ by the structure of sub-regressions, we are able to define a marginal regression model $P(\boldsymbol{Y}|\boldsymbol{X}_f,S)$ based on the reduced set of independent covariates $\boldsymbol{X}_f$ without significant information loss. 
 	\\
Using the partition $\boldsymbol{X}=[\boldsymbol{X}_f,\boldsymbol{X}_r]$ we can rewrite (\ref{regressionsimple}):
	\begin{equation}
			\boldsymbol{Y}_{|\boldsymbol{X}_f,\boldsymbol{X}_r,S}=\boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{\varepsilon_Y} \label{MainR}
		\end{equation}
		where $\boldsymbol{\beta}=(\boldsymbol{\beta}_f,\boldsymbol{\beta}_r) \in  \mathcal{R}^p$ is the vector of the regression coefficients associated respectively to $\boldsymbol{X}_f$ and $\boldsymbol{I}_n$ the identity matrix. 
We note that (\ref{SR}) and (\ref{MainR}) give also by simple integration on $\boldsymbol{X}_r$ a marginal regression model on $\boldsymbol{Y}$ {\it depending only on uncorrelated covariates $\boldsymbol{X}_f$}:
\begin{eqnarray}
	\boldsymbol{Y}_{|\boldsymbol{X}_f,S}&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j)+  \sum_{j \in I_r}\beta_{j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{Trueexpl} \\
	&=&\boldsymbol{X}_f\boldsymbol{\beta}_f^*+\boldsymbol{\varepsilon}_Y^*=\boldsymbol{X}\boldsymbol{\beta}^*+\boldsymbol{\varepsilon}_Y^* \textrm{ where }\boldsymbol{\beta}^*=(\boldsymbol{\beta}_f^*,\boldsymbol{\beta}_r^*) \textrm{ and } \boldsymbol{\beta}_r^*=\boldsymbol{0}\label{modexpl}
\end{eqnarray}
We note that it is simply a linear regression on some of the original covariates so we only made a pretreatment on the dataset by selecting $\boldsymbol{X}_f$ because of the correlations given by $S$.  It is a variable pre-selection independent of the response $\boldsymbol{Y}$. \\
Our running example gives here $\boldsymbol{Y}= 2\boldsymbol{x}_1+2\boldsymbol{x}_2+\boldsymbol{x}_4+\boldsymbol{x}_5+\boldsymbol{\varepsilon}_3 +\boldsymbol{\varepsilon}_Y$
\subsection{Strategy of use: pretreatment before classical estimation/selection methods}\label{interpretation}

As a pretreatment, it allows usage of any method in a second time to estimate $\boldsymbol{\beta}^*$, even with variable selection methods like LASSO or stepwise \cite{seber2012linear}.

After selection and estimation we will obtain a model with { \it two steps of variable selection}: the decorrelation step (coerced selection associated to redundant information defined in $S$) and the classical selection step, with different meanings for obtained zeros in $\hat{\boldsymbol{\beta}}^*_f$ (irrelevant covariates) and for $\hat{\boldsymbol{\beta}}^*_r=0$ (redundant information). 
 Thus we are able to distinguish the reasons of selection and consistency issues don't mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.


The explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset and to take better actions. Expert knowledge can even be added to the structure, physical models for example.


The structure obtained gives a system of linear regression that can be viewed as a recursive Simultaneous Equation Model (\textsc{SEM})\cite{davidson1993estimation}. Such a system is easy to interpret but estimation don't take advantage of the explicit structure \cite{TIMM} when it is basic (recursive \textsc{SEM}).
  	Some methods take into account correlations but they only consider covariances between residuals \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) or  endogenous variables like \textsc{SPRING} (Structured selection of Primordial Relationships IN the General linear model \cite{chiquetconf}) without explicit expression of the correlations and thus a smaller interpretation potential.
  	\\ Here we suppose independence between the residuals but in other cases it remains the possibility to use such methods to estimate the $\boldsymbol{\alpha}_j$ and $\sigma_j$.


Moreover, the uncrossing constraint (partition of $\boldsymbol{X}$) guarantee to keep a simple structure easily interpretable (no cycles and no chain-effect) and straightforward readable.

	
			There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypothesis. We play on the bias-variance tradeoff.
			 
	\subsection{Illustration of the tradeoff conveyed by the pretreatment}	
	We compare the OLS estimator on $\boldsymbol{X}$ defined in section \ref{sectionOLS} with the estimator obtained by the pretreatment that is $\boldsymbol{X}_f$ selection.
  
For the marginal regression model defined in (\ref{modexpl})
%	\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}_f}= \boldsymbol{X}_f\boldsymbol{\beta}_f^*+ \boldsymbol{\varepsilon}_Y^*
%	\end{equation}			
%		So 
we have the \textsc{OLS} unbiased estimator of $\boldsymbol{\beta}^*$: 
		\begin{equation}
			\hat{\boldsymbol{\beta}}_f^* = (\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{Y}  \textrm{ and }\boldsymbol{\hat\beta}_r^* = \boldsymbol{0}
		\end{equation}
		We see in (\ref{Trueexpl}) that it gives an unbiased estimation of $\boldsymbol{Y}$ and $\boldsymbol{\beta^*}$
		but in terms of $\boldsymbol{\beta}$ this estimator is biased:
		\begin{equation}
			\operatorname{E}[\hat{\boldsymbol{\beta}}_f^*|\boldsymbol{X}_f]=\boldsymbol{\beta}_f+\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \textrm{ and }\operatorname{E}[\hat{\boldsymbol{\beta}}_r^*|\boldsymbol{X}_f]=\boldsymbol{0}
		\end{equation}
		with variance:
		\begin{equation}
			\operatorname{Var}[\hat{\boldsymbol{\beta}}_f^*|\boldsymbol{X}_f]= (\sigma^2_Y+\sum_{j \in I_r}\sigma^2_{j}\beta_{j}^2 )(\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1}  \textrm{ and }\operatorname{Var}[\hat{\boldsymbol{\beta}}_r^*|\boldsymbol{X}_f]= \boldsymbol{0} 
		\end{equation}
		We see that the variance is reduced compared to OLS described in equation (\ref{eqOLS})(no correlations and smaller matrix give better conditioning ) for small values of $\sigma_j$ $i.e.$ strong correlations. So we play on the bias-variance tradeoff, reducing the variance by adding a bias. 				  
		  
		  
	 The Mean Squared Error (\textsc{MSE}) on $\hat{\boldsymbol{\beta}}$ is:
	\begin{eqnarray}
		\textsc{MSE}(\hat{\boldsymbol{\beta}}|\boldsymbol{X})&=&\parallel \operatorname{Bias}\parallel_2^2+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}})) \\
			\textsc{MSE}(\hat{\boldsymbol{\beta}}_{OLS}|\boldsymbol{X})&=& 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}) %\textrm{ for OLS, and then for the marginal model:}
			 \\
			\textsc{MSE}(\hat{\boldsymbol{\beta}}^*_{OLS}|\boldsymbol{X})&=& \parallel\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \parallel_2^2 +\parallel \boldsymbol{\beta}_r\parallel^2_2 + (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}\beta_{j}^2 ) \operatorname{Tr}((\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1})
	\end{eqnarray}	 
	To better illustrate the bias-variance tradeoff, we look at the running example. We observe the theoretical Mean Squared Error (MSE) of the estimator of both OLS and \textsc{CorReg}'s marginal  model for several values of $\sigma_3$ (strength of the sub-regression) and $n$. Figure \ref{MQE1} shows the theoretical MSE evolution with the strength of the sub-regression:
	\begin{equation}
		1-\mathcal{R}^2=\frac{\operatorname{Var}(\boldsymbol{\varepsilon)_3}}{\operatorname{Var}(\boldsymbol{x}_3)}=\frac{\sigma_3^2}{\sigma_3^2+2}
	\end{equation}
	
\begin{figure}[h!]
%	\begin{minipage}[l]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn15sigmaY10.png} 
%			\caption{For $n=15$. Dotted: \textsc{Correg}, plain: OLS}\label{MQE1}
%	\end{minipage} \hfill
%	\begin{minipage}[c]{.32\linewidth}
%			\includegraphics[ width=170px]{figures/MQEn100sigmaY10.png} 
%			\caption{For $n=100$. Dotted: \textsc{Correg}, plain: OLS}
%	\end{minipage} \hfill
%   \begin{minipage}[r]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn1000sigmaY10.png} 
%			\caption{For $n=1000$. Dotted: \textsc{Correg}, plain: OLS.} \label{MQE3}
%   \end{minipage} 
	\includegraphics[width=500px]{figures/MQEexplOLSp5.png}\label{MQE1}
	\caption{MSE of OLS and CorReg (dotted) estimators for varying $(1-R^2)$ of the sub-regression, $n$ and $\sigma_Y$.}
\end{figure} 
It is clear in Figure \ref{MQE1} that the marginal model is more robust than \textsc{OLS} on $\boldsymbol{X}$. And when sub-regression get weaker ($1-\mathcal{R}^2$ tends to 1) it remains stable until extreme values (sub-regression nearly fully explained by the noise). We also see that the error implied by strong correlations shrinks with the rise of $n$. 
We see that $\sigma_Y$ multiplies $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}))=\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{f}))+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))$ for both models but for the marginal model $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))=0$.
 Thus, when $\sigma_Y^2$ rises it increases the advantage of \textsc{CorReg} versus \textsc{OLS}. It illustrates the importance of dimension reduction when the model has a strong noise (very usual case on real datasets where true model is not even exactly linear). Further results are provided in sections \ref{sectionsimul} and \ref{sectionrealcase}.

	
\section{Sub-regressions model selection}	
Structural equations models like \textsc{SEM} are often used in social sciences and economy where a structure is supposed "by hand" but here we want to find it automatically. Graphical LASSO \cite{friedman2008sparse} offers a method to obtain a structure based on the precision matrix (inverse of the variance-covariance matrix), setting some coefficients of the precision matrix to zero. But the resulting matrix is symmetric and we need an oriented structure for $S$ to avoid cycles.

Cross-validation is very time-consuming and thus not friendly with combinatory problematics. Moreover, we need a criterion compatible with structures of different sizes (varying $p_r$) and not related with $\boldsymbol{Y}$ because the structure is inherent to $\boldsymbol{X}$ only. Thus it must be a global criterion. 	
Because it is about model selection and we are able to provide a full generative model (section \ref{sectionfullgen}), we decide to follow a Bayesian approach (\cite{raftery1995bayesian}, \cite{andrieu1999joint},\cite{chipman2001practical}).  
	
We want to find the most probable structure $S$ knowing the dataset, so we search for the structure that maximizes $P(S|\boldsymbol{X})$ and we have:	
	\begin{eqnarray}
	 \label{approxBIC} P(S|\boldsymbol{X})&\propto & P(\boldsymbol{X}|S)P(S)
	=P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)P(\boldsymbol{X}_f|S)P(S)
	\end{eqnarray}
So we will try to maximize $\psi(\boldsymbol{X},S)=P(\boldsymbol{X}|S)P(S)$.
	

	\subsection{Modeling the uncorrelated covariates: a full generative approach on $P(\boldsymbol{X})$} \label{sectionfullgen}
	To be able to compare structures with $P(S|\boldsymbol{X})$, we need a full generative model on $\boldsymbol{X}$. Sub-regressions give $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S) $ but $P(\boldsymbol{X}_f|S)$ is still undefined. We suppose that variables in $\boldsymbol{X}_f$ follow Gaussian mixtures of $k_j \in \mathbf{N}^*$ components: 
	\begin{equation}
			\forall \boldsymbol{X}^j \notin \boldsymbol{X}_r : \boldsymbol{X}^j_{|S} \sim f(\boldsymbol{\theta}_j)=\mathcal{GM}(\boldsymbol{\pi}_j;\boldsymbol{\mu}_j;\boldsymbol{\sigma}^2_j) \textrm{ with } \boldsymbol{\pi}_j,\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j \textrm{ vectors of size } K_j. \label{mixtureX1}
		\end{equation}
		The great flexibility \cite{mclachlan2004finite} of such models makes our model more robust. Gaussian case is just a special case ($K_j=1$) of Gaussian mixture so it is included in our hypothesis but identifiability of $S$ requires to have at least one Gaussian mixture with at least two distinct components in each sub-regression (derived from the identifiability of the SR model in \cite{maugis2009variable}, more details in the Appendices \ref{preuveident}).
				
		Remark:  Identifiability of $S$ is not necessary to use a given structure but helps to find it.

		Variables in $\boldsymbol{X}_f$ are in the followings supposed to be independent Gaussian mixtures with at least two distinct components each.
	%Thus if one have some hypothesis on the distribution of some variables (exponentially distributed for example) it is possible to use it without impacting the model in other ways. %compute corresponding $\psi$ according to it. %and then improve the walk (will keep a structure only if it is really relevant).%and give it as an input of \textsc{CorReg} and then improve the efficiency of the algorithm (it will find a structure only if it is really relevant).
	We now have a full generative model.
	
	\subsection{Penalization of the integrated likelihood by $P(S)$} \label{compstruct}

  Our full generative generative model allows us to compare structures with criterions like the Bayesian Information Criterion ($BIC$) which penalize the log-likelihood of the joint law on $\boldsymbol{X}$ according to the complexity of the structure~\cite{BIChuard}. 

Uniform law on $P(S)$ gives $\psi(\boldsymbol{X},S)\propto P(\boldsymbol{X}|S)$ so it is equivalent to a minimization of the $BIC$.
	We note $\boldsymbol{\Theta}$ the set of the parameters of the generative model
	\begin{eqnarray}
		-2\log P(\boldsymbol{X}|S)&\approx & BIC=-2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+|\boldsymbol{\Theta}|\log(n)  
	\end{eqnarray}
	But $BIC$ tends to give too complex structures because we test a great range of models. Thus we choose to penalise the complexity a bit more.
	
	We note $I_r$ the set of indices of endogenous variables in $\boldsymbol{X}$ (explained ones).
We also define $I_f=\{I_f^1,\dots,I_f^p \}$ the set of the sets of indices of exogenous covariates (explaining ones $=\boldsymbol{X}_f$) with $\forall j \notin I_r, I_f^j=\emptyset$. 
We see that $I_f$ defines the non-null coefficients in $\boldsymbol{\alpha}_j$ (each sub-regression can be very parsimonious).
Then we have the explicit structure characterized by $S=(I_f,I_r,p_f,p_r)$ where $p_r=|I_r|$, $\boldsymbol{p}_f=(p_f^1,\dots,p_f^{p_r})$ is the vector of the number of covariates in each sub-regression  and $p_f^j=|I_f^j|$, with $|.|$ the cardinal of an ensemble. Our running example is then described by $S= \left( \{ \{1,2\}\},\{3\},(2),(1)\right)$
\\
	 We suppose a hierarchical uniform {\it a priori} distribution $P(S)=P(I_f | \boldsymbol{p}_f,I_r,p_r)P(\boldsymbol{p}_f|I_r,p_r)P(I_r|p_r)P(p_r)$  instead of the simple uniform law on $S$ that is generally used and provides no penalty.
	 Thus we have :
		\begin{eqnarray}
		BIC_+(X|S)&=&BIC(X|S) -\ln(P(S)) \label{Bicstar}
	\end{eqnarray}		
	It increases penalty on complexity for $p_r\leq\frac{p}{2}$ and $p_f^j\leq\frac{p}{2}$ . Hence %when using $BIC*$ 
	this constraint on $\hat{p}_r$ and $\hat{p}_f^j$ is given in the research algorithm when the Hierarchical Uniform hypothesis is made instead of Uniform one in numerical experiments (section \ref{sectionsimul} and \ref{sectionrealcase}).
		$BIC_+$ does not change $BIC$ but only $P(S)$ so the properties of $BIC_+$ are the same as classical $BIC$ but we obtain better results when the constraints on the complexity are verified.  %With the Hierarchical Uniform hypothesis we maximize $\psi(\boldsymbol{X},S)\approx BIC + P(S)$.

%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	


\subsection{MCMC algorithm}
	Now we have a comparison criterion $\psi(\boldsymbol{X},S)$, we define an MCMC algorithm to find the structure (R package \textsc{CorReg} on CRAN). 
	\subsubsection{The neighbourhood}
	Let's define $\mathcal{S}$ the ensemble of feasible structures (those with $I_f\cap I_r=\emptyset$).
	\\
	For each step, starting from $S \in \mathcal{S}$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}_{S,j}&=& \{S \}\cup \{ S^{(i,j)} |1\leq i \leq p, i\neq j  \} \\
		\textrm{where }\ \ j &\sim & \mathcal{U}(\{1,\dots,p\}) 
	\end{eqnarray}	
	With $S^{(i,j)}$ defined by the following algorithm :
	\begin{itemize}
		\item if $i \notin I_f^j$ (add): 
			\begin{itemize}
				\item $I_f^j=I_f^j\cup \{i\}$
				\item $I_f^i=\emptyset$ (explicative variables can't depend on others : column-wise relaxation)
				\item $I_f=I_f \setminus \{j\}$ (dependent variables can't explain others : row-wise relaxation) 
			\end{itemize}			 
		\item else (remove): $I_f^j=I_f^j\setminus \{i\}$
	\end{itemize}
	
	\smallskip
	At every moment, coherence between $I_f$ and others parts of $S$ can be done by $\forall 1\leq j\leq p :  p_f^j=|I_f^j|$, $I_r=\{j |p_f^j>0 \}$, $p_r= |I_r|$ .
		
	\subsubsection{Transition probabilities}
	
	The walk follows a time-homogeneous Markov Chain whose transition matrix $\mathcal{P}$ has $|\mathcal{S}|$ rows and columns (combinatory so we just compute the probabilities when we need them).
	At each step the markov chain moves with probabiliy:
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=&\sum_{j=1}^p \mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S,j}\} }\frac{\exp(-\frac{1}{2}\psi(\boldsymbol{X},\tilde{S}))}{\sum_{S_l\in \mathcal{V}_{S,j}}\exp(-\frac{1}{2}\psi(\boldsymbol{X},S_l))} 
	\end{eqnarray}
	And $\mathcal{S}$ is a finite state space.%la relaxation rend P non symétrique mais ne remets  pas en cause l'homogénéité	
	 
Because the walk follows a regular and thus ergodic markov chain with a finite state space, it has exactly one stationary distribution \cite{grinstead1997introduction} %: $\pi$ and every rows of $\operatorname{lim}_{k\rightarrow \infty}\mathcal{P}^k=W$ equals $\pi$.
%	
%	
%	With $\forall S \in  \mathcal{S}$ :
%	\begin{eqnarray}		
%		0 \leq &\pi (S)& \leq 1 \nonumber \\
%		\sum_{S \in \mathcal{S}}\pi(S) &=&1 \nonumber \\
%		\pi (S) &=&\sum_{\tilde{S}\in \mathcal{S}} \pi(\tilde{S})\mathcal{P}(\tilde{S},S) \\%définition de la lois stationnaire
%	\end{eqnarray}
%		
and the output will be the best structure in terms of $P(S|\boldsymbol{X})$ which weights each candidate. Practically speaking, \textsc{CorReg} returns the best structure seen during the walk.
Numerical results (Section 4) illustrates the efficiency of the walk when the true model really contains a linear structure or no structure at all (Table (\ref{compZvrai})) and when the structure is not linear (Table \ref{compZnonlin})).

 \subsubsection{Initialisation(s)}
 If we have some knowledge about some sub-regressions (physical models for example) we can add them in the found and/or initial structure. So the model is really expert-friendly.
The initial structure can be based on a first warming algorithm taking the correlations into account. Coefficients are randomly placed into $I_f$, weighted by the absolute value of the correlations. We do so in the followings. Then this structure could be for example reduced by the Hadamard product with the binary matrix obtained by Graphical Lasso\cite{friedman2008sparse} that makes selection in the precision matrix but it is time consuming.

	One would rather test multiple short chains than lose time in initialisation or long chains \cite{gilks1996markov}. It also helps to face local extrema. In the followings, the chain wzs launched with twenty initialisations each time.
	
\section{Numerical results on simulated datasets} \label{sectionsimul}


	\subsection{The datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if \textsc{CorReg} 	keeps its promises.
	The \textsc{CorReg} package has been tested on simulated datasets. 
Section \ref{compZ} shows the results obtained in terms of $\hat{S}$. Sections \ref{tableMSEsimtout} and \ref{tableMSEsimgauche} show the results obtained using only \textsc{CorReg}, or \textsc{CorReg} combined with other methods. Tables give both mean and standard deviation of the observed Mean Squared Errors (MSE) on a validation sample of $1 000$ individuals. For each simulation,  $p=40$, $\sigma_Y=10$, $\sigma=0.001$, variables in $\boldsymbol{X}_f$ follow Gaussian mixture models of $\lambda=5$ classes which means follow Poisson's law of parameter $\lambda$ and which standard deviation also is $\lambda$. The $\beta_j$ and the coefficients of the $\boldsymbol{\alpha}_j$ are generated according to the same Poisson law but with a random sign. $S$ only contains binary relationships but \textsc{CorReg} was only constrained to $\max (\hat{p}_f^j)=5$.  
	We used \textsc{Rmixmod} to estimate the densities of each covariate. For each configuration, the walk was launched on $20$ initial structures with a maximum of 9 000 steps each time.
	When $n<p$, a frequently used method is the Moore-Penrose generalized inverse \cite{katsikis2008fast}, thus OLS can obtain some results even with $n<p$ (see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
		\subsection{Finding the structure}
		\subsubsection{How to evaluate found structure?}
			The first criterion is $\psi(\boldsymbol{X},S)$ which is maximized in the MCMC. But in our case, it is estimated by the likelihood (see (\ref{approxBIC}))whose value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $S$ we define some indicators to compare the true model $S$ and the found one $\hat{S}$.
			Global indicators :
			\begin{itemize}
				\item $TL$ (True left) : the number of found dependent variables that really are dependent $TL=|I_r\cap \hat{I}_r|$ 
				\item $WL$ (Wrong left) : the number of found dependent variables that are not dependent $WL=|\hat{I}_r|-TL$
				\item $ML$ (Missing left) : the number of really dependent variables not found $ML=|I_r|-TL$
				\item $\Delta p_r$ : the gap between the number of sub-regression in both model : $\Delta p_r=|I_r|-|\hat{I}_r|$. The sign defines if $\hat{S}$ is too complex or too simple
				\item $\Delta compl$ : the difference in complexity between both model : $\Delta compl=\sum_{j \in p_r}p_f^j-\sum_{j \in \hat{p}_r}\hat{p}_f^j$
			\end{itemize}
		\subsubsection{Results on $S$}	\label{compZ}
In table \ref{compZvrai} we compare found structures in different contexts with both Uniform and Hierarchical Uniform {\it a priori} law on $P(S)$, noted respectively $BIC$ and $BIC_+$. 
We see that usage of $BIC_+$ gives sparser models even with $\max (\hat{p}_f^j)=5$. Moreover, this constraint is not active because observed complexities are smaller (this constraint only serves to accelerate the walk by reducing the dimension of $\mathcal{S}$ because each configuration was computed a hundred times), meaning that the stronger penalty implied by $BIC_+$ really is efficient. The datasets used for $BIC$ and $BIC_+$ are the same to keep the comparison  meaningful. These datasets and $\hat{S}$ are those used for tables 
 \ref{YXlinOLS} to \ref{YX2linstep}.
 
 It is also notable that $BIC_+$ has a greater computational cost than $BIC$. 
	We notice that the MCMC is faster when there are numerous correlations (rejecting more candidates). 
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c}{Configuration}  &  \multicolumn{2}{|c}{Computing Time}  & \multicolumn{3}{|c}{Quality} & \multicolumn{2}{|c|}{Complexity}\\
\hline
$n$ & $p_r$ & $P(S)$ &  Time Mixmod  & Time MCMC  & $TL$ & $WL$ & $ML$ & $\Delta p_r$ & $\Delta compl$ \\
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & $BIC$ &0.4104 & 3.2428 & 0 & 5.43 & 0 & -5.43 & 22.55  \\
& & & (0.0275) & (0.3711) & (0) & (1.9346) & (0) & (1.9346) & (8.0884) \\
 &  &HU &0.4104 & 8.2338 & 0 & 0.53 & 0 & -0.53 & 2.27  \\
& & & (0.0275) & (0.9045) & (0) & (0.7844) & (0) & (0.7844) & (3.3024) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & $BIC$ &0.4182 & 2.7735 & 10.96 & 5.93 & 4.98 & -0.95 & 38.16  \\
& & & (0.0329) & (0.1498) & (1.9844) & (2.0313) & (1.9948) & (0.9987) & (6.499) \\
 &  &HU &0.4182 & 4.1876 & 11.61 & 4.57 & 4.33 & -0.24 & 16.48  \\
& & & (0.0329) & (0.2178) & (1.8743) & (1.9502) & (1.8752) & (0.4948) & (5.4892) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 0.4456 & 2.9154 & 25.23 & 1.92 & 6.5 & 4.58 & 28  \\
& & & (0.0429) & (0.1331) & (1.4761) & (1.0888) & (1.4668) & (0.9866) & (5.0831) \\
 &  & $BIC_+$ & 0.4456 & 4.0233 & 16.96 & 3.04 & 14.77 & 11.73 & 4.35  \\
& & & (0.0429) & (0.1091) & (1.3993) & (1.3993) & (1.4692) & (0.5478) & (5.8833) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & $BIC$ &0.5229 & 4.7068 & 0 & 4.2 & 0 & -4.2 & 13.35  \\
& & & (0.0519) & (0.5865) & (0) & (1.7233) & (0) & (1.7233) & (5.6468) \\
 &  & $BIC_+$ &0.5229 & 10.1198 & 0 & 0.13 & 0 & -0.13 & 0.32  \\
& & & (0.0519) & (0.5541) & (0) & (0.3667) & (0) & (0.3667) & (0.9732) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &0.5205 & 3.3681 & 11.15 & 5.42 & 4.72 & -0.7 & 22.85  \\	
& & & (0.0451) & (0.3123) & (1.93) & (1.9132) & (1.886) & (0.7317) & (5.7742) \\
 &  &HU &0.5205 & 4.909 & 11.42 & 4.59 & 4.45 & -0.14 & 7.55  \\
& & & (0.0451) & (0.4556) & (1.9079) & (1.8968) & (1.8333) & (0.3487) & (4.0611) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ & 0.5833 & 3.2683 & 28.17 & 1.38 & 3.7 & 2.32 & 12.74  \\
& & & (0.0628) & (0.316) & (1.3711) & (0.9077) & (1.3143) & (0.8394) & (4.3359) \\
 &  &HU &0.5833 & 4.3599 & 17.27 & 2.73 & 14.6 & 11.87 & -2.61  \\
& & & (0.0628) & (0.3729) & (1.1708) & (1.1708) & (1.2792) & (0.338) & (4.4854) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & $BIC$ & 0.9623 & 12.9373 & 0 & 2.83 & 0 & -2.83 & 6.23  \\
& & & (0.077) & (1.7778) & (0) & (1.2953) & (0) & (1.2953) & (3.1999) \\
 &  &HU & 0.9623 & 20.9817 & 0 & 0.01 & 0 & -0.01 & 0.02  \\
& & & (0.077) & (1.9421) & (0) & (0.1) & (0) & (0.1) & (0.2) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ & 1.1223 & 6.9647 & 11.67 & 4.8 & 4.25 & -0.55 & 12.58  \\
& & & (0.1122) & (0.5473) & (2.0003) & (2.0646) & (1.956) & (0.7833) & (3.9471) \\
 &  &HU & 1.1223 & 8.8486 & 12.04 & 3.95 & 3.88 & -0.07 & 3.75  \\
& & & (0.1122) & (0.7174) & (1.9223) & (1.9404) & (1.9137) & (0.2564) & (2.2625) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ & 1.4343 & 5.9626 & 30.14 & 0.84 & 1.61 & 0.77 & 6.96  \\
& & & (0.2528) & (0.3136) & (1.3928) & (0.8495) & (1.2941) & (0.7086) & (3.0975) \\
 &  &HU & 1.4343 & 7.3741 & 17.49 & 2.51 & 14.26 & 11.75 & -3.76  \\
& & & (0.2528) & (0.2748) & (1.1849) & (1.1849) & (1.2441) & (0.4794) & (4.4859) \\
\hline
\end{tabular} 
\caption{Results of the Markov chain with  constraint $\hat{p}_f\leq 5$. Mean observed and standard deviation (sd). } \label{compZvrai}
\end{table}


\clearpage
\subsection{Results on prediction}
	\subsubsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	 \label{tableMSEsimtout}	
We first try the method with a response depending on all covariates (\textsc{CorReg} reduces the dimension and can't give the true model if there is a structure). The datasets used here were those from table \ref{compZvrai}.

We observe that \textsc{CorReg} is better than classical methods especially with the Hierarchical Uniform law. When the complexity of the true model is higher than $\frac{p}{2}$ Uniform hypothesis logically is better but (HU) still beats classical methods, so it is robust. We also observe in table \ref{YXlinstep} that simple methods like stepwise (here from the package \textsc{lars}) can give good results in prediction.

When using penalized estimators for selection, a last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}) and is applied for both classical and marginal model.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$& $P(S)$ & indicator &OLS  &    \textsc{CorReg} $\hat S$ + OLS & \textsc{CorReg} $S$ + OLS\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & $BIC$ & MSE (sd) & 262627.57 (732019) & 5928332.4 (49005690.2) & 262627.57 (732019) \\
& & & cpl (sd) & 30 (0) & 29.99 (0.1) & 30 (0) \\
 &  &HU &MSE (sd) & 262627.57 (732019) & 10381962.64 (90962496.9) & 262627.57 (732019) \\
& & & cpl (sd) & 30 (0) & 30 (0) & 30 (0) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & $BIC$ &MSE (sd) & 510747.53 (2287539.8) & 635.42 (335.2) & 610.32 (424.9) \\
& & & cpl (sd) & 30 (0) & 24.11 (1) & 25 (0) \\
 &  &HU &MSE (sd) & 510747.53 (2287539.8) & 603.24 (415.6) & 610.32 (424.9) \\
& & & cpl (sd) & 30 (0) & 24.82 (0.6) & 25 (0) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 178323.95 (1426610.2) & 180.02 (44.9) & 141.03 (27.8) \\
& & & cpl (sd) & 30 (0) & 13.85 (0.9) & 9 (0) \\
 &  & $BIC_+$ & MSE (sd) & 178323.95 (1426610.2) & 330.31 (134.8) & 141.03 (27.8) \\
& & & cpl (sd) & 30 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & $BIC$ &MSE (sd) & 528.08 (228.6) & 886.54 (364.3) & 528.08 (228.6) \\
& & & cpl (sd) & 41 (0) & 36.8 (1.7) & 41 (0) \\
 &  & $BIC_+$ &MSE (sd) & 528.08 (228.6) & 542.29 (235) & 528.08 (228.6) \\
& & & cpl (sd) & 41 (0) & 40.87 (0.4) & 41 (0) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &MSE (sd) & 612.72 (291.5) & 239.17 (89.4) & 200.32 (42.6) \\	
& & & cpl (sd) & 41 (0) & 24.43 (0.7) & 25 (0) \\
 &  &HU &MSE (sd) & 612.72 (291.5) & 207.6 (51.3) & 200.32 (42.6) \\
& & & cpl (sd) & 41 (0) & 24.99 (0.5) & 25 (0) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ &MSE (sd) & 555.44 (262.5) & 128.98 (18.1) & 121.08 (11.9) \\
& & & cpl (sd) & 41 (0) & 11.45 (0.9) & 9 (0) \\
 &  &HU &MSE (sd) & 555.44 (262.5) & 171.9 (31.1) & 121.08 (11.9) \\
& & &cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & $BIC$ & MSE (sd) & 167.71 (20.6) & 323.44 (124.1) & 167.71 (20.6) \\
& & & cpl (sd) & 41 (0) & 38.17 (1.3) & 41 (0) \\
 &  &HU &MSE (sd) & 167.71 (20.6) & 167.98 (20.8) & 167.71 (20.6) \\  
& & & cpl (sd) & 41 (0) & 40.99 (0.1) & 41 (0) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ &  MSE (sd) & 168.68 (22.4) & 158.51 (51.6) & 133.49 (12.2) \\
& & & cpl (sd) & 41 (0) & 24.53 (0.7) & 25 (0) \\
 &  &HU &  MSE (sd) & 168.68 (22.4) & 137.03 (20.3) & 133.49 (12.2) \\
& & & cpl (sd) & 41 (0) & 25.01 (0.4) & 25 (0) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ &  MSE (sd) & 173.25 (22.9) & 112.8 (10.7) & 110.63 (6.9) \\
& & & cpl (sd) & 41 (0) & 10.02 (0.8) & 9 (0) \\
 &  &HU & MSE (sd) & 173.25 (22.9) & 127.4 (11.9) & 110.63 (6.9) \\
& & & cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\end{tabular} 
\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator &LASSO  &    \textsc{CorReg} $\hat S$ + LASSO& \textsc{CorReg} $S$ + LASSO\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & $BIC$ &MSE (sd) & 1246.35 (350.5) & 1433.98 (526.7) & 1246.35 (350.5) \\
& & & cpl (sd) & 17.84 (5.5) & 15.79 (5.6) & 17.84 (5.5) \\
 &  &HU &MSE (sd) & 1246.35 (350.5) & 1248.84 (341.4) & 1246.35 (350.5) \\
& & & cpl (sd) & 17.84 (5.5) & 17.63 (5.6) & 17.84 (5.5) \\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 &16& U & MSE (sd) & 712.79 (405.2) & 566.32 (226.4) & 554.01 (257.1) \\
& & & cpl (sd) & 16.68 (4.4) & 15.35 (4.3) & 16.14 (4.5) \\
 &  & $BIC_+$ & MSE (sd) & 712.79 (405.2) & 551.92 (242.4) & 554.01 (257.1) \\
& & & cpl (sd) & 16.68 (4.4) & 15.96 (4.4) & 16.14 (4.5) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 216.12 (128.3) & 157.78 (39.4) & 147.04 (28.4) \\
& & & cpl (sd) & 11.23 (4.4) & 8.56 (1.8) & 7.91 (1.3) \\
 &  & $BIC_+$ & MSE (sd) & 216.12 (128.3) & 178.88 (68.9) & 147.04 (28.4) \\
& & & cpl (sd) & 11.23 (4.4) & 9.86 (3) & 7.91 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & $BIC$ &MSE (sd) & 658.38 (221.4) & 872.32 (239.4) & 658.38 (221.4) \\
& & & cpl (sd) & 28.2 (6) & 22.95 (5.8) & 28.2 (6) \\
 &  & $BIC_+$ &MSE (sd) & 658.38 (221.4) & 652.72 (211.9) & 658.38 (221.4) \\
& & & cpl (sd) & 28.2 (6) & 28.36 (5.9) & 28.2 (6) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &MSE (sd) & 274.34 (101.7) & 268.76 (106.3) & 225.64 (67.5) \\	
& & & cpl (sd) & 22.14 (4.1) & 19.52 (2.9) & 20.47 (2.8) \\
 &  &HU &MSE (sd) & 274.34 (101.7) & 233.8 (78.9) & 225.64 (67.5) \\
& & & cpl (sd) & 22.14 (4.1) & 20.19 (2.8) & 20.47 (2.8) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ &MSE (sd) & 165.6 (73.7) & 128.34 (18.8) & 123.73 (12.8) \\
& & & cpl (sd) & 12.93 (6.4) & 8.51 (1.5) & 8.09 (1.1) \\
 &  &HU &MSE (sd) & 165.6 (73.7) & 135.84 (24.1) & 123.73 (12.8) \\
& & &cpl (sd) & 12.93 (6.4) & 10.17 (3.1) & 8.09 (1.1) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & $BIC$ &  MSE (sd) & 183.33 (31.1) & 357.15 (133.4) & 183.33 (31.1) \\
& & & cpl (sd) & 37.78 (2.4) & 32.93 (3.7) & 37.78 (2.4) \\
 &  &HU &  MSE (sd) & 183.33 (31.1) & 183.78 (31.6) & 183.33 (31.1) \\
& & &  cpl (sd) & 37.78 (2.4) & 37.75 (2.4) & 37.78 (2.4) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ & MSE (sd) & 148.83 (18.6) & 164.44 (54.3) & 139.57 (16.1) \\
& & & cpl (sd) & 25.22 (4) & 21.72 (2) & 22.3 (1.7) \\
 &  &HU &  MSE (sd) & 148.83 (18.6) & 142.97 (22.1) & 139.57 (16.1) \\
& & & cpl (sd) & 25.22 (4) & 22.24 (1.8) & 22.3 (1.7) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ & MSE (sd) & 124.29 (19.9) & 113.36 (11) & 111.54 (7.2) \\
& & & cpl (sd) & 13.18 (5.9) & 8.68 (1.2) & 8.53 (1) \\
 &  &HU & MSE (sd) & 124.29 (19.9) & 118.33 (12) & 111.54 (7.2) \\
& & & cpl (sd) & 13.18 (5.9) & 11.02 (2.9) & 8.53 (1) \\
\hline
\end{tabular} 
\caption{LASSO (with LAR) combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. LASSO is better than OLS but is improved by \textsc{CorReg} even for large values of $n$.}\label{YXlinLASSO}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$&indicator &Elasticnet  &    \textsc{CorReg} $\hat S$ + Elasticnet& \textsc{CorReg} $S$ + Elasticnet\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & $BIC$ &MSE (sd) & 1 326.54 (388.4) & 1 356.98 (331.4) & 1 326.54 (388.4) \\
& & & cpl (sd) & 12.14 (5.1) & 12.5 (5.2) & 12.14 (5.1) \\
 &  &HU & MSE (sd) & 1326.54 (388.4) & 1307.96 (356.6) & 1326.54 (388.4) \\
& & & cpl (sd) & 12.14 (5.1) & 12.27 (5.1) & 12.14 (5.1) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & $BIC$ &MSE (sd) & 1 400.56 (1598.2) & 668.57 (274.6) & 653.59 (283.7) \\
& & & cpl (sd) & 13.86 (6.8) & 14.31 (5.7) & 14.83 (5.3) \\
 &  &HU &MSE (sd) & 1 400.56 (1598.2) & 643.25 (277.9) & 653.59 (283.7) \\
& & & cpl (sd) & 13.86 (6.8) & 14.83 (5.6) & 14.83 (5.3) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 855.74 (582.6) & 181.08 (51.9) & 146.79 (32.7) \\
& & & cpl (sd) & 15.57 (6.3) & 11.19 (2.4) & 8.37 (1.3) \\
 &  & $BIC_+$ & MSE (sd) & 855.74 (582.6) & 311.57 (163.9) & 146.79 (32.7) \\
& & & cpl (sd) & 15.57 (6.3) & 14.87 (3.9) & 8.37 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & $BIC$ &MSE (sd) & 738.94 (254.5) & 914.56 (283.7) & 738.94 (254.5) \\
& & & cpl (sd) & 25.85 (8.7) & 20.97 (7.8) & 25.85 (8.7) \\
 &  & $BIC_+$ &MSE (sd) & 738.94 (254.5) & 751.06 (262.2) & 738.94 (254.5) \\
& & & cpl (sd) & 25.85 (8.7) & 25.43 (8.8) & 25.85 (8.7) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &	MSE (sd) & 516.36 (226.6) & 276.6 (133.6) & 228.85 (89.3) \\
& & & cpl (sd) & 26.72 (7.4) & 20.49 (3.8) & 21.8 (3.3) \\
 &  &HU &MSE (sd) & 516.36 (226.6) & 239.77 (103.7) & 228.85 (89.3) \\
& & & cpl (sd) & 26.72 (7.4) & 21.57 (3.4) & 21.8 (3.3) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ &MSE (sd) & 328.7 (146.3) & 130.61 (20.6) & 124.01 (15.4) \\
& & & cpl (sd) & 24.7 (7.1) & 9.95 (1.5) & 8.22 (1.2) \\
 &  &HU &MSE (sd) & 328.7 (146.3) & 169.13 (40.2) & 124.01 (15.4) \\
& & &cpl (sd) & 24.7 (7.1) & 16.04 (3.8) & 8.22 (1.2) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & $BIC$ & MSE (sd) & 175.93 (25.9) & 355.79 (143.8) & 175.93 (25.9) \\
& & & cpl (sd) & 39.71 (1.7) & 34.11 (4.6) & 39.71 (1.7) \\
 &  &HU &  MSE (sd) & 175.93 (25.9) & 176.2 (26.3) & 175.93 (25.9) \\
& & &  cpl (sd) & 39.71 (1.7) & 39.69 (1.7) & 39.71 (1.7) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ &  MSE (sd) & 173.65 (25.1) & 164.96 (59) & 139.51 (17.6) \\
& & &  cpl (sd) & 35.54 (4.2) & 22.58 (2.2) & 23.26 (1.9) \\
 &  &HU &  MSE (sd) & 173.65 (25.1) & 143.04 (23.2) & 139.51 (17.6) \\
& & & cpl (sd) & 35.54 (4.2) & 23.16 (1.9) & 23.26 (1.9) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ &  MSE (sd) & 161.92 (25.2) & 113.61 (10.9) & 111.46 (7.2) \\
& & &  cpl (sd) & 30.78 (6.4) & 9.25 (1.2) & 8.67 (1) \\
 &  &HU &  MSE (sd) & 161.92 (25.2) & 127.44 (13.3) & 111.46 (7.2) \\
& & &cpl (sd) & 30.78 (6.4) & 17.85 (2.7) & 8.67 (1) \\
\hline
\end{tabular} 
\caption{Elasticnet combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. LASSO was better.}\label{YXlinenet}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator &Stepwise  &    \textsc{CorReg} $\hat S$ + Stepwise& \textsc{CorReg} $S$ + Stepwise\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & $BIC$ & MSE (sd) & 1 919.43 (861.8) & 2 014.13 (683.6) & 1 919.43 (861.8) \\
& & &cpl (sd) & 22.8 (3.3) & 19.34 (4.9) & 22.8 (3.3) \\
 &  &HU &MSE (sd) & 1 919.43 (861.8) & 1 885.39 (814) & 1 919.43 (861.8) \\
& & & cpl (sd) & 22.8 (3.3) & 22.73 (3.1) & 22.8 (3.3) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & $BIC$ &MSE (sd) & 696.46 (492.9) & 662.07 (285.9) & 660.93 (354) \\
& & & cpl (sd) & 15.9 (3.8) & 15.1 (3.7) & 15.76 (3.7) \\
 &  &HU &MSE (sd) & 696.46 (492.9) & 655.94 (325.7) & 660.93 (354) \\
& & & cpl (sd) & 15.9 (3.8) & 15.72 (3.6) & 15.76 (3.7) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 394.93 (356) & 155.18 (36.4) & 147.81 (30.3) \\
& & &cpl (sd) & 16.1 (6.3) & 8.26 (1.7) & 7.77 (1.3) \\
 &  & $BIC_+$ &  MSE (sd) & 394.93 (356) & 189.49 (79) & 147.81 (30.3) \\
& & & cpl (sd) & 16.1 (6.3) & 10.23 (3) & 7.77 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & $BIC$ &MSE (sd) & 745.88 (241.7) & 962.12 (319.3) & 745.88 (241.7) \\
& & & cpl (sd) & 27.19 (4.6) & 22.9 (4.5) & 27.19 (4.6) \\
 &  & $BIC_+$ &MSE (sd) & 745.88 (241.7) & 749.08 (244.2) & 745.88 (241.7) \\
& & & cpl (sd) & 27.19 (4.6) & 27.21 (4.5) & 27.19 (4.6) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &	MSE (sd) & 279.82 (151.1) & 290.13 (117.3) & 239.61 (69.5) \\
& & & cpl (sd) & 21.24 (5.2) & 18.13 (2.8) & 19.23 (2.7) \\
 &  &HU &MSE (sd) & 279.82 (151.1) & 252.35 (108.5) & 239.61 (69.5) \\
& & & cpl (sd) & 21.24 (5.2) & 18.97 (2.7) & 19.23 (2.7) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ &MSE (sd) & 199.3 (158.7) & 126.81 (17.4) & 124.19 (13) \\
& & & cpl (sd) & 14.31 (6.2) & 8.16 (1.2) & 8.04 (1) \\
 &  &HU &MSE (sd) & 199.3 (158.7) & 136.32 (25.2) & 124.19 (13) \\
& & &cpl (sd) & 14.31 (6.2) & 9.59 (2.6) & 8.04 (1) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & $BIC$ &   MSE (sd) & 193.26 (34.9) & 387.18 (151.6) & 193.26 (34.9) \\
& & & cpl (sd) & 36.3 (2.5) & 30.94 (4.2) & 36.3 (2.5) \\
 &  &HU &   MSE (sd) & 193.26 (34.9) & 194.43 (37.3) & 193.26 (34.9) \\
& & &  cpl (sd) & 36.3 (2.5) & 36.22 (2.7) & 36.3 (2.5) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ &  MSE (sd) & 145.46 (20.1) & 165.74 (56.1) & 140.27 (17.5) \\
& & & cpl (sd) & 23.44 (3.8) & 21.23 (2.1) & 21.93 (1.7) \\
 &  &HU &  MSE (sd) & 145.46 (20.1) & 143.74 (23.2) & 140.27 (17.5) \\
& & & cpl (sd) & 23.44 (3.8) & 21.86 (1.7) & 21.93 (1.7) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ & MSE (sd) & 128.44 (18.4) & 113.27 (10.7) & 111.77 (7.2) \\
& & &  cpl (sd) & 13.58 (5.2) & 8.59 (1.1) & 8.48 (1) \\
 &  &HU &  MSE (sd) & 128.44 (18.4) & 118.61 (11.5) & 111.77 (7.2) \\
& & & cpl (sd) & 13.58 (5.2) & 10.75 (2.8) & 8.48 (1) \\
\hline
\end{tabular} 
\caption{Stepwise  combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins but stepwise is quite good compared to elasticnet.}\label{YXlinstep}
\end{table}



\clearpage
	\subsubsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $\boldsymbol{X}_r$. The datasets used here were still those from \ref{compZvrai}.
Depending only on $\boldsymbol{X}_r$ imply sparsity and impossibility to obtain the true model when using the true structure. \textsc{CorReg} is still better than classical methods. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&$P(S)$ &indicator &OLS  &    \textsc{CorReg} $\hat S$ + OLS& \textsc{CorReg} $S$ + OLS\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & $BIC$ &MSE (sd) & 81781.54 (247281.7) & 507.1 (269.2) & 593.03 (428.3) \\
& & & cpl (sd) & 30 (0) & 24.11 (1) & 25 (0) \\
 &  &HU &MSE (sd) & 81781.54 (247281.7) & 587.07 (425.6) & 593.03 (428.3) \\
& & & cpl (sd) & 30 (0) & 24.82 (0.6) & 25 (0) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 74136.81 (297716.1) & 189.45 (48.1) & 144.51 (31.4) \\
& & & cpl (sd) & 30 (0) & 13.85 (0.9) & 9 (0) \\
 &  & $BIC_+$ & MSE (sd) & 74136.81 (297716.1) & 340.53 (169.4) & 144.51 (31.4) \\
& & & cpl (sd) & 30 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &	MSE (sd) & 684.04 (438.1) & 190.97 (37.7) & 197.32 (40) \\
& & & cpl (sd) & 41 (0) & 24.43 (0.7) & 25 (0) \\
 &  &HU &MSE (sd) & 684.04 (438.1) & 196.36 (40.5) & 197.32 (40) \\
& & & cpl (sd) & 41 (0) & 24.99 (0.5) & 25 (0) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ &MSE (sd) & 596.32 (323) & 126.94 (15.5) & 119.39 (12.4) \\
& & & cpl (sd) & 41 (0) & 11.45 (0.9) & 9 (0) \\
 &  &HU &MSE (sd) & 596.32 (323) & 168.03 (27.7) & 119.39 (12.4) \\
& & &cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ &  MSE (sd) & 168.35 (20.1) & 133.75 (12.4) & 135.08 (13.1) \\
& & & cpl (sd) & 41 (0) & 24.53 (0.7) & 25 (0) \\
 &  &HU &  MSE (sd) & 168.35 (20.1) & 135.04 (13.1) & 135.08 (13.1) \\
& & & cpl (sd) & 41 (0) & 25.01 (0.4) & 25 (0) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ &  MSE (sd) & 168.07 (21.4) & 109.61 (7.4) & 108.64 (7.2) \\
& & &  cpl (sd) & 41 (0) & 10.02 (0.8) & 9 (0) \\
 &  &HU & MSE (sd) & 168.07 (21.4) & 124.23 (11.1) & 108.64 (7.2) \\
& & & cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\end{tabular} 
\caption{OLS and OLS combined with constrained \textsc{CorReg}.$\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X_r}$. Sometimes $\hat{S}$ gives better results than $S$ because $S$ is penalized by the fact that it relies only on covariates not in the true model. } \label{YX2linOLS}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator &LAR  &    \textsc{CorReg} $\hat S$ + LAR& \textsc{CorReg} $S$ + LAR\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & $BIC$ &MSE (sd) & 367.45 (198.4) & 298.32 (122.4) & 292.53 (102.2) \\
& & & cpl (sd) & 14.37 (5.1) & 12.9 (3.8) & 12.84 (3.8) \\
 &  &HU &MSE (sd) & 367.45 (198.4) & 298.1 (119.1) & 292.53 (102.2) \\
& & & cpl (sd) & 14.37 (5.1) & 12.81 (3.8) & 12.84 (3.8) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 273.12 (267.2) & 166.93 (49) & 152.45 (39.5) \\
& & & cpl (sd) & 11.7 (5.8) & 8.7 (2.5) & 7.45 (1.4) \\
 &  & $BIC_+$ & MSE (sd) & 273.12 (267.2) & 175.68 (57.5) & 152.45 (39.5) \\
& & & cpl (sd) & 11.7 (5.8) & 9.19 (2.9) & 7.45 (1.4) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &	MSE (sd) & 189.64 (52.5) & 171.97 (45.9) & 176.58 (46.8) \\
& & & cpl (sd) & 14.15 (3.9) & 13.4 (2.8) & 13.5 (3.1) \\
 &  &HU &MSE (sd) & 189.64 (52.5) & 174.36 (45.3) & 176.58 (46.8) \\
& & & cpl (sd) & 14.15 (3.9) & 13.57 (3) & 13.5 (3.1) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ &MSE (sd) & 163.88 (67.4) & 124.53 (17) & 121.99 (16.6) \\
& & & cpl (sd) & 13.06 (7.2) & 8.29 (1.6) & 7.73 (1.2) \\
 &  &HU &MSE (sd) & 163.88 (67.4) & 133.7 (26.4) & 121.99 (16.6) \\
& & &cpl (sd) & 13.06 (7.2) & 9.65 (3.3) & 7.73 (1.2) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ & MSE (sd) & 129.65 (14.8) & 127.11 (12.1) & 127.46 (12.2) \\
& & &  cpl (sd) & 14.86 (3.1) & 13.97 (2.5) & 14.04 (2.5) \\
 &  &HU &  MSE (sd) & 129.65 (14.8) & 127.46 (12.2) & 127.46 (12.2) \\
& & & cpl (sd) & 14.86 (3.1) & 14.02 (2.5) & 14.04 (2.5) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ &  MSE (sd) & 121.21 (20.1) & 109.66 (7.9) & 109.04 (7.8) \\
& & & cpl (sd) & 12.84 (5.2) & 8.42 (1.1) & 8.11 (0.8) \\
 &  &HU &  MSE (sd) & 121.21 (20.1) & 112.56 (11.2) & 109.04 (7.8) \\
& & & cpl (sd) & 12.84 (5.2) & 9.77 (2.3) & 8.11 (0.8) \\
\hline
\end{tabular} 
\caption{LASSO (with LAR) combined with constrained \textsc{CorReg}.$\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X_r}$. }\label{YX2linLASSO}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator & Elasticnet  &    \textsc{CorReg} $\hat S$ + Elasticnet& \textsc{CorReg} $S$ + Elasticnet\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & $BIC$ &MSE (sd) & 499.32 (218.8) & 311.41 (137.4) & 305.09 (123.5) \\
& & & cpl (sd) & 14.08 (6.1) & 11.75 (4.8) & 11.49 (4.7) \\
 &  &HU &MSE (sd) & 499.32 (218.8) & 307.6 (128.7) & 305.09 (123.5) \\
& & & cpl (sd) & 14.08 (6.1) & 11.63 (4.7) & 11.49 (4.7) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 691.55 (503.2) & 193.45 (54.2) & 150.17 (35) \\
& & & cpl (sd) & 12.54 (7.2) & 10.26 (2.7) & 7.81 (1.4) \\
 &  & $BIC_+$ & MSE (sd) & 691.55 (503.2) & 279.21 (117.3) & 150.17 (35) \\
& & & cpl (sd) & 12.54 (7.2) & 13 (4.3) & 7.81 (1.4) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &	MSE (sd) & 282.05 (134.8) & 180.64 (47.6) & 181.49 (48.6) \\
& & & cpl (sd) & 20.37 (6.3) & 13.68 (4) & 14.05 (4) \\
 &  &HU &MSE (sd) & 282.05 (134.8) & 180.84 (47.5) & 181.49 (48.6) \\
& & & cpl (sd) & 20.37 (6.3) & 14.03 (4) & 14.05 (4) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ &MSE (sd) & 308.13 (142.9) & 129.39 (19.6) & 122.71 (17.8) \\
& & & cpl (sd) & 22.94 (7.7) & 9.37 (2.1) & 7.91 (1.4) \\
 &  &HU &MSE (sd) & 308.13 (142.9) & 159.14 (30.4) & 122.71 (17.8) \\
& & &cpl (sd) & 22.94 (7.7) & 15.42 (4) & 7.91 (1.4) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ & MSE (sd) & 150.14 (19.7) & 127.48 (14) & 128.21 (15) \\
& & &  cpl (sd) & 25.6 (5.4) & 14.73 (3.8) & 14.73 (3.9) \\
 &  &HU &  MSE (sd) & 150.14 (19.7) & 128.21 (14.9) & 128.21 (15) \\
& & & cpl (sd) & 25.6 (5.4) & 14.7 (3.9) & 14.73 (3.9) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ & MSE (sd) & 157.18 (23.5) & 110.24 (8) & 109.24 (7.8) \\
& & &  cpl (sd) & 30.41 (6.7) & 8.96 (1.3) & 8.3 (1) \\
 &  &HU &  MSE (sd) & 157.18 (23.5) & 123.8 (12.5) & 109.24 (7.8) \\
& & & cpl (sd) & 30.41 (6.7) & 17.18 (3.3) & 8.3 (1) \\
\hline
\end{tabular} 
\caption{Elasticnet (with Elasticnet) combined with constrained \textsc{CorReg}.$\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X_2}$.}\label{YX2linenet}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator &Stepwise  &    \textsc{CorReg} $\hat S$ + stepwise & \textsc{CorReg} $S$ + stepwise \\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & $BIC$ &MSE (sd) & 394.59 (454.6) & 341.47 (153.2) & 351.14 (148) \\
& & & cpl (sd) & 13.09 (3.2) & 12.9 (2.9) & 12.92 (3) \\
 &  &HU &MSE (sd) & 394.59 (454.6) & 353.99 (157) & 351.14 (148) \\
& & & cpl (sd) & 13.09 (3.2) & 12.83 (2.9) & 12.92 (3) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 410.11 (343.5) & 163.05 (45.4) & 152.08 (38.1) \\
& & & cpl (sd) & 15.8 (6.5) & 8.29 (2.2) & 7.34 (1.3) \\
 &  & $BIC_+$ & MSE (sd) & 410.11 (343.5) & 178.94 (69.7) & 152.08 (38.1) \\
& & & cpl (sd) & 15.8 (6.5) & 8.89 (3) & 7.34 (1.3) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & $BIC$ &	MSE (sd) & 180.67 (52.7) & 173.02 (38.3) & 174.84 (36.8) \\
& & & cpl (sd) & 13.56 (3.7) & 12.78 (2.4) & 13.07 (2.4) \\
 &  &HU &MSE (sd) & 180.67 (52.7) & 175.3 (38.1) & 174.84 (36.8) \\
& & & cpl (sd) & 13.56 (3.7) & 13.01 (2.4) & 13.07 (2.4) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & $BIC$ &MSE (sd) & 188.91 (88.5) & 124.38 (17.4) & 122.38 (16.7) \\
& & & cpl (sd) & 14.74 (6.6) & 8.07 (1.6) & 7.65 (1.2) \\
 &  &HU &MSE (sd) & 188.91 (88.5) & 134.22 (23.6) & 122.38 (16.7) \\
& & &cpl (sd) & 14.74 (6.6) & 9.5 (2.8) & 7.65 (1.2) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & $BIC$ &  MSE (sd) & 128.18 (13.7) & 127.43 (13.7) & 127.71 (13.3) \\
& & &  cpl (sd) & 13.78 (2.8) & 13.51 (2) & 13.59 (2.1) \\
 &  &HU & MSE (sd) & 128.18 (13.7) & 127.71 (13.3) & 127.71 (13.3) \\
& & & cpl (sd) & 13.78 (2.8) & 13.59 (2.1) & 13.59 (2.1) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & $BIC$ & MSE (sd) & 126.15 (20.6) & 109.47 (8) & 109.04 (7.8) \\
& & & cpl (sd) & 13.25 (4.4) & 8.3 (1) & 8.12 (0.8) \\
 &  &HU &  MSE (sd) & 126.15 (20.6) & 113.96 (11.4) & 109.04 (7.8) \\
& & & cpl (sd) & 13.25 (4.4) & 10.08 (2.6) & 8.12 (0.8) \\
\hline
\end{tabular} 
\caption{Stepwise  combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X_r}$.}\label{YX2linstep}
\end{table}

\clearpage
\subsection{Robustess of the model}
 We have generated non linear structures (Tables \ref{compZnonlin} and \ref{resYnonlin}). Variables in $\boldsymbol{X_r}$ depends on the $\log$ or the square (randomly choosen with equiprobability) of a variable in $\boldsymbol{X_f}$ . Dependencies are still real but non linear. \textsc{CorReg} still found dependencies. One example of non-linear structure found with \textsc{CorReg} on real datasets is given in section \ref{sectionexfos}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c}{configuration}  &  \multicolumn{2}{|c}{Computing Time}  & \multicolumn{3}{|c}{Quality} & \multicolumn{2}{|c|}{Complexity}\\
\hline
$n$ & $p_r$ & $P(S)$&Time Mixmod  & Time MCMC  & $TL$ & $WL$ & $ML$ & $\Delta p_2$ & $\Delta compl$ \\
\hline
30 & 16 & $BIC$ & 0.4313 & 2.7769 & 9.96 & 2.45 & 5.95 & 3.5 & 29.42  \\
& & &(0.0391) & (0.0883) & (1.3175) & (1.3808) & (1.3056) & (1.453) & (6.2639) \\
 &  & $BIC_+$ & 0.4313 & 4.9079 & 7.28 & 1.25 & 8.63 & 7.38 & 5.77  \\ 
& & &(0.0391) & (0.4353) & (1.5769) & (0.9987) & (1.5548) & (1.6316) & (6.1297) \\ 
\hline
\end{tabular} 
\caption{Results of the Markov chain for non linear structure ($\log$ and square). Mean observed and standard deviation (sd). } \label{compZnonlin}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$& $P(S)$ &indicator &LASSO  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %nonlin0.5global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_55_33.csv
30 & 16 & $BIC$ &MSE (sd) & 996.06 (323.7) & 1100.17 (391.4) & 1501.25 (905.9) \\
& & &cpl (sd) & 17.66 (5.4) & 14.7 (4.7) & 11.97 (4.9) \\
 &  &HU &MSE (sd) & 996.06 (323.7) & 1094.5 (621.8) & 1501.25 (905.9) \\
& & & cpl (sd) & 17.66 (5.4) & 16.98 (4.8) & 11.97 (4.9) \\ 
\hline
\end{tabular} 
\caption{ $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} is not too far from LASSO even if it stays behind. } \label{resYnonlin}
\end{table}


	\clearpage	
\section{Numerical results on real datasets} \label{sectionrealcase}
\subsection{Quality case study} \label{sectionexfos}
This work takes place in steel industry context, with quality oriented objective : to understand and prevent quality problems on finished product, knowing the whole process. The correlations are strong here (many parameters of the whole process without any a priori and highly correlated because of physical laws, process rules, {\it etc.}). 
		
We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item 205 variables from the whole process to explain it.
			\item The stakes : a hundred euros per ton (for information: Dunkerque's  site aims to produce up to 7.5 millions tons a year)
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/mixmod.png} 
			\caption{Example of non-Gaussian real variable easily modeled by a Gaussian mixture}\label{graphMixmod}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/histR2exfos.png} 
			\caption{$R^2_{adj}$ of the 76 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/correlexfoshist.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEexfos}
   \end{minipage}
\end{figure}   			
	We get a training set of $n=3 000$ products described by $p=205$ variables from the industrial process and a validation sample of $847$ products.
	Let's note $\rho$ the absolute value of correlations between two covariates. Industrial variables are naturally highly correlated as the width and the weight of a steel slab ($\rho=0.905$), the temperature before and after some tool ($\rho=0.983$), the  roughness of both faces of the product ($\rho= 0.919$), a mean and a max ($\rho=0.911$). \textsc{CorReg} also found more complex structures describing physical models, like   Width = f (Mean.flow , Mean.speed.CC) even if the true Physcial model is not linear : Width = flow / (speed * thickness) (here thickness is constant). Non linear regulation models used to optimize the process were also found (but are confidential). These first results are easily understandable and meet metallurgists expertise.  
			The algorithm gives a structure of $p_r=76$ subregressions with a mean of $\bar{\boldsymbol{p}}_f=5.17$ regressors. In $\boldsymbol{X}_f$ the number of $\rho>0.7$ is $\textbf{79.33\%}$ smaller than in $\boldsymbol{X}$.		
	
			It is now time to look at the predictive results (Figure \ref{compareMSEexfos}).
				The best model found when not using \textsc{CorReg} is given by the LASSO. But when using \textsc{CorReg} elasticnet produces a better model in terms of prediction. LASSO gives a model with 21 non-zero coefficients and elasticnet with \textsc{CorReg} gives a model with 40 non-zero parameters but $6.40\%$ better in prediction on the validation sample (847 products). $14$ non-zero coefficients are common between the two models.
				Elasticnet alone get a model with 78 parameters that is improved by $9.75\%$ in prediction when used with \textsc{CorReg}. When using LASSO with \textsc{CorReg} we obtain a model with 24 non-zero coefficients that is $4.11\%$ better than LASSO alone. We also computed the OLS model (without selection) and the naive one (estimating the response by the mean of the learning set). All the MSE were modified here to obtain a value of 100 for the best (to preserve confidentiality). Elasticnet with \textsc{CorReg} is $13.51\%$ better than OLS.
		\begin{figure}[h]
			\centering
				\label{barplotMSEexfos}
				\includegraphics[width=400px]{figures/MSEfinal.png}
			\caption{MSE comparison on industrial dataset. Learning set : 3 000 products, validation set : 847 products}
		\end{figure}		
		\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	OLS & 115.63 & 206 \\ 
	\hline 
	\textsc{CorReg} + OLS & 109.59&130 \\ 
	\hline  
	LASSO & 106.84 & 21 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 102.45 &24 \\ 
	\hline 
	elasticnet & 110.81 & 78\\ 
	\hline 
	\textsc{CorReg} + elasticnet & 100 &40 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained on a validation sample.}	
\end{table}

		In terms of interpretation, the main regression comes with the family of regression so it gives a better understanding of the consequences of corrective actions on the whole process. It typically permits to determine the \textit{tuning parameters} whereas LASSO would point variables we can't directly act on.	So it becomes easier to take corrective actions on the process to reach the goal. The stakes are so important that even a little improvement leads to consequent benefits, and we don't even talk of the impact on the market shares that is even more important.
		\clearpage
		
		\subsection{Production case study}
This second example is about a phenomenon that impacts the productivity of a steel plan.
We have :
		\begin{itemize}
			\item a (confidential)  response variable,
			\item $p=145$ variables from the whole process to explain it but only $n=100$ individuals.
			\item The stakes : $20\%$ of productivity to gain
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[width=150px]{figures/correlbeforeafter.png} 
			\caption{Correlations between the covariates in $\boldsymbol{X}$ (upper) and $\hat{\boldsymbol{X}}_f$ (lower).}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/R2corregBVBI.png} 
			\caption{$R^2_{adj}$ of the 67 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/histcorrelBVBI.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} 
   \end{minipage}
\end{figure}   			
	Here $n<p$ so we only compare the leave-one-out cross-validation MSE.
	\textsc{CorReg} improves LASSO by $5.24\%$ and elasticnet by $8.60\%$. \textsc{CorReg} combined with LASSO gives the best result but it is only a leave-on-out MSE.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	LASSO & 105.54 & 34 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 100 & 18 \\ 
	\hline 
	elasticnet & 129.94 & 13 \\ 
	\hline 
	\textsc{CorReg} + elasticnet & 118.76 & 21 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained with leave-one out cross-validation. $n=100, p=145$.}	
\end{table}
In this precise case, \textsc{CorReg} found a structure that helped to decorrelate covariates in interpretation and to find the relevant part of the process to optimize.


\section{Conclusion and perspectives} \label{conclusion}
	We have seen that correlations can lead to serious estimation and variable selection problems in linear regression and that in such a context, it can be useful to explicitly model the structure between the covariates and to use this structure (even sequentially) to avoid correlations issues. We also show that real industrial context faces this kind of situations so our model can help to understand and predict physical phenomenon efficiently. But for now we still need a full dataset to learn the structure between the covariates and even if correlations are strong, some information is lost. Further work is needed to face these two challenges.
	
	\textsc{CorReg} is accessible on CRAN and has already proved its efficiency on real regression problematics in industry. \textsc{CorReg}'s strength is its great interpretability of the model, composed of several short linear regression easily managed by non-statisticians while strongly reducing correlations issues that are everywhere in industry.
	Nevertheless, we need to enlarge its application field to missing values, also very commons in industry. The structure can be used to estimate missing values in $\boldsymbol{X}_r$ but the actual generative model allows to go further (to manage missing values even in the MCMC algorithm) without supplementary hypothesis and this also is a strength of \textsc{CorReg}. 
	
	Another perspective would be to take back lost information (the residual of each sub-regression) to improve predictive efficiency when needed. It would only consists in a second step of linear regression between the residuals and would thus still be able to use any selection method.
	
	This paper only treats linear regression but such a pretreatment could be used for logistic regression, {\it etc.}
	So the subject is still wide opened.	

\section{Acknowledgements}
	We want to thanks ArcelorMittal Atlantique \& Lorraine that has granted this work, given the chance to use \textsc{CorReg} on real dataset and authorized the package to be open-sourced licensed (\textsc{CeCILL}), especially Dunkerque's site where most of the work has been done.
\bibliography{biblio}{}
\bibliographystyle{plain}
\section{Appendices}
	\subsection{Identifiability of the structure} \label{preuveident}
	The model presented above relies on a discrete structure $S$ between the covariates. But to find it we need identifiability property to insure the MCMC will asymptotically find the true model. Identifiability of the structure is asked in following terms: Is it possible to find another structure $\tilde{S}$ of linear regression between the covariates leading to the same joint distribution and marginal distributions? 
	
		If there are exact sub-regressions ($\sigma^2_j=0$), the structure won't be identifiable. But it only means that several structure will have the same likelihood and they will have the same interpretation. So it's not really a problem. Moreover, when an exact sub-regression is found, we can delete one of the implied variables without any loss of information and the structure will define a list of variable from which to delete. \textsc{CorReg} (Our R package) prints a warning to point out exact regressions when found.
	In the followings we suppose $\sigma^2_j\neq 0$, then $\boldsymbol{X}_f'\boldsymbol{X}_f$ and $\boldsymbol{X}'\boldsymbol{X}$ are of full rank (but the later is ill-conditioned for small values of $\sigma^2_j$)  .
	
	We introduce the matricial notation
		$\boldsymbol{X}_r=\boldsymbol{X}_f\boldsymbol{\alpha} + \boldsymbol{\varepsilon}$
		we note $\Theta$ the parameter of the model. $\boldsymbol{\alpha}$ is the $(p-p_r)\times p_r$ matrix whose columns are the $\boldsymbol{\alpha}_j$.
		\\ We want to know if $P(\boldsymbol{X}|S,\Theta)=P(\boldsymbol{X}|\tilde{S},\tilde{\Theta})$ does imply $(S,\Theta)=(\tilde{S},\tilde{\Theta})$.
		
%	\subsubsection{Identifiability of $\Theta$}		
%		We know by identifiability of the Gaussian mixtures and linear regression ($\boldsymbol{X}_f'\boldsymbol{X}_f$ is of full rank) that if $P(\boldsymbol{X}|S,\Theta)=P(\boldsymbol{X}|S,\tilde{\Theta})$ then $\Theta=\tilde{\Theta}$ up to a permutation of the components. But the identifiability of the discrete parameter $S$ is still pending.
%		
%	\subsubsection{Identifiability of $S$, quick look}
%	
%	In the following, $\otimes$ and $\oplus$ denote respectively the kronecker product and sum.
%	\begin{eqnarray}
%			\forall j \in I_r : \  \boldsymbol{X}^j_{|\boldsymbol{X}_f}&\sim & \mathcal{N}(\boldsymbol{X}_f\boldsymbol{\alpha}_j,\sigma^2_j\boldsymbol{I}_n) \label{densitycondgauche}\\
%			\forall j \in I_f : \boldsymbol{X}^j_{|S} &\sim& \mathcal{GM}(\boldsymbol{\pi}_j;\boldsymbol{\mu}_j;\boldsymbol{\sigma}^2_j) \textrm{ with } \boldsymbol{\pi}_j,\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j \textrm{ vectors of size } K_j  \label{densitydroite}\\
%			\textrm{And we obtain, } \forall j \in I_r : \  \boldsymbol{X}^j&\sim & 
%						\mathcal{GM}(\bigotimes_{\substack{i \in I_f^j  }}\boldsymbol{\pi}_i \ ; 
%			                         \bigoplus_{\substack{i \in I_f^j  }} \alpha_{i,j} \boldsymbol{\mu}_i \ ; 
%			                         \boldsymbol{\sigma}_j^2+\bigoplus_{\substack{i \in I_f^j  }}\alpha_{i,j}^2 \boldsymbol{\sigma}_i^2 )\label{densitygauche}          
%		\end{eqnarray}
%		So when we compare (\ref{densitydroite}) and (\ref{densitygauche}) we see that the number of components in $I_r$ variables differs when subregressions are of length $>1$ (almost 2 predictors) with multiple-class predictors (so the kronecker product is effective). We call this difference in component number "identifiability" in the sense that we try to find the model with the fewest component in $\boldsymbol{X}_f$ (we use the BIC so when comparing two models with the same likelihood, the one with fewer parameters will win). So in this case we have a "better" model (the simplest one) in the group of equivalent-meaning models (permuting variables in some sub-regression).
%	\\
%	
If all the variables in $\boldsymbol{X}$ are Gaussian mixtures with at least two distinct components, then the model of sub-regression is identifiable (from the identifiability of the SR model by Maugis \cite{maugis2009variable}).
		
But making this hypothesis on $\boldsymbol{X}_r$ is sufficient. 
In the followings, we call Gaussian mixture the Gaussian mixtures with at least two distinct components, and Gaussian mixtures with equal components are called Gaussian (in fact they are).

First, we observe that if each variable in $\boldsymbol{X}_r$ is a Gaussian mixture, then there must be at least one Gaussian mixture on the right of each sub-regression. So we have $\boldsymbol{X}^G \subsetneq \boldsymbol{X}_f$ containing Gaussian variables and we note the Gaussian mixtures $\boldsymbol{X}^{G^c}\neq \emptyset$ its complement in $\boldsymbol{X}_f$.


The theorem from Maugis guarantee that a sub-regression between Gaussian mixtures is identifiable.
		\begin{eqnarray}
			\forall j \in I_r, \boldsymbol{X}^j_{|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_j^G+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c}+ \boldsymbol{\varepsilon}_j \\
			\boldsymbol{X}^j_{|\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c} + \tilde{\boldsymbol{\varepsilon}}_j \textrm{ is identifiable where} \\
			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_j^{G}+ \boldsymbol{\varepsilon}_j \textrm{ is Gaussian. And we have the unique decomposition} \\
			f(\boldsymbol{X}^j|\boldsymbol{X}^G,\boldsymbol{X}^{G^c})&=&f(\boldsymbol{X}^j|\boldsymbol{X}^{G^c})f(\boldsymbol{X}^{G})
		\end{eqnarray}
		We have the unique (Gaussian are allowed only on the right of the sub-regressions) decomposition $f(\boldsymbol{X}_r|\boldsymbol{X}_f)=f(\boldsymbol{X}_r|\boldsymbol{X}^G,\boldsymbol{X}^{G^c})=f(\boldsymbol{X}_r|\boldsymbol{X}^{G^c})f(\boldsymbol{X}^{G}) $ 
		where  $f(\boldsymbol{X}_r|\boldsymbol{X}^{G^c})$ is identifiable. Moreover, $\boldsymbol{X}^{G}$ is a set (empty or not) of independent Gaussian variables, whose parameters are identifiable.
		So the structure is identifiable.
		
		
		
%	\subsubsection{Identifiability: Theorem and proof} 
%		A necessary and sufficient condition for identifiability of $S$ is $\forall j \in I_r: K_j\geq 2$ with at least two distinct components in $(\boldsymbol{\mu}_j,\boldsymbol{\sigma}_j)$
%\\
%	{\it Proof}:
%	We know that for a Gaussian multivariate distribution $(\boldsymbol{X}_r,\boldsymbol{X}_f)$ the conditional distributions $(\boldsymbol{X}_r|\boldsymbol{X}_f)$ and $(\boldsymbol{X}_f|\boldsymbol{X}_r)$ are both Gaussian (multivariate or not according to $p_r$). 
%	
%		If $\exists j \in I_r: k_j=1$ (Gaussian) then all the covariates on the right of the associated sub-regression (with non-zero coefficients) are Gaussian and then the sub-regression can be permuted and is not identifiable. So if $\boldsymbol{X}^j$ are Gaussian mixtures, it is a necessary condition for identifiability to have $\forall j \in I_r: K_j\geq 2$ with at least two distinct components in $(\boldsymbol{\mu}_j,\boldsymbol{\sigma}_j)$.
%
%	Gaussian mixtures in $\boldsymbol{X_r}$ with at least two distinct components does imply the presence of at least one Gaussian mixture in each sub-regression (with at least two components).
%%	\begin{equation}
%%	 \forall j \in I_f :    \boldsymbol{X}^j_{|S}=\sum_{k=1}^{K_j}\pi_j(k)\mathcal{N}\left(\mu_j(k),\sigma_j^2(k)\boldsymbol{I}_n\right) \\
%%	\end{equation}
%%	$\forall j \in I_r$ we can define $K_{j}$ the number of distinct components of $\boldsymbol{X}_j$.
%%	\begin{eqnarray}
%%				\boldsymbol{X}^{j}_{|S}&=&\sum_{k=1}^{K_j}\pi_j(k)\mathcal{N}\left(\mu_j(k),\sigma_j^2(k)\boldsymbol{I}_n\right)
%%	\end{eqnarray}
%	The joint distribution of the $p$ Gaussian mixtures is a multivariate Gaussian mixture $\boldsymbol{X}$ with $K\geq 2$ distinct components $(\boldsymbol{m}_k,\boldsymbol{\Sigma}_k)$ where $\boldsymbol{m}_k \in \mathbf{R}^p$ and $\boldsymbol{\Sigma}_k$ is a $p \times p$ matrix.
%		We decompose the joint distribution based on the components:
%		
%		\begin{equation}
%			f(\boldsymbol{X}_r,\boldsymbol{X}_f)= \sum_{k=1}^K p_kf(\boldsymbol{X}_r,\boldsymbol{X}_f|k)
%	\end{equation}
%			where  $\forall 1\leq k \leq K$, $ f(\boldsymbol{X}_r,\boldsymbol{X}_f|k)$  is a multivariate Gaussian.
%			\\
%			Then,  $\forall j \in I_r, \forall i \in I_f^j$ we have:
%			\begin{equation}
%			 f(\boldsymbol{X}_r,\boldsymbol{X}_f)= \sum_{k=1}^K p_kf(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i,k)f(\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i|k)
%	\end{equation}
%				where the $f(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i,k)$ are Gaussian.
%				
%Hence,
%\begin{equation}
%				f(\boldsymbol{X}^i|\boldsymbol{X}\setminus\boldsymbol{X}^i)=f(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i)=\sum_{k=1}^Kp_kf(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i,k)
%				\end{equation}
%				
%				is a Gaussian mixture.				
%				
%				Identifiability of $S$ means we don't have a linear regression of $\boldsymbol{X}^i$ on $(\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i)$. So we need a non-linear relationship or a not-Gaussian residual $f(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}\setminus\boldsymbol{X}^i)$, {\it i.e. } the Gaussian mixture $f(\boldsymbol{X}^i|\boldsymbol{X}\setminus\boldsymbol{X}^i)$ should have at least two distinct components.
%	Linear relationship means $\exists \boldsymbol{\gamma} \in \mathbf{R}^{p-1}$ and $\sigma_i \in \mathbf{R}$ for which
%	\begin{equation}
%		\forall 1\leq k \leq K, \boldsymbol{X}^i_{|\boldsymbol{X}_r,\boldsymbol{X}\setminus\boldsymbol{X}^i,k}=\mathcal{N}(\boldsymbol{m}_k^{\bar{i}}\boldsymbol{\gamma},\sigma_i^2)
%	\end{equation}
%					
%				
%				\begin{equation}
%				f(\boldsymbol{X}^i|\boldsymbol{X}\setminus\boldsymbol{X}^i)=f(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i)=\sum_{k=1}^Kp_kf(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i,k)
%				\end{equation}
%				
%				If it is a Gaussian mixture with at least two distinct components when $\boldsymbol{X}^j \in \boldsymbol{X}_r$ has at least two distinct components.
%				Then $\boldsymbol{X}^i$ can't be explained by a linear regression on the other covariates (non-Gaussian residual).		
%				 So we have identifiability of the structure $S$ even for trivial sub-regressions if we keep the constraint to have distinct components in $\boldsymbol{X}_r$ (equivalent to have at least one Gaussian mixture with at least two components in each sub-regression).
%			%f(\boldsymbol{X}_r,\boldsymbol{X}_f)&=& sum_{j=1}^K 
%
%The distribution of $\boldsymbol{X}_r$ is a Gaussian mixture defined in (\ref{densitygauche}) thus
%			$\forall 1\leq k \leq K$,  $(\boldsymbol{X}_r|k)$ is Gaussian: 
%		
%
%
%In the followings, $N_{11}$ and $N_{12}$ Gaussian (multivariate or not). $\boldsymbol{X}_r$ is here supposed to be univariate (identifiability in such a case will be generalisable).
%		\begin{eqnarray}
%			f(\boldsymbol{X_f})&=& p_{11} N_{11}+p_{12}N_{12}\\
%			\boldsymbol{X}_r&=&\boldsymbol{X}_f\boldsymbol{\alpha}_r +\boldsymbol{\varepsilon}_r \\
%			f(\boldsymbol{X}_r)&=&p_{11}(N_{11}+\boldsymbol{\varepsilon})+p_{12}(N_{12}+\boldsymbol{\varepsilon}) \\
%			f(\boldsymbol{X}_r|\boldsymbol{X}_f)&=&\boldsymbol{\varepsilon}\\
%			f(\boldsymbol{X}_r,\boldsymbol{X}_f)&=&f(\boldsymbol{X}_r|\boldsymbol{X}_f)f(\boldsymbol{X}_f)=f(\boldsymbol{X}_f|\boldsymbol{X}_r)f(\boldsymbol{X}_r) \\
%			f(\boldsymbol{X}_f|\boldsymbol{X}_r)&=&\frac{f(\boldsymbol{X}_r|\boldsymbol{X}_f)f(\boldsymbol{X}_f)}{f(\boldsymbol{X}_r)} \\
%			&=&\frac{(p_{11} N_{11}+p_{12}N_{12})\boldsymbol{\varepsilon}}{p_{11}(N_{11}+\boldsymbol{\varepsilon})+p_{12}(N_{12}+\boldsymbol{\varepsilon})} \textrm{ with }p_{11}+p_{12}=1\\
%			&=&\frac{(p_{11} N_{11}+p_{12}N_{12})\boldsymbol{\varepsilon}}{(p_{11} N_{11}+p_{12}N_{12})+\boldsymbol{\varepsilon}}
%		\end{eqnarray}
%	So we have a quotient of two multivariate Gaussian mixtures and the question is to know if it is Gaussian (in such a case we wouldn't have identifiability).
%If $\boldsymbol{X}_f$ is a Gaussian mixture with $K>1$ distinct components, then so will be $\boldsymbol{X}_r$.
%	
%	If the sufficient condition is not satisfied but we have different variances for $\boldsymbol{X}_f$ and $\boldsymbol{X}_r$, then we don't have identifiability but reverse regressions don't give the same results as the true model, as illustrated in the well-known salary discrimination study \cite{goldberger1984reverse} and explained in books and papers (\cite{leamer1978least},\cite{cameron2005microeconometrics}).
%	
	
	
%	 As a set of Gaussian mixtures and linear combination of Gaussian mixtures, $\boldsymbol{X}$ follows a multivariate Gaussian mixture model with $K$ components:
%		\begin{eqnarray}
%			f(\boldsymbol{X}|S,\theta)=\sum_{k=1}^Kp_k \Phi(\boldsymbol{X}_r|\boldsymbol{X}_f\boldsymbol{\alpha},S,\theta)\Phi(\boldsymbol{X}_f|S,\theta)
%		\end{eqnarray}	
%	

	
	\subsection{The \textsc{CorReg} package}
\subsubsection{Alternative neighbourhoods for the MCMC}
	We have here at each step $|\mathcal{V}_{S,j}|=p$ candidates but some other constraints can be added on the definition of $\mathcal{S}$ and will consequently modify the size of the neighbourhood (for example a maximum complexity for the internal regressions or the whole structure, a maximum number of internal regressions, {\it etc.}). \textsc{CorReg} allows to modify this neighbourhood to better fit users constraints. Relaxation (column-wise and row-wise) is optional but gives more stability to the number of feasible candidates at each step and allows to modify several parts of $I_f$ in only one step when needed. Hence it improves efficiency by a significant reinforcement of the irreductibility of the Markov chain. Rejecting candidates instead of doing the relaxation steps will  however reduce the number of evaluated candidates and thus accelerate the walk. So it can be used for a warming phase when $n$ is great and time is missing.
	
	The hierchical uniform hypothesis made above for $P(S)$ implies $p_r<\frac{p}{2}$ and $p_f^j<\frac{p}{2}$ so candidates may be rejected to satisfy this hypothesis. Stronger constraints on $p_r$ and/or $p_f^j$ can be given in \textsc{CorReg} if relevant.
	
If the algorithm did not have time to converge (stationnarity), it can be continued with a few step for which the neighboorhood would only contain smaller candidates (in terms of complexity). It is equivalent to ask for each element in $I_f$ if the criterion $P(S|\boldsymbol{X})$ would be better without it. Thus it can be seen as a final cleaning step. But in fact, it's just continuing the MCMC with a reduced neighbourhood.	
	
	
	
\end{document}