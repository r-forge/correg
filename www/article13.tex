\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[table]{xcolor}
 \graphicspath{{figures/}}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Clément THERY}
\title{CorReg pretreatment : Regression for correlated variables and application in steel industry}

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%


\definecolor{darkgreen}{rgb}{0,0.4,0}
	 \definecolor{darkred}{rgb}{0.75,0,0}
	 \definecolor{darkblue}{rgb}{0,0,0.4}

\begin{document}
\maketitle
\bigskip
{\bf Abstract.} Linear regression generally suppose to have decorrelated covariates. This hypothesis is often irrealist with industrial datasets that contains many highly correlated covariates due to the process, physcial laws,  {\it etc}. The proposed generative model consists in explicit modeling of the correlations with a family of linear regressions between the covariates permitting to obtain by marginalization a parsimonious correlation-free regression model, easily understandable and compatible with variable selection methods. The structure of correlations is found with an MCMC algorithm. An R package (\textsc{CorReg}) available on the CRAN implements this new method which will be illustrated on both simulated datasets and real-life datasets from steel industry.
\smallskip

{\bf Keywords.} Regression, correlations, industry, variable selection, generative models

\section{Introduction}
%la régression et ses problèmes


When one wants to explain a phenomenon based on some covariates, the first statistical method tried frequently is the linear regression. It provides a predictive model with a good interpretability and is simple to learn for non-statistician. Therefore, linear regression is used in nearly all the fields where statistics are made, from industry (ballistic models to calibrate the process) to sociology (predicting some numerical properties of a population).
Linear regression is a very classic situation that faces an also classical problem : the variance of the estimators.
When estimating the parameters of the regression we have to compute the inverse of a matrix\cite{saporta2006probabilites} which will be ill-conditioned or even singular if some covariates depend linearly from each other. For a model defined by 
	\begin{equation}
		Y_{|X}=X\beta + \varepsilon
	\end{equation}
	where $X$ is the $n\times p$ matrix of the explicative variables, $Y$ the response vector and $\varepsilon \sim \mathcal{N}(0,\sigma_Y^2)$ 
	we have the following Ordinary Least Squares (OLS) estimators :
	\begin{equation}
		\hat{\beta}=\left(X'X \right) ^{-1}X'Y
	\end{equation}
	With variance 
	\begin{equation}
		\operatorname{Var}(\hat{\beta})=\sigma_Y^2(X'X)^{-1}
	\end{equation}
	And when correlations between covariates are strong, the matrix to invert is ill-conditioned and the variance explodes.
This variance increases based on two aspects :
\begin{itemize}
	\item The dimension $p$ (number of covariates) of the model  : the more covariates you have the greater variance you get.
	\item The correlations within the covariates : strongly correlated covariates give bad-conditioning and increase variance of the estimators .
\end{itemize}

	With the rise of informatic, datasets contains more and more covariates and thus more and more useless covariates. So dimension reduction becomes a necessity. Moreover, when you use more covariates, you increase the chance to have correlated ones. For example, this work takes place in an industrial (steel industry) context with a big set of covariates (many parameters of the whole process without any a priori) highly correlated (physical laws, process rules, etc). In such a context, variance of the estimators can lead to arbitrary results or even no results at all. Prediction and interpretation are both strongly needed, with a preference for interpretation in industrial context (better to improve the process when possible than to only predict defects).
		~\\	~\\
		
%bibliographie	
	Because OLS is the minimum-variance unbiased estimator, penalized methods try to reduce the variance introducing some bias to improve the bias-variance trade-off and get better prediction.
Moreover, real datasets implies many irrelevant variables (datasets based on the whole process without any a priori) so we have to use variable selection methods.

In the following we note classical norms: $\parallel\beta\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$ and $\parallel\beta \parallel_1=\sum_{i=1}^p|\beta_i| $.

	Ridge regression\cite{marquardt1975ridge} proposes a biased estimator that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \hat{\beta}=\operatorname{argmin} \left\lbrace \parallel Y-X\beta\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\beta\parallel_2^2\leq k
	\end{equation}
	But Ridge regression is not efficient to select covariates (it's an assumed choice) because coefficients tends to 0 but don't reach 0.
	So it gives difficult interpretations for large values of $p$ and is not adapted for our industrial context. We need to reduce the dimension of the model. Our goal is not just to predict but also to understand the response variable.
	

	
	
	
	The Least Absolute Shrinkage and Selection Operator (LASSO)\cite{tibshirani1996regression} consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty.
		\begin{equation}
		 \hat{\beta}=\operatorname{argmin} \left\lbrace \parallel Y-X\beta\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\beta\parallel_1\leq \lambda
		\end{equation}	
	 The Least Angle Regression\cite{efron2004least} (LAR) Algorithm offers a very efficient way to obtain the whole LASSO path and is very attractive. It requires only the same order of magnitude of computational effort as OLS applied to the full set of covariates. And it  really selects covariates with coeffients set exactly to 0.
	 But LASSO also faces consistency problems \cite{Zhao2006MSC} when confronted  with correlated covariates. Another limitation of the LASSO is that it preserves at most $n$ predictors (troublesome when in high dimension). Some recent variants of the LASSO does exist for the choice of the penalization coefficient like the adaptative LASSO \cite{zou2006adaptive} or the random LASSO \cite{wang2011random}.
	 \\
	 
	Elastic net\cite{zou2005regularization}	is a method developped to be a compromise between Ridge regression and the LASSO. 
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	Elastic net can be written:
	\begin{equation}
		\hat{\beta}=(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel Y-X\beta \parallel_2^2 \right\rbrace, \textrm{ subject to } (1-\alpha)\parallel\beta\parallel_1+\alpha\parallel\beta\parallel_2^2\leq t \textrm{ for some } t
	\end{equation}
	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	It seems to give good predictions. But it is based on the grouping effect and if the dataset contains two identical variables they will obtain the same coefficient whereas LASSO will choose between one of them and will then obtain same predictions with a more parsimonious model. 
	\\
	
	Another way of reducing the dimension is to consider clusters of variables with the same coefficients, like the Octogonal Shrinkage and Clustering Algorithm for Regression (OSCAR) \cite{bondell2008simultaneous}.
	The CLusterwise Effect REgression\cite{yengo2012variable} (CLERE) describes the $\beta_j$ no longer as fixed effect parameters but as unobserved independant random variables whith grouped $\beta_j$ following a Gaussian Mixture distribution. 

The idea is to hope that the model have a small number of groups of covariates and that the mixture will have few enough components to have a number of parameters to estimate significantly lower than $p$. In such a case, it improves interpretability and ability to yeld reliable prediction with a smaller variance on $\hat{\beta}$. But it requires to suppose having many covariates with the same level of effect on the response variable and seems to stay less efficient in prediction than elastic net. Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the $\beta_j$ and gives a subset of covariates (not grouped) on which to compute OLS but has no specific protection against correlations issues.
	~\\	~\\
% Principe de la méthode

 When some try to reduce the dimension and then just hope to have small correlations in the remaining dimensions,  we propose to focus on the correlations, giving a model with orthogonal covariates and an explicit structure between covariates. In fact we search the greatest set of orthogonal covariates to keep the maximum information but with an orthogonality constraint. This can be viewed as a pretreatment on the dataset allowing to use then other dimension reduction tools without suffering from correlations. We only consider strong correlations (i.e. : problematic ones) thus we keep most of the information contained in the dataset. 
 %We will in a second time be able to use the remaining part of the information (sequential approach).	
 
	Our work is based on the assumption that if we know that correlations are a problem and if we precisely know the correlations, we could use this knowledge to avoid the problem.
	The idea is to suppose explicitly a linear structure between the covariates. 
	It gives a system of linear regression that can be viewed as a recursive Simultaneous Equation Model (SEM)\cite{davidson1993estimation}. Such a system is easy to interpret but estimation don't take advantage of the explicit structure \cite{TIMM} when the structure is straight forward (recursive SEM).
  	Other methods already rely on linear structure and start to take into account correlations but they only consider covariances between the residuals (SUR) \cite{SURzellner} or covariances between the endogenous variables like SPRING (Structured selection of Primordial Relationships IN the General linear model) \cite{chiquetconf}.

 	
	  In this work, we decide to distinguish the response variable from the other endogenous variables (that are on the left of a regression). Thus we don't have a system of regressions but one regression on our response variable and a system of subregressions (without the response variable). And we consider correlations between the explicative covariates of the main regression, not between the residuals.
	  The structure is supposed to be the source of the correlations and allows us to define a reduced set of independent covariates. Thus we reduce dimension and correlations in the same time. The structure justifies the eviction of the redundant covariates without significant information loss. It can be seen as a pretreatment on the dataset based on the hypothesis of a strong structure between the covariates (i.e. : small $\varepsilon_X$).
	  
	We can use any variable selection method on the reduced dataset with improved efficiency (reduced variance) due to dimension reduction and correlation suppression. So we obtain two kinds of zeros in our first model : coerced zeros due to correlations (redundant information) and estimated ones with classical variable selection methods applied on remaining variables. This two kinds of zero won't be interpreted in the same way and thus consistency issues don't mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.
 	 	
But to work, we need an explicit structure between the covariates. SEM are often used in social sciences and economy where a structure is supposed "by hand" but here we want to be able to find a structure without any a priori (possibility to include some known structure remains). Graphical LASSO \cite{friedman2008sparse} offers a method to obtain a structure based on the precision matrix (inverse of the variance-covariance matrix). It consists in a selection in the precision matrix, setting some covariances to zero. But the resulting matrix is symmetric and we need an oriented graph for our SEM. So we developped an MCMC algorithm to find it (R package CorReg on CRAN). However, Graphical LASSO can be used in the initialization step of our MCMC. This structure is based on gaussian mixture models to fit better real datasets and to allow identifiability of the structure in terms of complexity (number of parameters).
 	\\
 	
 	%plan
 	This paper will first present the reduced model and its properties before describing in Section 3 the algorithm used to find the structure.
 	We will then look at some numerical results on simulated (Section 4) and real industrial datasets (Section 5) before concluding and giving some perspectives in the sixth part.
	
\section{Model to decorrelate the covariates}

\subsection{Modelisation of the correlations}
Let $Y \in \mathcal{R}^n$ be a response variable we want to explain with a set $X \in \mathcal{R}^{n\times p}$ of $p$ correlated covariates.
We propose to explicitly define a family of $p_2$ internal regressions between covariates whith $I_2$ the set of indices of endogenous variables in $X$ (explained ones) and $I_1=\{I_1^1,\dots,I_1^p \}$ the set of the sets of indices of exogenous covariates (explaining ones) with $\forall j \notin I_2, I_1^j=\emptyset$. Then we have an explicit structure $S=(I_1,I_2,p_1,p_2)$ where $p_1=(p_1^1,\dots,p_1^{p_2})$ is the vector of the number of covariates in each internal regression. Thus we have $p_2=| I_2|$ and $p_1^j=|I_1^j|$ where $|.|$ represents the cardinal of an ensemble.

In the following, we note $X^j$ the $j^{th}$ column of a matrix $X$.
For lighter notation we define $X_2=X^{I_2}$ the matrix of the endogenous covariates and $X_1=X\setminus X_2$ the matrix of the remaining exogenous covariates. 


We can now write the generative model:
\begin{itemize}
	\item Main regression between $Y$ and $X$:
		\begin{equation}
			Y_{|S}=XA+\varepsilon_Y =X_1A_1+X_2A_2+\varepsilon_Y \textrm{ whith } \varepsilon_Y \sim \mathcal{N}(0,\sigma^2_YI_n);	\label{MainR}
		\end{equation}
		where $A=(A_1,A_2) \in  \mathcal{R}^p$ is the vector of the regression coefficients and $I_n$ the identity matrix, 
	\item Family of $p_2$ regressions within correlated covariates in $X$:
		\begin{equation}
			\forall j \in I_2: X^j_{|X_1,S}=X_1B^j_1+\varepsilon_j \textrm{ with } \varepsilon_j \sim(0,\sigma^2_jI_n); \label{SR}
		\end{equation}
		where $B_1^j \in \mathcal{R}^{(p-p_2)}$ are the vectors of the regression coefficients between the covariates (containing some zeros according to $I_1^j$).
	\item  $p-p_2$ remaining independent exogenous variables:
		\begin{equation}
			\forall j \notin I_2 : X^j_{|S} \sim f(\theta_j)%=\mathcal{GM}(\pi_j;\mu_j;\sigma^2_j) \textrm{ with } \pi_j,\mu_j,\sigma^2_j \textrm{ vectors of size } k_j; \label{mixtureX1v0}
		\end{equation}
\end{itemize}

We make the hypothesis of the uncrossing rule $I_1\cap I_2=\emptyset$ {\it i.e.} endogenous variables don't explain other covariates, thus we have a partition $X=[X_1,X_2]$.


This generative model is conditionnal to $S$, the discrete structure model that is identifiable because we can't permute some regressions in (\ref{SR}) and obtain the same joint distribution $P(X,Y)$, the residuals ($\varepsilon_j$) would not stay Gaussian. These residuals are in the following supposed independent but one can suppose dependencies between them and then use appropriate tools to estimate them and the $B_1^j$ like SUR with Feasible Generalized Least Squares (FGLS) by Zellner \cite{SURzellner} or SPRING \cite{chiquetconf} as mentioned in the introduction.
\\
If there are exact regressions ($\sigma^2_j=0$) in (\ref{SR}), the structure won't be identifiable. But it only means that several structure will have the same likelihood and they will have the same interpretation. So it's not really a problem. Moreover, when an exact subregression is found, we can delete one of the implied variables without any loss of information and the structure will define a list of variable from which to delete. CorReg (Our R package) prints a warning to point out exact regressions when found.
	


\subsection{Estimator and properties}
	We note that (\ref{MainR}) and (\ref{SR}) also give by simple integration on $X_2$ a regression model on $Y$ {\it depending only on uncorrelated covariates $X_1$}:
	\begin{equation}
		Y=X_1 (A_{1}+ \sum_{j \in I_2}B^{j}_{1}A_{j})+  \sum_{j \in I_2}\varepsilon_{j}A_{j}+\varepsilon_Y 
		 = X_1\alpha_1+ \varepsilon_{\alpha}\label{Trueexpl}
	\end{equation}			
		So we have the unbiased estimator: 
		\begin{equation}
			\hat{\alpha}_{1} = (X'_{1} X^{1})^{-1}X'_{1}Y % \textrm{ and }\hat{\alpha}_{2} = 0
		\end{equation}
		with variance:
		\begin{equation}
			\operatorname{Var}[\hat{\alpha}_{1}|X]= (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}A_{j}^2 )(X'_{1} X^{1})^{-1} % \textrm{ and }\operatorname{Var}[\hat{\alpha}_{2}|X]= 0 
		\end{equation}
		We see that the variance is reduced (no correlations and smaller matrix give better conditioning) for small values of $\sigma_j$ $i.e.$ strong correlations.					
			%There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypothesis. Therefore we made some simulations to compare both methods (see the end of this paper).
			Moreover, the explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset. Expert knowledge can even be added to the structure.
			 This new model is reduced even without variable selection and is just a linear regression so every method for variable selection in linear regression can be used. Hence we hope to obtain a parsimonious model with { \it two kinds of zeros}: those from decorrelating step and those from selection step, with different meanings.
			 \\
			 The \textsc{CorReg} package proposes to make selection with both LASSO (with LAR), elasticnet, adaptative lasso, and others more classical methods. A last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}).
		 
	\subsection{Advantages for interpretation}
	
		Grouping effect is the fact that correlated covariates get similar coefficient and are selected together. It's the case with elasticnet \cite{zou2005regularization} for example.	
			If $X_2=X_1+e$ and we have the grouping effect, we will obtain a model like $\hat{Y}=\hat{a}X_1+\hat{a}X_2$. 
			Then, if you try to modify the response value, you will modify one of the covariates and both will change so you won't get expected results. 
			
		At the opposite, methods like the LASSO with LAR will only keep one covariate for each group of correlated covariates  so we get $\hat{Y}=2aX_1$ and think that $X_2 \perp Y$ but $Y$ depends on $X_2$.
		
		
			Nothing constrains us to give only one equation. It is clearly better to give the user another equation (or system for more complex models) describing the correlations. 
			So you get the following model : $Y=aX_1+aX_2$ and $X_1=X_2+e$. Then you have more information and are able to decide better actions. 
			With such a model, grouping effect is no more useful because when saying $Y=2aX_1$ and $X_2=X_1+e$ with the information that $X_2$ has been removed locally because of its correlation with $X_1$, you don't get misleading interpretations anymore. In fact, we have two kinds of zeros (those from selection and those from correlations) and we know for each zero which kind is it.
			So it is possible to combine the advantages of grouping effect and selection just giving several equations. 
			Each equation here is very simple (only linear regressions) so you don't really increase complexity of the model. 
			Moreover, the uncrossing constraint ($I_1\cap I_2=\emptyset$) guarantee to keep a simple structure easily interpretable.

	
	\section{Estimating structure of sub-regressions with a Markov chain}	

	\subsection{Structure comparison} \label{compstruct}
  All our work is based on $S$, the linear structure between the covariates.
	Our generative model allows us to compare structures with criterions like the Bayesian Information Criterion ($BIC$) which penalize the log-likelihood according to the complexity of the structure~\cite{BIChuard}. We will prefer this kind of comparison criterion instead of cross-validation that is very time-consuming and thus not friendly with combinatory problematics.
	We note $\Theta$ the set of the parameters of the generative model
	\begin{eqnarray}
		P(S|X)&\propto& P(X|S)P(S) \\
		BIC&=&-2\mathcal{L}(X,S,\Theta)+|\Theta|\log(n) \approx -2\log P(X|S)
	\end{eqnarray}
	But $BIC$ tends to give too complex structures because we test a great range of models. 
	Thus \mbox{\textsc{CorReg}} allows to choose to penalise the complexity a bit more with a hierarchical uniform {\it a priori} law $P(S)=P(I_1 | p_1,I_2,p_2)P(p_1|I_2,p_2)P(I_2|p_2)P(p_2)$  instead of a simple uniform law on $S$.
%	 Thus we have :
%		\begin{eqnarray}
%		BIC^*(X|S)&=&BIC(X|S) +\ln(P(S)) \label{Bicstar}
%	\end{eqnarray}		
	It increases penalty on complexity for $p_2<\frac{p}{2}$ and $p_1^j<\frac{p}{2}$ . Hence %when using $BIC*$ 
	\textsc{CorReg} adds this constraint on $\hat{p}_2$ and $\hat{p}_1^j$ in the MCMC when hierarchical hypothesis is made (it will be the case in the followings).			
	But we can imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
\\	
	
		We will now use the following notation: $\psi(S)=\psi(X|S)$	where $\psi(X|S)$ is the chosen criterion, that will be $BIC$ with hierarchical uniform hypothesis in our numerical results.
\subsection{Detailed generative model}
		
	To better fit industrial variables (Figure \ref{graphMixmod}), we suppose  that variables in $X_1$ follow Gaussian mixtures. The great flexibility \cite{mclachlan2004finite} of such models makes our model more robust. But one can use other laws if needed. Gaussian case is just a special case ($k_j=1$) of Gaussian mixture so it is included in our hypothesis.
\begin{equation}
			\forall j \notin I_2 : X^j_{|S} \sim f(\theta_j)=\mathcal{GM}(\pi_j;\mu_j;\sigma^2_j) \textrm{ with } \pi_j,\mu_j,\sigma^2_j \textrm{ vectors of size } k_j; \label{mixtureX1}
		\end{equation}
		Variables in $X_1$ are supposed to be independent.
	Thus if one have some hypothesis on the distribution of some variables (exponentially distributed for example) it is possible to compute corresponding $\psi$ separately, give it as an input of \textsc{CorReg} and then improve the efficiency of the algorithm (it will find a structure only if it is really relevant).
		
	\subsection{The neighbourhood}
		\subsubsection{Definition}
	Let's define $\mathcal{S}$ the ensemble of feasible structures (those with $I_1\cap I_2=\emptyset$).
	\\
	For each step, starting from $S \in \mathcal{S}$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}_{S,j}&=& \{S \}\cup \{ S^{(i,j)} |1\leq i \leq p, i\neq j  \} \\
		\textrm{where }\ \ j &\sim & \mathcal{U}(\{1,\dots,p\}) 
	\end{eqnarray}	
	With $S^{(i,j)}$ defined by the following algorithm :
	\begin{itemize}
		\item if $i \notin I_i^j$ (add): 
			\begin{itemize}
				\item $I_1^j=I_1^j\cup \{i\}$
				\item $I_1^i=\emptyset$ (explicative variables can't depend on others : column-wise relaxation)
				\item $I_1=I_1 \setminus \{j\}$ (dependent variables can't explain others : row-wise relaxation) 
			\end{itemize}			 
		\item else (remove): $I_1^j=I_1^j\setminus \{i\}$
	\end{itemize}
	
	\smallskip
	At every moment, coherence between $I_1$ and others parts of $S$ can be done by $\forall 1\leq j\leq p :  p_1^j=|I_1^j|$, $I_2=\{j |p_1^j>0 \}$, $p_2= |I_2|$, .
		\subsubsection{Alternatives}
	We have here at each step $|\mathcal{V}_{S,j}|=p$ candidates but some other constraints can be added on the definition of $\mathcal{S}$ and will consequently modify the size of the neighbourhood (for example a maximum complexity for the internal regressions or the whole structure, a maximum number of internal regressions, {\it etc.}). \textsc{CorReg} allows to modify this neighbourhood to better fit users constraints. Relaxation (column-wise and row-wise) is optional but gives more stability to the number of feasible candidates at each step and allows to modify several parts of $I_1$ in only one step when needed. Hence it improves efficiency by a significant reinforcement of the irreductibility of the Markov chain. Rejecting candidates instead of doing the relaxation steps will  however reduce the number of evaluated candidates and thus accelerate the walk. So it can be used for a warming phase when $n$ is great and time is missing.
	
	The hierchical uniform hypothesis made above for $P(S)$ implies $p_2<\frac{p}{2}$ and $p_1^j<\frac{p}{2}$ so candidates may be rejected to satisfy this hypothesis. Strongest constraints on $p_2$ and/or $p_1$ can be given in \textsc{CorReg} if relevant.
	
If the algorithm did not have time to converge, it can be continued with a few step for which the neighboorhood would only contain smaller candidates (in terms of complexity). It is equivalent to ask for each element in $I_1$ if the criterion $\psi$ would be better without it. Thus it can be seen as a final cleaning step. But in fact, it's just continuing the MCMC with a reduced neighbourhood.	
	\subsection{The walk}
	\subsubsection{Transition probabilities}
	We first make the approximation
	\begin{equation}
		P(S|X)\approx exp(\psi(S)).
	\end{equation}
	The algorithm follows a time-homogeneous markov chain whose transition matrix $\mathcal{P}$ has $|\mathcal{S}|$ rows and columns (combinatory so we'll just compute the probabilities when we need them).
	At each step the markov chain moves with probabiliy:
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=&\sum_{j=1}^p \mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S,j}\} }\frac{\exp(-\frac{1}{2} \psi(\tilde{S}))}{\sum_{S_l\in \mathcal{V}_{S,j}}\exp(-\frac{1}{2}\psi(S_l))} \\
	\end{eqnarray}
	And $\mathcal{S}$ is a finite state space.%la relaxation rend P non symétrique mais ne remets  pas en cause l'homogénéité	
	 
Because the walk follows a regular and thus ergodic markov chain with a finite state space, it has exactly one stationary distribution \cite{grinstead1997introduction} %: $\pi$ and every rows of $\operatorname{lim}_{k\rightarrow \infty}\mathcal{P}^k=W$ equals $\pi$.
%	
%	
%	With $\forall S \in  \mathcal{S}$ :
%	\begin{eqnarray}		
%		0 \leq &\pi (S)& \leq 1 \nonumber \\
%		\sum_{S \in \mathcal{S}}\pi(S) &=&1 \nonumber \\
%		\pi (S) &=&\sum_{\tilde{S}\in \mathcal{S}} \pi(\tilde{S})\mathcal{P}(\tilde{S},S) \\%définition de la lois stationnaire
%	\end{eqnarray}
%		
and the output will be the best structure in terms of $\psi$ which weights each candidate. Practically speaking, \textsc{CorReg} returns the best structure seen during the walk.
Numerical results (Section 4) illustrates the efficiency of the walk when the true model really contains a linear structure or no structure at all (Table (\ref{compZvrai})) and when the structure is not linear (Table \ref{compZnonlin})).

 \subsubsection{Initialisation(s)}

 If we have some knowledge about some sub-regressions (physical models for example) we can add them in the found and/or initial structure. So the model is really expert-friendly.
The initial structure can be based on a first warming algorithm taking the correlations into account. coefficients are randomly placed into $I_1$, weighted by the absolute value of the correlations. We do so in the followings. Then this structure can be reduced by the hadamard product with the binary matrix obtained by Graphical Lasso\cite{friedman2008sparse}. Graphical LASSO is time consuming so we only used it on real datasets (each simulation was computed 100 times).		

	One would rather test multiple short chains than lose time in initialisation or long chains \cite{gilks1996markov}. It also helps to face local extrema. In the followings, the chain was launched with twenty initialisations.
	
\section{Numerical results on simulated datasets}
	\subsection{The datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if \textsc{CorReg} 	keeps its promises.
	The \textsc{CorReg} package has been tested on simulated datasets. 
Section \ref{compZ} show the results obtained in terms of $\hat{S}$. Sections \ref{tableMSEsimtout} et \ref{tableMSEsimgauche} show the results obtained using only \textsc{CorReg}, or \textsc{CorReg} combined with other methods. Tables give both mean and standard deviation of the observed Mean Squared Errors (MSE) on a validation sample of $1 000$ individuals. For each simulation ,  $p=40$, $\sigma_Y=10$, $\sigma=0.001$, variables $X_1$ follow Gaussian mixture models of $\lambda=5$ classes which means follow Poisson's law of parameter $\lambda$ and which standard deviation also is $\lambda$. The $B_1^j$ are generated according to the same Poisson law but with a random sign. $S$ only contains binary relationships but \textsc{CorReg} was only constrained to $\max (\hat{p}_1^j)=5$.  
	We used \textsc{Rmixmod} to estimate the densities of each covariate.
	
		\subsection{Finding the structure}
		\subsubsection{How to evaluate found structure ?}
			The first criterion is $\psi$ which is minimised in the MCMC. But in our case, $\psi$ is based on the likelihood whose value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $S$ we define some indicators to compare the true model $S$ and the found one $\hat{S}$.
			Global indicators :
			\begin{itemize}
				\item $TL$ (True left) : the number of found dependent variables that really are dependent $TL=|I_2\cap \hat{I}_2|$ 
				\item $WL$ (Wrong left) : the number of found dependent variables that are not dependent $WL=|\hat{I}_2|-TL$
				\item $ML$ (Missing left) : the number of really dependent variables not found $ML=|I_2|-TL$
				\item $\Delta p_2$ : the gap between the number of sub-regression in both model : $\Delta p_2=|I_2|-|\hat{I}_2|$. The sign defines if $\hat{S}$ is too complex or too simple
				\item $\Delta compl$ : the difference in complexity between both model : $\Delta compl=\sum_{j \in p_2}p_1^j-\sum_{j \in \hat{p}_2}\hat{p}_1^j$
			\end{itemize}
		\subsubsection{Results on $S$}	\label{compZ}
In table \ref{compZvrai} we compare found structures in different contexts with both Uniform (U) and Hierarchical Uniform (HU) a priori law on $P(S)$. 
We see that (HU) hypothesis gives sparser models even if $\max (\hat{p}_1^j)=5$. $p=40$ so the maximum value for $\hat{p_2}$ is $20$ in (HU) case and we see that this max is not reached. The stronger penalty implied by (HU) really is efficient. The datasets used for (U) and (HU) are the same to keep the comparison  meaningful.
 
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
$n$ & $p_2$ & $\psi$ &  Time Mixmod  & Time MCMC  & $TL$ & $WL$ & $ML$ & $\Delta p_2$ & $\Delta compl$ \\
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&0.4104 & 3.2428 & 0 & 5.43 & 0 & -5.43 & 22.55  \\
& & & (0.0275) & (0.3711) & (0) & (1.9346) & (0) & (1.9346) & (8.0884) \\
 &  &HU &0.4104 & 8.2338 & 0 & 0.53 & 0 & -0.53 & 2.27  \\
& & & (0.0275) & (0.9045) & (0) & (0.7844) & (0) & (0.7844) & (3.3024) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&0.4182 & 2.7735 & 10.96 & 5.93 & 4.98 & -0.95 & 38.16  \\
& & & (0.0329) & (0.1498) & (1.9844) & (2.0313) & (1.9948) & (0.9987) & (6.499) \\
 &  &HU &0.4182 & 4.1876 & 11.61 & 4.57 & 4.33 & -0.24 & 16.48  \\
& & & (0.0329) & (0.2178) & (1.8743) & (1.9502) & (1.8752) & (0.4948) & (5.4892) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 0.4456 & 2.9154 & 25.23 & 1.92 & 6.5 & 4.58 & 28  \\
& & & (0.0429) & (0.1331) & (1.4761) & (1.0888) & (1.4668) & (0.9866) & (5.0831) \\
 &  & HU & 0.4456 & 4.0233 & 16.96 & 3.04 & 14.77 & 11.73 & 4.35  \\
& & & (0.0429) & (0.1091) & (1.3993) & (1.3993) & (1.4692) & (0.5478) & (5.8833) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&0.5229 & 4.7068 & 0 & 4.2 & 0 & -4.2 & 13.35  \\
& & & (0.0519) & (0.5865) & (0) & (1.7233) & (0) & (1.7233) & (5.6468) \\
 &  & HU &0.5229 & 10.1198 & 0 & 0.13 & 0 & -0.13 & 0.32  \\
& & & (0.0519) & (0.5541) & (0) & (0.3667) & (0) & (0.3667) & (0.9732) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&0.5205 & 3.3681 & 11.15 & 5.42 & 4.72 & -0.7 & 22.85  \\	
& & & (0.0451) & (0.3123) & (1.93) & (1.9132) & (1.886) & (0.7317) & (5.7742) \\
 &  &HU &0.5205 & 4.909 & 11.42 & 4.59 & 4.45 & -0.14 & 7.55  \\
& & & (0.0451) & (0.4556) & (1.9079) & (1.8968) & (1.8333) & (0.3487) & (4.0611) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U& 0.5833 & 3.2683 & 28.17 & 1.38 & 3.7 & 2.32 & 12.74  \\
& & & (0.0628) & (0.316) & (1.3711) & (0.9077) & (1.3143) & (0.8394) & (4.3359) \\
 &  &HU &0.5833 & 4.3599 & 17.27 & 2.73 & 14.6 & 11.87 & -2.61  \\
& & & (0.0628) & (0.3729) & (1.1708) & (1.1708) & (1.2792) & (0.338) & (4.4854) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U& 0.9623 & 12.9373 & 0 & 2.83 & 0 & -2.83 & 6.23  \\
& & & (0.077) & (1.7778) & (0) & (1.2953) & (0) & (1.2953) & (3.1999) \\
 &  &HU & 0.9623 & 20.9817 & 0 & 0.01 & 0 & -0.01 & 0.02  \\
& & & (0.077) & (1.9421) & (0) & (0.1) & (0) & (0.1) & (0.2) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& 1.1223 & 6.9647 & 11.67 & 4.8 & 4.25 & -0.55 & 12.58  \\
& & & (0.1122) & (0.5473) & (2.0003) & (2.0646) & (1.956) & (0.7833) & (3.9471) \\
 &  &HU & 1.1223 & 8.8486 & 12.04 & 3.95 & 3.88 & -0.07 & 3.75  \\
& & & (0.1122) & (0.7174) & (1.9223) & (1.9404) & (1.9137) & (0.2564) & (2.2625) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& 1.4343 & 5.9626 & 30.14 & 0.84 & 1.61 & 0.77 & 6.96  \\
& & & (0.2528) & (0.3136) & (1.3928) & (0.8495) & (1.2941) & (0.7086) & (3.0975) \\
 &  &HU & 1.4343 & 7.3741 & 17.49 & 2.51 & 14.26 & 11.75 & -3.76  \\
& & & (0.2528) & (0.2748) & (1.1849) & (1.1849) & (1.2441) & (0.4794) & (4.4859) \\
\hline
\end{tabular} 
\caption{Results of the Markov chain with  constraint $\hat{p}_1\leq 5$. Mean observed and standard deviation (sd). } \label{compZvrai}
\end{table}
With no constraints on $\hat{p}_1$ when $n<p$ we can have a perfect overlearning, so it is recommended to constrain each $\hat{p}_1^j <n$. Table \ref{compZvraip1max5} shows the impact on the research of such a constraint.


For  \ref{compZnonlin} we have generated non linear structures. Variables in $X_2$ depends on the $\log$ or the square (randomly choosen with equiprobability) of a variable in $X_1$ . Dependencies are still real but non linear.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
$n$ & $p_2$ &  $BIC(S)$ & $BIC(\emptyset)$ &$BIC(\hat{S})$  & $TL$ & $WL$ & $ML$ & $\Delta p_2$ & $\Delta compl$ \\
\hline
30 & 16 & ? & ? & ?  & ?  &?  & ?  & ? & ? \\
& &(?) & (?) & (?) & (?) & (?) & (?) & (?) & (?) \\
\hline
\end{tabular} 
\caption{Results of the Markov chain for non linear structure ($\log$ and square). Mean observed and standard deviation (sd). } \label{compZnonlin}
\end{table}

\clearpage

	\subsection{$Y$ depends on all variables in $X$}	 \label{tableMSEsimtout}	
We first try the method with a response depending on all covariates (\textsc{CorReg} reduces the dimension and can't give the true model if there is a structure). The datasets used here were those from \ref{compZvrai}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$& \psi & indicator &OLS  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U& MSE (sd) & 262627.57 (732019) & 5928332.4 (49005690.2) & 262627.57 (732019) \\
& & & cpl (sd) & 30 (0) & 29.99 (0.1) & 30 (0) \\
 &  &HU &MSE (sd) & 262627.57 (732019) & 10381962.64 (90962496.9) & 262627.57 (732019) \\
& & & cpl (sd) & 30 (0) & 30 (0) & 30 (0) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 510747.53 (2287539.8) & 635.42 (335.2) & 610.32 (424.9) \\
& & & cpl (sd) & 30 (0) & 24.11 (1) & 25.06 (0.2) \\
 &  &HU &MSE (sd) & 510747.53 (2287539.8) & 603.24 (415.6) & 610.32 (424.9) \\
& & & cpl (sd) & 30 (0) & 24.82 (0.6) & 25.06 (0.2) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 
& & & 
 &  & HU & 
& & & 
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&
& & & 
 &  & HU &
& & & 
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	
& & & 
 &  &HU &
& & & 
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&
& & & 
 &  &HU &
& & &
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U&   \\
& & & \\
 &  &HU &   \\
& & &  \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline
\end{tabular} 
\caption{OLS and OLS combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  \psi &indicator &LAR  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&MSE (sd) & 1246.35 (350.5) & 1433.98 (526.7) & 1246.35 (350.5) \\
& & & cpl (sd) & 17.84 (5.5) & 15.79 (5.6) & 17.84 (5.5) \\
 &  &HU &MSE (sd) & 1246.35 (350.5) & 1248.84 (341.4) & 1246.35 (350.5) \\
& & & cpl (sd) & 17.84 (5.5) & 17.63 (5.6) & 17.84 (5.5) \\ \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&
& & & 
 &  &HU &
& & & 
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 712.79 (405.2) & 566.32 (226.4) & 554.01 (257.1) \\
& & & cpl (sd) & 16.68 (4.4) & 15.35 (4.3) & 16.14 (4.5) \\
 &  & HU & MSE (sd) & 712.79 (405.2) & 551.92 (242.4) & 554.01 (257.1) \\
& & & cpl (sd) & 16.68 (4.4) & 15.96 (4.4) & 16.14 (4.5) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&
& & & 
 &  & HU &
& & & 
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	
& & & 
 &  &HU &
& & & 
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&
& & & 
 &  &HU &
& & &
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U&   \\
& & & \\
 &  &HU &   \\
& & &  \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline
\end{tabular} 
\caption{LASSO (with LAR) combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X$. \textsc{CorReg} logically wins}\label{YXlinLASSO}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  \psi &indicator &Elasticnet  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&MSE (sd) & 1326.54 (388.4) & 1356.98 (331.4) & 1326.54 (388.4) \\
& & & cpl (sd) & cpl (sd) & 12.14 (5.1) & 12.5 (5.2) & 12.14 (5.1) \\
 &  &HU & MSE (sd) & 1326.54 (388.4) & 1307.96 (356.6) & 1326.54 (388.4) \\
& & & cpl (sd) & 12.14 (5.1) & 12.27 (5.1) & 12.14 (5.1) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 1400.56 (1598.2) & 668.57 (274.6) & 653.59 (283.7) \\
& & & cpl (sd) & 13.86 (6.8) & 14.31 (5.7) & 14.83 (5.3) \\
 &  &HU &MSE (sd) & 1400.56 (1598.2) & 643.25 (277.9) & 653.59 (283.7) \\
& & & cpl (sd) & 13.86 (6.8) & 14.83 (5.6) & 14.83 (5.3) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 
& & & 
 &  & HU & 
& & & 
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&
& & & 
 &  & HU &
& & & 
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	
& & & 
 &  &HU &
& & & 
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&
& & & 
 &  &HU &
& & &
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U&   \\
& & & \\
 &  &HU &   \\
& & &  \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline
\end{tabular} 
\caption{Elasticnet (with Elasticnet) combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X$. \textsc{CorReg} logically wins}\label{YXlinenet}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  \psi &indicator &Stepwise  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U& MSE (sd) & 1919.43 (861.8) & 2014.13 (683.6) & 1919.43 (861.8) \\
& & &cpl (sd) & 22.8 (3.3) & 19.34 (4.9) & 22.8 (3.3) \\
 &  &HU &MSE (sd) & 1919.43 (861.8) & 1885.39 (814) & 1919.43 (861.8) \\
& & & cpl (sd) & 22.8 (3.3) & 22.73 (3.1) & 22.8 (3.3) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 696.46 (492.9) & 662.07 (285.9) & 660.93 (354) \\
& & & cpl (sd) & 15.9 (3.8) & 15.1 (3.7) & 15.76 (3.7) \\
 &  &HU &MSE (sd) & 696.46 (492.9) & 655.94 (325.7) & 660.93 (354) \\
& & & cpl (sd) & 15.9 (3.8) & 15.72 (3.6) & 15.76 (3.7) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 
& & & 
 &  & HU & 
& & & 
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&
& & & 
 &  & HU &
& & & 
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	
& & & 
 &  &HU &
& & & 
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&
& & & 
 &  &HU &
& & &
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U&   \\
& & & \\
 &  &HU &   \\
& & &  \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline
\end{tabular} 
\caption{Stepwise  combined with constrained \textsc{CorReg}. $Y$ depends on all variables in $X$. \textsc{CorReg} logically wins}\label{YXlinstep}
\end{table}



\clearpage
	\subsection{$Y$ depends only on covariates in $X_2$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $X_2$. The datasets used here were still those from \ref{compZvrai}.
Depending only on $X_2$ imply sparsity and impossibility to obtain the true model when using the true structure. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  \psi &indicator &OLS  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 81781.54 (247281.7) & 507.1 (269.2) & 593.03 (428.3) \\
& & & cpl (sd) & 30 (0) & 24.11 (1) & 25.06 (0.2) \\
 &  &HU &MSE (sd) & 81781.54 (247281.7) & 587.07 (425.6) & 593.03 (428.3) \\
& & & cpl (sd) & 30 (0) & 24.82 (0.6) & 25.06 (0.2) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 
& & & 
 &  & HU & 
& & & 
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	
& & & 
 &  &HU &
& & & 
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&
& & & 
 &  &HU &
& & &
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline
\end{tabular} 
\caption{OLS and OLS combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X2$. } \label{YX2linOLS}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  \psi &indicator &LAR  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 367.45 (198.4) & 298.32 (122.4) & 292.53 (102.2) \\
& & & cpl (sd) & 14.37 (5.1) & 12.9 (3.8) & 12.84 (3.8) \\
 &  &HU &MSE (sd) & 367.45 (198.4) & 298.1 (119.1) & 292.53 (102.2) \\
& & & cpl (sd) & 14.37 (5.1) & 12.81 (3.8) & 12.84 (3.8) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 
& & & 
 &  & HU & 
& & & 
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	
& & & 
 &  &HU &
& & & 
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&
& & & 
 &  &HU &
& & &
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline
\end{tabular} 
\caption{LASSO (with LAR) combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X2$. }\label{YX2linLASSO}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  \psi &indicator & Elasticnet  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 499.32 (218.8) & 311.41 (137.4) & 305.09 (123.5) \\
& & & cpl (sd) & 14.08 (6.1) & 11.75 (4.8) & 11.49 (4.7) \\
 &  &HU &MSE (sd) & 499.32 (218.8) & 307.6 (128.7) & 305.09 (123.5) \\
& & & cpl (sd) & 14.08 (6.1) & 11.63 (4.7) & 11.49 (4.7) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 
& & & 
 &  & HU & 
& & & 
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	
& & & 
 &  &HU &
& & & 
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&
& & & 
 &  &HU &
& & &
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline
\end{tabular} 
\caption{Elasticnet (with Elasticnet) combined with constrained \textsc{CorReg}.$Y$  depends on all variables in $X2$.}\label{YX2linenet}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_2$&  \psi &indicator &Stepwise  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 394.59 (454.6) & 341.47 (153.2) & 351.14 (148) \\
& & & cpl (sd) & 13.09 (3.2) & 12.9 (2.9) & 12.92 (3) \\
 &  &HU &MSE (sd) & 394.59 (454.6) & 353.99 (157) & 351.14 (148) \\
& & & cpl (sd) & 13.09 (3.2) & 12.83 (2.9) & 12.92 (3) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 
& & & 
 &  & HU & 
& & & 
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	
& & & 
 &  &HU &
& & & 
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&
& & & 
 &  &HU &
& & &
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  \\
& & &  \\
 &  &HU &  \\
& & & \\
\hline
\end{tabular} 
\caption{Stepwise  combined with constrained \textsc{CorReg}. $Y$ depends on all variables in $X2$.}\label{YX2linstep}
\end{table}


	\clearpage	
\section{Numerical results on real datasets}
\subsection{Quality case study}
This work takes place in steel industry context, with quality oriented objective : to understand and prevent quality problems on finished product, knowing the whole process. 
		
We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item 205 variables from the whole process to explain it.
			\item The stakes : a hundred euros per ton (for information: Dunkerque's  site aims to produce up to 7.5 millions tons a year)
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/mixmod.png} 
			\caption{Example of non-Gaussian real variable easily modeled by a Gaussian mixture}\label{graphMixmod}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/histR2exfos.png} 
			\caption{$R^2_{adj}$ of the 76 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/correlexfoshist.png} 
			\caption{Histogram of correlations in $X$.} \label{compareMSEexfos}
   \end{minipage}
\end{figure}   			
	We get a training set of $n=3 000$ products described by $p=205$ variables from the industrial process and a validation sample of $847$ products.
	Let's note $\rho$ the absolute value of correlations between two covariates. Industrial variables are naturally highly correlated as the width and the weight of a steel slab ($\rho=0.905$), the temperature before and after some tool ($\rho=0.983$), the  roughness of both faces of the product ($\rho= 0.919$), a mean and a max ($\rho=0.911$). \textsc{CorReg} also found more complex structures describing physical models, like   Width = f (Mean.flow , Mean.speed.CC) even if the true Physcial model is not linear : Width = flow / (speed * thickness) (here thickness is constant). Regulation models used to optimize the process were also found. These first results are easily understandable and meet metallurgists expertise.  
			The algorithm gives a structure of $p_2=76$ subregressions with a mean of $\bar{p_1}=5.17$ regressors. In $X_1$ the number of $\rho>0.7$ is $\textbf{79.33\%}$ smaller than in $X$.		
	
			It is now time to look at the predictive results (Figure \ref{compareMSEexfos}).
				The best model found when not using \textsc{CorReg} is given by the LASSO. But when using \textsc{CorReg} elasticnet produces a better model in terms of prediction. LASSO gives a model with 21 non-zero coefficients and elasticnet with \textsc{CorReg} gives a model with 40 non-zero parameters but $6.40\%$ better in prediction on the validation sample (847 products). $14$ non-zero coefficients are common between the two models.
				Elasticnet alone get a model with 78 parameters that is improved by $9.75\%$ in prediction when used with \textsc{CorReg}. When using LASSO with \textsc{CorReg} we obtain a model with 24 non-zero coefficients that is $4.11\%$ better than LASSO alone. We also computed the OLS model (without selection) and the naive one (estimating the response by the mean of the learning set). All the MSE were modified here to obtain a value of 100 for the best (to preserve confidentiality). Elasticnet with \textsc{CorReg} is $13.51\%$ better than OLS.
		\begin{figure}[h]
			\centering
				\label{compareMSEexfos}
				\includegraphics[width=400px]{figures/MSEfinal.png}
			\caption{MSE comparison on industrial dataset. Learning set : 3 000 products, validation set : 847 products}
		\end{figure}		
		\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	OLS & 115.63 & 206 \\ 
	\hline 
	\textsc{CorReg} + OLS & 109.59&130 \\ 
	\hline  
	LASSO & 106.84 & 21 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 102.45 &24 \\ 
	\hline 
	elasticnet & 110.81 & 78\\ 
	\hline 
	\textsc{CorReg} + elasticnet & 100 &40 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained on a validation sample.}	
\end{table}

		In terms of interpretation, the main regression comes with the family of regression so it gives a better understanding of the consequences of corrective actions on the whole process. It typically permits to determine the \textit{tuning parameters} whereas LASSO would point variables we can't directly act on.	So it becomes easier to take corrective actions on the process to reach the goal. The stakes are so important that even a little improvement leads to consequent benefits, and we don't even talk of the impact on the market shares that is even more important.
		\clearpage
		
		\subsection{Production case study}
This second example is about a phenomenon that impacts the productivity of a steel plan.
We have :
		\begin{itemize}
			\item a (confidential)  response variable,
			\item $p=145$ variables from the whole process to explain it but only $n=100$ individuals.
			\item The stakes : $20\%$ of productivity to gain
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[width=150px]{figures/correl0BVBI.png} 
			\caption{Correlations between the covariates}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/R2corregBVBI.png} 
			\caption{$R^2_{adj}$ of the 67 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/histcorrelBVBI.png} 
			\caption{Histogram of correlations in $X$.} 
   \end{minipage}
\end{figure}   			
	Here $n<p$ so we only have the leave one out cross-validation MSE.
	\textsc{CorReg} improves LASSO by $5.24\%$ and elasticnet by $8.60\%$. \textsc{CorReg} combined with LASSO gives the best result but it is only a leave on out MSE.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	LASSO & 105.54 & 34 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 100 & 18 \\ 
	\hline 
	elasticnet & 129.94 & 13 \\ 
	\hline 
	\textsc{CorReg} + elasticnet & 118.76 & 21 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained with leave-one out cross-validation. $n=100, p=145$.}	
\end{table}
In this precise case, \textsc{CorReg} found a structure that helped to decorrelate covariates in interpretation and to find the relevant part of the process to optimize.


\section{Conclusion and perspectives}
	We have seen that correlations can lead to serious estimation and variable selection problems in linear regression and that in such a context, it can be useful to explicitly model the structure between the covariates and to use this structure (even sequentially) to avoid correlations issues. We also show that real industrial context faces this kind of situations so our model can help to interprete and predict physical phenomenon efficiently and to help to manage missing values. But for now we still need a full dataset to learn the structure between the covariates and even if correlations are strong, some information is lost. Further work is needed to face these two challenges.
	
	\textsc{CorReg} is accessible on CRAN and has already proved its efficiency on real regression problematics in industry. \textsc{CorReg}'s strength is its great interpretability of the model, composed of several short linear regression easily managed by non-statisticians while strongly reducing correlations issues that are everywhere in industry.
	Nevertheless, we need to enlarge its application field to missing values, also very commons in industry. The actual generative model allows such a functionality without supplementary hypothesis and this also is a strength of \textsc{CorReg}. 
	
	Another perspective would be to take back lost information (the residual of each sub-regression) to improve predictive efficiency when needed. It would only consists in a second step of linear regression between the residuals and would thus still be able to use any selection method.
	
	This paper only treats linear regression but such a pretreatment could be used for logistic regression, {\it etc.}
	So the subject is still wide opened.	
	
\bibliography{biblio}{}
\bibliographystyle{plain}

\end{document}
