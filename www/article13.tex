\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[table]{xcolor}
 \graphicspath{{figures/}}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%\author{Clément THERY, Christophe BIERNACKI, Gaétan LORIDANT}
\title{Model-based pretreatment for regression with correlated variables.}

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%


\definecolor{darkgreen}{rgb}{0,0.4,0}
	 \definecolor{darkred}{rgb}{0.75,0,0}
	 \definecolor{darkblue}{rgb}{0,0,0.4}

\begin{document}
\begin{center}
{\Large
	{\sc Model-based pretreatment for regression \\ with correlated variables.}
}
\bigskip

  Clément Théry$^{1}$ \& Christophe Biernacki$^{2}$ \& Gaétan Loridant$^{3}$
\bigskip

{\it
$^{1}$ ArcelorMittal, Université Lille 1, Inria, CNRS, clement.thery@arcelormittal.com
 
$^{2}$ Université Lille 1, Inria, CNRS, christophe.biernacki@math.univ-lille1.fr

$^{3}$ Etudes Industrielles ArcelorMittal Dunkerque, gaetan.loridant@arcelormittal.com\textbf{}
}
\end{center}
\bigskip

{\bf Abstract.} Linear regression outcomes are known to be damaged by highly correlated covariates. However many modern datasets are expected to convey more and more highly correlated covariates due to the global increase of the amount of variables in datasets. We propose to explicitly model the correlations by a family of linear regressions between the covariates, some covariates explaining others. It allows then to obtain by marginalisation on the explained covariates a parsimonious correlation-free regression model, easily understandable. 
It corresponds to a kind of variable selection preliminary step which has then to be followed by standard linear estimation methods including variables selection procedures for instance. The structure of correlations is found with an MCMC algorithm aiming at optimizing a specific BIC criterion.
 An R package (\textsc{CorReg}) available on the CRAN implements this new method which will be illustrated on both simulated datasets and real-life datasets from steel industry where correlated variables are frequent.
\smallskip

{\bf Keywords.} Regression, correlations, industry, variable selection, generative models

\section{Introduction}
%la régression et ses problèmes
Linear regression is a very standard and efficient method providing a predictive model with a good interpretability even  for non-statistician. Therefore, linear regression is used in nearly all the fields where statistics are made \cite{montgomery2012introduction}, from industry and astronomy \cite{isobe1990linear} to sociology \cite{longford2012revision}.
Linear regression is a very classic situation but sometimes it has to face an also classical problem: the variance of the estimators. We note the linear regression model:
\begin{equation}
		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \label{regressionsimple}
	\end{equation}
	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables, $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $p\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{OLS}). %As shown in section \ref{sectionOLS}, 
	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
Conditionning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects : the dimension $p$ (number of covariates) of the model (the more covariates you have the greater variance you get)
	 and the correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .

	With the rise of informatics, datasets contain more and more covariates and thus dimension reduction becomes a necessity. Moreover, increasing the number of covariates also increases the chance to have correlated ones. It will be illustrated with examples from steel industry in the last section of this paper (many parameters of the whole process without any a priori) highly correlated (physical laws, process rules, etc). 
	 In such a context, variance of the estimators can lead to arbitrary results whereas interpretation and prediction are both strongly needed, depending on the context. For instance, in an industrial context interpretation could be favored to improve the process instead of only predict defects, so robust estimation is needed.
		~\\	~\\
		
%bibliographie	
	Because \textsc{OLS} is the minimum-variance linear unbiased estimator, penalized methods try to reduce the variance introducing some bias to improve the bias-variance trade-off and get better prediction.


In the following we note classical norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$ and $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^p|\beta_i| $.

	Ridge regression\cite{marquardt1975ridge} proposes a biased estimator that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \lambda \textrm{ with } \lambda>0
	\end{equation}
	But Ridge regression do not aim at selecting covariates because coefficients tends to 0 but don't reach 0.
	So it gives difficult interpretations for large values of $p$.	
	Moreover, real datasets imply many irrelevant variables so we have to use variable selection methods.
	
	The Least Absolute Shrinkage and Selection Operator (\textsc{LASSO} \cite{tibshirani1996regression}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0
		\end{equation}	
	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) Algorithm offers a very efficient way to obtain the whole LASSO path and is very attractive. It requires only the same order of magnitude of computational effort as \textsc{OLS} applied to the full set of covariates. And it  really selects covariates with coefficients set exactly to 0.
	 
%	 Another limitation of the \textsc{LASSO} is that it preserves at most $n$ predictors (troublesome when in high dimension). 
	 Some recent variants of the \textsc{LASSO} do exist for the choice of the penalization coefficient like the adaptative \textsc{LASSO} \cite{zou2006adaptive} or the random \textsc{LASSO} \cite{wang2011random}.
	 \\
	 
	Elastic net\cite{zou2005regularization}	is a method developed to be a compromise between Ridge regression and the \textsc{LASSO}. 
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	Elastic net can be written:
	\begin{equation}
		\boldsymbol{\hat{\beta}}=(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta} \parallel_2^2 \right\rbrace, \textrm{ subject to } (1-\alpha)\parallel\boldsymbol{\beta}\parallel_1+\alpha\parallel\boldsymbol{\beta}\parallel_2^2\leq t \textrm{ for some } t
	\end{equation}
	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	%It seems to give good predictions. 
	But it is based on the grouping effect so correlated covariates get similar coefficients and are selected together whereas LASSO will choose between one of them and will then obtain same predictions with a more parsimonious model. Hence interpretations are not the same and nothing explains why. So it can be very confusing. 
	\\
	
	Another way of reducing the dimension is to consider clusters of variables with the same coefficients, like the Octogonal Shrinkage and Clustering Algorithm for Regression (\textsc{OSCAR} \cite{bondell2008simultaneous}).
	The CLusterwise Effect REgression (\textsc{CLERE} \cite{yengo2012variable}) describes the $\beta_j$ no longer as fixed effect parameters but as unobserved independant random variables with grouped $\beta_j$ following a Gaussian Mixture distribution. The idea is to hope that the model have a small number of groups of covariates and that the mixture will have few enough components to have a number of parameters to estimate significantly lower than $p$. In such a case, it improves interpretability and ability to yeld reliable prediction with a smaller variance on $\boldsymbol{\hat{\beta}}$. 
	
	But it requires to suppose having many covariates with the same level of effect on the response variable and seems to stay less efficient in prediction than elastic net. Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the $\beta_j$ and gives a subset of covariates (not grouped) on which to compute \textsc{OLS} but has no specific protection against correlations issues.
	 \textsc{LASSO} also faces consistency problems \cite{Zhao2006MSC} when confronted  with correlated covariates and we see that none of the above methods take the correlations into account.

	~\\	~\\
% Principe de la méthode

 Where some reduce the dimension without looking at correlations,  we propose to focus on the correlations, giving an explicit structure between covariates that helps to define a marginal model with uncorrelated covariates. Thus we have variable selection with explanation and improve interpretation and not only prediction.
 Our work is based on the assumption that if we know explicitly the source of correlations, we could use this knowledge to avoid the problem.
 In fact we search the greatest set of orthogonal covariates to keep the maximum information but with an orthogonality constraint. This can be viewed as a pretreatment on the dataset allowing to use then other tools for dimension reduction and estimation without suffering from correlations. Practically speaking we use a system of linear regressions between the covariates to model the structure of the correlations. This structure is obtained by a MCMC algorithm optimizing the penalized likelihood of the joint law on the dataset $\boldsymbol{X}$. This algorithm is part of the R package \textsc{CorReg} accessible on \textsc{CRAN}. %We only consider strong correlations (i.e. : problematic ones) thus we keep most of the information contained in the dataset. 
 %We will in a second time be able to use the remaining part of the information (sequential approach).	
 
	
 	
 	%plan
 	This paper will first present the marginal regression model and its properties before describing in Section 3 the random walk used to find the structure.
 	We will then look at some numerical results on simulated (Section \ref{sectionsimul}) and real industrial datasets (Section \ref{sectionrealcase}) before concluding and giving some perspectives in Section \ref{conclusion}.
	
\section{Model to select decorrelated covariates}
\subsection{A classical problem: correlations in regression}\label{sectionOLS}
For a model defined by 
	\begin{equation}
		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X\beta} + \boldsymbol{\varepsilon}
	\end{equation}
	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables, $\boldsymbol{Y}$ the response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I_n})$ with $\boldsymbol{I_n}$ the $n$-sized identity matrix,
	we have the following Ordinary Least Squares (\textsc{OLS}) estimator:
	\begin{equation}
		\boldsymbol{\hat{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\boldsymbol{\hat{\beta}})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1} \label{eqOLS}
	\end{equation}
	and without any bias.
	When correlations between covariates are strong, the matrix to invert is ill-conditioned and the variance increases, giving unstable and unusable estimator \cite{hoerl1970ridge}.
	Another problem is that matricial inversion requires $n\geq p$. 
\subsection{Our proposal: modelisation of the correlations}
Let now $\boldsymbol{X} \in \mathcal{R}^{n\times p}$ be a set of $p$ covariates.
In the following, we note $\boldsymbol{X^j}$ the $j^{th}$ column of $\boldsymbol{X}$.

We propose to explicitly define a family of $p_r$ internal regressions between covariates with $I_r$ the set of indices of endogenous variables in $\boldsymbol{X}$ (explained ones). We note $\boldsymbol{X}_r=\boldsymbol{X}^{I_r}$ the sub-matrix of the redundant covariates (explained by others) and  $\boldsymbol{X}_f$ the remaining submatrix of free covariates (independent explaining covariates).
We focus here on the expression of $P(\boldsymbol{X}_r|\boldsymbol{X}_f)$.

We have $I_f=\{I_f^1,\dots,I_f^p \}$ the set of the sets of indices of exogenous covariates (explaining ones $=\boldsymbol{X}_f$) with $\forall j \notin I_r, I_f^j=\emptyset$. Then we have an explicit structure $S=(I_f,I_r,p_f,p_r)$ where $p_r=|I_r|$, $\boldsymbol{p}_f=(p_f^1,\dots,p_f^{p_r})$ is the vector of the number of covariates in each internal regression  and $p_f^j=|I_f^j|$. Here $|.|$ represents the cardinal of an ensemble.



The family of $p_r$ regressions within correlated covariates in $\boldsymbol{X}$ is noted:
	\begin{equation}
		\boldsymbol{X}_{r|\boldsymbol{X}_f,S} \textrm{ defined by }\forall j \in I_r: \boldsymbol{X}^j_{|\boldsymbol{X}_f,S}=\boldsymbol{X}_f\boldsymbol{\alpha}_j+\boldsymbol{\varepsilon}_j \textrm{ with } \boldsymbol{\varepsilon}_j \sim(\boldsymbol{0},\sigma^2_j\boldsymbol{I}_n) \label{SR}
	\end{equation}
		where $\boldsymbol{\alpha}_j \in \mathcal{R}^{(p-p_2)}$ are the vectors of the regression coefficients between the covariates (containing some zeros according to $I_f^j$).

We make the hypothesis of the uncrossing rule $I_f\cap I_r=\emptyset$ {\it i.e.} endogenous variables don't explain other covariates, thus $\boldsymbol{X}_f$ and $\boldsymbol{X}_f$ give a partition of $\boldsymbol{X}$, so $\boldsymbol{X}_f=\boldsymbol{X}\setminus \boldsymbol{X}_r$.
The discrete structure $S$ is identifiable because we can't permute some regressions in (\ref{SR}) and obtain the same joint distribution $P(\boldsymbol{X},\boldsymbol{Y})$, the residuals ($\boldsymbol{\varepsilon}_j$) would not stay Gaussian (see the appendices).


%ancienne intro

The structure obtained gives a system of linear regression that can be viewed as a recursive Simultaneous Equation Model (\textsc{SEM})\cite{davidson1993estimation}. Such a system is easy to interpret but estimation don't take advantage of the explicit structure \cite{TIMM} when the structure is straight forward (recursive \textsc{SEM}).
  	Some methods take into account correlations but they only consider covariances between residuals \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) or  endogenous variables like \textsc{SPRING} (Structured selection of Primordial Relationships IN the General linear model \cite{chiquetconf}) without explicit expression of the correlations and thus a smaller interpretation potential.
  	\\ Here we suppose independency between the residuals but in other cases it remains the possibility to use such methods to estimate the $\boldsymbol{\alpha}_j$ and $\sigma_j$.

 	
	  We make the choice to distinguish the response variable from the other endogenous variables (that are on the left of a regression). Thus we have one regression on our response variable ($P(\boldsymbol{Y}|\boldsymbol{X})$ and a system of sub-regressions (without the response variable: $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$). Then we consider correlations between the explicative covariates of the main regression, not between the residuals.
	 
%ancienne intro


	
\subsection{A by-product model: marginal regression with decorrelated covariates}
Now we know $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$ by the structure of sub-regressions, we are able to define a marginal regression model $P(\boldsymbol{Y}|\boldsymbol{X}_f,S)$ based on the reduced set of independent covariates $\boldsymbol{X}_f$ without significant information loss. 
 	\\
Using the partition $\boldsymbol{X}=[\boldsymbol{X}_f,\boldsymbol{X}_r]$ we can rewrite \ref{regressionsimple}:
	\begin{equation}
			\boldsymbol{Y}_{|\boldsymbol{X}_f,\boldsymbol{X}_r,S}=\boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{\varepsilon_Y} \textrm{ with } \boldsymbol{\varepsilon_Y} \sim \mathcal{N}(\boldsymbol{0},\sigma^2_Y\boldsymbol{I}_n);	\label{MainR}
		\end{equation}
		where $\boldsymbol{\beta}=(\boldsymbol{\beta}_f,\boldsymbol{\beta}_r) \in  \mathcal{R}^p$ is the vector of the regression coefficients associated respectively to $\boldsymbol{X}_f$ and $\boldsymbol{I}_n$ the identity matrix. 
We note that (\ref{SR}) and (\ref{MainR}) give also by simple integration on $\boldsymbol{X}_r$ a marginal regression model on $\boldsymbol{Y}$ {\it depending only on uncorrelated covariates $\boldsymbol{X}_f$}:
\begin{eqnarray}
	\boldsymbol{Y}_{|\boldsymbol{X}_f,S}&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j)+  \sum_{j \in I_r}\beta_{j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{Trueexpl} \\
	&=&\boldsymbol{X}_f\boldsymbol{\beta}_f^*+\boldsymbol{\varepsilon}_Y^*=\boldsymbol{X}\boldsymbol{\beta}^*+\boldsymbol{\varepsilon}_Y^* \textrm{ where }\boldsymbol{\beta}^*=(\boldsymbol{\beta}_f^*,\boldsymbol{\beta}_r^*) \textrm{ and } \hat{\boldsymbol{\beta}}_r^*=\boldsymbol{\beta}_r^*=\boldsymbol{0}\label{modexpl}
\end{eqnarray}
We note that it is simply a linear regression on some of the original covariates so we only made a pretreatment on the dataset by setting some coefficients to $0$ because of correlations. These $0$ won't be interpreted as independence with the response variable but as redundant information due to correlations between the covariates. It is a variable pre-selection independent of the response $\boldsymbol{Y}$. 

\subsection{Strategy of use: pretreatment before classical estimation/selection methods}\label{interpretation}
As a pretreatment, it allows usage of any method in a second time to estimate $\boldsymbol{\beta}^*$, even with variable selection methods like LASSO or even stepwise \cite{seber2012linear}.

After selection and estimation we will obtain a model with { \it two steps of variable selection}: the decorrelation step (coerced zeros associated to redundant information) and the classical selection step, with different meanings for obtained zeros in $\hat{\boldsymbol{\beta}}^*$. 
 This two kinds of zero won't be interpreted in the same way and thus consistency issues don't mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.


The explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset. Expert knowledge can even be added to the structure.

Moreover, the uncrossing constraint ($I_f\cap I_r=\emptyset$) guarantee to keep a simple structure easily interpretable (no cycles and no chain-effect) and straightforward readable.

	For the marginal regression model defined in (\ref{modexpl})
%	\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}_f}= \boldsymbol{X}_f\boldsymbol{\beta}_f^*+ \boldsymbol{\varepsilon}_Y^*
%	\end{equation}			
%		So 
we have the \textsc{OLS} unbiased estimator of $\boldsymbol{\beta}^*$: 
		\begin{equation}
			\hat{\boldsymbol{\beta}}_f^* = (\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{Y}  \textrm{ and }\boldsymbol{\hat\beta}_r^* = \boldsymbol{0}
		\end{equation}
		We see in (\ref{Trueexpl}) that it gives an unbiased estimation of $\boldsymbol{Y}$ and $\boldsymbol{\beta^*}$
		but in terms of $\boldsymbol{\beta}$ this estimator is biased:
		\begin{equation}
			\operatorname{E}[\hat{\boldsymbol{\beta}}_f^*|\boldsymbol{X}_f]=\boldsymbol{\beta}_f+\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \textrm{ and }\operatorname{E}[\hat{\boldsymbol{\beta}}_r^*|\boldsymbol{X}_f]=\boldsymbol{0}
		\end{equation}
		with variance:
		\begin{equation}
			\operatorname{Var}[\hat{\boldsymbol{\beta}}_f^*|\boldsymbol{X}_f]= (\sigma^2_Y+\sum_{j \in I_r}\sigma^2_{j}\beta_{j}^2 )(\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1}  \textrm{ and }\operatorname{Var}[\hat{\boldsymbol{\beta}}_r^*|\boldsymbol{X}_f]= \boldsymbol{0} 
		\end{equation}
		We see that the variance is reduced compared to OLS described in equation (\ref{eqOLS})(no correlations and smaller matrix give better conditioning ) for small values of $\sigma_j$ $i.e.$ strong correlations. So we play on the bias-variance tradeoff, reducing the variance by adding a bias. 		
			%There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypothesis. Therefore we made some simulations to compare both methods (see the end of this paper).
			 \\
	\subsection{Illustration of the tradeoff conveyed by the pretreatment}		  
	 The Mean Squared Error (\textsc{MSE}) on $\hat{\boldsymbol{\beta}}$ is:
	\begin{eqnarray}
		\textsc{MSE}(\hat{\boldsymbol{\beta}}|\boldsymbol{\beta},\boldsymbol{X})&=&\parallel \operatorname{Bias}\parallel_2^2+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}})) \\
			&=& 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}) \textrm{ for OLS, and then for the marginal model:} \\
			&=& \parallel\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \parallel_2^2 +\parallel \boldsymbol{\beta}_r\parallel^2_2 + (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}\beta_{j}^2 ) \operatorname{Tr}((\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1})
	\end{eqnarray}	 
	To better illustrate the bias-variance tradeoff, we look at a simple example with $p=3$ variables. $\boldsymbol{X}_f$ is composed by two independent scaled Gaussian $\mathcal{N}(0,1)$, $\boldsymbol{x}_3=\boldsymbol{x}_1+\boldsymbol{x}_2+\boldsymbol{\varepsilon}_3$ where $\boldsymbol{\varepsilon}_3\sim{\mathcal{N}(\boldsymbol{0},\sigma_3^2\boldsymbol{I}_n)}$. We also have $\boldsymbol{\beta}=(1,1,1)$ and $\sigma_Y \in \{1,10\}$  . Then we observe the theoretical Mean Squared Error (MSE) of the estimator of both OLS and \textsc{CorReg}'s marginal  model for several values of $\sigma_3$ (strength of the sub-regression) and $n$. Figure \ref{MQE1} shows the theoretical MSE evolution with the strength of the sub-regression:
	\begin{equation}
		1-\mathcal{R}^2=\frac{\operatorname{Var}(\boldsymbol{\varepsilon)_3}}{\operatorname{Var}(\boldsymbol{x}_3)}=\frac{\sigma_3^2}{\sigma_3^2+2}
	\end{equation}
	
\begin{figure}[h!]
%	\begin{minipage}[l]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn15sigmaY10.png} 
%			\caption{For $n=15$. Dotted: \textsc{Correg}, plain: OLS}\label{MQE1}
%	\end{minipage} \hfill
%	\begin{minipage}[c]{.32\linewidth}
%			\includegraphics[ width=170px]{figures/MQEn100sigmaY10.png} 
%			\caption{For $n=100$. Dotted: \textsc{Correg}, plain: OLS}
%	\end{minipage} \hfill
%   \begin{minipage}[r]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn1000sigmaY10.png} 
%			\caption{For $n=1000$. Dotted: \textsc{Correg}, plain: OLS.} \label{MQE3}
%   \end{minipage} 
	\includegraphics[width=500px]{figures/MQEexplOLS.png}\label{MQE1}
	\caption{MSE of OLS and CorReg (dotted) estimators for varying $(1-R^2)$ of the sub-regression, $n$ and $\sigma_Y$.}
\end{figure} 
It is clear in Figure \ref{MQE1} that the marginal model is more robust than \textsc{OLS} on $X$. And when sub-regression get weaker ($1-\mathcal{R}^2$ tends to 1) it remains stable until extreme values (sub-regression nearly fully explained by the noise). We also see that the error implied by strong correlations shrinks with the rise of $n$. 
We see that $\sigma_Y$ multiplies $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}))=\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{f}))+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))$ for both models but for the marginal model $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))=0$.
 Thus, as $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))=0$ when $\sigma_Y^2$ rises it increases the advantage of \textsc{CorReg} versus \textsc{OLS}. It illustrates the importance of dimension reduction when the model has a strong noise (very usual case on real datasets where true model is not even exactly linear).

	
\section{Sub-regressions model selection}	
Structural equations models like \textsc{SEM} are often used in social sciences and economy where a structure is supposed "by hand" but here we want to find it automatically. Graphical LASSO \cite{friedman2008sparse} offers a method to obtain a structure based on the precision matrix (inverse of the variance-covariance matrix), setting some coefficients of the precision matrix to zero. But the resulting matrix is symmetric and we need an oriented structure for $S$ to avoid cycles.

Cross-validation is very time-consuming and thus not friendly with combinatory problematics. Moreover, we need a criterion compatible with structures of different sizes (varying $p_2$) and not related with $Y$ because the structure is inherent to $\boldsymbol{X}$ only. Thus it must be a global criterion. 	
	


We want to find the structure $S$ that maximizes $P(S|\boldsymbol{X})$ and we have:	
	\begin{eqnarray}
	 \label{approxBIC} P(S|\boldsymbol{X})&\propto & P(\boldsymbol{X}|S)P(S)
	=P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)P(\boldsymbol{X}_f|S)P(S)
	\end{eqnarray}
	
	\subsection{Modeling the uncorrelated covariates: a full generative approach on $P(\boldsymbol{X})$}
	To be able to compare structures with probabilistic likelihood-based criterions, we need a full generative model on $\boldsymbol{X}$. Sub-regressions give $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S) $ but $P(\boldsymbol{X}_f|S)$ is still undefined. We suppose that variables in $\boldsymbol{X}_f$ follow Gaussian mixtures of $k_j \in \mathbf{N}^*$ components: 
	\begin{equation}
			\forall j \notin I_r : \boldsymbol{X}^j_{|S} \sim f(\boldsymbol{\theta}_j)=\mathcal{GM}(\boldsymbol{\pi}_j;\boldsymbol{\mu}_j;\boldsymbol{\sigma}^2_j) \textrm{ with } \boldsymbol{\pi}_j,\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j \textrm{ vectors of size } k_j. \label{mixtureX1}
		\end{equation}
		The great flexibility \cite{mclachlan2004finite} of such models makes our model more robust but one can use other laws if needed (exponentially distributed for example). Gaussian case is just a special case ($k_j=1$) of Gaussian mixture so it is included in our hypothesis.

		Variables in $\boldsymbol{X}_f$ are in the followings supposed to be independent.
	%Thus if one have some hypothesis on the distribution of some variables (exponentially distributed for example) it is possible to use it without impacting the model in other ways. %compute corresponding $\psi$ according to it. %and then improve the walk (will keep a structure only if it is really relevant).%and give it as an input of \textsc{CorReg} and then improve the efficiency of the algorithm (it will find a structure only if it is really relevant).
	We now have a full generative model.
	
	\subsection{Penalization of the integrated likelihood by $P(S)$} \label{compstruct}

Because it is about model selection and we are able to provide a full generative model, we decide to follow a Bayesian approach (\cite{raftery1995bayesian}, \cite{andrieu1999joint},\cite{chipman2001practical}).  
	
  Our full generative generative model allows us to compare structures with criterions like the Bayesian Information Criterion ($BIC$) which penalize the log-likelihood of the joint law on $\boldsymbol{X}$ according to the complexity of the structure~\cite{BIChuard}. 

	We note $\boldsymbol{\Theta}$ the set of the parameters of the generative model
	\begin{eqnarray}
		-2\log P(\boldsymbol{X}|S)&\approx & BIC=-2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+|\boldsymbol{\Theta}|\log(n)  
	\end{eqnarray}
	But $BIC$ tends to give too complex structures because we test a great range of models. 
	Thus we choose to penalise the complexity a bit more with a hierarchical uniform {\it a priori} distribution $P(S)=P(I_f | \boldsymbol{p}_f,I_r,p_r)P(\boldsymbol{p}_f|I_r,p_r)P(I_r|p_r)P(p_r)$  instead of a simple uniform law on $S$ that is generally used and provides no penalty.
%	 Thus we have :
%		\begin{eqnarray}
%		BIC^*(X|S)&=&BIC(X|S) +\ln(P(S)) \label{Bicstar}
%	\end{eqnarray}		
	It increases penalty on complexity for $p_r<\frac{p}{2}$ and $p_f^j<\frac{p}{2}$ . Hence %when using $BIC*$ 
	this constraint on $\hat{p}_r$ and $\hat{p}_f^j$ is given in the research algorithm when the Hierarchical Uniform (HU) hypothesis is made instead of Uniform one (U) in numerical experiments (section \ref{sectionsimul} and \ref{sectionrealcase}).
		We do not change $BIC$ but only $P(S)$ so the properties are the same as classical $BIC$ but we obtain better results when (HU) this constraint is verified.

%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	

	%ancienne intro	
	%ancienne intro

\subsection{MCMC algorithm}
	Now we have a comparison criterion, we define an MCMC algorithm to find the structure (R package \textsc{CorReg} on CRAN). 
	\subsubsection{The neighbourhood}
	Let's define $\mathcal{S}$ the ensemble of feasible structures (those with $I_f\cap I_r=\emptyset$).
	\\
	For each step, starting from $S \in \mathcal{S}$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}_{S,j}&=& \{S \}\cup \{ S^{(i,j)} |1\leq i \leq p, i\neq j  \} \\
		\textrm{where }\ \ j &\sim & \mathcal{U}(\{1,\dots,p\}) 
	\end{eqnarray}	
	With $S^{(i,j)}$ defined by the following algorithm :
	\begin{itemize}
		\item if $i \notin I_f^j$ (add): 
			\begin{itemize}
				\item $I_f^j=I_f^j\cup \{i\}$
				\item $I_f^i=\emptyset$ (explicative variables can't depend on others : column-wise relaxation)
				\item $I_f=I_f \setminus \{j\}$ (dependent variables can't explain others : row-wise relaxation) 
			\end{itemize}			 
		\item else (remove): $I_f^j=I_f^j\setminus \{i\}$
	\end{itemize}
	
	\smallskip
	At every moment, coherence between $I_f$ and others parts of $S$ can be done by $\forall 1\leq j\leq p :  p_f^j=|I_f^j|$, $I_r=\{j |p_f^j>0 \}$, $p_r= |I_r|$ .
		
	\subsubsection{Transition probabilities}
	
	The walk follows a time-homogeneous Markov Chain whose transition matrix $\mathcal{P}$ has $|\mathcal{S}|$ rows and columns (combinatory so we just compute the probabilities when we need them).
	At each step the markov chain moves with probabiliy:
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=&\sum_{j=1}^p \mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S,j}\} }\frac{\exp(-\frac{1}{2}P(\tilde{S}|\boldsymbol{X}))}{\sum_{S_l\in \mathcal{V}_{S,j}}\exp(-\frac{1}{2}P(S_l|\boldsymbol{X}))} 
	\end{eqnarray}
	And $\mathcal{S}$ is a finite state space.%la relaxation rend P non symétrique mais ne remets  pas en cause l'homogénéité	
	 
Because the walk follows a regular and thus ergodic markov chain with a finite state space, it has exactly one stationary distribution \cite{grinstead1997introduction} %: $\pi$ and every rows of $\operatorname{lim}_{k\rightarrow \infty}\mathcal{P}^k=W$ equals $\pi$.
%	
%	
%	With $\forall S \in  \mathcal{S}$ :
%	\begin{eqnarray}		
%		0 \leq &\pi (S)& \leq 1 \nonumber \\
%		\sum_{S \in \mathcal{S}}\pi(S) &=&1 \nonumber \\
%		\pi (S) &=&\sum_{\tilde{S}\in \mathcal{S}} \pi(\tilde{S})\mathcal{P}(\tilde{S},S) \\%définition de la lois stationnaire
%	\end{eqnarray}
%		
and the output will be the best structure in terms of $P(S|\boldsymbol{X})$ which weights each candidate. Practically speaking, \textsc{CorReg} returns the best structure seen during the walk.
Numerical results (Section 4) illustrates the efficiency of the walk when the true model really contains a linear structure or no structure at all (Table (\ref{compZvrai})) and when the structure is not linear (Table \ref{compZnonlin})).

 \subsubsection{Initialisation(s)}
 If we have some knowledge about some sub-regressions (physical models for example) we can add them in the found and/or initial structure. So the model is really expert-friendly.
The initial structure can be based on a first warming algorithm taking the correlations into account. coefficients are randomly placed into $I_1$, weighted by the absolute value of the correlations. We do so in the followings. Then this structure could be for example reduced by the hadamard product with the binary matrix obtained by Graphical Lasso\cite{friedman2008sparse} that makes selection in the precision matrix but it is time consuming.

	One would rather test multiple short chains than lose time in initialisation or long chains \cite{gilks1996markov}. It also helps to face local extrema. In the followings, the chain was launched with twenty initialisations.
	
\section{Numerical results on simulated datasets} \label{sectionsimul}


	\subsection{The datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if \textsc{CorReg} 	keeps its promises.
	The \textsc{CorReg} package has been tested on simulated datasets. 
Section \ref{compZ} shows the results obtained in terms of $\hat{S}$. Sections \ref{tableMSEsimtout} and \ref{tableMSEsimgauche} show the results obtained using only \textsc{CorReg}, or \textsc{CorReg} combined with other methods. Tables give both mean and standard deviation of the observed Mean Squared Errors (MSE) on a validation sample of $1 000$ individuals. For each simulation,  $p=40$, $\sigma_Y=10$, $\sigma=0.001$, variables in $\boldsymbol{X}_f$ follow Gaussian mixture models of $\lambda=5$ classes which means follow Poisson's law of parameter $\lambda$ and which standard deviation also is $\lambda$. The $\beta_j$ and the coefficients of the $\boldsymbol{\alpha}_j$ are generated according to the same Poisson law but with a random sign. $S$ only contains binary relationships but \textsc{CorReg} was only constrained to $\max (\hat{p}_f^j)=5$.  
	We used \textsc{Rmixmod} to estimate the densities of each covariate. For each configuration, the walk was launched on $20$ initial structures with a maximum of 9 000 steps each time.
	When $n<p$, a frequently used method is the Moore-Penrose generalized inverse \cite{katsikis2008fast}, thus OLS can obtain some results even with $n<p$ (see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
		\subsection{Finding the structure}
		\subsubsection{How to evaluate found structure?}
			The first criterion is $P(S|\boldsymbol{X})$ which is maximized in the MCMC. But in our case, it is estimated by the likelihood (see (\ref{approxBIC}))whose value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $S$ we define some indicators to compare the true model $S$ and the found one $\hat{S}$.
			Global indicators :
			\begin{itemize}
				\item $TL$ (True left) : the number of found dependent variables that really are dependent $TL=|I_r\cap \hat{I}_r|$ 
				\item $WL$ (Wrong left) : the number of found dependent variables that are not dependent $WL=|\hat{I}_r|-TL$
				\item $ML$ (Missing left) : the number of really dependent variables not found $ML=|I_r|-TL$
				\item $\Delta p_r$ : the gap between the number of sub-regression in both model : $\Delta p_r=|I_r|-|\hat{I}_r|$. The sign defines if $\hat{S}$ is too complex or too simple
				\item $\Delta compl$ : the difference in complexity between both model : $\Delta compl=\sum_{j \in p_r}p_f^j-\sum_{j \in \hat{p}_r}\hat{p}_f^j$
			\end{itemize}
		\subsubsection{Results on $S$}	\label{compZ}
In table \ref{compZvrai} we compare found structures in different contexts with both Uniform (U) and Hierarchical Uniform (HU) a priori law on $P(S)$. 
We see that (HU) hypothesis gives sparser models even with $\max (\hat{p}_f^j)=5$. Moreover, this constraint is not active because observed comlexity are smaller (this constraint only serves to accelerate the walk by reducing the dimension of $\mathcal{S}$ because each configuration was computed a hundred times), meaning that the stronger penalty implied by (HU) really is efficient. The datasets used for (U) and (HU) are the same to keep the comparison  meaningful. These datasets and $\hat{S}$ are those used for tables 
 \ref{YXlinOLS} to \ref{YX2linstep}.
 
 It is also notable that (HU) has a greater computational cost than (U). 
	We also notice that the MCMC is faster when there are numerous correlations (rejecting more candidates). 
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c}{Configuration}  &  \multicolumn{2}{|c}{Computing Time}  & \multicolumn{3}{|c}{Quality} & \multicolumn{2}{|c|}{Complexity}\\
\hline
$n$ & $p_r$ & $P(S)$ &  Time Mixmod  & Time MCMC  & $TL$ & $WL$ & $ML$ & $\Delta p_r$ & $\Delta compl$ \\
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&0.4104 & 3.2428 & 0 & 5.43 & 0 & -5.43 & 22.55  \\
& & & (0.0275) & (0.3711) & (0) & (1.9346) & (0) & (1.9346) & (8.0884) \\
 &  &HU &0.4104 & 8.2338 & 0 & 0.53 & 0 & -0.53 & 2.27  \\
& & & (0.0275) & (0.9045) & (0) & (0.7844) & (0) & (0.7844) & (3.3024) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&0.4182 & 2.7735 & 10.96 & 5.93 & 4.98 & -0.95 & 38.16  \\
& & & (0.0329) & (0.1498) & (1.9844) & (2.0313) & (1.9948) & (0.9987) & (6.499) \\
 &  &HU &0.4182 & 4.1876 & 11.61 & 4.57 & 4.33 & -0.24 & 16.48  \\
& & & (0.0329) & (0.2178) & (1.8743) & (1.9502) & (1.8752) & (0.4948) & (5.4892) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & 0.4456 & 2.9154 & 25.23 & 1.92 & 6.5 & 4.58 & 28  \\
& & & (0.0429) & (0.1331) & (1.4761) & (1.0888) & (1.4668) & (0.9866) & (5.0831) \\
 &  & HU & 0.4456 & 4.0233 & 16.96 & 3.04 & 14.77 & 11.73 & 4.35  \\
& & & (0.0429) & (0.1091) & (1.3993) & (1.3993) & (1.4692) & (0.5478) & (5.8833) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&0.5229 & 4.7068 & 0 & 4.2 & 0 & -4.2 & 13.35  \\
& & & (0.0519) & (0.5865) & (0) & (1.7233) & (0) & (1.7233) & (5.6468) \\
 &  & HU &0.5229 & 10.1198 & 0 & 0.13 & 0 & -0.13 & 0.32  \\
& & & (0.0519) & (0.5541) & (0) & (0.3667) & (0) & (0.3667) & (0.9732) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&0.5205 & 3.3681 & 11.15 & 5.42 & 4.72 & -0.7 & 22.85  \\	
& & & (0.0451) & (0.3123) & (1.93) & (1.9132) & (1.886) & (0.7317) & (5.7742) \\
 &  &HU &0.5205 & 4.909 & 11.42 & 4.59 & 4.45 & -0.14 & 7.55  \\
& & & (0.0451) & (0.4556) & (1.9079) & (1.8968) & (1.8333) & (0.3487) & (4.0611) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U& 0.5833 & 3.2683 & 28.17 & 1.38 & 3.7 & 2.32 & 12.74  \\
& & & (0.0628) & (0.316) & (1.3711) & (0.9077) & (1.3143) & (0.8394) & (4.3359) \\
 &  &HU &0.5833 & 4.3599 & 17.27 & 2.73 & 14.6 & 11.87 & -2.61  \\
& & & (0.0628) & (0.3729) & (1.1708) & (1.1708) & (1.2792) & (0.338) & (4.4854) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U& 0.9623 & 12.9373 & 0 & 2.83 & 0 & -2.83 & 6.23  \\
& & & (0.077) & (1.7778) & (0) & (1.2953) & (0) & (1.2953) & (3.1999) \\
 &  &HU & 0.9623 & 20.9817 & 0 & 0.01 & 0 & -0.01 & 0.02  \\
& & & (0.077) & (1.9421) & (0) & (0.1) & (0) & (0.1) & (0.2) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& 1.1223 & 6.9647 & 11.67 & 4.8 & 4.25 & -0.55 & 12.58  \\
& & & (0.1122) & (0.5473) & (2.0003) & (2.0646) & (1.956) & (0.7833) & (3.9471) \\
 &  &HU & 1.1223 & 8.8486 & 12.04 & 3.95 & 3.88 & -0.07 & 3.75  \\
& & & (0.1122) & (0.7174) & (1.9223) & (1.9404) & (1.9137) & (0.2564) & (2.2625) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& 1.4343 & 5.9626 & 30.14 & 0.84 & 1.61 & 0.77 & 6.96  \\
& & & (0.2528) & (0.3136) & (1.3928) & (0.8495) & (1.2941) & (0.7086) & (3.0975) \\
 &  &HU & 1.4343 & 7.3741 & 17.49 & 2.51 & 14.26 & 11.75 & -3.76  \\
& & & (0.2528) & (0.2748) & (1.1849) & (1.1849) & (1.2441) & (0.4794) & (4.4859) \\
\hline
\end{tabular} 
\caption{Results of the Markov chain with  constraint $\hat{p}_f\leq 5$. Mean observed and standard deviation (sd). } \label{compZvrai}
\end{table}


\clearpage
\subsection{Results on prediction}
	\subsubsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	 \label{tableMSEsimtout}	
We first try the method with a response depending on all covariates (\textsc{CorReg} reduces the dimension and can't give the true model if there is a structure). The datasets used here were those from table \ref{compZvrai}.

We observe that \textsc{CorReg} is better than classical methods especially with the Hierarchical Uniform law. When the complexity of the true model is higher than $\frac{p}{2}$ Uniform hypothesis logically is better but (HU) still beats classical methods, so it is robust. We also observe in table \ref{YXlinstep} that simple methods like stepwise (here from the package \textsc{lars}) can give good results in prediction.

When using penalized estimators for selection, a last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}) and is applied for both classical and marginal model.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$& $P(S)$ & indicator &OLS  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U& MSE (sd) & 262627.57 (732019) & 5928332.4 (49005690.2) & 262627.57 (732019) \\
& & & cpl (sd) & 30 (0) & 29.99 (0.1) & 30 (0) \\
 &  &HU &MSE (sd) & 262627.57 (732019) & 10381962.64 (90962496.9) & 262627.57 (732019) \\
& & & cpl (sd) & 30 (0) & 30 (0) & 30 (0) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 510747.53 (2287539.8) & 635.42 (335.2) & 610.32 (424.9) \\
& & & cpl (sd) & 30 (0) & 24.11 (1) & 25 (0) \\
 &  &HU &MSE (sd) & 510747.53 (2287539.8) & 603.24 (415.6) & 610.32 (424.9) \\
& & & cpl (sd) & 30 (0) & 24.82 (0.6) & 25 (0) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 178323.95 (1426610.2) & 180.02 (44.9) & 141.03 (27.8) \\
& & & cpl (sd) & 30 (0) & 13.85 (0.9) & 9 (0) \\
 &  & HU & MSE (sd) & 178323.95 (1426610.2) & 330.31 (134.8) & 141.03 (27.8) \\
& & & cpl (sd) & 30 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&MSE (sd) & 528.08 (228.6) & 886.54 (364.3) & 528.08 (228.6) \\
& & & cpl (sd) & 41 (0) & 36.8 (1.7) & 41 (0) \\
 &  & HU &MSE (sd) & 528.08 (228.6) & 542.29 (235) & 528.08 (228.6) \\
& & & cpl (sd) & 41 (0) & 40.87 (0.4) & 41 (0) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&MSE (sd) & 612.72 (291.5) & 239.17 (89.4) & 200.32 (42.6) \\	
& & & cpl (sd) & 41 (0) & 24.43 (0.7) & 25 (0) \\
 &  &HU &MSE (sd) & 612.72 (291.5) & 207.6 (51.3) & 200.32 (42.6) \\
& & & cpl (sd) & 41 (0) & 24.99 (0.5) & 25 (0) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 555.44 (262.5) & 128.98 (18.1) & 121.08 (11.9) \\
& & & cpl (sd) & 41 (0) & 11.45 (0.9) & 9 (0) \\
 &  &HU &MSE (sd) & 555.44 (262.5) & 171.9 (31.1) & 121.08 (11.9) \\
& & &cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U& MSE (sd) & 167.71 (20.6) & 323.44 (124.1) & 167.71 (20.6) \\
& & & cpl (sd) & 41 (0) & 38.17 (1.3) & 41 (0) \\
 &  &HU &MSE (sd) & 167.71 (20.6) & 167.98 (20.8) & 167.71 (20.6) \\  
& & & cpl (sd) & 41 (0) & 40.99 (0.1) & 41 (0) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 168.68 (22.4) & 158.51 (51.6) & 133.49 (12.2) \\
& & & cpl (sd) & 41 (0) & 24.53 (0.7) & 25 (0) \\
 &  &HU &  MSE (sd) & 168.68 (22.4) & 137.03 (20.3) & 133.49 (12.2) \\
& & & cpl (sd) & 41 (0) & 25.01 (0.4) & 25 (0) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  MSE (sd) & 173.25 (22.9) & 112.8 (10.7) & 110.63 (6.9) \\
& & & cpl (sd) & 41 (0) & 10.02 (0.8) & 9 (0) \\
 &  &HU & MSE (sd) & 173.25 (22.9) & 127.4 (11.9) & 110.63 (6.9) \\
& & & cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\end{tabular} 
\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator &LASSO  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&MSE (sd) & 1246.35 (350.5) & 1433.98 (526.7) & 1246.35 (350.5) \\
& & & cpl (sd) & 17.84 (5.5) & 15.79 (5.6) & 17.84 (5.5) \\
 &  &HU &MSE (sd) & 1246.35 (350.5) & 1248.84 (341.4) & 1246.35 (350.5) \\
& & & cpl (sd) & 17.84 (5.5) & 17.63 (5.6) & 17.84 (5.5) \\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 &16& U & MSE (sd) & 712.79 (405.2) & 566.32 (226.4) & 554.01 (257.1) \\
& & & cpl (sd) & 16.68 (4.4) & 15.35 (4.3) & 16.14 (4.5) \\
 &  & HU & MSE (sd) & 712.79 (405.2) & 551.92 (242.4) & 554.01 (257.1) \\
& & & cpl (sd) & 16.68 (4.4) & 15.96 (4.4) & 16.14 (4.5) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 216.12 (128.3) & 157.78 (39.4) & 147.04 (28.4) \\
& & & cpl (sd) & 11.23 (4.4) & 8.56 (1.8) & 7.91 (1.3) \\
 &  & HU & MSE (sd) & 216.12 (128.3) & 178.88 (68.9) & 147.04 (28.4) \\
& & & cpl (sd) & 11.23 (4.4) & 9.86 (3) & 7.91 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&MSE (sd) & 658.38 (221.4) & 872.32 (239.4) & 658.38 (221.4) \\
& & & cpl (sd) & 28.2 (6) & 22.95 (5.8) & 28.2 (6) \\
 &  & HU &MSE (sd) & 658.38 (221.4) & 652.72 (211.9) & 658.38 (221.4) \\
& & & cpl (sd) & 28.2 (6) & 28.36 (5.9) & 28.2 (6) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&MSE (sd) & 274.34 (101.7) & 268.76 (106.3) & 225.64 (67.5) \\	
& & & cpl (sd) & 22.14 (4.1) & 19.52 (2.9) & 20.47 (2.8) \\
 &  &HU &MSE (sd) & 274.34 (101.7) & 233.8 (78.9) & 225.64 (67.5) \\
& & & cpl (sd) & 22.14 (4.1) & 20.19 (2.8) & 20.47 (2.8) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 165.6 (73.7) & 128.34 (18.8) & 123.73 (12.8) \\
& & & cpl (sd) & 12.93 (6.4) & 8.51 (1.5) & 8.09 (1.1) \\
 &  &HU &MSE (sd) & 165.6 (73.7) & 135.84 (24.1) & 123.73 (12.8) \\
& & &cpl (sd) & 12.93 (6.4) & 10.17 (3.1) & 8.09 (1.1) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U&  MSE (sd) & 183.33 (31.1) & 357.15 (133.4) & 183.33 (31.1) \\
& & & cpl (sd) & 37.78 (2.4) & 32.93 (3.7) & 37.78 (2.4) \\
 &  &HU &  MSE (sd) & 183.33 (31.1) & 183.78 (31.6) & 183.33 (31.1) \\
& & &  cpl (sd) & 37.78 (2.4) & 37.75 (2.4) & 37.78 (2.4) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& MSE (sd) & 148.83 (18.6) & 164.44 (54.3) & 139.57 (16.1) \\
& & & cpl (sd) & 25.22 (4) & 21.72 (2) & 22.3 (1.7) \\
 &  &HU &  MSE (sd) & 148.83 (18.6) & 142.97 (22.1) & 139.57 (16.1) \\
& & & cpl (sd) & 25.22 (4) & 22.24 (1.8) & 22.3 (1.7) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& MSE (sd) & 124.29 (19.9) & 113.36 (11) & 111.54 (7.2) \\
& & & cpl (sd) & 13.18 (5.9) & 8.68 (1.2) & 8.53 (1) \\
 &  &HU & MSE (sd) & 124.29 (19.9) & 118.33 (12) & 111.54 (7.2) \\
& & & cpl (sd) & 13.18 (5.9) & 11.02 (2.9) & 8.53 (1) \\
\hline
\end{tabular} 
\caption{LASSO (with LAR) combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. LASSO is better than OLS but is improved by \textsc{CorReg} even for large values of $n$.}\label{YXlinLASSO}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$&indicator &Elasticnet  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U&MSE (sd) & 1 326.54 (388.4) & 1 356.98 (331.4) & 1 326.54 (388.4) \\
& & & cpl (sd) & 12.14 (5.1) & 12.5 (5.2) & 12.14 (5.1) \\
 &  &HU & MSE (sd) & 1326.54 (388.4) & 1307.96 (356.6) & 1326.54 (388.4) \\
& & & cpl (sd) & 12.14 (5.1) & 12.27 (5.1) & 12.14 (5.1) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 1 400.56 (1598.2) & 668.57 (274.6) & 653.59 (283.7) \\
& & & cpl (sd) & 13.86 (6.8) & 14.31 (5.7) & 14.83 (5.3) \\
 &  &HU &MSE (sd) & 1 400.56 (1598.2) & 643.25 (277.9) & 653.59 (283.7) \\
& & & cpl (sd) & 13.86 (6.8) & 14.83 (5.6) & 14.83 (5.3) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 855.74 (582.6) & 181.08 (51.9) & 146.79 (32.7) \\
& & & cpl (sd) & 15.57 (6.3) & 11.19 (2.4) & 8.37 (1.3) \\
 &  & HU & MSE (sd) & 855.74 (582.6) & 311.57 (163.9) & 146.79 (32.7) \\
& & & cpl (sd) & 15.57 (6.3) & 14.87 (3.9) & 8.37 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&MSE (sd) & 738.94 (254.5) & 914.56 (283.7) & 738.94 (254.5) \\
& & & cpl (sd) & 25.85 (8.7) & 20.97 (7.8) & 25.85 (8.7) \\
 &  & HU &MSE (sd) & 738.94 (254.5) & 751.06 (262.2) & 738.94 (254.5) \\
& & & cpl (sd) & 25.85 (8.7) & 25.43 (8.8) & 25.85 (8.7) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 516.36 (226.6) & 276.6 (133.6) & 228.85 (89.3) \\
& & & cpl (sd) & 26.72 (7.4) & 20.49 (3.8) & 21.8 (3.3) \\
 &  &HU &MSE (sd) & 516.36 (226.6) & 239.77 (103.7) & 228.85 (89.3) \\
& & & cpl (sd) & 26.72 (7.4) & 21.57 (3.4) & 21.8 (3.3) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 328.7 (146.3) & 130.61 (20.6) & 124.01 (15.4) \\
& & & cpl (sd) & 24.7 (7.1) & 9.95 (1.5) & 8.22 (1.2) \\
 &  &HU &MSE (sd) & 328.7 (146.3) & 169.13 (40.2) & 124.01 (15.4) \\
& & &cpl (sd) & 24.7 (7.1) & 16.04 (3.8) & 8.22 (1.2) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U& MSE (sd) & 175.93 (25.9) & 355.79 (143.8) & 175.93 (25.9) \\
& & & cpl (sd) & 39.71 (1.7) & 34.11 (4.6) & 39.71 (1.7) \\
 &  &HU &  MSE (sd) & 175.93 (25.9) & 176.2 (26.3) & 175.93 (25.9) \\
& & &  cpl (sd) & 39.71 (1.7) & 39.69 (1.7) & 39.71 (1.7) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 173.65 (25.1) & 164.96 (59) & 139.51 (17.6) \\
& & &  cpl (sd) & 35.54 (4.2) & 22.58 (2.2) & 23.26 (1.9) \\
 &  &HU &  MSE (sd) & 173.65 (25.1) & 143.04 (23.2) & 139.51 (17.6) \\
& & & cpl (sd) & 35.54 (4.2) & 23.16 (1.9) & 23.26 (1.9) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  MSE (sd) & 161.92 (25.2) & 113.61 (10.9) & 111.46 (7.2) \\
& & &  cpl (sd) & 30.78 (6.4) & 9.25 (1.2) & 8.67 (1) \\
 &  &HU &  MSE (sd) & 161.92 (25.2) & 127.44 (13.3) & 111.46 (7.2) \\
& & &cpl (sd) & 30.78 (6.4) & 17.85 (2.7) & 8.67 (1) \\
\hline
\end{tabular} 
\caption{Elasticnet combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. LASSO was better.}\label{YXlinenet}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator &Stepwise  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_51_44.csv
30 & 0 & U& MSE (sd) & 1 919.43 (861.8) & 2 014.13 (683.6) & 1 919.43 (861.8) \\
& & &cpl (sd) & 22.8 (3.3) & 19.34 (4.9) & 22.8 (3.3) \\
 &  &HU &MSE (sd) & 1 919.43 (861.8) & 1 885.39 (814) & 1 919.43 (861.8) \\
& & & cpl (sd) & 22.8 (3.3) & 22.73 (3.1) & 22.8 (3.3) \\
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 696.46 (492.9) & 662.07 (285.9) & 660.93 (354) \\
& & & cpl (sd) & 15.9 (3.8) & 15.1 (3.7) & 15.76 (3.7) \\
 &  &HU &MSE (sd) & 696.46 (492.9) & 655.94 (325.7) & 660.93 (354) \\
& & & cpl (sd) & 15.9 (3.8) & 15.72 (3.6) & 15.76 (3.7) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 394.93 (356) & 155.18 (36.4) & 147.81 (30.3) \\
& & &cpl (sd) & 16.1 (6.3) & 8.26 (1.7) & 7.77 (1.3) \\
 &  & HU &  MSE (sd) & 394.93 (356) & 189.49 (79) & 147.81 (30.3) \\
& & & cpl (sd) & 16.1 (6.3) & 10.23 (3) & 7.77 (1.3) \\
\hline
\hline %global50p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_02_25_40.csv
50 & 0 & U&MSE (sd) & 745.88 (241.7) & 962.12 (319.3) & 745.88 (241.7) \\
& & & cpl (sd) & 27.19 (4.6) & 22.9 (4.5) & 27.19 (4.6) \\
 &  & HU &MSE (sd) & 745.88 (241.7) & 749.08 (244.2) & 745.88 (241.7) \\
& & & cpl (sd) & 27.19 (4.6) & 27.21 (4.5) & 27.19 (4.6) \\
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 279.82 (151.1) & 290.13 (117.3) & 239.61 (69.5) \\
& & & cpl (sd) & 21.24 (5.2) & 18.13 (2.8) & 19.23 (2.7) \\
 &  &HU &MSE (sd) & 279.82 (151.1) & 252.35 (108.5) & 239.61 (69.5) \\
& & & cpl (sd) & 21.24 (5.2) & 18.97 (2.7) & 19.23 (2.7) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 199.3 (158.7) & 126.81 (17.4) & 124.19 (13) \\
& & & cpl (sd) & 14.31 (6.2) & 8.16 (1.2) & 8.04 (1) \\
 &  &HU &MSE (sd) & 199.3 (158.7) & 136.32 (25.2) & 124.19 (13) \\
& & &cpl (sd) & 14.31 (6.2) & 9.59 (2.6) & 8.04 (1) \\
\hline
\hline %global100p40ratio0BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_10_47_32.csv
100 & 0 & U&   MSE (sd) & 193.26 (34.9) & 387.18 (151.6) & 193.26 (34.9) \\
& & & cpl (sd) & 36.3 (2.5) & 30.94 (4.2) & 36.3 (2.5) \\
 &  &HU &   MSE (sd) & 193.26 (34.9) & 194.43 (37.3) & 193.26 (34.9) \\
& & &  cpl (sd) & 36.3 (2.5) & 36.22 (2.7) & 36.3 (2.5) \\
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 145.46 (20.1) & 165.74 (56.1) & 140.27 (17.5) \\
& & & cpl (sd) & 23.44 (3.8) & 21.23 (2.1) & 21.93 (1.7) \\
 &  &HU &  MSE (sd) & 145.46 (20.1) & 143.74 (23.2) & 140.27 (17.5) \\
& & & cpl (sd) & 23.44 (3.8) & 21.86 (1.7) & 21.93 (1.7) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& MSE (sd) & 128.44 (18.4) & 113.27 (10.7) & 111.77 (7.2) \\
& & &  cpl (sd) & 13.58 (5.2) & 8.59 (1.1) & 8.48 (1) \\
 &  &HU &  MSE (sd) & 128.44 (18.4) & 118.61 (11.5) & 111.77 (7.2) \\
& & & cpl (sd) & 13.58 (5.2) & 10.75 (2.8) & 8.48 (1) \\
\hline
\end{tabular} 
\caption{Stepwise  combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins but stepwise is quite good compared to elasticnet.}\label{YXlinstep}
\end{table}



\clearpage
	\subsubsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $\boldsymbol{X_r}$. The datasets used here were still those from \ref{compZvrai}.
Depending only on $\boldsymbol{X_r}$ imply sparsity and impossibility to obtain the true model when using the true structure. \textsc{CorReg} is still better than classical methods. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&$P(S)$ &indicator &OLS  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 81781.54 (247281.7) & 507.1 (269.2) & 593.03 (428.3) \\
& & & cpl (sd) & 30 (0) & 24.11 (1) & 25 (0) \\
 &  &HU &MSE (sd) & 81781.54 (247281.7) & 587.07 (425.6) & 593.03 (428.3) \\
& & & cpl (sd) & 30 (0) & 24.82 (0.6) & 25 (0) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 74136.81 (297716.1) & 189.45 (48.1) & 144.51 (31.4) \\
& & & cpl (sd) & 30 (0) & 13.85 (0.9) & 9 (0) \\
 &  & HU & MSE (sd) & 74136.81 (297716.1) & 340.53 (169.4) & 144.51 (31.4) \\
& & & cpl (sd) & 30 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 684.04 (438.1) & 190.97 (37.7) & 197.32 (40) \\
& & & cpl (sd) & 41 (0) & 24.43 (0.7) & 25 (0) \\
 &  &HU &MSE (sd) & 684.04 (438.1) & 196.36 (40.5) & 197.32 (40) \\
& & & cpl (sd) & 41 (0) & 24.99 (0.5) & 25 (0) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 596.32 (323) & 126.94 (15.5) & 119.39 (12.4) \\
& & & cpl (sd) & 41 (0) & 11.45 (0.9) & 9 (0) \\
 &  &HU &MSE (sd) & 596.32 (323) & 168.03 (27.7) & 119.39 (12.4) \\
& & &cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 168.35 (20.1) & 133.75 (12.4) & 135.08 (13.1) \\
& & & cpl (sd) & 41 (0) & 24.53 (0.7) & 25 (0) \\
 &  &HU &  MSE (sd) & 168.35 (20.1) & 135.04 (13.1) & 135.08 (13.1) \\
& & & cpl (sd) & 41 (0) & 25.01 (0.4) & 25 (0) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  MSE (sd) & 168.07 (21.4) & 109.61 (7.4) & 108.64 (7.2) \\
& & &  cpl (sd) & 41 (0) & 10.02 (0.8) & 9 (0) \\
 &  &HU & MSE (sd) & 168.07 (21.4) & 124.23 (11.1) & 108.64 (7.2) \\
& & & cpl (sd) & 41 (0) & 21 (0) & 9 (0) \\
\hline
\end{tabular} 
\caption{OLS and OLS combined with constrained \textsc{CorReg}.$\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X_r}$. Sometimes $\hat{S}$ gives better results than $S$ because $S$ is penalized by the fact that it relies only on covariates not in the true model. } \label{YX2linOLS}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator &LAR  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 367.45 (198.4) & 298.32 (122.4) & 292.53 (102.2) \\
& & & cpl (sd) & 14.37 (5.1) & 12.9 (3.8) & 12.84 (3.8) \\
 &  &HU &MSE (sd) & 367.45 (198.4) & 298.1 (119.1) & 292.53 (102.2) \\
& & & cpl (sd) & 14.37 (5.1) & 12.81 (3.8) & 12.84 (3.8) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 273.12 (267.2) & 166.93 (49) & 152.45 (39.5) \\
& & & cpl (sd) & 11.7 (5.8) & 8.7 (2.5) & 7.45 (1.4) \\
 &  & HU & MSE (sd) & 273.12 (267.2) & 175.68 (57.5) & 152.45 (39.5) \\
& & & cpl (sd) & 11.7 (5.8) & 9.19 (2.9) & 7.45 (1.4) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 189.64 (52.5) & 171.97 (45.9) & 176.58 (46.8) \\
& & & cpl (sd) & 14.15 (3.9) & 13.4 (2.8) & 13.5 (3.1) \\
 &  &HU &MSE (sd) & 189.64 (52.5) & 174.36 (45.3) & 176.58 (46.8) \\
& & & cpl (sd) & 14.15 (3.9) & 13.57 (3) & 13.5 (3.1) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 163.88 (67.4) & 124.53 (17) & 121.99 (16.6) \\
& & & cpl (sd) & 13.06 (7.2) & 8.29 (1.6) & 7.73 (1.2) \\
 &  &HU &MSE (sd) & 163.88 (67.4) & 133.7 (26.4) & 121.99 (16.6) \\
& & &cpl (sd) & 13.06 (7.2) & 9.65 (3.3) & 7.73 (1.2) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& MSE (sd) & 129.65 (14.8) & 127.11 (12.1) & 127.46 (12.2) \\
& & &  cpl (sd) & 14.86 (3.1) & 13.97 (2.5) & 14.04 (2.5) \\
 &  &HU &  MSE (sd) & 129.65 (14.8) & 127.46 (12.2) & 127.46 (12.2) \\
& & & cpl (sd) & 14.86 (3.1) & 14.02 (2.5) & 14.04 (2.5) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U&  MSE (sd) & 121.21 (20.1) & 109.66 (7.9) & 109.04 (7.8) \\
& & & cpl (sd) & 12.84 (5.2) & 8.42 (1.1) & 8.11 (0.8) \\
 &  &HU &  MSE (sd) & 121.21 (20.1) & 112.56 (11.2) & 109.04 (7.8) \\
& & & cpl (sd) & 12.84 (5.2) & 9.77 (2.3) & 8.11 (0.8) \\
\hline
\end{tabular} 
\caption{LASSO (with LAR) combined with constrained \textsc{CorReg}.$\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X_r}$. }\label{YX2linLASSO}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator & Elasticnet  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 499.32 (218.8) & 311.41 (137.4) & 305.09 (123.5) \\
& & & cpl (sd) & 14.08 (6.1) & 11.75 (4.8) & 11.49 (4.7) \\
 &  &HU &MSE (sd) & 499.32 (218.8) & 307.6 (128.7) & 305.09 (123.5) \\
& & & cpl (sd) & 14.08 (6.1) & 11.63 (4.7) & 11.49 (4.7) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 691.55 (503.2) & 193.45 (54.2) & 150.17 (35) \\
& & & cpl (sd) & 12.54 (7.2) & 10.26 (2.7) & 7.81 (1.4) \\
 &  & HU & MSE (sd) & 691.55 (503.2) & 279.21 (117.3) & 150.17 (35) \\
& & & cpl (sd) & 12.54 (7.2) & 13 (4.3) & 7.81 (1.4) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 282.05 (134.8) & 180.64 (47.6) & 181.49 (48.6) \\
& & & cpl (sd) & 20.37 (6.3) & 13.68 (4) & 14.05 (4) \\
 &  &HU &MSE (sd) & 282.05 (134.8) & 180.84 (47.5) & 181.49 (48.6) \\
& & & cpl (sd) & 20.37 (6.3) & 14.03 (4) & 14.05 (4) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 308.13 (142.9) & 129.39 (19.6) & 122.71 (17.8) \\
& & & cpl (sd) & 22.94 (7.7) & 9.37 (2.1) & 7.91 (1.4) \\
 &  &HU &MSE (sd) & 308.13 (142.9) & 159.14 (30.4) & 122.71 (17.8) \\
& & &cpl (sd) & 22.94 (7.7) & 15.42 (4) & 7.91 (1.4) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U& MSE (sd) & 150.14 (19.7) & 127.48 (14) & 128.21 (15) \\
& & &  cpl (sd) & 25.6 (5.4) & 14.73 (3.8) & 14.73 (3.9) \\
 &  &HU &  MSE (sd) & 150.14 (19.7) & 128.21 (14.9) & 128.21 (15) \\
& & & cpl (sd) & 25.6 (5.4) & 14.7 (3.9) & 14.73 (3.9) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& MSE (sd) & 157.18 (23.5) & 110.24 (8) & 109.24 (7.8) \\
& & &  cpl (sd) & 30.41 (6.7) & 8.96 (1.3) & 8.3 (1) \\
 &  &HU &  MSE (sd) & 157.18 (23.5) & 123.8 (12.5) & 109.24 (7.8) \\
& & & cpl (sd) & 30.41 (6.7) & 17.18 (3.3) & 8.3 (1) \\
\hline
\end{tabular} 
\caption{Elasticnet (with Elasticnet) combined with constrained \textsc{CorReg}.$\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X_2}$.}\label{YX2linenet}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$&  $P(S)$ &indicator &Stepwise  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_49_58.csv
30 & 16 & U&MSE (sd) & 394.59 (454.6) & 341.47 (153.2) & 351.14 (148) \\
& & & cpl (sd) & 13.09 (3.2) & 12.9 (2.9) & 12.92 (3) \\
 &  &HU &MSE (sd) & 394.59 (454.6) & 353.99 (157) & 351.14 (148) \\
& & & cpl (sd) & 13.09 (3.2) & 12.83 (2.9) & 12.92 (3) \\
\hline %global30p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_50_41.csv
30 & 32 & U & MSE (sd) & 410.11 (343.5) & 163.05 (45.4) & 152.08 (38.1) \\
& & & cpl (sd) & 15.8 (6.5) & 8.29 (2.2) & 7.34 (1.3) \\
 &  & HU & MSE (sd) & 410.11 (343.5) & 178.94 (69.7) & 152.08 (38.1) \\
& & & cpl (sd) & 15.8 (6.5) & 8.89 (3) & 7.34 (1.3) \\
\hline
\hline %global50p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_50_48.csv
50 & 16 & U&	MSE (sd) & 180.67 (52.7) & 173.02 (38.3) & 174.84 (36.8) \\
& & & cpl (sd) & 13.56 (3.7) & 12.78 (2.4) & 13.07 (2.4) \\
 &  &HU &MSE (sd) & 180.67 (52.7) & 175.3 (38.1) & 174.84 (36.8) \\
& & & cpl (sd) & 13.56 (3.7) & 13.01 (2.4) & 13.07 (2.4) \\
\hline %global50p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_23_49_39.csv
50 & 32 & U&MSE (sd) & 188.91 (88.5) & 124.38 (17.4) & 122.38 (16.7) \\
& & & cpl (sd) & 14.74 (6.6) & 8.07 (1.6) & 7.65 (1.2) \\
 &  &HU &MSE (sd) & 188.91 (88.5) & 134.22 (23.6) & 122.38 (16.7) \\
& & &cpl (sd) & 14.74 (6.6) & 9.5 (2.8) & 7.65 (1.2) \\
\hline
\hline %global100p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_33_40.csv
100 & 16 & U&  MSE (sd) & 128.18 (13.7) & 127.43 (13.7) & 127.71 (13.3) \\
& & &  cpl (sd) & 13.78 (2.8) & 13.51 (2) & 13.59 (2.1) \\
 &  &HU & MSE (sd) & 128.18 (13.7) & 127.71 (13.3) & 127.71 (13.3) \\
& & & cpl (sd) & 13.78 (2.8) & 13.59 (2.1) & 13.59 (2.1) \\
\hline % global100p40ratio0.8BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_13mars2014_04_10_09.csv
100 & 32 & U& MSE (sd) & 126.15 (20.6) & 109.47 (8) & 109.04 (7.8) \\
& & & cpl (sd) & 13.25 (4.4) & 8.3 (1) & 8.12 (0.8) \\
 &  &HU &  MSE (sd) & 126.15 (20.6) & 113.96 (11.4) & 109.04 (7.8) \\
& & & cpl (sd) & 13.25 (4.4) & 10.08 (2.6) & 8.12 (0.8) \\
\hline
\end{tabular} 
\caption{Stepwise  combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X_r}$.}\label{YX2linstep}
\end{table}

\clearpage
\subsection{Robustess of the model}
 We have generated non linear structures (Tables \ref{compZnonlin} and \ref{resYnonlin}). Variables in $\boldsymbol{X_r}$ depends on the $\log$ or the square (randomly choosen with equiprobability) of a variable in $\boldsymbol{X_f}$ . Dependencies are still real but non linear. \textsc{CorReg} still found dependencies. One example of non-linear structure found with \textsc{CorReg} on real datasets is given in section \ref{sectionexfos}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c}{configuration}  &  \multicolumn{2}{|c}{Computing Time}  & \multicolumn{3}{|c}{Quality} & \multicolumn{2}{|c|}{Complexity}\\
\hline
$n$ & $p_r$ & $P(S)$&Time Mixmod  & Time MCMC  & $TL$ & $WL$ & $ML$ & $\Delta p_2$ & $\Delta compl$ \\
\hline
30 & 16 & U& 0.4313 & 2.7769 & 9.96 & 2.45 & 5.95 & 3.5 & 29.42  \\
& & &(0.0391) & (0.0883) & (1.3175) & (1.3808) & (1.3056) & (1.453) & (6.2639) \\
 &  & HU & 0.4313 & 4.9079 & 7.28 & 1.25 & 8.63 & 7.38 & 5.77  \\ 
& & &(0.0391) & (0.4353) & (1.5769) & (0.9987) & (1.5548) & (1.6316) & (6.1297) \\ 
\hline
\end{tabular} 
\caption{Results of the Markov chain for non linear structure ($\log$ and square). Mean observed and standard deviation (sd). } \label{compZnonlin}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
$n$ & $p_r$& $P(S)$ &indicator &LASSO  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
\hline %nonlin0.5global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_55_33.csv
30 & 0 & U&MSE (sd) & 996.06 (323.7) & 1100.17 (391.4) & 1501.25 (905.9) \\
& & &cpl (sd) & 17.66 (5.4) & 14.7 (4.7) & 11.97 (4.9) \\
 &  &HU &MSE (sd) & 996.06 (323.7) & 1094.5 (621.8) & 1501.25 (905.9) \\
& & & cpl (sd) & 17.66 (5.4) & 16.98 (4.8) & 11.97 (4.9) \\ 
\hline
\end{tabular} 
\caption{ $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} is not too far from LASSO even if it stays behind. } \label{resYnonlin}
\end{table}


	\clearpage	
\section{Numerical results on real datasets} \label{sectionrealcase}
\subsection{Quality case study} \label{sectionexfos}
This work takes place in steel industry context, with quality oriented objective : to understand and prevent quality problems on finished product, knowing the whole process. 
		
We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item 205 variables from the whole process to explain it.
			\item The stakes : a hundred euros per ton (for information: Dunkerque's  site aims to produce up to 7.5 millions tons a year)
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/mixmod.png} 
			\caption{Example of non-Gaussian real variable easily modeled by a Gaussian mixture}\label{graphMixmod}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/histR2exfos.png} 
			\caption{$R^2_{adj}$ of the 76 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/correlexfoshist.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEexfos}
   \end{minipage}
\end{figure}   			
	We get a training set of $n=3 000$ products described by $p=205$ variables from the industrial process and a validation sample of $847$ products.
	Let's note $\rho$ the absolute value of correlations between two covariates. Industrial variables are naturally highly correlated as the width and the weight of a steel slab ($\rho=0.905$), the temperature before and after some tool ($\rho=0.983$), the  roughness of both faces of the product ($\rho= 0.919$), a mean and a max ($\rho=0.911$). \textsc{CorReg} also found more complex structures describing physical models, like   Width = f (Mean.flow , Mean.speed.CC) even if the true Physcial model is not linear : Width = flow / (speed * thickness) (here thickness is constant). Non linear regulation models used to optimize the process were also found (but are confidential). These first results are easily understandable and meet metallurgists expertise.  
			The algorithm gives a structure of $p_r=76$ subregressions with a mean of $\bar{\boldsymbol{p}}_f=5.17$ regressors. In $\boldsymbol{X}_f$ the number of $\rho>0.7$ is $\textbf{79.33\%}$ smaller than in $\boldsymbol{X}$.		
	
			It is now time to look at the predictive results (Figure \ref{compareMSEexfos}).
				The best model found when not using \textsc{CorReg} is given by the LASSO. But when using \textsc{CorReg} elasticnet produces a better model in terms of prediction. LASSO gives a model with 21 non-zero coefficients and elasticnet with \textsc{CorReg} gives a model with 40 non-zero parameters but $6.40\%$ better in prediction on the validation sample (847 products). $14$ non-zero coefficients are common between the two models.
				Elasticnet alone get a model with 78 parameters that is improved by $9.75\%$ in prediction when used with \textsc{CorReg}. When using LASSO with \textsc{CorReg} we obtain a model with 24 non-zero coefficients that is $4.11\%$ better than LASSO alone. We also computed the OLS model (without selection) and the naive one (estimating the response by the mean of the learning set). All the MSE were modified here to obtain a value of 100 for the best (to preserve confidentiality). Elasticnet with \textsc{CorReg} is $13.51\%$ better than OLS.
		\begin{figure}[h]
			\centering
				\label{barplotMSEexfos}
				\includegraphics[width=400px]{figures/MSEfinal.png}
			\caption{MSE comparison on industrial dataset. Learning set : 3 000 products, validation set : 847 products}
		\end{figure}		
		\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	OLS & 115.63 & 206 \\ 
	\hline 
	\textsc{CorReg} + OLS & 109.59&130 \\ 
	\hline  
	LASSO & 106.84 & 21 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 102.45 &24 \\ 
	\hline 
	elasticnet & 110.81 & 78\\ 
	\hline 
	\textsc{CorReg} + elasticnet & 100 &40 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained on a validation sample.}	
\end{table}

		In terms of interpretation, the main regression comes with the family of regression so it gives a better understanding of the consequences of corrective actions on the whole process. It typically permits to determine the \textit{tuning parameters} whereas LASSO would point variables we can't directly act on.	So it becomes easier to take corrective actions on the process to reach the goal. The stakes are so important that even a little improvement leads to consequent benefits, and we don't even talk of the impact on the market shares that is even more important.
		\clearpage
		
		\subsection{Production case study}
This second example is about a phenomenon that impacts the productivity of a steel plan.
We have :
		\begin{itemize}
			\item a (confidential)  response variable,
			\item $p=145$ variables from the whole process to explain it but only $n=100$ individuals.
			\item The stakes : $20\%$ of productivity to gain
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[width=150px]{figures/correlbeforeafter.png} 
			\caption{Correlations between the covariates in $\boldsymbol{X}$ (upper) and $\hat{\boldsymbol{X}}_f$ (lower).}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/R2corregBVBI.png} 
			\caption{$R^2_{adj}$ of the 67 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/histcorrelBVBI.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} 
   \end{minipage}
\end{figure}   			
	Here $n<p$ so we only compare the leave-one-out cross-validation MSE.
	\textsc{CorReg} improves LASSO by $5.24\%$ and elasticnet by $8.60\%$. \textsc{CorReg} combined with LASSO gives the best result but it is only a leave-on-out MSE.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	LASSO & 105.54 & 34 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 100 & 18 \\ 
	\hline 
	elasticnet & 129.94 & 13 \\ 
	\hline 
	\textsc{CorReg} + elasticnet & 118.76 & 21 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained with leave-one out cross-validation. $n=100, p=145$.}	
\end{table}
In this precise case, \textsc{CorReg} found a structure that helped to decorrelate covariates in interpretation and to find the relevant part of the process to optimize.


\section{Conclusion and perspectives} \label{conclusion}
	We have seen that correlations can lead to serious estimation and variable selection problems in linear regression and that in such a context, it can be useful to explicitly model the structure between the covariates and to use this structure (even sequentially) to avoid correlations issues. We also show that real industrial context faces this kind of situations so our model can help to interprete and predict physical phenomenon efficiently and to help to manage missing values. But for now we still need a full dataset to learn the structure between the covariates and even if correlations are strong, some information is lost. Further work is needed to face these two challenges.
	
	\textsc{CorReg} is accessible on CRAN and has already proved its efficiency on real regression problematics in industry. \textsc{CorReg}'s strength is its great interpretability of the model, composed of several short linear regression easily managed by non-statisticians while strongly reducing correlations issues that are everywhere in industry.
	Nevertheless, we need to enlarge its application field to missing values, also very commons in industry. The actual generative model allows such a functionality without supplementary hypothesis and this also is a strength of \textsc{CorReg}. 
	
	Another perspective would be to take back lost information (the residual of each sub-regression) to improve predictive efficiency when needed. It would only consists in a second step of linear regression between the residuals and would thus still be able to use any selection method.
	
	This paper only treats linear regression but such a pretreatment could be used for logistic regression, {\it etc.}
	So the subject is still wide opened.	

\section{Acknowledgements}
	We want to thanks ArcelorMittal Atlantique \& Lorraine that has granted this work, given the chance to use \textsc{CorReg} on real dataset and authorized the package to be open-sourced licensed (\textsc{CeCILL}), especially Dunkerque's site where most of the work has been done.
\bibliography{biblio}{}
\bibliographystyle{plain}
\section{Appendices}
	\subsection{Identifiability of the structure}
	The model presented above relies on a discrete structure $S$ between the covariates. But to find it we need identifiability property to insure the MCMC will asymptotically find the true model. Identifiability of the discrete structure is asked in following terms: Is it possible to find another structure $\tilde{S}$ of linear regression between the covariates leading to the same joint distribution and marginal distributions? The answer is no. Thus $S$ is identifiable.
	
	
	If there are exact regressions ($\sigma^2_j=0$) in (\ref{SR}), the structure won't be identifiable. But it only means that several structure will have the same likelihood and they will have the same interpretation. So it's not really a problem. Moreover, when an exact sub-regression is found, we can delete one of the implied variables without any loss of information and the structure will define a list of variable from which to delete. %\textsc{CorReg} (Our R package) prints a warning to point out exact regressions when found.
	In the followings we suppose $\sigma^2_j\neq 0$.
	
	\subsection{The \textsc{CorReg} package}
\subsubsection{Alternative neighbourhoods for the MCMC}
	We have here at each step $|\mathcal{V}_{S,j}|=p$ candidates but some other constraints can be added on the definition of $\mathcal{S}$ and will consequently modify the size of the neighbourhood (for example a maximum complexity for the internal regressions or the whole structure, a maximum number of internal regressions, {\it etc.}). \textsc{CorReg} allows to modify this neighbourhood to better fit users constraints. Relaxation (column-wise and row-wise) is optional but gives more stability to the number of feasible candidates at each step and allows to modify several parts of $I_f$ in only one step when needed. Hence it improves efficiency by a significant reinforcement of the irreductibility of the Markov chain. Rejecting candidates instead of doing the relaxation steps will  however reduce the number of evaluated candidates and thus accelerate the walk. So it can be used for a warming phase when $n$ is great and time is missing.
	
	The hierchical uniform hypothesis made above for $P(S)$ implies $p_r<\frac{p}{2}$ and $p_f^j<\frac{p}{2}$ so candidates may be rejected to satisfy this hypothesis. Stronger constraints on $p_r$ and/or $p_f^j$ can be given in \textsc{CorReg} if relevant.
	
If the algorithm did not have time to converge (stationnarity), it can be continued with a few step for which the neighboorhood would only contain smaller candidates (in terms of complexity). It is equivalent to ask for each element in $I_f$ if the criterion $P(S|\boldsymbol{X})$ would be better without it. Thus it can be seen as a final cleaning step. But in fact, it's just continuing the MCMC with a reduced neighbourhood.	
	
	
	
\end{document}