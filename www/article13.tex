\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[table]{xcolor}
 \graphicspath{{figures/}}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%%\author{Clément THERY, Christophe BIERNACKI, Gaétan LORIDANT}
%\title{Model-based variable selection for regression with highly correlated variables.}

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%


\definecolor{darkgreen}{rgb}{0,0.4,0}
	 \definecolor{darkred}{rgb}{0.75,0,0}
	 \definecolor{darkblue}{rgb}{0,0,0.4}

\begin{document}
\begin{center}
{\Large
	{\sc Model-based variable selection in regression \\ with highly correlated variables.}
}
\bigskip

  Clément Théry$^{1}$ \& Christophe Biernacki$^{2}$ \& Gaétan Loridant$^{3}$
\bigskip

{\it
$^{1}$ ArcelorMittal, Université Lille 1, Inria, CNRS, clement.thery@arcelormittal.com
 
$^{2}$ Université Lille 1, Inria, CNRS, christophe.biernacki@math.univ-lille1.fr

$^{3}$ Etudes Industrielles ArcelorMittal Dunkerque, gaetan.loridant@arcelormittal.com\textbf{}
}
\end{center}
\bigskip

{\bf Abstract.} Linear regression outcomes are known to be damaged by highly correlated covariates. However many modern datasets are expected to convey more and more highly correlated covariates due to the global increase of the amount of variables in datasets. We propose to explicitly model the correlations by a family of linear regressions between the covariates, some covariates explaining others. It allows then to obtain by marginalisation on the explained covariates a parsimonious correlation-free regression model. 
It corresponds to a kind of variable selection preliminary step which has then to be followed by standard linear estimation methods including classical variables selection procedures for instance. The structure of correlations is found with an MCMC algorithm aiming at optimizing a specific BIC criterion.
 An R package (\textsc{CorReg}) available on the CRAN implements this new method which will be illustrated on both simulated datasets and real-life datasets from steel industry where correlated variables are frequent.
\smallskip

{\bf Keywords.} Regression, correlations, industry, variable selection, generative models

\section{Introduction}
%la régression et ses problèmes
Linear regression is a very standard and efficient method providing a predictive model with a good interpretability even  for non-statisticians. Therefore, it is used in nearly all the fields where statistics are made \cite{montgomery2012introduction},  industry (illustrated in the present paper), astronomy \cite{isobe1990linear}, sociology \cite{longford2012revision} \dots
%However, linear regression has to face the problem of the variance of the estimators.
%Linear regression is a very classic situation but sometimes it has to face an also classical problem: the variance of the estimators. 
%We note the linear regression model:
%\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \label{regressionsimple}
%	\end{equation}
%	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables, $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $p\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{OLS}). %As shown in section \ref{sectionOLS}, 
%	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
%Conditionning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects : the dimension $p$ (number of covariates) of the model (the more covariates you have the greater variance you get)
%	 and the correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .

	With the rise of informatics, datasets contain more and more covariates, % and thus dimension reduction becomes a necessity. Moreover, 
	increasing the chance to have correlated ones.  
		Many estimators rely on the inversion of matrix that will be ill-conditioned in such a context, increasing the variance of these estimators and leading to misleading interpretations and reduced prediction efficiency.
		%whereas interpretation and prediction are both strongly needed, depending on the context. For instance, interpretation could be favoured to improve a situation instead of only predict problems, so robust estimation is a real stake.
		~\\	~\\
		
%bibliographie	
	We know the minimum-variance linear unbiased estimator for linear regression that is the Ordinary Least Squares (\textsc{OLS}) estimator but it suffers from ill-conditioned matrices inversion when the covariates are highly correlated (section \ref{sectionOLS}). 
	Many traditional try to reduce the variance introducing some bias to improve the bias-variance tradeoff and get better prediction by selection of only some covariates, grouping of some covariates, {\it etc.}


%In the following we note classical norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$ and $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^p|\beta_i| $.

	 Ridge regression \cite{marquardt1975ridge} proposes a biased estimator that can be written in terms of a parametric $L_2$ penalty but
	it is the same for each covariate no matter how much it is correlated or not to others.%and will be too large for independent covariates and/or too small for correlated ones.  
	Moreover, coefficients tend to 0 but don't reach 0 so it gives difficult interpretations for large number of covariates. The absence of variable selection is not compatible with the need to find small set of relevant covariates to explain the response variable. 
	
	Real datasets often imply many irrelevant variables so variable selection should be favoured when possible to reduce the dimension and obtain models small enough to be understood. Moreover, selection may keep only uncorrelated covariates and thus resolve the correlations problem.	Variable selection methods may add some bias by deleting some relevant covariates but reduce the variance by the dimension reduction.
	The Least Absolute Shrinkage and Selection Operator (\textsc{LASSO} \cite{tibshirani1996regression}) consists in a shrinkage of the regression coefficients based on a parametric $L_1$ penalty to shrink some coefficients exactly to zero.
%	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) algorithm offers a very efficient way to obtain the whole LASSO path and is very attractive. It requires only the same order of magnitude of computational effort as \textsc{OLS} applied to the full set of covariates. 
But like the ridge regression, the penalty does not distinguish correlated and independent covariates so there is no guarantee to have less correlated covariates. It only produces a parsimonious model, that is a gain for interpretation but only half the way.
%	 Another limitation of the \textsc{LASSO} is that it preserves at most $n$ predictors (troublesome when in high dimension). 
	% Some recent variants of the \textsc{LASSO} do exist for the choice of the penalization coefficient like the adaptative \textsc{LASSO} \cite{zou2006adaptive} or the random \textsc{LASSO} \cite{wang2011random}.  But
	 Indeed, \textsc{LASSO} is also known to face consistency problems \cite{Zhao2006MSC} when confronted with correlated covariates. So the quality of interpretation is compromised.
	 \\
	 
	Elastic net \cite{zou2005regularization} is a method developed to be a compromise between Ridge regression and the \textsc{LASSO} by a linear combination of $L_1$ and $L_2$ penalties.
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	%Elastic net can be written:
%	\begin{equation}
%		\boldsymbol{\hat{\beta}}=(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta} \parallel_2^2 \right\rbrace, \textrm{ subject to } (1-\alpha)\parallel\boldsymbol{\beta}\parallel_1+\alpha\parallel\boldsymbol{\beta}\parallel_2^2\leq t \textrm{ for some } t
%	\end{equation}
%	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	%It seems to give good predictions. 
	But it is based on the grouping effect so correlated covariates get similar coefficients and are selected together whereas LASSO will choose between one of them and will then obtain similar predictions with a more parsimonious model. Once again, nothing specifically aims to reduce the correlations. Interpretation will be misleading in both LASSO and Elastic net cases because nothing differentiates correlated and uncorrelated covariates.
	%Hence, when comparing the two models, interpretations are not the same and nothing explicitly explains why. So it can be very confusing. 
	\\
	
	Another way of improving the conditioning and the understandability is to consider clusters of variables with the same coefficients, like the Octogonal Shrinkage and Clustering Algorithm for Regression (\textsc{OSCAR} \cite{bondell2008simultaneous}) to reduce the dimension and also the correlations if correlated covariates are in the same clusters. The bias is added by the dimension reduction inherent to the coefficients clustering.
	The CLusterwise Effect REgression (\textsc{CLERE} \cite{yengo2012variable}) describes the regression coefficients no longer as fixed effect parameters but as unobserved independent random variables with grouped coefficients following a Gaussian Mixture distribution. The idea is that if the model have a small number of groups of covariates and the mixture have few enough components the model will have a number of parameters to estimate significantly lower than the number of covariates. In such a case, it improves interpretability and ability to yeld reliable prediction with a smaller variance on the coefficients estimator. 
	
	But it requires to suppose having many covariates with the same level of effect on the response variable and seems to stay less efficient in prediction than elastic net.
	 Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the regression coefficients and gives a subset of covariates (not grouped) on which to compute \textsc{OLS} but has no specific protection against correlations issues. It is again a probable increase of the bias by variable selection to reduce the variance by dimension reduction.
	  %\textsc{SPRING} (Structured selection of Primordial Relationships IN the General linear model \cite{chiquetconf}) takes into account correlations between endogenous covariates but without explicit expression of the correlations and thus a reduced interpretation potential.
	We see that none of the above methods takes explicitly the correlations into account, even if the clustering methods may group the correlated covariates together.
	These methods are not directly guided by the correlations whereas knowing the correlations between the covariates facilitates interpretation.


	~\\	~\\
% Principe de la méthode
Outside of the linear regression field, model-based explicit correlations by linear sub-regressions between covariates  claims  to obtain good results in both interpretation and estimation quality  \cite{maugis2009variable}. But this is made in a clustering context only, with only irrelevant covariates being dependent from relevant ones (no sub-regressions between relevant or between irrelevant covariates) and the algorithm used to find the structure is a stepwise-like algorithm without protection against correlations \cite{raftery2006variable} even if it is known to be often unstable \cite{miller2002subset}. We propose to transpose this method for linear regression with a specifically adapted algorithm to find the structure of sub-regression.
 
  The idea is that if we know explicitly the correlations, we could use this knowledge to avoid the problem. 
	We use the correlations as new information to reduce the variance without adding any bias.  
  More precisely, we model the correlations with a system of linear sub-regressions for the joint distribution of the covariates to model the structure of the correlations. It helps to define the greatest set of orthogonal covariates to keep the maximum information but with an orthogonality constraint.
Then we can define a marginal model with independent covariates. 
Thus we have an explained variable selection guided by the correlations and it improves interpretation, not only prediction. This marginal model still is the true model but in a different probability space, and OLS still gives an estimator without bias but with a reduced variance for the estimators. The model depends only on independent covariates because it is a marginal model on these covariates.
 This can be viewed as a pretreatment on the dataset that will be followed then by any other tools for estimation and dimension reduction without suffering from correlations. This pretreatment is specifically done to decorrelate the covariates, unlike methods described above. 
The linear structure is obtained by a MCMC algorithm optimizing the penalized likelihood of the joint distribution on the covariates, independently from the response variable. This algorithm is part of the R package \textsc{CorReg} accessible on \textsc{CRAN}. %We only consider strong correlations (i.e. : problematic ones) thus we keep most of the information contained in the dataset. 
 %We will in a second time be able to use the remaining part of the information (sequential approach).	
 
	
 	
 	%plan
 	This paper will first present the linear modelisation of the correlations and the by-product marginal regression model before describing in Section 3 the random walk used to find the structure.
 	We will then look at some numerical results on simulated (Section \ref{sectionsimul}) and real industrial datasets (Section \ref{sectionrealcase}) before concluding and giving some perspectives in Section \ref{conclusion}.
	
\section{Model to select decorrelated covariates}
\subsection{A classical problem: correlations in regression}\label{sectionOLS}
We note the linear regression model:
\begin{equation}
		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \label{regressionsimple}
	\end{equation}
	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables (that is a sub-matrix of $\tilde{\boldsymbol{X}}$ the $n\times \tilde{p}$ matrix of provided covariates), $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $p\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{OLS}): %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1} \label{eqOLS}
	\end{equation}
	and without any bias.
	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
Conditionning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects: the dimension $p$ (number of covariates) of the model (the more covariates you have the greater variance you get)
	 and the correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .
	When correlations between covariates are strong, the matrix to invert is ill-conditioned and the variance increases, giving unstable and unusable estimator \cite{hoerl1970ridge}.
	Another problem is that matrix inversion requires $n\geq p$. 	
\paragraph{Running example:} we look at a simple case with $p=5$ variables defined by four independent scaled Gaussian $\mathcal{N}(0,1)$ named $\boldsymbol{x}_1,\boldsymbol{x}_2$ and $\boldsymbol{x}_3=\boldsymbol{x}_1+\boldsymbol{x}_2+\boldsymbol{\varepsilon}_3$ where $\boldsymbol{\varepsilon}_3\sim{\mathcal{N}(\boldsymbol{0},\sigma_3^2\boldsymbol{I}_n)}$. We also define another couple $\boldsymbol{x}_4,\boldsymbol{x}_5$ of covariates that are {\it i.i.d. } with $(\boldsymbol{x}_1,\boldsymbol{x}_2)$ and two {\it scenarii} for $\boldsymbol{Y}$ with $\boldsymbol{\beta}=(1,1,1,1,1)$ and $\sigma_Y \in \{10,20\}$ .
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_3$ gets smaller.
	
	
\subsection{Our proposal: modelisation of the correlations}
We make the hypothesis that $\boldsymbol{X}$ can be described by a partition $\boldsymbol{X}=(\boldsymbol{X}_f,\boldsymbol{X}_r) $ given by an explicit structure $S$ where variables in $\boldsymbol{X}_r$ are endogenous covariates resulting from linear sub-regressions based on $\boldsymbol{X}_f$, the submatrix of mutually independent exogenous covariates.
So we model the correlations by $P(\boldsymbol{X}_r|\boldsymbol{X}_f) $ with $\boldsymbol{X}_f$ orthogonals.
 Then $\boldsymbol{X}_r$ is the $n\times p_r$ submatrix of $0\leq p_r <p$ redundent covariates and $\boldsymbol{X}_f$ the $n\times (p-p_r)$ submatrix of the free (independent) covariates.
 
 
 
In the following, we note $\boldsymbol{X}^j$ the $j^{th}$ column of $\boldsymbol{X}$.
The structure $S$ of $p_r$ regressions within correlated covariates in $\boldsymbol{X}$ is described by:
	\begin{equation}
		\boldsymbol{X}_{r|\boldsymbol{X}_f,S} \textrm{ defined by }\forall \boldsymbol{X}^j \subset \boldsymbol{X}_r: \boldsymbol{X}^j_{|\boldsymbol{X}_f,S}=\boldsymbol{X}_f\boldsymbol{\alpha}_j+\boldsymbol{\varepsilon}_j \textrm{ with } \boldsymbol{\varepsilon}_j \sim\mathcal{N}(\boldsymbol{0},\sigma^2_j\boldsymbol{I}_n) \label{SR}
	\end{equation}
		where $\boldsymbol{\alpha}_j \in \mathcal{R}^{(p-p_r)}$ are the sparse vectors of the regression coefficients between the covariates (each sub-regression freely implies different covariates). 
\\
\\


The partition of $\boldsymbol{X}$ implies the uncrossing rule  $\boldsymbol{X}_r \cap \boldsymbol{X}_f$ 
{\it i.e.} endogenous variables don't explain other covariates. This hypothesis ensures that $S$ contains no cycle and is straightforward readable (no need to order the sub-regressions). It is not so restrictive because cyclic structures have no sense and any non-cyclic structure can be associated with a structure that verifies the uncrossing constraint by just successively replacing endogenous covariates by their sub-regression when they are also exogenous in some other sub-regressions.

	
	  We make the choice to distinguish the response variable from the other endogenous variables (that are on the left of a sub-regression). Thus we have one regression on the response variable ($P(\boldsymbol{Y}|\boldsymbol{X}))$ and a system of sub-regressions (without the response variable: $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$). Then we consider correlations between the explicative covariates of the main regression, not between the residuals. We see that the $S$ does not depend on $\boldsymbol{Y}$ so it can be learnt independently, even with a larger dataset (if missing values in $\boldsymbol{Y}$).
	 
The structure obtained gives a system of linear regression that can be viewed as a recursive Simultaneous Equation Model (\textsc{SEM})\cite{davidson1993estimation} \cite{TIMM}.
  	Here we suppose the $\boldsymbol{\varepsilon}_j$ independent but in other cases \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) takes into account correlations between residuals \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) and could be used to estimate the $\boldsymbol{\alpha}_j$. 
		 
	 
\paragraph{In the running example:}$\boldsymbol{X}_r=\boldsymbol{x}_3$, $\boldsymbol{X}_f=\{\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_4,\boldsymbol{x}_5 \}$, $p_r=1$ and $\boldsymbol{\alpha}_3=(1,1,0,0)'$

	

\subsection{A by-product model: marginal regression with decorrelated covariates}
Now we know $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$ by the structure of sub-regressions, we are able to define a marginal regression model $P(\boldsymbol{Y}|\boldsymbol{X}_f,S)$ based on the reduced set of independent covariates $\hat{\boldsymbol{\beta}}_f$ without significant information loss. We use the information of the correlations structure to rewrite the true model without bias in the marginal space defined by the independent covariates.
 	\\
Using the partition $\boldsymbol{X}=[\boldsymbol{X}_f,\boldsymbol{X}_r]$ we can rewrite (\ref{regressionsimple}):
	\begin{equation}
			\boldsymbol{Y}_{|\boldsymbol{X}_f,\boldsymbol{X}_r,S}=\boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{\varepsilon_Y} \label{MainR}
		\end{equation}
		where $\boldsymbol{\beta}=(\boldsymbol{\beta}_f,\boldsymbol{\beta}_r) \in  \mathcal{R}^p$ is the vector of the regression coefficients associated respectively to $\boldsymbol{X}_f$ and $\boldsymbol{I}_n$ the identity matrix. 
We note that (\ref{SR}) and (\ref{MainR}) give also by simple integration on $\boldsymbol{X}_r$ a marginal regression model on $\boldsymbol{Y}$ {\it depending only on uncorrelated covariates $\boldsymbol{X}_f$}:
\begin{eqnarray}
	\boldsymbol{Y}_{|\boldsymbol{X}_f,S}&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j)+  \sum_{j \in I_r}\beta_{j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{Trueexpl} \\
	&=&\boldsymbol{X}_f\boldsymbol{\beta}_f^*+\boldsymbol{\varepsilon}_Y^*\label{modexpl}
\end{eqnarray}
 This model is still the true model and OLS estimator will still give an unbiased estimator, but its variance will be reduced by both dimension reduction and decorrelation (variables in $\boldsymbol{X}_f$ are independent so the matrix $\boldsymbol{X}_f'\boldsymbol{X}_f$ will be well-conditioned). So the information given by the structure $S$ allows to reduce the variance without adding bias, by simple marginalization.
\\
Nevertheless, to be able to compare the bias-variance tradeoff, we can see this model as a variable pre-selection independent of the response in $\boldsymbol{Y}_{|\boldsymbol{X}}$.
We note that it is simply a linear regression on some of the original covariates so we only made a pretreatment on the dataset by selecting $\boldsymbol{X}_f$ because of the correlations given by $S$. So we also get the model
\begin{equation}
\boldsymbol{Y}_{|\boldsymbol{X},S}=\boldsymbol{X}\boldsymbol{\beta}^*+\boldsymbol{\varepsilon}_Y^* \textrm{ where }\boldsymbol{\beta}^*=(\boldsymbol{\beta}_f^*,\boldsymbol{\beta}_r^*) \textrm{ and } \boldsymbol{\beta}_r^*=\boldsymbol{0}
\end{equation}
	for which OLS estimator of the coefficients may be biased.  

\paragraph{Running example:} $\boldsymbol{Y}_{|\boldsymbol{X}_f}= 2\boldsymbol{x}_1+2\boldsymbol{x}_2+\boldsymbol{x}_4+\boldsymbol{x}_5+\boldsymbol{\varepsilon}_3 +\boldsymbol{\varepsilon}_Y$
\subsection{Strategy of use: pretreatment before classical estimation/selection methods}\label{interpretation}

As a pretreatment, the model allows usage of any method in a second time to estimate $\boldsymbol{\beta}_f^*$, even with variable selection methods like LASSO or a best subset algorithm like stepwise \cite{seber2012linear}. However, we always have $\boldsymbol{X}_r=\boldsymbol{0}$

After selection and estimation we will obtain a model with { \it two steps of variable selection}: the decorrelation step by marginalization(coerced selection associated to redundant information defined in $S$) and the classical selection step, with different meanings for obtained zeros in $\hat{\boldsymbol{\beta}}^*_f$ (irrelevant covariates) and for $\hat{\boldsymbol{\beta}}^*_r=0$ (redundant information). 
 Thus we are able to distinguish the reasons of selection and consistency issues don't mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.


The explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset and to take better actions. Expert knowledge can even be added to the structure, physical models for example.

Moreover, the uncrossing constraint (partition of $\boldsymbol{X}$) guarantee to keep a simple structure easily interpretable (no cycles and no chain-effect) and straightforward readable.

	
			There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypothesis. We just play on the traditional bias-variance tradeoff.
			 
	\subsection{Illustration of the tradeoff conveyed by the pretreatment}	
	We compare the OLS estimator on $\boldsymbol{X}$ defined in section \ref{sectionOLS} with the estimator obtained by the pretreatment that is $\boldsymbol{X}_f$ selection.
  
For the marginal regression model defined in (\ref{modexpl})
%	\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}_f}= \boldsymbol{X}_f\boldsymbol{\beta}_f^*+ \boldsymbol{\varepsilon}_Y^*
%	\end{equation}			
%		So 
we have the \textsc{OLS} unbiased estimator of $\boldsymbol{\beta}^*$: 
		\begin{equation}
			\hat{\boldsymbol{\beta}}_f^* = (\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{Y}  \textrm{ and }\boldsymbol{\hat\beta}_r^* = \boldsymbol{0}
		\end{equation}
		We see in (\ref{Trueexpl}) that it gives an unbiased estimation of $\boldsymbol{Y}$ and $\boldsymbol{\beta^*}$
		but in terms of $\boldsymbol{\beta}$ this estimator is biased:
		\begin{equation}
			\operatorname{E}[\hat{\boldsymbol{\beta}}_f^*|\boldsymbol{X}_f]=\boldsymbol{\beta}_f+\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \textrm{ and }\operatorname{E}[\hat{\boldsymbol{\beta}}_r^*|\boldsymbol{X}_f]=\boldsymbol{0}
		\end{equation}
		with variance:
		\begin{equation}
			\operatorname{Var}[\hat{\boldsymbol{\beta}}_f^*|\boldsymbol{X}_f]= (\sigma^2_Y+\sum_{j \in I_r}\sigma^2_{j}\beta_{j}^2 )(\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1}  \textrm{ and }\operatorname{Var}[\hat{\boldsymbol{\beta}}_r^*|\boldsymbol{X}_f]= \boldsymbol{0} 
		\end{equation}
		We see that the variance is reduced compared to OLS described in equation (\ref{eqOLS})(no correlations and smaller matrix give better conditioning ) for small values of $\sigma_j$ $i.e.$ strong correlations. So we play on the bias-variance tradeoff, reducing the variance by adding a bias. 				  
		  
		  
	 The Mean Squared Error (\textsc{MSE}) on $\hat{\boldsymbol{\beta}}$ is:
	\begin{eqnarray}
		\textsc{MSE}(\hat{\boldsymbol{\beta}}|\boldsymbol{X})&=&\parallel \operatorname{Bias}\parallel_2^2+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}})) \\
			\textsc{MSE}(\hat{\boldsymbol{\beta}}_{OLS}|\boldsymbol{X})&=& 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}) %\textrm{ for OLS, and then for the marginal model:}
			 \\
			\textsc{MSE}(\hat{\boldsymbol{\beta}}^*_{OLS}|\boldsymbol{X})&=& \parallel\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \parallel_2^2 +\parallel \boldsymbol{\beta}_r\parallel^2_2 + (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}\beta_{j}^2 ) \operatorname{Tr}((\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1})
	\end{eqnarray}	 
	To better illustrate the bias-variance tradeoff, we look at the running example. We observe the theoretical Mean Squared Error (MSE) of the estimator of both OLS and \textsc{CorReg}'s marginal  model for several values of $\sigma_3$ (strength of the sub-regression) and $n$. Figure \ref{MQE1} shows the theoretical MSE evolution with the strength of the sub-regression:
	\begin{equation}
		1-\mathcal{R}^2=\frac{\operatorname{Var}(\boldsymbol{\varepsilon)_3}}{\operatorname{Var}(\boldsymbol{x}_3)}=\frac{\sigma_3^2}{\sigma_3^2+2}
	\end{equation}
	
\begin{figure}[h!]
%	\begin{minipage}[l]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn15sigmaY10.png} 
%			\caption{For $n=15$. Dotted: \textsc{Correg}, plain: OLS}\label{MQE1}
%	\end{minipage} \hfill
%	\begin{minipage}[c]{.32\linewidth}
%			\includegraphics[ width=170px]{figures/MQEn100sigmaY10.png} 
%			\caption{For $n=100$. Dotted: \textsc{Correg}, plain: OLS}
%	\end{minipage} \hfill
%   \begin{minipage}[r]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn1000sigmaY10.png} 
%			\caption{For $n=1000$. Dotted: \textsc{Correg}, plain: OLS.} \label{MQE3}
%   \end{minipage} 
	\includegraphics[width=500px]{figures/MQEexplOLSp5.png}\label{MQE1}
	\caption{MSE of OLS and CorReg (dotted) estimators for varying $(1-R^2)$ of the sub-regression, $n$ and $\sigma_Y$.}
\end{figure} 
It is clear in Figure \ref{MQE1} that the marginal model is more robust than \textsc{OLS} on $\boldsymbol{X}$. And when sub-regression get weaker ($1-\mathcal{R}^2$ tends to 1) it remains stable until extreme values (sub-regression nearly fully explained by the noise). We also see that the error implied by strong correlations shrinks with the rise of $n$. 
We see that $\sigma_Y$ multiplies $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}))=\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{f}))+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))$ for both models but for the marginal model $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))=0$.
 Thus, when $\sigma_Y^2$ rises it increases the advantage of \textsc{CorReg} versus \textsc{OLS}. It illustrates the importance of dimension reduction when the model has a strong noise (very usual case on real datasets where true model is not even exactly linear). Further results are provided in sections \ref{sectionsimul} and \ref{sectionrealcase}.

	
\section{Sub-regressions model selection}	
Structural equations models like \textsc{SEM} are often used in social sciences and economy where a structure is supposed "by hand" but here we want to find it automatically. Graphical LASSO \cite{friedman2008sparse} offers a method to obtain a structure based on the precision matrix (inverse of the variance-covariance matrix), setting some coefficients of the precision matrix to zero. But the resulting matrix is symmetric and we need an oriented structure for $S$ to avoid cycles.

Cross-validation is very time-consuming and thus not friendly with combinatory problematics. Moreover, we need a criterion compatible with structures of different sizes (varying $p_r$) and not related with $\boldsymbol{Y}$ because the structure is inherent to $\boldsymbol{X}$ only. Thus it must be a global criterion. 	
Because it is about model selection and we are able to provide a full generative model (section \ref{sectionfullgen}), we decide to follow a Bayesian approach (\cite{raftery1995bayesian}, \cite{andrieu1999joint},\cite{chipman2001practical}).  
	
We want to find the most probable structure $S$ knowing the dataset, so we search for the structure that maximizes $P(S|\boldsymbol{X})$ and we have:	
	\begin{eqnarray}
	 \label{approxBIC} P(S|\boldsymbol{X})&\propto & P(\boldsymbol{X}|S)P(S)
	=P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)P(\boldsymbol{X}_f|S)P(S)
	\end{eqnarray}
So we will try to maximize $\psi(\boldsymbol{X},S)=P(\boldsymbol{X}|S)P(S)$.
	

	\subsection{Modeling the uncorrelated covariates: a full generative approach on $P(\boldsymbol{X})$} \label{sectionfullgen}
	To be able to compare structures with $P(S|\boldsymbol{X})$, we need a full generative model on $\boldsymbol{X}$. Sub-regressions give $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S) $ but $P(\boldsymbol{X}_f|S)$ is still undefined. We suppose that variables in $\boldsymbol{X}_f$ follow Gaussian mixtures of $k_j \in \mathbf{N}^*$ components: 
	\begin{equation}
			\forall \boldsymbol{X}^j \notin \boldsymbol{X}_r : \boldsymbol{X}^j_{|S} \sim f(\boldsymbol{\theta}_j)=\mathcal{GM}(\boldsymbol{\pi}_j;\boldsymbol{\mu}_j;\boldsymbol{\sigma}^2_j) \textrm{ with } \boldsymbol{\pi}_j,\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j \textrm{ vectors of size } K_j. \label{mixtureX1}
		\end{equation}
		The great flexibility \cite{mclachlan2004finite} of such models makes our model more robust. Gaussian case is just a special case ($K_j=1$) of Gaussian mixture so it is included in our hypothesis but identifiability of $S$ requires to have Gaussian mixtures with at least two distinct components in each sub-regression (derived from the identifiability of the SR model in \cite{maugis2009variable}, more details in the Appendices \ref{preuveident}).
				
		Remark:  Identifiability of $S$ is not necessary to use a given structure but helps to find it.

		Variables in $\boldsymbol{X}$ are in the followings supposed to be independent Gaussian mixtures with at least two distinct components each. 
	%Thus if one have some hypothesis on the distribution of some variables (exponentially distributed for example) it is possible to use it without impacting the model in other ways. %compute corresponding $\psi$ according to it. %and then improve the walk (will keep a structure only if it is really relevant).%and give it as an input of \textsc{CorReg} and then improve the efficiency of the algorithm (it will find a structure only if it is really relevant).
	We now have a full generative model.
	
	\subsection{Penalization of the integrated likelihood by $P(S)$} \label{compstruct}

  Our full generative generative model allows us to compare structures with criterions like the Bayesian Information Criterion ($BIC$) which penalize the log-likelihood of the joint law on $\boldsymbol{X}$ according to the complexity of the structure~\cite{BIChuard}. 

Uniform law on $P(S)$ gives $\psi(\boldsymbol{X},S)\propto P(\boldsymbol{X}|S)$ so it is equivalent to a minimization of the $BIC$.
	We note $\boldsymbol{\Theta}$ the set of the parameters of the generative model
	\begin{eqnarray}
		-2\log P(\boldsymbol{X}|S)&\approx & BIC=-2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+|\boldsymbol{\Theta}|\log(n)  
	\end{eqnarray}
	But $BIC$ tends to give too complex structures because we test a great range of models. Thus we choose to penalise the complexity a bit more.
	
	We note $I_r$ the set of indices of endogenous variables in $\boldsymbol{X}$ (explained ones).
We also define $I_f=\{I_f^1,\dots,I_f^p \}$ the set of the sets of indices of exogenous covariates (explaining ones $=\boldsymbol{X}_f$) with $\forall j \notin I_r, I_f^j=\emptyset$. 
We see that $I_f$ defines the non-null coefficients in $\boldsymbol{\alpha}_j$ (each sub-regression can be very parsimonious).
Then we have the explicit structure characterized by $S=(I_f,I_r,p_f,p_r)$ where $p_r=|I_r|$, $\boldsymbol{p}_f=(p_f^1,\dots,p_f^{p_r})$ is the vector of the number of covariates in each sub-regression  and $p_f^j=|I_f^j|$, with $|.|$ the cardinal of an ensemble. Our running example is then described by $S= \left( \{ \{1,2\}\},\{3\},(2),(1)\right)$
\\
	 We suppose a hierarchical uniform {\it a priori} distribution $P(S)=P(I_f | \boldsymbol{p}_f,I_r,p_r)P(\boldsymbol{p}_f|I_r,p_r)P(I_r|p_r)P(p_r)$  instead of the simple uniform law on $S$ that is generally used and provides no penalty.
	 Thus we have :
		\begin{eqnarray}
		BIC_+(X|S)&=&BIC(X|S) -\ln(P(S)) \label{Bicstar}
	\end{eqnarray}		
	It increases penalty on complexity for $p_r\leq\frac{p}{2}$ and $p_f^j\leq\frac{p}{2}$ . Hence %when using $BIC*$ 
	this constraint on $\hat{p}_r$ and $\hat{p}_f^j$ is given in the research algorithm when the Hierarchical Uniform hypothesis is made instead of Uniform one in numerical experiments (section \ref{sectionsimul} and \ref{sectionrealcase}).
		$BIC_+$ does not change $BIC$ but only $P(S)$ so the properties of $BIC_+$ are the same as classical $BIC$ but we obtain better results when the constraints on the complexity are verified.  %With the Hierarchical Uniform hypothesis we maximize $\psi(\boldsymbol{X},S)\approx BIC + P(S)$.

%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	


\subsection{MCMC algorithm}
	Now we have a comparison criterion $\psi(\boldsymbol{X},S)$, we define an MCMC algorithm to find the structure (R package \textsc{CorReg} on CRAN). 
	\subsubsection{The neighbourhood}
	Let's define $\mathcal{S}$ the ensemble of feasible structures (those with $I_f\cap I_r=\emptyset$).
	\\
	For each step, starting from $S \in \mathcal{S}$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}_{S,j}&=& \{S \}\cup \{ S^{(i,j)} |1\leq i \leq p, i\neq j  \} \\
		\textrm{where }\ \ j &\sim & \mathcal{U}(\{1,\dots,p\}) 
	\end{eqnarray}	
	With $S^{(i,j)}$ defined by the following algorithm :
	\begin{itemize}
		\item if $i \notin I_f^j$ (add): 
			\begin{itemize}
				\item $I_f^j=I_f^j\cup \{i\}$
				\item $I_f^i=\emptyset$ (explicative variables can't depend on others : column-wise relaxation)
				\item $I_f=I_f \setminus \{j\}$ (dependent variables can't explain others : row-wise relaxation) 
			\end{itemize}			 
		\item else (remove): $I_f^j=I_f^j\setminus \{i\}$
	\end{itemize}
	
	\smallskip
	At every moment, coherence between $I_f$ and others parts of $S$ can be done by $\forall 1\leq j\leq p :  p_f^j=|I_f^j|$, $I_r=\{j |p_f^j>0 \}$, $p_r= |I_r|$ .
		
	\subsubsection{Transition probabilities}
	
	The walk follows a time-homogeneous Markov Chain whose transition matrix $\mathcal{P}$ has $|\mathcal{S}|$ rows and columns (combinatory so we just compute the probabilities when we need them).
	At each step the markov chain moves with probabiliy:
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=&\sum_{j=1}^p \mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S,j}\} }\frac{\exp(-\frac{1}{2}\psi(\boldsymbol{X},\tilde{S}))}{\sum_{S_l\in \mathcal{V}_{S,j}}\exp(-\frac{1}{2}\psi(\boldsymbol{X},S_l))} 
	\end{eqnarray}
	And $\mathcal{S}$ is a finite state space.%la relaxation rend P non symétrique mais ne remets  pas en cause l'homogénéité	
	 
Because the walk follows a regular and thus ergodic markov chain with a finite state space, it has exactly one stationary distribution \cite{grinstead1997introduction} %: $\pi$ and every rows of $\operatorname{lim}_{k\rightarrow \infty}\mathcal{P}^k=W$ equals $\pi$.
%	
%	
%	With $\forall S \in  \mathcal{S}$ :
%	\begin{eqnarray}		
%		0 \leq &\pi (S)& \leq 1 \nonumber \\
%		\sum_{S \in \mathcal{S}}\pi(S) &=&1 \nonumber \\
%		\pi (S) &=&\sum_{\tilde{S}\in \mathcal{S}} \pi(\tilde{S})\mathcal{P}(\tilde{S},S) \\%définition de la lois stationnaire
%	\end{eqnarray}
%		
and the output will be the best structure in terms of $P(S|\boldsymbol{X})$ which weights each candidate. Practically speaking, \textsc{CorReg} returns the best structure seen during the walk.
Numerical results (Section 4) illustrates the efficiency of the walk when the true model really contains a linear structure or no structure at all (Table (\ref{compZvrai})) and when the structure is not linear (Table \ref{compZnonlin})).

 \subsubsection{Initialisation(s)}
 If we have some knowledge about some sub-regressions (physical models for example) we can add them in the found and/or initial structure. So the model is really expert-friendly.
The initial structure can be based on a first warming algorithm taking the correlations into account. Coefficients are randomly placed into $I_f$, weighted by the absolute value of the correlations. We do so in the followings. Then this structure could be for example reduced by the Hadamard product with the binary matrix obtained by Graphical Lasso\cite{friedman2008sparse} that makes selection in the precision matrix but it is time consuming.

	One would rather test multiple short chains than lose time in initialisation or long chains \cite{gilks1996markov}. It also helps to face local extrema. In the followings, the chain wzs launched with twenty initialisations each time.
	
\section{Numerical results on simulated datasets} \label{sectionsimul}


	\subsection{The datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if \textsc{CorReg} 	keeps its promises.
	The \textsc{CorReg} package has been tested on simulated datasets. 
Section \ref{compZ} shows the results obtained in terms of $\hat{S}$. Sections \ref{tableMSEsimtout} and \ref{tableMSEsimgauche} show the results obtained using only \textsc{CorReg}, or \textsc{CorReg} combined with other methods. Tables give both mean and standard deviation of the observed Mean Squared Errors (MSE) on a validation sample of $1 000$ individuals. For each simulation,  $p=40$, the $R^2$ of the main regression is $0.4$, variables in $\boldsymbol{X}_f$ follow Gaussian mixture models of $\lambda=5$ classes which means follow Poisson's law of parameter $\lambda=5$ and which standard deviation is $\lambda$. The $\beta_j$ and the coefficients of the $\boldsymbol{\alpha}_j$ are generated according to the same Poisson law but with a random sign. $\forall j \in I_r, p_1^j=2$ (sub-regressions of length 2) and we have $p_r=16$ sub-regressions. The datasets were then scaled so that covariates $X_r$ don't have a greater variance or mean.
	We used \textsc{Rmixmod} to estimate the densities of each covariate. For each configuration, the MCMC walk was launched on $10$ initial structures with a maximum of 1 000 steps each time.
	When $n<p$, a frequently used method is the Moore-Penrose generalized inverse \cite{katsikis2008fast}, thus OLS can obtain some results even with $n<p$. %(see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
	When using penalized estimators for selection, a last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}) and is applied for both classical and marginal model.
	We compare different methods with and without CorReg as a pretreatment. All the results are provided by the CorReg package.
		\subsection{Finding the structure}
		\subsubsection{How to evaluate found structure?}
			The first criterion is $\psi(\boldsymbol{X},S)$ which is maximized in the MCMC. But in our case, it is estimated by the likelihood (see (\ref{approxBIC}))whose value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $S$ we define some indicators to compare the true model $S$ and the found one $\hat{S}$.
			Global indicators :
			\begin{itemize}
				\item $TL$ (True left) : the number of found dependent variables that really are dependent $TL=|I_r\cap \hat{I}_r|$ 
				\item $WL$ (Wrong left) : the number of found dependent variables that are not dependent $WL=|\hat{I}_r|-TL$
				\item $ML$ (Missing left) : the number of really dependent variables not found $ML=|I_r|-TL$
				\item $\Delta p_r$ : the gap between the number of sub-regression in both model : $\Delta p_r=|I_r|-|\hat{I}_r|$. The sign defines if $\hat{S}$ is too complex or too simple
				\item $\Delta compl$ : the difference in complexity between both model : $\Delta compl=\sum_{j \in p_r}p_f^j-\sum_{j \in \hat{p}_r}\hat{p}_f^j$
			\end{itemize}
		\subsubsection{Results on $S$}	\label{compZ}


\begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/BIC_p2.png} 
			\caption{Quality of the subregressions found with classical $BIC$ criterion}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/BICSTAR_P2.png} 
			\caption{Quality of the subregressions found with our $BIC_+$ criterion} 
   \end{minipage}
\end{figure}






\clearpage
\subsection{Results on prediction}

\subsubsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$ (best case for us)}	 \label{tableMSEsimgauche}
\begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X1_compl_NB.png} 
			\caption{Comparison of the complexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
 
%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X1_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X1_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X1_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
   \end{minipage}
\end{figure}

\clearpage
	\subsubsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	 	
We then try the method with a response depending on all covariates (\textsc{CorReg} reduces the dimension and can't give the true model if there is a structure). The datasets used here were those from table \ref{compZvrai}. 
 
 
 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_tout_compl_NB.png} 
			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
 
%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_tout_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_tout_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_tout_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
   \end{minipage}
\end{figure}

We see that CorReg tends to give more parsimonious models and better predictions, even if the true model is not parsomious. We logically observe that when $n$ rises, all the models get better and the correlations cease to be a problem so the complete model starts to be better (CorReg does not allow the true model to be choosen).




\clearpage
	\subsubsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $\boldsymbol{X}_r$. The datasets used here were still those from \ref{compZvrai}.
Depending only on $\boldsymbol{X}_r$ implies sparsity and impossibility to obtain the true model when using the true structure. 

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X2_compl_NB.png} 
			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
\textsc{CorReg} is still better than OLS for strong correlations and limited values of $n$. 
 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X2_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X2_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X2_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise}
   \end{minipage}
\end{figure}

\clearpage
\subsection{Robustess of the model}
% We have generated non linear structures (Tables \ref{compZnonlin} and \ref{resYnonlin}). Variables in $\boldsymbol{X_r}$ depends on the $\log$ or the square (randomly choosen with equiprobability) of a variable in $\boldsymbol{X_f}$ . Dependencies are still real but non linear. \textsc{CorReg} still found dependencies. One example of non-linear structure found with \textsc{CorReg} on real datasets is given in section \ref{sectionexfos}.
%\begin{table}[h!]
%\centering
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%\hline
%\multicolumn{3}{|c}{configuration}  &  \multicolumn{2}{|c}{Computing Time}  & \multicolumn{3}{|c}{Quality} & \multicolumn{2}{|c|}{Complexity}\\
%\hline
%$n$ & $p_r$ & $P(S)$&Time Mixmod  & Time MCMC  & $TL$ & $WL$ & $ML$ & $\Delta p_2$ & $\Delta compl$ \\
%\hline
%30 & 16 & $BIC$ & 0.4313 & 2.7769 & 9.96 & 2.45 & 5.95 & 3.5 & 29.42  \\
%& & &(0.0391) & (0.0883) & (1.3175) & (1.3808) & (1.3056) & (1.453) & (6.2639) \\
% &  & $BIC_+$ & 0.4313 & 4.9079 & 7.28 & 1.25 & 8.63 & 7.38 & 5.77  \\ 
%& & &(0.0391) & (0.4353) & (1.5769) & (0.9987) & (1.5548) & (1.6316) & (6.1297) \\ 
%\hline
%\end{tabular} 
%\caption{Results of the Markov chain for non linear structure ($\log$ and square). Mean observed and standard deviation (sd). } \label{compZnonlin}
%\end{table}
%
%
%\begin{table}[h!]
%\centering
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline 
%$n$ & $p_r$& $P(S)$ &indicator &LASSO  &    \textsc{CorReg} $\hat S$& \textsc{CorReg} $S$\\ 
%\hline %nonlin0.5global30p40ratio0.4BICTRUEBICstarTRUEp1max5lambda5100Amaxp2max100sigma_sousreg0.001maxit9000maxtry20max_compl1scaleTRUEcandidates-1_12mars2014_19_55_33.csv
%30 & 16 & $BIC$ &MSE (sd) & 996.06 (323.7) & 1100.17 (391.4) & 1501.25 (905.9) \\
%& & &cpl (sd) & 17.66 (5.4) & 14.7 (4.7) & 11.97 (4.9) \\
% &  &$BIC_+$ &MSE (sd) & 996.06 (323.7) & 1094.5 (621.8) & 1501.25 (905.9) \\
%& & & cpl (sd) & 17.66 (5.4) & 16.98 (4.8) & 11.97 (4.9) \\ 
%\hline
%\end{tabular} 
%\caption{ $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} is not too far from LASSO even if it stays behind. } \label{resYnonlin}
%\end{table}


We have generated a non-linear structure to test the robustess of the model. $\boldsymbol{X}_f$ is a set of 6 independent Gaussian mixtures defined as previously but with random signs for the components means. $\boldsymbol{X}_r=\boldsymbol{X}_7=a\boldsymbol{X}_1^2+\boldsymbol{X}_2-2\boldsymbol{X}_3+ \varepsilon$. The matrix $\boldsymbol{X}$ is then scaled and we get $\boldsymbol{Y}=\sum_{i=1}^7\boldsymbol{X}_i+\varepsilon_Y$. We let $a$ vary between $0$ and $10$ to increase progressively the non-linear part of the sub-regression.

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/robust_S.png} 
			\caption{Quality of the structure found when the paramater $a$ increases}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/robust_MSE.png} 
			\caption{MSE on the main regression.}
   \end{minipage}
\end{figure}


	\clearpage	
\section{Numerical results on real datasets} \label{sectionrealcase}
\subsection{Quality case study} \label{sectionexfos}
This work takes place in steel industry context, with quality oriented objective : to understand and prevent quality problems on finished product, knowing the whole process. The correlations are strong here (many parameters of the whole process without any a priori and highly correlated because of physical laws, process rules, {\it etc.}). 
		
We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item 205 variables from the whole process to explain it.
			\item The stakes : a hundred euros per ton (for information: Dunkerque's  site aims to produce up to 7.5 millions tons a year)
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/mixmod.png} 
			\caption{Example of non-Gaussian real variable easily modeled by a Gaussian mixture}\label{graphMixmod}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/histR2exfos.png} 
			\caption{$R^2_{adj}$ of the 76 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/correlexfoshist.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEexfos}
   \end{minipage}
\end{figure}   			
	We get a training set of $n=3 000$ products described by $p=205$ variables from the industrial process and a validation sample of $847$ products.
	Let's note $\rho$ the absolute value of correlations between two covariates. Industrial variables are naturally highly correlated as the width and the weight of a steel slab ($\rho=0.905$), the temperature before and after some tool ($\rho=0.983$), the  roughness of both faces of the product ($\rho= 0.919$), a mean and a max ($\rho=0.911$). \textsc{CorReg} also found more complex structures describing physical models, like   Width = f (Mean.flow , Mean.speed.CC) even if the true Physcial model is not linear : Width = flow / (speed * thickness) (here thickness is constant). Non linear regulation models used to optimize the process were also found (but are confidential). These first results are easily understandable and meet metallurgists expertise.  
			The algorithm gives a structure of $p_r=76$ subregressions with a mean of $\bar{\boldsymbol{p}}_f=5.17$ regressors. In $\boldsymbol{X}_f$ the number of $\rho>0.7$ is $\textbf{79.33\%}$ smaller than in $\boldsymbol{X}$.		
	
			It is now time to look at the predictive results (Figure \ref{compareMSEexfos}).
				The best model found when not using \textsc{CorReg} is given by the LASSO. But when using \textsc{CorReg} elasticnet produces a better model in terms of prediction. LASSO gives a model with 21 non-zero coefficients and elasticnet with \textsc{CorReg} gives a model with 40 non-zero parameters but $6.40\%$ better in prediction on the validation sample (847 products). $14$ non-zero coefficients are common between the two models.
				Elasticnet alone get a model with 78 parameters that is improved by $9.75\%$ in prediction when used with \textsc{CorReg}. When using LASSO with \textsc{CorReg} we obtain a model with 24 non-zero coefficients that is $4.11\%$ better than LASSO alone. We also computed the OLS model (without selection) and the naive one (estimating the response by the mean of the learning set). All the MSE were modified here to obtain a value of 100 for the best (to preserve confidentiality). Elasticnet with \textsc{CorReg} is $13.51\%$ better than OLS.
		\begin{figure}[h]
			\centering
				\label{barplotMSEexfos}
				\includegraphics[width=430px]{figures/MSEfinal.png}
			\caption{MSE comparison on industrial dataset. Learning set : 3 000 products, validation set : 847 products}
		\end{figure}		
		\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	OLS & 115.63 & 206 \\ 
	\hline 
	\textsc{CorReg} + OLS & 109.59&130 \\ 
	\hline  
	LASSO & 106.84 & 21 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 102.45 &24 \\ 
	\hline 
	elasticnet & 110.81 & 78\\ 
	\hline 
	\textsc{CorReg} + elasticnet & 100 &40 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained on a validation sample.}	
\end{table}

		In terms of interpretation, the main regression comes with the family of regression so it gives a better understanding of the consequences of corrective actions on the whole process. It typically permits to determine the \textit{tuning parameters} whereas LASSO would point variables we can't directly act on.	So it becomes easier to take corrective actions on the process to reach the goal. The stakes are so important that even a little improvement leads to consequent benefits, and we don't even talk of the impact on the market shares that is even more important.
		\clearpage
		
		\subsection{Production case study}
This second example is about a phenomenon that impacts the productivity of a steel plan.
We have :
		\begin{itemize}
			\item a (confidential)  response variable,
			\item $p=145$ variables from the whole process to explain it but only $n=100$ individuals.
			\item The stakes : $20\%$ of productivity to gain
		\end{itemize}
		
\begin{figure}[h!]
	\begin{minipage}[l]{.30\linewidth}
			\includegraphics[width=150px]{figures/correlbeforeafter.png} 
			\caption{Correlations between the covariates in $\boldsymbol{X}$ (upper) and $\hat{\boldsymbol{X}}_f$ (lower).}
	\end{minipage} \hfill
	\begin{minipage}[c]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/R2corregBVBI.png} 
			\caption{$R^2_{adj}$ of the 67 sub-regressions.}
	\end{minipage} \hfill
   \begin{minipage}[r]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/histcorrelBVBI.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} 
   \end{minipage}
\end{figure}   			
	Here $n<p$ so we only compare the leave-one-out cross-validation MSE.
	\textsc{CorReg} improves LASSO by $5.24\%$ and elasticnet by $8.60\%$. \textsc{CorReg} combined with LASSO gives the best result but it is only a leave-on-out MSE.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
	\hline 
	Model & MSE & Complexity (with intercept) \\ 
	\hline 
	LASSO & 105.54 & 34 \\ 
	\hline 
	\textsc{CorReg} + LASSO & 100 & 18 \\ 
	\hline 
	elasticnet & 129.94 & 13 \\ 
	\hline 
	\textsc{CorReg} + elasticnet & 118.76 & 21 \\ 
	\hline 
\end{tabular} 
\caption{Results obtained with leave-one out cross-validation. $n=100, p=145$.}	
\end{table}
In this precise case, \textsc{CorReg} found a structure that helped to decorrelate covariates in interpretation and to find the relevant part of the process to optimize.


\section{Conclusion and perspectives} \label{conclusion}
	We have seen that correlations can lead to serious estimation and variable selection problems in linear regression and that in such a context, it can be useful to explicitly model the structure between the covariates and to use this structure (even sequentially) to avoid correlations issues. We also show that real industrial context faces this kind of situations so our model can help to understand and predict physical phenomenon efficiently. But for now we still need a full dataset to learn the structure between the covariates and even if correlations are strong, some information is lost. Further work is needed to face these two challenges.
	
	\textsc{CorReg} is accessible on CRAN and has already proved its efficiency on real regression problematics in industry. \textsc{CorReg}'s strength is its great interpretability of the model, composed of several short linear regression easily managed by non-statisticians while strongly reducing correlations issues that are everywhere in industry.
	Nevertheless, we need to enlarge its application field to missing values, also very commons in industry. The structure can be used to estimate missing values in $\boldsymbol{X}_r$ but the actual generative model allows to go further (to manage missing values even in the MCMC algorithm) without supplementary hypothesis and this also is a strength of \textsc{CorReg}. 
	
	Another perspective would be to take back lost information (the residual of each sub-regression) to improve predictive efficiency when needed. It would only consists in a second step of linear regression between the residuals and would thus still be able to use any selection method.
	
	This paper only treats linear regression but such a pretreatment could be used for logistic regression, {\it etc.}
	So the subject is still wide opened.	

\section{Acknowledgements}
	We want to thanks ArcelorMittal Atlantique \& Lorraine that has granted this work, given the chance to use \textsc{CorReg} on real dataset and authorized the package to be open-sourced licensed (\textsc{CeCILL}), especially Dunkerque's site where most of the work has been done.
\bibliography{biblio}{}
\bibliographystyle{plain}
\section{Appendices}
	\subsection{Identifiability of the structure} \label{preuveident}
	The model presented above relies on a discrete structure $S$ between the covariates. But to find it we need identifiability property to insure the MCMC will asymptotically find the true model. Identifiability of the structure is asked in following terms: Is it possible to find another structure $\tilde{S}$ of linear regression between the covariates leading to the same joint distribution and marginal distributions? 
	
		If there are exact sub-regressions ($\sigma^2_j=0$), the structure won't be identifiable. But it only means that several structure will have the same likelihood and they will have the same interpretation. So it's not really a problem. Moreover, when an exact sub-regression is found, we can delete one of the implied variables without any loss of information and the structure will define a list of variable from which to delete. \textsc{CorReg} (Our R package) prints a warning to point out exact regressions when found.
	In the followings we suppose $\sigma^2_j\neq 0$, then $\boldsymbol{X}_f'\boldsymbol{X}_f$ and $\boldsymbol{X}'\boldsymbol{X}$ are of full rank (but the later is ill-conditioned for small values of $\sigma^2_j$).
	\\
	
Our full generative model is a $p$-sized Gaussian mixture model of $K$ distinct components and 
%	\begin{equation}
%	f(\boldsymbol{X}|K,S)=f(\boldsymbol{X}_f|K,S)f(\boldsymbol{X}_r|\boldsymbol{X}_f,S)
%	\end{equation}	
%	
	can be seen as a $\mathbf{SR}$ model defined by Maugis \cite{maugis2009variable}. In this section, $S$ will denote the set of variable as in the paper from Maugis and we call Gaussian mixtures the Gaussian mixtures with at least two distinct components. The equivalence with Maugis's model is defined by:
	$\boldsymbol{X}_r=\boldsymbol{y}^{S^c}$ and $\boldsymbol{X}_f=\boldsymbol{y}^R$. We have supposed independence between variables in $\boldsymbol{X}_f$ so the identifiability theorem from Maugis tells that our model is identifiable if variables in $\boldsymbol{X}_f$ are Gaussian mixtures (what we supposed in section \ref{sectionfullgen}).
	\\
	
	
%First, we observe that if each variable in $\boldsymbol{X}_r$ is a Gaussian mixture, then there must be at least one Gaussian mixture on the right of each sub-regression. 
We define $\boldsymbol{X}^G \subsetneq \boldsymbol{X}_f$ containing Gaussian variables and we note the Gaussian mixtures $\boldsymbol{X}^{G^c}\neq \emptyset$ its complement in $\boldsymbol{X}_f$.
We suppose that variables in $\boldsymbol{X}_r$ are all Gaussian mixtures. It implies that $\forall j  \in I_r,\exists i \in I_f^j $ so that $\boldsymbol{X}^i \subset \boldsymbol{X}^{G^c} $ since any linear combination of Gaussian variable would only give a Gaussian (so each sub-regression contain at least one Gaussian mixture as a regressor).
\\
	We introduce the matricial notation
		$\boldsymbol{X}_r=\boldsymbol{X}_f\boldsymbol{\alpha} + \boldsymbol{\varepsilon}$ where
		 $\boldsymbol{\alpha}$ is the $(p-p_r)\times p_r$ matrix whose columns are the $\boldsymbol{\alpha}_j$ and $\boldsymbol{\varepsilon}$ is the $n\times p_r$ matrix whose columns are the $\boldsymbol{\varepsilon}_j$
		%\\We note $\Theta$ the parameter of the model.We want to know if $P(\boldsymbol{X}|S,\Theta)=P(\boldsymbol{X}|\tilde{S},\tilde{\Theta})$ does imply $(S,\Theta)=(\tilde{S},\tilde{\Theta})$.
		
The theorem from Maugis guarantee that a sub-regression between Gaussian mixtures is identifiable in terms of which one is regressed by others.
		\begin{eqnarray}
%			\forall j \in I_r, \boldsymbol{X}^j_{|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_j^G+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c}+ \boldsymbol{\varepsilon}_j \\
%			\boldsymbol{X}^j_{|\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c} + \tilde{\boldsymbol{\varepsilon}}_j \textrm{ is identifiable where} \\
%			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_j^{G}+ \boldsymbol{\varepsilon}_j \textrm{ is Gaussian. 
		 \boldsymbol{X}_{r|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_G+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_{G^c}+ \boldsymbol{\varepsilon} \\
			\boldsymbol{X}_{r|\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_{G^c} + \tilde{\boldsymbol{\varepsilon}} \textrm{ is identifiable where} \\
			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_j^{G}+ \boldsymbol{\varepsilon}_j \textrm{ is Gaussian.}  
		\end{eqnarray}
%		We have the unique (Gaussian are allowed only on the right of the sub-regressions) decomposition $f(\boldsymbol{X}_r|\boldsymbol{X}_f)=f(\boldsymbol{X}_r|\boldsymbol{X}^G,\boldsymbol{X}^{G^c})=f(\boldsymbol{X}_r|\boldsymbol{X}^{G^c})f(\boldsymbol{X}^{G}) $ 
%		where  $f(\boldsymbol{X}_r|\boldsymbol{X}^{G^c})$ is identifiable. Moreover, $\boldsymbol{X}^{G}$ is a set (empty or not) of independent Gaussian variables, whose parameters are identifiable.
%		So the structure is identifiable.
%		\begin{eqnarray}
%			f(\boldsymbol{X})&=&f(\boldsymbol{X}\setminus\boldsymbol{X}^{G},\boldsymbol{X}^{G})=f(\boldsymbol{X}_r|\boldsymbol{X}^{G^c},\boldsymbol{X}^{G})f(\boldsymbol{X}^{G^c},\boldsymbol{X}^{G}) \\
%			&=& f(\boldsymbol{X}_r|\boldsymbol{X}^{G^c},\boldsymbol{X}^{G})f(\boldsymbol{X}^{G^c})f(\boldsymbol{X}^{G}) \\
%		\end{eqnarray}
%		
%		\begin{equation}
%			
%		\end{equation}
	%It can also be extended to the $(S,R,U,W)$ model with variables of $\boldsymbol{X}_f$ independent from $\boldsymbol{X}_r$ defining ...
	
Here, $\otimes$ and $\oplus$ denote respectively the kronecker product and sum.
	\begin{eqnarray}
			\forall j \in I_r : \  \boldsymbol{X}^j_{|\boldsymbol{X}_f}&\sim & \mathcal{N}(\boldsymbol{X}_f\boldsymbol{\alpha}_j,\sigma^2_j\boldsymbol{I}_n) \label{densitycondgauche}\\
			\forall j \in I_f : \boldsymbol{X}^j_{|S} &\sim& \mathcal{GM}(\boldsymbol{\pi}_j;\boldsymbol{\mu}_j;\boldsymbol{\sigma}^2_j) \textrm{ with } \boldsymbol{\pi}_j,\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j \textrm{ vectors of size } K_j  \label{densitydroite}\\
			\textrm{And we obtain, } \forall j \in I_r : \  \boldsymbol{X}^j&\sim & 
						\mathcal{GM}(\bigotimes_{\substack{i \in I_f^j  }}\boldsymbol{\pi}_i \ ; 
			                         \bigoplus_{\substack{i \in I_f^j  }} \alpha_{i,j} \boldsymbol{\mu}_i \ ; 
			                         \boldsymbol{\sigma}_j^2+\bigoplus_{\substack{i \in I_f^j  }}\alpha_{i,j}^2 \boldsymbol{\sigma}_i^2 )\label{densitygauche}          
		\end{eqnarray}
		So when we compare (\ref{densitydroite}) and (\ref{densitygauche}) we see that the number of components in $I_r$ variables differs when subregressions are of length $>1$ (almost 2 predictors) with multiple-class predictors (so the kronecker product is effective).	
	
	The presence of two Gaussian mixtures on the right of the sub-regression gives the identifiability of it (even with also Gaussian variables) by the strict increase of the component number when combining Gaussian mixtures.
	So a sub-regression is identifiable if it has only Gaussian mixtures (even only one) on the right or if it has at least two Gaussian mixtures (even with additional Gaussian variables)
	
	
	
	

%	\subsubsection{Identifiability of $\Theta$}		
%		We know by identifiability of the Gaussian mixtures and linear regression ($\boldsymbol{X}_f'\boldsymbol{X}_f$ is of full rank) that if $P(\boldsymbol{X}|S,\Theta)=P(\boldsymbol{X}|S,\tilde{\Theta})$ then $\Theta=\tilde{\Theta}$ up to a permutation of the components. But the identifiability of the discrete parameter $S$ is still pending.
%		
%	\subsubsection{Identifiability of $S$, quick look}
%	
%	In the following, $\otimes$ and $\oplus$ denote respectively the kronecker product and sum.
%	\begin{eqnarray}
%			\forall j \in I_r : \  \boldsymbol{X}^j_{|\boldsymbol{X}_f}&\sim & \mathcal{N}(\boldsymbol{X}_f\boldsymbol{\alpha}_j,\sigma^2_j\boldsymbol{I}_n) \label{densitycondgauche}\\
%			\forall j \in I_f : \boldsymbol{X}^j_{|S} &\sim& \mathcal{GM}(\boldsymbol{\pi}_j;\boldsymbol{\mu}_j;\boldsymbol{\sigma}^2_j) \textrm{ with } \boldsymbol{\pi}_j,\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j \textrm{ vectors of size } K_j  \label{densitydroite}\\
%			\textrm{And we obtain, } \forall j \in I_r : \  \boldsymbol{X}^j&\sim & 
%						\mathcal{GM}(\bigotimes_{\substack{i \in I_f^j  }}\boldsymbol{\pi}_i \ ; 
%			                         \bigoplus_{\substack{i \in I_f^j  }} \alpha_{i,j} \boldsymbol{\mu}_i \ ; 
%			                         \boldsymbol{\sigma}_j^2+\bigoplus_{\substack{i \in I_f^j  }}\alpha_{i,j}^2 \boldsymbol{\sigma}_i^2 )\label{densitygauche}          
%		\end{eqnarray}
%		So when we compare (\ref{densitydroite}) and (\ref{densitygauche}) we see that the number of components in $I_r$ variables differs when subregressions are of length $>1$ (almost 2 predictors) with multiple-class predictors (so the kronecker product is effective). We call this difference in component number "identifiability" in the sense that we try to find the model with the fewest component in $\boldsymbol{X}_f$ (we use the BIC so when comparing two models with the same likelihood, the one with fewer parameters will win). So in this case we have a "better" model (the simplest one) in the group of equivalent-meaning models (permuting variables in some sub-regression).
%	\\
%	
%If all the variables in $\boldsymbol{X}$ are Gaussian mixtures with at least two distinct components, then the model of sub-regression is identifiable (from the identifiability of the SR model by Maugis \cite{maugis2009variable}).
%		
%But making this hypothesis on $\boldsymbol{X}_r$ is sufficient. 
%In the followings, we call Gaussian mixture the Gaussian mixtures with at least two distinct components, and Gaussian mixtures with equal components are called Gaussian (in fact they are).

		
%	\subsubsection{Identifiability: Theorem and proof} 
%		A necessary and sufficient condition for identifiability of $S$ is $\forall j \in I_r: K_j\geq 2$ with at least two distinct components in $(\boldsymbol{\mu}_j,\boldsymbol{\sigma}_j)$
%\\
%	{\it Proof}:
%	We know that for a Gaussian multivariate distribution $(\boldsymbol{X}_r,\boldsymbol{X}_f)$ the conditional distributions $(\boldsymbol{X}_r|\boldsymbol{X}_f)$ and $(\boldsymbol{X}_f|\boldsymbol{X}_r)$ are both Gaussian (multivariate or not according to $p_r$). 
%	
%		If $\exists j \in I_r: k_j=1$ (Gaussian) then all the covariates on the right of the associated sub-regression (with non-zero coefficients) are Gaussian and then the sub-regression can be permuted and is not identifiable. So if $\boldsymbol{X}^j$ are Gaussian mixtures, it is a necessary condition for identifiability to have $\forall j \in I_r: K_j\geq 2$ with at least two distinct components in $(\boldsymbol{\mu}_j,\boldsymbol{\sigma}_j)$.
%
%	Gaussian mixtures in $\boldsymbol{X_r}$ with at least two distinct components does imply the presence of at least one Gaussian mixture in each sub-regression (with at least two components).
%%	\begin{equation}
%%	 \forall j \in I_f :    \boldsymbol{X}^j_{|S}=\sum_{k=1}^{K_j}\pi_j(k)\mathcal{N}\left(\mu_j(k),\sigma_j^2(k)\boldsymbol{I}_n\right) \\
%%	\end{equation}
%%	$\forall j \in I_r$ we can define $K_{j}$ the number of distinct components of $\boldsymbol{X}_j$.
%%	\begin{eqnarray}
%%				\boldsymbol{X}^{j}_{|S}&=&\sum_{k=1}^{K_j}\pi_j(k)\mathcal{N}\left(\mu_j(k),\sigma_j^2(k)\boldsymbol{I}_n\right)
%%	\end{eqnarray}
%	The joint distribution of the $p$ Gaussian mixtures is a multivariate Gaussian mixture $\boldsymbol{X}$ with $K\geq 2$ distinct components $(\boldsymbol{m}_k,\boldsymbol{\Sigma}_k)$ where $\boldsymbol{m}_k \in \mathbf{R}^p$ and $\boldsymbol{\Sigma}_k$ is a $p \times p$ matrix.
%		We decompose the joint distribution based on the components:
%		
%		\begin{equation}
%			f(\boldsymbol{X}_r,\boldsymbol{X}_f)= \sum_{k=1}^K p_kf(\boldsymbol{X}_r,\boldsymbol{X}_f|k)
%	\end{equation}
%			where  $\forall 1\leq k \leq K$, $ f(\boldsymbol{X}_r,\boldsymbol{X}_f|k)$  is a multivariate Gaussian.
%			\\
%			Then,  $\forall j \in I_r, \forall i \in I_f^j$ we have:
%			\begin{equation}
%			 f(\boldsymbol{X}_r,\boldsymbol{X}_f)= \sum_{k=1}^K p_kf(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i,k)f(\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i|k)
%	\end{equation}
%				where the $f(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i,k)$ are Gaussian.
%				
%Hence,
%\begin{equation}
%				f(\boldsymbol{X}^i|\boldsymbol{X}\setminus\boldsymbol{X}^i)=f(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i)=\sum_{k=1}^Kp_kf(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i,k)
%				\end{equation}
%				
%				is a Gaussian mixture.				
%				
%				Identifiability of $S$ means we don't have a linear regression of $\boldsymbol{X}^i$ on $(\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i)$. So we need a non-linear relationship or a not-Gaussian residual $f(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}\setminus\boldsymbol{X}^i)$, {\it i.e. } the Gaussian mixture $f(\boldsymbol{X}^i|\boldsymbol{X}\setminus\boldsymbol{X}^i)$ should have at least two distinct components.
%	Linear relationship means $\exists \boldsymbol{\gamma} \in \mathbf{R}^{p-1}$ and $\sigma_i \in \mathbf{R}$ for which
%	\begin{equation}
%		\forall 1\leq k \leq K, \boldsymbol{X}^i_{|\boldsymbol{X}_r,\boldsymbol{X}\setminus\boldsymbol{X}^i,k}=\mathcal{N}(\boldsymbol{m}_k^{\bar{i}}\boldsymbol{\gamma},\sigma_i^2)
%	\end{equation}
%					
%				
%				\begin{equation}
%				f(\boldsymbol{X}^i|\boldsymbol{X}\setminus\boldsymbol{X}^i)=f(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i)=\sum_{k=1}^Kp_kf(\boldsymbol{X}^i|\boldsymbol{X}_r,\boldsymbol{X}_f\setminus\boldsymbol{X}^i,k)
%				\end{equation}
%				
%				If it is a Gaussian mixture with at least two distinct components when $\boldsymbol{X}^j \in \boldsymbol{X}_r$ has at least two distinct components.
%				Then $\boldsymbol{X}^i$ can't be explained by a linear regression on the other covariates (non-Gaussian residual).		
%				 So we have identifiability of the structure $S$ even for trivial sub-regressions if we keep the constraint to have distinct components in $\boldsymbol{X}_r$ (equivalent to have at least one Gaussian mixture with at least two components in each sub-regression).
%			%f(\boldsymbol{X}_r,\boldsymbol{X}_f)&=& sum_{j=1}^K 
%
%The distribution of $\boldsymbol{X}_r$ is a Gaussian mixture defined in (\ref{densitygauche}) thus
%			$\forall 1\leq k \leq K$,  $(\boldsymbol{X}_r|k)$ is Gaussian: 
%		
%
%
%In the followings, $N_{11}$ and $N_{12}$ Gaussian (multivariate or not). $\boldsymbol{X}_r$ is here supposed to be univariate (identifiability in such a case will be generalisable).
%		\begin{eqnarray}
%			f(\boldsymbol{X_f})&=& p_{11} N_{11}+p_{12}N_{12}\\
%			\boldsymbol{X}_r&=&\boldsymbol{X}_f\boldsymbol{\alpha}_r +\boldsymbol{\varepsilon}_r \\
%			f(\boldsymbol{X}_r)&=&p_{11}(N_{11}+\boldsymbol{\varepsilon})+p_{12}(N_{12}+\boldsymbol{\varepsilon}) \\
%			f(\boldsymbol{X}_r|\boldsymbol{X}_f)&=&\boldsymbol{\varepsilon}\\
%			f(\boldsymbol{X}_r,\boldsymbol{X}_f)&=&f(\boldsymbol{X}_r|\boldsymbol{X}_f)f(\boldsymbol{X}_f)=f(\boldsymbol{X}_f|\boldsymbol{X}_r)f(\boldsymbol{X}_r) \\
%			f(\boldsymbol{X}_f|\boldsymbol{X}_r)&=&\frac{f(\boldsymbol{X}_r|\boldsymbol{X}_f)f(\boldsymbol{X}_f)}{f(\boldsymbol{X}_r)} \\
%			&=&\frac{(p_{11} N_{11}+p_{12}N_{12})\boldsymbol{\varepsilon}}{p_{11}(N_{11}+\boldsymbol{\varepsilon})+p_{12}(N_{12}+\boldsymbol{\varepsilon})} \textrm{ with }p_{11}+p_{12}=1\\
%			&=&\frac{(p_{11} N_{11}+p_{12}N_{12})\boldsymbol{\varepsilon}}{(p_{11} N_{11}+p_{12}N_{12})+\boldsymbol{\varepsilon}}
%		\end{eqnarray}
%	So we have a quotient of two multivariate Gaussian mixtures and the question is to know if it is Gaussian (in such a case we wouldn't have identifiability).
%If $\boldsymbol{X}_f$ is a Gaussian mixture with $K>1$ distinct components, then so will be $\boldsymbol{X}_r$.
%	
%	If the sufficient condition is not satisfied but we have different variances for $\boldsymbol{X}_f$ and $\boldsymbol{X}_r$, then we don't have identifiability but reverse regressions don't give the same results as the true model, as illustrated in the well-known salary discrimination study \cite{goldberger1984reverse} and explained in books and papers (\cite{leamer1978least},\cite{cameron2005microeconometrics}).
%	
	
	
%	 As a set of Gaussian mixtures and linear combination of Gaussian mixtures, $\boldsymbol{X}$ follows a multivariate Gaussian mixture model with $K$ components:
%		\begin{eqnarray}
%			f(\boldsymbol{X}|S,\theta)=\sum_{k=1}^Kp_k \Phi(\boldsymbol{X}_r|\boldsymbol{X}_f\boldsymbol{\alpha},S,\theta)\Phi(\boldsymbol{X}_f|S,\theta)
%		\end{eqnarray}	
%	

	
	\subsection{The \textsc{CorReg} package}
\subsubsection{Alternative neighbourhoods for the MCMC}
	We have here at each step $|\mathcal{V}_{S,j}|=p$ candidates but some other constraints can be added on the definition of $\mathcal{S}$ and will consequently modify the size of the neighbourhood (for example a maximum complexity for the internal regressions or the whole structure, a maximum number of internal regressions, {\it etc.}). \textsc{CorReg} allows to modify this neighbourhood to better fit users constraints. Relaxation (column-wise and row-wise) is optional but gives more stability to the number of feasible candidates at each step and allows to modify several parts of $I_f$ in only one step when needed. Hence it improves efficiency by a significant reinforcement of the irreductibility of the Markov chain. Rejecting candidates instead of doing the relaxation steps will  however reduce the number of evaluated candidates and thus accelerate the walk. So it can be used for a warming phase when $n$ is great and time is missing.
	
	The hierchical uniform hypothesis made above for $P(S)$ implies $p_r<\frac{p}{2}$ and $p_f^j<\frac{p}{2}$ so candidates may be rejected to satisfy this hypothesis. Stronger constraints on $p_r$ and/or $p_f^j$ can be given in \textsc{CorReg} if relevant.
	
If the algorithm did not have time to converge (stationnarity), it can be continued with a few step for which the neighboorhood would only contain smaller candidates (in terms of complexity). It is equivalent to ask for each element in $I_f$ if the criterion $P(S|\boldsymbol{X})$ would be better without it. Thus it can be seen as a final cleaning step. But in fact, it's just continuing the MCMC with a reduced neighbourhood.	
	
	
	
\end{document}