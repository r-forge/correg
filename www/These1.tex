\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Clément THERY}
\title{\textsc{CoMPASS}: \textbf{Co}rrelation \textbf{M}odeling for \textbf{P}retreatment by \textbf{A}utomated \textbf{S}tructure \textbf{S}election}
\begin{document}
\maketitle
\newpage
\itshape To my sons
\upshape
\tableofcontents
\chapter*{Abstract}
\chapter*{Acknowledgments}
\chapter{The industrial context}
	This work takes place in a steel industry context. The main objective is to be able to solve quality crisis when they occur. In such a case, a new type of unknown quality issue is observed and we have no idea of its origin. The defects, even generated at the beginning of the process, are often detected in its last part. The steel-making process includes several sub-process, each implying a whole manufactory. Thus we have many covariates and no a priori on the relevant ones. Moreover, the values of each covariates essentially depends on the characteristics of the final product, and many physical laws and tuning models are implied in the process. Therefore the covariates are highly correlated.
	We have several constraints :
	\begin{itemize}
		\item To be able to predict the defect and stop the process as early as possible to gain time (and money)
		\item To be able to understand the origin of the defect to try to optimize the process
		\item To be able to find parameters that can be changed because the objective is not only to understand but to correct the problematic part of the process.
		\item It also must be fast and automatic (without any a priori).
	\end{itemize}
	We will see in the state of the art that correlations are a real issue and that the number of variables increases the problem.	
	The stakes are very high because of the high productivity of the steel plants but also because steel making is now well-known and optimized thus new defects only appears on innovative steels with high value. Any improvement on such crisis can have important impact on the market shares and when the customer is implied, each day won by the automation of the data mining process can lead to a gain of hundreds of thousands of euros, sometimes more. So we really need a kind of automatic method, able to manage the correlations without any a priori and giving an easily understandable and flexible model.
	
	
\chapter{State of the art}
In the following we note classical norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$, $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^p|\beta_i| $ and $\parallel\boldsymbol{\beta} \parallel_{\infty}=\operatorname{max}(|\beta_1|,\dots,|\beta_p|)$.
	\section{Ordinary least squares and associated problems}\label{sectionOLS}
We note the linear regression model:
\begin{equation}
		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \label{regressionsimple}
	\end{equation}
	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables (that is a sub-matrix of $\tilde{\boldsymbol{X}}$ the $n\times \tilde{p}$ matrix of provided covariates), $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $p\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{OLS}): %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1} \label{eqOLS}
	\end{equation}
	and without any bias.
	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
Conditionning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects: the dimension $p$ (number of covariates) of the model (the more covariates you have the greater variance you get)
	 and the correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .
	When correlations between covariates are strong, the matrix to invert is ill-conditioned and the variance increases, giving unstable and unusable estimator \cite{hoerl1970ridge}.
	Another problem is that matrix inversion requires $n\geq p$. 	
	\section{Penalized models}
		\subsection{Ridge regression}
			%\cite{hoerl1970ridge}
			%\cite{marquardt1975ridge}
Ridge regression \cite{marquardt1975ridge} proposes a biased estimator that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \lambda \textrm{ with } \lambda>0
	\end{equation}
	But this penalty is not guided by the correlations. It is the same for each covariates and will be too large for independent covariates and/or too small for correlated ones. So the efficiency of such a method is limited. 
	Moreover, coefficients tend to 0 but don't reach 0 so it gives difficult interpretations for large values of $p$. 
				
			
		\subsection{LASSO: Least Absolute Shrinkage and Selection Operator }
			\cite{tibshiranilasso}  
			\cite{tibshirani1996regression} 
			\cite{efron2004least} %LAR
			\cite{Zhao2006MSC}%problèmes du lasso/lars en correlations
			\cite{SAM10088}%lars necessite OLS en surcouche
The Least Absolute Shrinkage and Selection Operator (\textsc{LASSO} \cite{tibshirani1996regression}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0
		\end{equation}	
	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) algorithm offers a very efficient way to obtain the whole LASSO path and is very attractive. It requires only the same order of magnitude of computational effort as \textsc{OLS} applied to the full set of covariates. But like the ridge regression, the penalty does not distinguish correlated and independent covariates so there is no guarantee to have less correlated covariates.



		\subsection{Adaptive LASSO and Random LASSO}
			\cite{zou2006adaptive}% adaptive lasso
			\cite{wang2011random}%random lasso
			 Some recent variants of the \textsc{LASSO} do exist for the choice of the penalization coefficient like the adaptive \textsc{LASSO} \cite{zou2006adaptive} or the random \textsc{LASSO} \cite{wang2011random}.  But \textsc{LASSO} also faces consistency problems \cite{Zhao2006MSC} when confronted with correlated covariates.
		\subsection{Elasticnet}
			\cite{zou2005regularization}
			Elastic net \cite{zou2005regularization} is a method developed to be a compromise between Ridge regression and the \textsc{LASSO}: 
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	%Elastic net can be written:
	\begin{equation}
		\boldsymbol{\hat{\beta}}=(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta} \parallel_2^2 \right\rbrace, \textrm{ subject to } (1-\alpha)\parallel\boldsymbol{\beta}\parallel_1+\alpha\parallel\boldsymbol{\beta}\parallel_2^2\leq t \textrm{ for some } t
	\end{equation}
	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	%It seems to give good predictions. 
	But it is based on the grouping effect so correlated covariates get similar coefficients and are selected together whereas LASSO will choose between one of them and will then obtain same predictions with a more parsimonious model. Once again, nothing specifically aims to reduce the correlations. %Hence, when comparing the two models, interpretations are not the same and nothing explicitly explains why. So it can be very confusing. 
		\subsection{OSCAR: Octogonal Shrinkage and Clustering Algorithm for Regression }
			%\cite{bondell2008simultaneous}%Oscar
			Like elasticnet, \textsc{OSCAR} \cite{bondell2008simultaneous} uses combination of two norms for its penalty. Here the objective is to group covariates with the same effect (by a pairwise $L_\infty$ norm) and give them exactly the same coefficient (reducing the dimension) with a simultaneous variable selection (implied by the $L_1$ norm).
			\begin{equation}
				\hat{\boldsymbol{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \parallel\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta} \parallel^2_2 \textrm{ subject to } \sum_{j=1}^p|\beta_j|+c\sum_{j<k}\operatorname{max}(|\beta_j|,|\beta_k|) \leq \lambda		
			\end{equation}						
			But \textsc{OSCAR} depends on two tuning parameters: $c$ anf $\lambda$. For a fixed $c$ the $\lambda$ can be found by the \textsc{LAR} algorithm but $c$ still has to be found "by hand" comparing final models for many values of $c$.
			Correlations are only implicitely taken into account and only pairwise. So it lacks of an efficient algorithm and need a supplementary study to interpret the groups found.
	\section{Modeling the parameters}	
		\subsection{CLERE: CLusterwise Effect REgression}
			\cite{yengo2012variable}%clere
			The CLusterwise Effect REgression (\textsc{CLERE} \cite{yengo2012variable}) describes the $\beta_j$ no longer as fixed effect parameters but as unobserved independant random variables with grouped $\beta_j$ following a Gaussian Mixture distribution. The idea is to hope that the model have a small number of groups of covariates and that the mixture will have few enough components to have a number of parameters to estimate significantly lower than $p$. In such a case, it improves interpretability and ability to yeld reliable prediction with a smaller variance on $\boldsymbol{\hat{\beta}}$. 
	
		\subsection{Spike and Slab}	
			\cite{ishwaran2005spike}%spike and slab
			Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the $\beta_j$ and gives a subset of covariates (not grouped) on which to compute \textsc{OLS} but has no specific protection against correlations issues.
	\section{Multiple Equations}
		\subsection{SEM and Path Analysis}
		\subsection{SUR: Seemingly Unrelated Regression}
			\cite{SURzellner}
		\subsection{SPRING: Structured selection of Primordial Relationships IN the General linear model}
			\cite{chiquetconf}			
			
		\subsection{Selvarclust: Linear regression within covariates for clustering}
			\cite{maugis2009variable}
			The idea is to allow covariates to have different roles : $(S,R,U,W)$.
			But:
			\begin{itemize}
				\item It is about clustering and not regression (not the same application field)
				\item No sub-regression allowed between relevant variables (in the True model)
				\item Using stepwise-like algorithm without protection against correlations \cite{raftery2006variable} even it is known to be often unstable \cite{miller2002subset}
			\end{itemize}	
			We provide an specific MCMC algorithm with the ability to have redundant covariates in the true model.		 
\part{Pretreatment for correlations}
\chapter{Decorrelating covariates by a generative model}

\paragraph{Running example:} we look at a simple case with $p=5$ variables defined by four independent scaled Gaussian $\mathcal{N}(0,1)$ named $\boldsymbol{x}_1,\boldsymbol{x}_2$ and $\boldsymbol{x}_3=\boldsymbol{x}_1+\boldsymbol{x}_2+\boldsymbol{\varepsilon}_3$ where $\boldsymbol{\varepsilon}_3\sim{\mathcal{N}(\boldsymbol{0},\sigma_3^2\boldsymbol{I}_n)}$. We also define another couple $\boldsymbol{x}_4,\boldsymbol{x}_5$ of covariates that are {\it i.i.d. } with $(\boldsymbol{x}_1,\boldsymbol{x}_2)$ and two {\it scenarii} for $\boldsymbol{Y}$ with $\boldsymbol{\beta}=(1,1,1,1,1)$ and $\sigma_Y \in \{10,20\}$ .
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_3$ gets smaller.
	
	
\section{Our proposal: modelisation of the correlations}
We make the hypothesis that $\boldsymbol{X}$ can be described by a partition $\boldsymbol{X}=(\boldsymbol{X}_f,\boldsymbol{X}_r) $ given by an explicit structure $S$ where variables in $\boldsymbol{X}_r$ are endogenous covariates resulting from linear sub-regressions based on $\boldsymbol{X}_f$, the submatrix of mutually independent exogenous covariates.
So we model the correlations by $P(\boldsymbol{X}_r|\boldsymbol{X}_f) $ with $\boldsymbol{X}_f$ orthogonals.
 Then $\boldsymbol{X}_r$ is the $n\times p_r$ submatrix of $0\leq p_r <p$ redundent covariates and $\boldsymbol{X}_f$ the $n\times (p-p_r)$ submatrix of the free (independent) covariates.
 
 
 
In the following, we note $\boldsymbol{X}^j$ the $j^{th}$ column of $\boldsymbol{X}$.
The structure $S$ of $p_r$ regressions within correlated covariates in $\boldsymbol{X}$ is described by:
	\begin{equation}
		\boldsymbol{X}_{r|\boldsymbol{X}_f,S} \textrm{ defined by }\forall \boldsymbol{X}^j \subset \boldsymbol{X}_r: \boldsymbol{X}^j_{|\boldsymbol{X}_f,S}=\boldsymbol{X}_f\boldsymbol{\alpha}_j+\boldsymbol{\varepsilon}_j \textrm{ with } \boldsymbol{\varepsilon}_j \sim\mathcal{N}(\boldsymbol{0},\sigma^2_j\boldsymbol{I}_n) \label{SR}
	\end{equation}
		where $\boldsymbol{\alpha}_j \in \mathcal{R}^{(p-p_r)}$ are the sparse vectors of the regression coefficients between the covariates (each sub-regression freely implies different covariates). 
\\
\\


The partition of $\boldsymbol{X}$ implies the uncrossing rule  $\boldsymbol{X}_r \cap \boldsymbol{X}_f$ 
{\it i.e.} endogenous variables don't explain other covariates. This hypothesis ensures that $S$ contains no cycle and is straightforward readable (no need to order the sub-regressions). It is not so restrictive because cyclic structures have no sense and any non-cyclic structure can be associated with a structure that verifies the uncrossing constraint by just successively replacing endogenous covariates by their sub-regression when they are also exogenous in some other sub-regressions.

	
	  We make the choice to distinguish the response variable from the other endogenous variables (that are on the left of a sub-regression). Thus we have one regression on the response variable ($P(\boldsymbol{Y}|\boldsymbol{X}))$ and a system of sub-regressions (without the response variable: $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$). Then we consider correlations between the explicative covariates of the main regression, not between the residuals. We see that the $S$ does not depend on $\boldsymbol{Y}$ so it can be learnt independently, even with a larger dataset (if missing values in $\boldsymbol{Y}$).
	 
The structure obtained gives a system of linear regression that can be viewed as a recursive Simultaneous Equation Model (\textsc{SEM})\cite{davidson1993estimation} \cite{TIMM}.
  	Here we suppose the $\boldsymbol{\varepsilon}_j$ independent but in other cases \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) takes into account correlations between residuals \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) and could be used to estimate the $\boldsymbol{\alpha}_j$. 
		 
	 
\paragraph{In the running example:}$\boldsymbol{X}_r=\boldsymbol{x}_3$, $\boldsymbol{X}_f=\{\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_4,\boldsymbol{x}_5 \}$, $p_r=1$ and $\boldsymbol{\alpha}_3=(1,1,0,0)'$

	

\section{A by-product model: marginal regression with decorrelated covariates}
Now we know $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$ by the structure of sub-regressions, we are able to define a marginal regression model $P(\boldsymbol{Y}|\boldsymbol{X}_f,S)$ based on the reduced set of independent covariates $\hat{\boldsymbol{\beta}}_f$ without significant information loss. We use the information of the correlations structure to rewrite the true model without bias in the marginal space defined by the independent covariates.
 	\\
Using the partition $\boldsymbol{X}=[\boldsymbol{X}_f,\boldsymbol{X}_r]$ we can rewrite (\ref{regressionsimple}):
	\begin{equation}
			\boldsymbol{Y}_{|\boldsymbol{X}_f,\boldsymbol{X}_r,S}=\boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{\varepsilon_Y} \label{MainR}
		\end{equation}
		where $\boldsymbol{\beta}=(\boldsymbol{\beta}_f,\boldsymbol{\beta}_r) \in  \mathcal{R}^p$ is the vector of the regression coefficients associated respectively to $\boldsymbol{X}_f$ and $\boldsymbol{I}_n$ the identity matrix. 
We note that (\ref{SR}) and (\ref{MainR}) give also by simple integration on $\boldsymbol{X}_r$ a marginal regression model on $\boldsymbol{Y}$ {\it depending only on uncorrelated covariates $\boldsymbol{X}_f$}:
\begin{eqnarray}
		P(\boldsymbol{Y}|\boldsymbol{X}_f)&=& \int_{\boldsymbol{X}_r}P(\boldsymbol{Y}|\boldsymbol{X}_r,\boldsymbol{X}_f)P(\boldsymbol{X}_r|\boldsymbol{X}_f) d \boldsymbol{X} \\
	\boldsymbol{Y}_{|\boldsymbol{X}_f,S}&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j)+  \sum_{j \in I_r}\beta_{j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{Trueexpl} \\
	&=&\boldsymbol{X}_f\boldsymbol{\beta}_f^*+\boldsymbol{\varepsilon}_Y^*\label{modexpl}
\end{eqnarray}
 This model is still the true model and OLS estimator will still give an unbiased estimator, but its variance will be reduced by both dimension reduction and decorrelation (variables in $\boldsymbol{X}_f$ are independent so the matrix $\boldsymbol{X}_f'\boldsymbol{X}_f$ will be well-conditioned). So the information given by the structure $S$ allows to reduce the variance without adding bias, by simple marginalization.
\\
Nevertheless, to be able to compare the bias-variance tradeoff, we can see this model as a variable pre-selection independent of the response in $\boldsymbol{Y}_{|\boldsymbol{X}}$.
We note that it is simply a linear regression on some of the original covariates so we only made a pre-treatment on the dataset by selecting $\boldsymbol{X}_f$ because of the correlations given by $S$. So we also get the model
\begin{equation}
\boldsymbol{Y}_{|\boldsymbol{X},S}=\boldsymbol{X}\boldsymbol{\beta}^*+\boldsymbol{\varepsilon}_Y^* \textrm{ where }\boldsymbol{\beta}^*=(\boldsymbol{\beta}_f^*,\boldsymbol{\beta}_r^*) \textrm{ and } \boldsymbol{\beta}_r^*=\boldsymbol{0}
\end{equation}
	for which OLS estimator of the coefficients may be biased.  

\paragraph{Running example:} $\boldsymbol{Y}_{|\boldsymbol{X}_f}= 2\boldsymbol{x}_1+2\boldsymbol{x}_2+\boldsymbol{x}_4+\boldsymbol{x}_5+\boldsymbol{\varepsilon}_3 +\boldsymbol{\varepsilon}_Y$
\section{Strategy of use: pre-treatment before classical estimation/selection methods}\label{interpretation}

As a pre-treatment, the model allows usage of any method in a second time to estimate $\boldsymbol{\beta}_f^*$, even with variable selection methods like LASSO or a best subset algorithm like stepwise \cite{seber2012linear}. However, we always have $\boldsymbol{X}_r=\boldsymbol{0}$

After selection and estimation we will obtain a model with { \it two steps of variable selection}: the decorrelation step by marginalization(coerced selection associated to redundant information defined in $S$) and the classical selection step, with different meanings for obtained zeros in $\hat{\boldsymbol{\beta}}^*_f$ (irrelevant covariates) and for $\hat{\boldsymbol{\beta}}^*_r=0$ (redundant information). 
 Thus we are able to distinguish the reasons of selection and consistency issues don't mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.


The explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset and to take better actions. Expert knowledge can even be added to the structure, physical models for example.

Moreover, the uncrossing constraint (partition of $\boldsymbol{X}$) guarantee to keep a simple structure easily interpretable (no cycles and no chain-effect) and straightforward readable.

	
			There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypothesis. We just play on the traditional bias-variance tradeoff.
			 
	\section{Illustration of the tradeoff conveyed by the pre-treatment}	
	We compare the OLS estimator on $\boldsymbol{X}$ defined in section \ref{sectionOLS} with the estimator obtained by the pre-treatment that is $\boldsymbol{X}_f$ selection.
  
For the marginal regression model defined in (\ref{modexpl})
%	\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}_f}= \boldsymbol{X}_f\boldsymbol{\beta}_f^*+ \boldsymbol{\varepsilon}_Y^*
%	\end{equation}			
%		So 
we have the \textsc{OLS} unbiased estimator of $\boldsymbol{\beta}^*$: 
		\begin{equation}
			\hat{\boldsymbol{\beta}}_f^* = (\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{Y}  \textrm{ and }\boldsymbol{\hat\beta}_r^* = \boldsymbol{0}
		\end{equation}
		We see in (\ref{Trueexpl}) that it gives an unbiased estimation of $\boldsymbol{Y}$ and $\boldsymbol{\beta^*}$
		but in terms of $\boldsymbol{\beta}$ this estimator is biased:
		\begin{equation}
			\operatorname{E}[\hat{\boldsymbol{\beta}}_f^*|\boldsymbol{X}_f]=\boldsymbol{\beta}_f+\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \textrm{ and }\operatorname{E}[\hat{\boldsymbol{\beta}}_r^*|\boldsymbol{X}_f]=\boldsymbol{0}
		\end{equation}
		with variance:
		\begin{equation}
			\operatorname{Var}[\hat{\boldsymbol{\beta}}_f^*|\boldsymbol{X}_f]= (\sigma^2_Y+\sum_{j \in I_r}\sigma^2_{j}\beta_{j}^2 )(\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1}  \textrm{ and }\operatorname{Var}[\hat{\boldsymbol{\beta}}_r^*|\boldsymbol{X}_f]= \boldsymbol{0} 
		\end{equation}
		We see that the variance is reduced compared to OLS described in equation (\ref{eqOLS})(no correlations and smaller matrix give better conditioning ) for small values of $\sigma_j$ $i.e.$ strong correlations. So we play on the bias-variance tradeoff, reducing the variance by adding a bias. 				  
		  
		  
	 The Mean Squared Error (\textsc{MSE}) on $\hat{\boldsymbol{\beta}}$ is:
	\begin{eqnarray}
		\textsc{MSE}(\hat{\boldsymbol{\beta}}|\boldsymbol{X})&=&\parallel \operatorname{Bias}\parallel_2^2+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}})) \\
			\textsc{MSE}(\hat{\boldsymbol{\beta}}_{OLS}|\boldsymbol{X})&=& 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}) %\textrm{ for OLS, and then for the marginal model:}
			 \\
			\textsc{MSE}(\hat{\boldsymbol{\beta}}^*_{OLS}|\boldsymbol{X})&=& \parallel\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \parallel_2^2 +\parallel \boldsymbol{\beta}_r\parallel^2_2 + (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}\beta_{j}^2 ) \operatorname{Tr}((\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1})
	\end{eqnarray}	 
	To better illustrate the bias-variance tradeoff, we look at the running example. We observe the theoretical Mean Squared Error (MSE) of the estimator of both OLS and \textsc{CorReg}'s marginal  model for several values of $\sigma_3$ (strength of the sub-regression) and $n$. Figure \ref{MQE1} shows the theoretical MSE evolution with the strength of the sub-regression:
	\begin{equation}
		1-\mathcal{R}^2=\frac{\operatorname{Var}(\boldsymbol{\varepsilon)_3}}{\operatorname{Var}(\boldsymbol{x}_3)}=\frac{\sigma_3^2}{\sigma_3^2+2}
	\end{equation}
	
\begin{figure}[h!]
%	\begin{minipage}[l]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn15sigmaY10.png} 
%			\caption{For $n=15$. Dotted: \textsc{Correg}, plain: OLS}\label{MQE1}
%	\end{minipage} \hfill
%	\begin{minipage}[c]{.32\linewidth}
%			\includegraphics[ width=170px]{figures/MQEn100sigmaY10.png} 
%			\caption{For $n=100$. Dotted: \textsc{Correg}, plain: OLS}
%	\end{minipage} \hfill
%   \begin{minipage}[r]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn1000sigmaY10.png} 
%			\caption{For $n=1000$. Dotted: \textsc{Correg}, plain: OLS.} \label{MQE3}
%   \end{minipage} 
	\includegraphics[width=500px]{figures/MQEexplOLSp5.png}\label{MQE1}
	\caption{MSE of OLS (plain) and CorReg (dotted) estimators for varying $(1-R^2)$ of the sub-regression, $n$ and $\sigma_Y$.}
\end{figure} 
It is clear in Figure \ref{MQE1} that the marginal model is more robust than \textsc{OLS} on $\boldsymbol{X}$. And when sub-regression get weaker ($1-\mathcal{R}^2$ tends to 1) it remains stable until extreme values (sub-regression nearly fully explained by the noise). We also see that the error implied by strong correlations shrinks with the rise of $n$. 
We see that $\sigma_Y$ multiplies $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}))=\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{f}))+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))$ for both models but for the marginal model $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{r}))=0$.
 Thus, when $\sigma_Y^2$ rises it increases the advantage of \textsc{CorReg} versus \textsc{OLS}. It illustrates the importance of dimension reduction when the model has a strong noise (very usual case on real datasets where true model is not even exactly linear). Further results are provided in sections \ref{sectionsimul} and \ref{sectionrealcase}.

	
\chapter{Estimation of the Structure of subregression by MCMC}
	\section{How to compare structures ?}
		\subsection{Bayesian criterion for quality}
		\subsection{Some indicators for proximity}
		The first criterion is $\psi(\boldsymbol{X},S)$ which is maximized in the MCMC. But in our case, it is estimated by the likelihood (see (\ref{approxBIC}))whose value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $S$ we define some indicators to compare the true model $S$ and the found one $\hat{S}$.
			Global indicators :
			\begin{itemize}
				\item $TL$ (True left) : the number of found dependent variables that really are dependent $TL=|I_r\cap \hat{I}_r|$ 
				\item $WL$ (Wrong left) : the number of found dependent variables that are not dependent $WL=|\hat{I}_r|-TL$
				\item $ML$ (Missing left) : the number of really dependent variables not found $ML=|I_r|-TL$
				\item $\Delta p_r$ : the gap between the number of sub-regression in both model : $\Delta p_r=|I_r|-|\hat{I}_r|$. The sign defines if $\hat{S}$ is too complex or too simple
				\item $\Delta compl$ : the difference in complexity between both model : $\Delta compl=\sum_{j \in p_r}p_f^j-\sum_{j \in \hat{p}_r}\hat{p}_f^j$
			\end{itemize}
	\section{Neighbourhood}
		\subsection{Classical}
		\subsection{Active relaxation of the constraints}
	\section{The walk}
	
	\section{Numerical results on simulated datasets} \label{sectionsimul}


	\subsection{The datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if \textsc{CorReg} 	keeps its promises.
	The \textsc{CorReg} package has been tested on simulated datasets. 
Section \ref{compZ} shows the results obtained in terms of $\hat{S}$. Sections \ref{tableMSEsimtout} and \ref{tableMSEsimgauche} show the results obtained using only \textsc{CorReg}, or \textsc{CorReg} combined with other methods. Tables give both mean and standard deviation of the observed Mean Squared Errors (MSE) on a validation sample of $1 000$ individuals. For each simulation,  $p=40$, the $R^2$ of the main regression is $0.4$, variables in $\boldsymbol{X}_f$ follow Gaussian mixture models of $\lambda=5$ classes which means follow Poisson's law of parameter $\lambda=5$ and which standard deviation is $\lambda$. The $\beta_j$ and the coefficients of the $\boldsymbol{\alpha}_j$ are generated according to the same Poisson law but with a random sign. $\forall j \in I_r, p_1^j=2$ (sub-regressions of length 2) and we have $p_r=16$ sub-regressions. The datasets were then scaled so that covariates $X_r$ don't have a greater variance or mean.
	We used \textsc{Rmixmod} to estimate the densities of each covariate. For each configuration, the MCMC walk was launched on $10$ initial structures with a maximum of 1 000 steps each time.
	When $n<p$, a frequently used method is the Moore-Penrose generalized inverse \cite{katsikis2008fast}, thus OLS can obtain some results even with $n<p$. %(see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
	When using penalized estimators for selection, a last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}) and is applied for both classical and marginal model.
	We compare different methods with and without CorReg as a pretreatment. All the results are provided by the CorReg package.
	
		\subsubsection{Results on $\hat S$}	\label{compZ}


\begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/BIC_p2.png} 
			\caption{Quality of the subregressions found with classical $BIC$ criterion}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/BICSTAR_P2.png} 
			\caption{Quality of the subregressions found with our $BIC_+$ criterion} 
   \end{minipage}
\end{figure}






\clearpage
\subsection{Results on prediction}

\subsubsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$ (best case for us)}	 \label{tableMSEsimgauche}
\begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X1_compl_NB.png} 
			\caption{Comparison of the complexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
 
%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X1_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X1_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X1_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
   \end{minipage}
\end{figure}

\clearpage
	\subsubsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	 	
We then try the method with a response depending on all covariates (\textsc{CorReg} reduces the dimension and can't give the true model if there is a structure). The datasets used here were those from table \ref{compZvrai}. 
 
 
 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_tout_compl_NB.png} 
			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
 
%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_tout_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_tout_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_tout_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
   \end{minipage}
\end{figure}

We see that CorReg tends to give more parsimonious models and better predictions, even if the true model is not parsomious. We logically observe that when $n$ rises, all the models get better and the correlations cease to be a problem so the complete model starts to be better (CorReg does not allow the true model to be choosen).




\clearpage
	\subsubsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $\boldsymbol{X}_r$. The datasets used here were still those from \ref{compZvrai}.
Depending only on $\boldsymbol{X}_r$ implies sparsity and impossibility to obtain the true model when using the true structure. 

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X2_compl_NB.png} 
			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
\textsc{CorReg} is still better than OLS for strong correlations and limited values of $n$. 
 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X2_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X2_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X2_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise}
   \end{minipage}
\end{figure}



\part{Further usage of the structure}	
\chapter{Missing values}
	Real datasets often have missing values and it is a very recurrent issue in industry. We note $\boldsymbol{M}$ the $n\times p$ binary matrix indicating whereas a value is missing (1) or not (0) in $\boldsymbol{X}$.
	We note $\boldsymbol{X}_M$ the missing values and $\boldsymbol{X}_{O}$ the observed values. $\Theta$ stands for the parameters of the Gaussian mixture followed by $\boldsymbol{X}$.
	$\boldsymbol{\alpha}$ is the matrix of the sub-regression coefficients with $\alpha_{i,j}$ the coefficients associated to $\boldsymbol{X}^i$ in the sub-regression explaining $\boldsymbol{X}^j$.\\ 
			Here we suppose that missing values are Missing Completely At Random (MCAR). 
	 Many methods does exist to manage such problems \cite{little1992regression} but they make approximation , add noise (imputation methods) or delete information (cutting methods).	
\section{Some results on missing values and Gaussian mixtures}
	\subsection{Decomposition of the integrated likelihood}
We start with the complete likelihood of $\boldsymbol{X}$
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})&=& \prod_{i=1}^n f(\boldsymbol{X}_i)= \prod_{i=1}^n\left[f(\boldsymbol{X}_i^{I_r}|\boldsymbol{X}_i^{I_f},\boldsymbol{\alpha},\Theta,S)f(\boldsymbol{X}_i^{I_f}|\boldsymbol{\alpha},\Theta,S) \right] \\
	&=&\prod_{i=1}^n\left[\prod_{j \in I_r}f(x_{i,j}|\boldsymbol{X}_i^{I_f},\boldsymbol{\alpha},\Theta,S)\prod_{j \notin I_r} f(x_{i,j}|\boldsymbol{\alpha},\Theta,S) \right] \\
	&=&\prod_{i=1}^n\left[\prod_{j \in I_r}f(x_{i,j}|\boldsymbol{X}_i^{I_f^j},\boldsymbol{\alpha},\Theta,S)\prod_{j \notin I_r} f(x_{i,j}|\boldsymbol{\alpha},\Theta,S) \right] \\
%	&=&\prod_{i=1}^n\left[\prod_{\substack{j \in I_r \\ \boldsymbol{M}_{i,j}=1}}P(x_{i,j}|\boldsymbol{X}_i^{I_f^j})\prod_{\substack{j \in I_r \\ \boldsymbol{M}_{i,j}=0}}P(x_{i,j}|\boldsymbol{X}_i^{I_f^j})
%			\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=1}} P(x_{i,j})\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}} P(x_{i,j}) \right] \\
	\mathcal{L}(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})&=&\sum_{i=1}^n\left[\sum_{j \in I_r}\log \left(f(x_{i,j}|\boldsymbol{X}_i^{I_f^j},\boldsymbol{\alpha},\Theta,S)\right)+\sum_{j \notin I_r} \log \left(f(x_{i,j}|\boldsymbol{\alpha},\Theta,S)\right) \right] \label{loglikmiss}
\end{eqnarray}
		In the MCMC we need to compute the likelihood of the dataset knowing the structure. When missing values occurs, we restrict the likelihood to the known values by integration on $\boldsymbol{X}_M$.
%		We have 
%	\begin{equation}
%		g(\boldsymbol{X}|\Theta)=\int_{\boldsymbol{X}_M}f(\boldsymbol{X}|\Theta)d \boldsymbol{X} \label{integralmiss}
%	\end{equation}
%For the covariates in $\boldsymbol{X}_f$, we use the density estimated  ($e.g.$ a Gaussian Mixture model estimated by \textsc{Mixmod}) or given as hypothesis. All individuals are supposed $iid$ so $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \notin I_r $: 
%				 \begin{equation}
%				 	g(x_{i,j}|\Theta)=f(x_{i,j}|\Theta)=\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k}) 
%				 \end{equation} with $k_j,\pi_{j,k}, \mu_{j,k}$ and $\Sigma_{j,k}$ estimated by Mixmod (for example). 
%\\				 		
				 		
%		Then we have
%		\begin{eqnarray}
%			g(\boldsymbol{X}|\Theta)&=& g(\boldsymbol{X}_r|\boldsymbol{X}_f,\Theta)g(\boldsymbol{X}_f|\Theta) \\
%			&=&\prod_{i=1}^n\left[ g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}} g(x_{i,j}|\Theta) \right]\\
%			&=&\prod_{i=1}^n\left[ g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)
%							\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}}\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k}) \right] \label{decomplikelimiss}
%		\end{eqnarray}
%		reminding that covariates in $\boldsymbol{X}_f$ are orthogonal. \\
%		
%	 Residuals of the sub-regressions are orthogonal but missing values can make the residuals dependent. We have to decompose more precisely $g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)$. To have a better view on the dependencies implied, we first write the marginal distributions. \\

We know that $\boldsymbol{X}$ is a Gaussian mixture ({\it iid} individuals, vectors of orthogonal Gaussian mixtures $\boldsymbol{X}^{I_f}$ and linear combinations of these Gaussian mixtures and some Gaussian for $\boldsymbol{X}^{I_r}$) with $K$ the number of its components.
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)&=&\int_{\boldsymbol{X}_M}L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})d\boldsymbol{X} \\
	&=&\int_{\boldsymbol{X}_M}\sum_{k=1}^K \pi_k \phi_k(\boldsymbol{X},\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} \\
	&=&\sum_{k=1}^K \pi_k \int_{\boldsymbol{X}_M}\phi_k(\boldsymbol{X},\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} \\
	&=&\sum_{k=1}^K \pi_k \int_{\boldsymbol{X}_M}\prod_{i=1}^n\phi_k(\boldsymbol{X}_i,\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} \\
	&=&\sum_{k=1}^K \pi_k \prod_{i=1}^n\int_{\boldsymbol{X}_{i,M}}\phi_k(\boldsymbol{X}_i,\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X}_i \\
	&=&\sum_{k=1}^K \pi_k \prod_{i=1}^n\phi_k(\boldsymbol{X}_{i,O},\boldsymbol{\alpha},\Theta,S)\\
	&=&\sum_{k=1}^K \pi_k \phi_k(\boldsymbol{X}_{O},\boldsymbol{\alpha},\Theta,S)=f(\boldsymbol{X}_{O},\boldsymbol{\alpha},\Theta,S)
\end{eqnarray}






% $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \notin I_r $:
%	 \begin{equation}
%	 \int_{\boldsymbol{X}_M}f(x_{i,j}|\boldsymbol{\alpha},\Theta,S) d\boldsymbol{X}=1
%	 \end{equation}	
To compute this likelihood, we will use the decomposition
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)&=&f(\boldsymbol{X}_{O},\boldsymbol{\alpha},\Theta,S)=\prod_{i=1}^nf(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)f(\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S) \\
	&=&\prod_{i=1}^nf(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)\prod_{\substack{j \in I_f \\ M_{i,j}=0}}f(x_{i,j};\boldsymbol{\alpha},\Theta,S)
\end{eqnarray}

with	  $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}= 0 \textrm{ and } j \notin I_r $:
	 \begin{equation}
	 f(x_{i,j};\boldsymbol{\alpha},\Theta,S)=\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k})
	 \end{equation} with $k_j,\pi_{j,k}, \mu_{j,k}$ and $\Sigma_{j,k}$ estimated by Mixmod (for example). 
%	 $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \in I_r $:
%	 \begin{equation}
%	 \int_{\boldsymbol{X}_M}f(x_{i,j}|\boldsymbol{X}_i^{I_f^j},\boldsymbol{\alpha},\Theta,S) d\boldsymbol{X}=1
%	 \end{equation}
\\
	 And, $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}= 0 \textrm{ and } j \in I_r $:
		\begin{eqnarray}
 f(x_{i,j}|\boldsymbol{X}_{i,O}^{I_f^j};\boldsymbol{\alpha},\Theta,S)&=& \sum_{k=1}^{K_{ij}}\pi_{ij,k}\Phi(x_{i,j},\mu_{ij,k},\Sigma_{ij,k}) \textrm{ where }  \label{Missingdensity}\\
				\boldsymbol{\pi}_{ij} &=& \bigotimes_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1 } } \boldsymbol{\pi}_l \textrm{ and  }K_{ij}=|\boldsymbol{\pi}_{ij}| ,\\
				\boldsymbol{\mu}_{ij}&=& \sum_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=0  }}\alpha_{l,j}x_{i,l} + \bigoplus_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1  }} \alpha_{l,j} \boldsymbol{\mu}_l \\
				\boldsymbol{\Sigma}_{ij} &=& \sigma_j^2 + \bigoplus_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1 }}\alpha_{i,l}^2 \boldsymbol{\Sigma}_l		
		\end{eqnarray}		 
		This could be easily used for imputation of the missing values in $\boldsymbol{X}^{I_r}$ knowing the parameters $\boldsymbol{\alpha}, \Theta$ and $S$. 
%		In first approximation we can suppose independence between the sub-regression:
%		\begin{equation}
%		\forall (j,j') \in I_r \times I_r, g(x_{i,j}| \boldsymbol{X}_{i}^{I_f},\Theta) \perp g(x_{i,j'}| \boldsymbol{X}_{i}^{I_f},\Theta)
%\end{equation}		 
%then we have the complete expression of the likelihood with \ref{decomplikelimiss} and \ref{Missingdensity}.
% Such approximation can be costless according to the position of the missing values ({\it e.g.} if they are all in $\boldsymbol{X}^{I_r}$). It is closer to the real model than the orthogonal hypothesis made by classical imputation by the mean. Moreover, sub-regressions are used only locally and errors don't cumulate whereas the true general decomposition combine many sub-regressions with cumulated noise of approximation. Thus, a general model would be better asymptotically but may not be efficient with finite dataset if the structure is complex. This first approximation is a good candidate to compare to the naive model (not taking into account the structure of sub-regression but making imputations by the mean for each covariate individually). 
%		
%		However, we write the real generalized expression for the log-likelihood.
%		Let $\mathcal{I}_r$ be a permutation of $I_r$ (arbitrary chosen, so the package will use identity). We define the general decomposition:
%		\begin{eqnarray}
%			g(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},\Theta)&=& \prod_{i=1}^n \left[g(x_{i,\mathcal{I}_r(p_r)}|\boldsymbol{X}_i^{I_f},\Theta)\prod_{j=1}^{p_r-1}g(x_{i,\mathcal{I}_r(j)}|\boldsymbol{X}_{i}^{\mathcal{I}_r(j+1)},\dots ,\boldsymbol{X}_{i}^{\mathcal{I}_r(p_r)}, \boldsymbol{X}_i^{I_f},\Theta)\right]
%		\end{eqnarray}
%		where we don't know the expression of $ g(x_{i,\mathcal{I}_r(j)}|\boldsymbol{X}_{i}^{\mathcal{I}_r(j+1)},\dots ,\boldsymbol{X}_{i}^{\mathcal{I}_r(p_r)}, \boldsymbol{X}_i^{I_f},\Theta)$ so the previous approximation stands still. 
%			
%	
%To estimate $\boldsymbol{\alpha}$ we use an EM algorithm. We start with an arbitrary value $\boldsymbol{\alpha}^{(0)}$, then:
%For the iteration $h$ of the algorithm at the E step we want 
%\begin{equation}
%	E_{\boldsymbol{X}_{\boldsymbol{M}}|\boldsymbol{X}_O,\boldsymbol{\alpha},\Theta,S}\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha}^{(h)},S,\Theta)\right]
%\end{equation}
%So we get $\boldsymbol{X}_M^{(h)}$ the  imputation for $\boldsymbol{X}_M$ and then the M step simply is
%\begin{equation}
%	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}E\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha},S,\Theta) \right]
%\end{equation}
%and we can use the same method as the one for classical case without missing values (OLS, SUR, {\it etc.}).
%		And we continue until convergence ($\parallel \boldsymbol{\alpha}^{(h+1)} - \boldsymbol{\alpha}^{(h)}\parallel < tol $ where $tol$ is the tolerance.\\
%		
%Imputation in $\boldsymbol{X}^{I_r}$ is made according to (\ref{Missingdensity}) and we have orthogonality in $\boldsymbol{X}^{I_f}$:
%\begin{displaymath}
%\forall j \notin I_r, P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f}\setminus \boldsymbol{X}^j)=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r})
%\end{displaymath}
%Moreover, $\forall j \notin I_r, \forall l \in I_r $ with $j \notin I_f^l, \boldsymbol{X}^j\perp \boldsymbol{X}^j$ so
%\begin{displaymath}
%\forall j \notin I_r, P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f}\setminus \boldsymbol{X}^j)=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r})=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r^j})
%\end{displaymath}
%where $I_r^j=\{i \in I_r| j \in I_f^j \}=\{i \in I_r|\alpha_{j,i}\neq 0 \}$ So we will make imputations (for E step only) according to $P(\boldsymbol{X}^j|\boldsymbol{X}_O^{I_r^j})$
%		\\
%$\forall 1 \leq i \leq n$	
%To use formulas on conditional distribution for Gaussian multivariate distribution we first write $P(\boldsymbol{X}_i^j,\boldsymbol{X}_i^{I_r^j})$ which is a Gaussian mixture with $K_{ij}$ components.
%\\
%$\forall j \notin I_r, \forall (l_1,l_2) \in I_r^j, P(x_{i,l_1}|x_{i,j},x_{i,l_2})=P(x_{i,l_1}|x_{i,j})$
%\begin{eqnarray}
%	P(x_{i,j},\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\alpha})&=& P(x_{i,j})\prod_{\substack{ l \in I_r^j \\M_{i,l=0} } }P(x_{i,l}|x_{i,j}) \\
%	&=&\sum_{k =1}^{ K_{ij}} \pi_{ij,k} \phi(x_{i,j},\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\mu}_{ij,k},\boldsymbol{\Sigma}_{ij,k}) \textrm{ where } \\
%	\boldsymbol{\pi}_{ij}&=&\boldsymbol{\pi}_j\otimes \left[ \bigotimes_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 } } \boldsymbol{\pi}_{ijl} \right]=\boldsymbol{\pi}_j\otimes\bigotimes_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 } } \left(\bigotimes_{\substack{h \in I_f^l \\ h \neq j} }\boldsymbol{\pi}_h \right)
%	 \textrm{ and  }K_{ij}=|\boldsymbol{\pi}_{ij}| ,\\
%	\boldsymbol{\mu}_{ij}&=&\boldsymbol{\mu}_j \times \left[\prod_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 }}\boldsymbol{\mu}_{ijl} \right]
%		=\boldsymbol{\mu}_j \times \prod_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 }}\left[\alpha_{j,l}x_{i,j}+\bigoplus_{\substack{h \in I_f^l}}\alpha_{h,l}\boldsymbol{\mu}_{h} \right] \\	
%		\boldsymbol{\Sigma}_{ij}&& \textrm{is the associated variance-covariance matrix}
%\end{eqnarray}
%		Cartesian product and power in the expression of the mean.
%		
%
%Then we have 
%\begin{eqnarray}
%	P(x_{i,j}|\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\alpha})&= &\sum_{k=1}^{K_{ij}}\pi_{ij,k}\phi(x_{i,j}|\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\mu}_{ij,k},\boldsymbol{\Sigma}_{ij,k}) \\
%	&=&\sum_{k=1}^{K_{ij}}\pi_{ij,k}\phi \left(x_{i,j},\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}},\Sigma_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}\right) \textrm{ with} \\
%	\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}&=& \mu_{j,k} + \operatorname{cov}(x_{i,j,k},\boldsymbol{X}_{i,O,k}^{I_r^j})\operatorname{var}(\boldsymbol{X}_{i,O,k}^{I_r^j})^{-1}(\boldsymbol{X}_{i,O}^{I_r^j}-\boldsymbol{\mu}_{\boldsymbol{X}_{i,O}^{I_r^j},k})\textrm{ and} \\
%	\Sigma_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}&=&\operatorname{var}(x_{i,j,k})-\operatorname{cov}(x_{i,j},\boldsymbol{X}_{i,O,k}^{I_r^j})\operatorname{var}(\boldsymbol{X}_{i,O,k}^{I_r^j})^{-1}\operatorname{cov}(\boldsymbol{X}_{i,O,k}^{I_r^j},x_{i,j,k})
%\end{eqnarray}
% 
%But we do not need to compute the variance because we only want $E_{\boldsymbol{X}_{\boldsymbol{M}}|\boldsymbol{X}_O,\boldsymbol{\alpha},\Theta,S}\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha}^{(h)},S,\Theta)\right]$ so the mean is sufficient, we impute  
%\begin{equation}
%	\hat{x}_{i,j}=\frac{1}{K_{ij}}\sum_{k=1}^{K_{ij}}\pi_{ij,k}\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}
%\end{equation}
%
%We have 
%\begin{eqnarray}
%	\forall l \in I_r^j,\forall 1\leq k \leq K_{ij}, \operatorname{cov}(x_{i,j,k},\boldsymbol{X}_{i,O,k}^{l})&=&\operatorname{cov}(x_{i,j,k},\sum_{h\in I_f^l}x_{i,h}\alpha_{h,l}+\varepsilon_{i,l})
%	=\alpha_{j,l}\sigma_{j,k}^2 \\
%	\forall l \in I_r^j,\forall 1\leq k \leq K_{ij}, \operatorname{var}(x_{i,l,k})&=&\sigma_l^2+\sum_{h \in I_f^l}\alpha_{h,l}^2\sigma_{h,k}^2 
%	\end{eqnarray}
%	$\forall l_1\neq l_2 \in I_r^j,\forall 1\leq k \leq K_{ij},$
%	\begin{eqnarray}
%	 \operatorname{cov}(x_{i,l_1,k},x_{i,l_2,k})&=&\operatorname{cov}(\sum_{h\in I_f^{l_1}}x_{i,h,k}\alpha_{h,l_1}+\varepsilon_{i,l_1},\sum_{h\in I_f^{l_2}}x_{i,h,k}\alpha_{h,l_2}+\varepsilon_{i,l_2})\\
%	&=& \operatorname{cov}(\sum_{h\in I_f^{l_1}}x_{i,h,k}\alpha_{h,l_1},\sum_{h\in I_f^{l_2}}x_{i,h,k}\alpha_{h,l_2}) \\
%	&=& \sum_{h \in I_f^{l_1}\cap I_f^{l_2}}\sigma_{h,k}^2\alpha_{h,l_1}\alpha_{h,l_2}
%\end{eqnarray}
%
%

\section{SEM}
	We use a SEM \cite{celeux1986algorithme} to estimate $\boldsymbol{\alpha}$ because the log-likelihood (\ref{loglikmiss}) is not linear. 
	\paragraph{initialization:} We start with imputation by the mean for each missing value (done only once for the MCMC). $\boldsymbol{\alpha}^{(0)}$ can be initialized by cutting method	(sparse structure) or using imputed values in $\boldsymbol{X}$.
	At iteration $h$,
	\paragraph{SE step:}
		We generate the missing values according to $P(\boldsymbol{X}_M|\boldsymbol{X}_O; \alpha^{(h)},\Theta,S)$, that is stochastic imputation.
	\paragraph{M step:}
		We estimate 
		\begin{equation}
	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}E\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha},S,\Theta) \right]
\end{equation}
and we can use the same method as the one for classical case without missing values (OLS, SUR, {\it etc.}).
		And we continue until convergence ($\parallel \boldsymbol{\alpha}^{(h+1)} - \boldsymbol{\alpha}^{(h)}\parallel < tol $ where $tol$ is the tolerance. Then we make $m$ iterations and take $\hat{\boldsymbol{\alpha}}$ as the mean of these $m$ last iterations.
		
	\subsection{Stochastic imputation by Gibbs sampling}
		We use a Gibbs sampling method to generate the missing values at the SE step. $\boldsymbol{X}$ follows a multivariate Gaussian mixture with $K$ component and we note $Z$ the set of the $Z_{i,j}$ indicating the component from which $x_{i,j}$ is generated.
		\paragraph{Initialisation:} all the $z_{i,j}$ are set to the first component (such an initialisation does not depend on $K$) and $\boldsymbol{X}_M$ are imputed by the marginal means.
		At each iteration of the Gibbs sampler: \\
			$\forall x_{i,j} \in \boldsymbol{X}_M^{I_r}$:  $x_{i,j}$ is generated according to 
			\begin{eqnarray}
			P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}},Z;\boldsymbol{\alpha}^{(h)},\Theta,S)&=&P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}};\alpha^{(h)},\Theta,S) \\
			&=&P(x_{i,j}|\boldsymbol{X}_i^{I_f^j};\boldsymbol{\alpha}^{(h)},\Theta,S)=\mathcal{N}(\boldsymbol{X}_i^{I_f^j}\boldsymbol{\alpha}^{(h)}_{I_f^j,j};\sigma_j^2 )
			\end{eqnarray}		
			We have $P(\boldsymbol{X}|Z)=\mathcal{N}(\boldsymbol{\mu}_{|Z},\boldsymbol{\Sigma}_{|Z})$. \\
			$\forall x_{i,j} \in \boldsymbol{X}_M^{I_f}$:  $x_{i,j}$ is generated according to 
			\begin{eqnarray}
			P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}},Z;\boldsymbol{\alpha}^{(h)},\Theta,S)&=&P(x_{i,j}|\boldsymbol{X}_{i,\bar{j}},Z_i;\boldsymbol{\alpha}^{(h)},\Theta,S)			\end{eqnarray}			
			\begin{eqnarray}
			=\mathcal{N}(\mu_{j|Z_{i,j}} + \boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}^{-1}_{X_{\bar{ij}},X_{\bar{ij}}|Z_i}(X_{\bar{ij}}-\boldsymbol{\mu}_{X_{\bar{ij}}|Z_i}) ;  \sigma_{j|Z_{i,j}}^2-\boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}^{-1}_{X_{\bar{ij}},X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}')
			\end{eqnarray}		
With $\forall j \in I_r$ 
\begin{equation}
	\operatorname{var}_{|Z_{i}}(x_{i,j})=\sigma_{j|Z_{i,j}}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{l|Z_i,l}^2=\sigma_{j}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{l|Z_i,l}^2
\end{equation}			
	$\forall j \notin I_r $
\begin{equation}
	\operatorname{var}_{|Z_{i}}(x_{i,j})=\sigma_{j|Z_{i,j}}^2
\end{equation}			
	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}\neq \emptyset $
	\begin{equation}
	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=\sum_{k\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{k,j_1}\alpha_{k,j_2}\operatorname{var}_{|Z_i}(x_{k|Z_i,k}) =\sum_{k\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{k,j_1}\alpha_{k,j_2}\sigma_{k|Z_{i,k}}^2
\end{equation}
	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}= \emptyset $
	\begin{equation}
	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=0
\end{equation}
	$\forall j_1 \in I_f, j_2 \in I_f$
	\begin{equation}
	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=0
\end{equation}
	$\forall j_1 \in I_r, j_2 \in I_f^{j_1}$
	\begin{equation}
	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})= \alpha_{j_2,j_1}\sigma^2_{j_2|Z_{i,j_2}}
\end{equation}
$\forall j_1 \in I_r, j_2 \notin I_f^{j_1}\cup I_r$
	\begin{equation}
	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})= 0
\end{equation}
We see that the $0$ in the variance-covariance matrix does not depend on $Z$ so the structure of sparsity of $\boldsymbol{\Sigma}$ can be stored and used back in each iteration for a given structure $S$ to reduce computing time.

	Then, $\forall 1\leq i \leq n, \forall j \in I_f$ we draw new values for $Z_{i,j}$ according to
	\begin{eqnarray}
		P(Z_{i,j}|\boldsymbol{X},Z_{\bar{i,j}};\Theta,\boldsymbol{\alpha},S)&=&P(Z_{i,j}|\boldsymbol{X}_i,Z_{i,\bar{j}};\Theta,\boldsymbol{\alpha},S)=\mathcal{M}(t_{i,j,1},\dots ,t_{i,j,K_j})
	\end{eqnarray}
		
	
	
	We see that $Z_{i,j}$ are not used if there is no missing values in $\boldsymbol{X}_i$ and others are not all needed so we can also optimize computation time by  computing only the $Z_{i,j}$ that are needed in the Gibbs.
	For the last iteration of the Gibbs, in the last iteration of the SEM, we do not need to draw $Z$.	
	
	Instead of using long chain for each Gibbs, we can use small chains because SEM iteration will simulate longer chains so it remains efficient with a smaller computation cost.
	
	Computation cost will be the main purpose here because we need an iterative algorithm (Gibbs sampler) at each iteration of another iterative algorithm (SEM) for each candidate of the MCMC.
	So alternative method should be preferred for large datasets with many missing values and only a small amount of time.
			
\subsection{Alternative E step}
	If we can't (or don't want to) compute the SE step described above, then we can use alternative imputation step for missing data based on $\boldsymbol{\alpha}$ (and keep the alternate optimisation to find the best $\boldsymbol{\alpha}$). 
	
	$\forall x_{i,j} \in \boldsymbol{X}_M $ we have:
	\\
if $j\in I_r$, Equation(\ref{Missingdensity}) gives: 
	\begin{eqnarray}
	E[x_{i,j}|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S]&=&E[\sum_{k=1}^{k_{ij}}\pi_{ij,k}\Phi(x_{i,j}|\mu_{ij,k},\Sigma_{ij,k})|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S] 
	\end{eqnarray}
	  
	Let $r_{i,j}=\{l \in I_r| \boldsymbol{\alpha}_{j,l}\neq 0, \boldsymbol{M}_{i,j}=0 \}$ the set of observed covariates for individual $i$ that are explained by $x_{i,j}$ according to $S$.
	\\
	If $j\notin I_r$ we can do:
	\begin{eqnarray}
	E[x_{i,j}|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S]&=&\frac{1}{|r_{i,j}|}\sum_{k \in r_{i,j}}E_{|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S}\left[\frac{1}{\alpha_{j,k}}\left(x_{i,k}-\varepsilon_{k}(i)-\sum_{l \in I_f^k} x_{i,l}\alpha_{l,k}\right)\right] \\
	&=& \frac{1}{|r_{i,j}|}\sum_{k \in r_{i,j}}E_{|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S}\left[\frac{1}{\alpha_{j,k}}\left(x_{i,k}- \sum_{l \in I_f^k} x_{i,l}\alpha_{l,k}\right)\right]
	\end{eqnarray}
	that is the mean of the expectations of the inverse sub-regressions implying $x_i,j$ with value in $\boldsymbol{X}^{I_r}_i$ not missing.



Another way is to only use the structure for $\boldsymbol{X}^{I_r}$ and use the distribution given by Mixmod for $\boldsymbol{X}^{I_f}$ along the MCMC. The full SEM would then be used only once with the final structure to make imputation in $\boldsymbol{X}$ before using variable selection methods like the LASSO.


%		\subsection{Estimation of the coefficients in each regression}
%			Estimating the $\boldsymbol{\alpha}_j$  with missing values is just estimating independent regressions with missing values. We have seen in equation (\ref{Missingdensity}) that we know the expression of this density for a given the $\boldsymbol{\alpha}_j$. So it's just about maximizing the likelihood of this density on the $\boldsymbol{\alpha}_j$. This can be done with an Expectation-Maximization (EM) algorithm \cite{dempster1977maximum} or one of its extensions \cite{mclachlan2007algorithm}.
%			
%step E: ($\Theta$ stands for the parameters of the gaussian mixtures for the marginal distributions, estimated once by Mixmod):
%\begin{equation}
%	\boldsymbol{X}^{(h)}=E[\boldsymbol{X}|\boldsymbol{X}_{O},\boldsymbol{\alpha}^{(h)},\boldsymbol{\varepsilon},\Theta,S]
%\end{equation}			
%	
%	step M:	
%\begin{equation}
%	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}(\mathcal{L}(\boldsymbol{X}^{(h)},\boldsymbol{\alpha},\boldsymbol{\varepsilon},\Theta,S)) \textrm{ by OLS}
%\end{equation}	
%	
%		
%			But estimation of the $\boldsymbol{\alpha}_j$ is the most critical part of the MCMC in terms of computational time so it could be a bad idea to put there another iterative algorithm. 
%			Alternatives does exist :
%			\begin{itemize}
%				\item Because sub-regression are supposed to be parsimonious, we could imagine to estimate each column of $\boldsymbol{\alpha}$ with full sub-matrices of $\boldsymbol{X}_f$. When relying on too much missing values, $\hat{\boldsymbol{\alpha}}$ would be a bad candidate and then penalized directly by the likelihood (and it could be a good thing). Computational cost would be reduced significantly.
%				\item To estimate the $\boldsymbol{\alpha}_j$ (and not for the global likelihood) we could use data imputation (by the mean) and then obtain a full matrix but still ignoring missing values when estimating the likelihood. Imputation only concerns the estimation of the sub-regression coefficients and because null coefficients in sub-regression are coerced at each step, imputation only concerns a few covariates each time.
%			\end{itemize}
%			
%			
%			 $\forall j \in I_r$, estimation of $\boldsymbol{\alpha}^j$ only depends on individuals not missing in $\boldsymbol{X}^j$ (individuals are {\it iid}).
%			 So we work with a restriction of $\boldsymbol{X}$ for each $\boldsymbol{\alpha}^j$. Thus in this section, to simplify the notation, we will consider no missing values in $\boldsymbol{X}_r$ but in fact we work with restrictions.
%			
%			The EM algorithm can be written here: we start with some $\Theta^{(0)}=(\boldsymbol{\alpha},\boldsymbol{\varepsilon}) $ initial value for $\Theta$. The $\pi_{ij,k}$ are estimated once for each covariate (for example by Mixmod) and stay the same during the EM algorithm.
%			Naive E step : estimation of 
%			\begin{equation}
%				\boldsymbol{X}^{(h)}=E(\boldsymbol{X}|\boldsymbol{X}_{\bar M},\Theta^{(h)}) \textrm{ so it simply is}
%			\end{equation}
%			$\forall (i,j), \boldsymbol{M}_{i,j}=1, j\neq I_r$, 			
%			\begin{equation}
%				x^{(h)}_{i,j}=E(x_{i,j}|\boldsymbol{X}_{\bar M},\Theta^{(h)}) =\sum_{k=1}^{k_{ij}}\pi_{ij,k}\mu_{ij,k}^{(h)} \label{Estep}
%			\end{equation}
%			where, $\forall j \in I_f, k_{ij}=k_j, \pi_{ij,k}=\pi_{j,k}, \mu_{ij,k}=\mu_{j,k}$ \\
%			M-step : we determine $\Theta^{(h+1)}$ as the solution of the equation
%			\begin{equation}
%				E(\boldsymbol{X}|\Theta)=\boldsymbol{X}^{(h)} \textrm{ done by OLS}
%			\end{equation}
%			So the M step is just computing linear regressions on the filled dataset.
%			
%			
%		real E step : individuals are $iid$ so we just look at the expression for one individual, and use it for all
%		$\forall 1\leq n \leq n , \forall j \notin I_r$, we note $\bar{\boldsymbol{X}}_{i,j}=(\boldsymbol{X}_{\bar M}\cap \boldsymbol{X}_{i} \setminus \boldsymbol{X}^j)$ 
%			\begin{eqnarray}
%				P(\boldsymbol{X}_{fi}^M,\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M} | \Theta)&=&
%					P(\boldsymbol{X}_{fi}^M|\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M}|\Theta) \\
%				&=&P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M}|\Theta) \\
%				P(\boldsymbol{X}_{fi}^M|\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M},\Theta)&=&\frac{P(\boldsymbol{X}_{ri}^M|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M}|\Theta)}{P(\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M}|\Theta)} \\
%				&=&\frac{P(\boldsymbol{X}_{ri}^{O}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M}|\Theta)P(\boldsymbol{X}_{fi}^{\bar M}|\Theta)
%				}{
%				P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{\bar M}|\Theta)
%				} \\
%				&=&\frac{P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M}|\Theta)}{P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{\bar M},\Theta)}
%			\end{eqnarray}
%			
%			No imputation for missing left. Imputations for missing right are just used to obtain $\hat{\boldsymbol{\alpha}}$ but not when computing the $BIC$ or $BIC_+$.
%			
		\subsection{Weighted penalty}
			Now we have defined the way to compute the likelihood, other questions remain : how to define the number of parameters in the structure ?		How to take into account missingness (structures relying on highly missing covariates should be penalized) ?
			We have seen that for a same covariate $X^j$ with $ j \in I_r$, the number of parameters is not the same for each individual depending whether or not $M_{i,j}=0$. But the penalty (for $\psi=BIC$) can't be added at the individual level (because $\log(1)=0$ so it would be annihilated). 
			
			To penalize models that suppose dependencies based only on a few individuals, we propose to use the mean of the complexities obtained for a given covariate.
			\begin{equation}
			k_j=\frac{1}{n}\sum_{i=1}^nk_{i,j}
\end{equation}						
			
			 Thus if a structure is only touched by one missing value the penalty will be smaller than another same shaped structure but with more missing values implied.
			Another way would be to use $\psi=RIC$ (see \cite{foster1994risk}) so the complexity is associated with $\log(p)$ and can be added individually. Another idea would be to make a compromise and penalize by $\frac{k_i\log(p)}{\log(n)}$.
		
%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	
					
			
			
	\section{Missing values in the main regression}
		The easier way would be to draw missing values with the SEM described above and then use classical methods on the completed dataset, with the possibility to repeat this procedure a few times and then take the mean. We should for example try multiple draw and LASSO for variable selection like variable selection by random forest.
		
		But another way would be to consider classical estimation methods as likelihood optimizer and then adapt them to the integrated likelihood of our model. Thus we can imagine to use LASSO without imputation. But the choice of the penalty using the LAR algorithm need also to adapt the LAR that is based on correlations that are computing on vectors with distinct number of individuals (due to missing values). So it requires a bit more reflexion but could be a good perspective for our method.
	\section{Numerical results}
		\subsection{Finding the structure}
		\subsection{Efficiency for main regression}
\chapter{Taking back the residuals}
	\section{The model}
	\section{Properties}
\begin{figure}[h!]
	\includegraphics[width=500px]{figures/res_these/MQE_toutOLSp5.png}\label{MQE2}
	\caption{MSE of OLS (plain black) and CorReg marginal(red dashed) and CorReg full (green dotted) estimators for varying $(1-R^2)$ of the sub-regression, $n$ and $\sigma_Y$.}
\end{figure}	
	
	\section{Consistency}
		\subsection{Consistency Issues}\label{consistency}
		Consistency issues of the LASSO are well known and Zhao \cite{Zhao2006MSC} gives a very simple example to illustrate it.
		We have taken the same example to show how our method is more consistent.
		Here $p=3$ and $n=1000$.We define $\boldsymbol{X}_f,\boldsymbol{X}_r,\boldsymbol{\varepsilon}_Y,\boldsymbol{\varepsilon}_{X} i.i.d. \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}_n)$ and then $X_3=\frac{2}{3}X_1+\frac{2}{3}X_2+\frac{1}{3}\varepsilon_X$ and $Y=2X_1+3X_2+\varepsilon_Y$.
		We compare consistencies of complete,explicative and predictive model with LASSO (and LAR) for selection.
		It happens that the algorithm don't find the true structure but a permuted one so we also look at the results obtained with the true $S$ (but $\hat{B}$ is used) and with the structure found by the Markov chain after a few seconds.
		
		True $S$ is found $340$ times on $1000$ tries.
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & \textsc{CorReg} Explicative & \textsc{CorReg} Predictive \\ 
		\hline 
		True $S$ &  1.006479 & \textbf{1.005468} & \textbf{1.006093} \\ 
		\hline 
		$\hat{Z}$ & \textbf{1.006479} & 1.884175 & 1.006517 \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observer on a validation sample (1000 individuals)}
		\end{table}

		We observe as we hoped that explicative model is better when using true $S$ (coercing real zeros) and that explicative with $\hat{S}$ is penalized (coercing wrong coefficients to be zeros).
		But the main point is that the predictive model stay better than the classical one whith the true $S$ and corrects enough the explicative model to follow the classical LASSO closely when using $\hat{S}$. 
		And when we look at the consistency :
		\begin{table}[h!]	
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  0 & 1000 & 830 \\ 
		\hline 
		$\hat{S}$ & 0 & 340 & \textbf{621} \\ 
		\hline 
		\end{tabular} 
		\caption{number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}
		\end{table}				
		
		$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		
		We also made the same experiment but with $X_1,X_2$ (and consequently $X_3$) following gaussian mixtures (to improve identifiability) randomly generated by our \textsc{CorReg} package for R. 
		True $S$ is now found $714$ times on $1000$ tries \label{testidentifiable}. So it confirms that non-gaussian models are easier to identify.
		
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  1.571029 & \textbf{1.569559} & \textbf{1.570801} \\ 
		\hline 
		$\hat{S}$ & 1.005402 & 1.465768 & \textbf{1.005066} \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observed on a validation sample (1000 individuals)}
		\end{table}

		And when we look at the consistency :
		\begin{table}[h!]
		\centering	
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  0 & 1000 & 789 \\ 
		\hline 
		$\hat{S}$ & 0 & 714 & \textbf{608} \\ 
		\hline 
		\end{tabular} 
		\caption{number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}
		\end{table}				
				
		
		$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		

	\section{Numerical results}

\chapter{CorReg: the R package}	
	
		\textsc{CorReg} is already downloadable on the CRAN under CeCILL Licensing. This package permits to generate datasets according to our generative model, to estimate the structure (C++ code) of regression within a given dataset and to estimate both explicative and predictive model with many regression tools (OLS,stepwise,LASSO,elasticnet,clere,spike and slab, adaptive lasso and every models in the \textsc{lars} package). So every simulation presented above can be done with \textsc{CorReg}.
	\textsc{CorReg} also provides tools to interpreat found structures and visualize the dataset (missing values and correlations). More informations can be found on the website www.correg.org which is dedicated to \textsc{CorReg}.
	
\chapter{Conclusion and perspectives}
	\section{Conclusion}
		Our model is easy to understand and to use. Usage of linear regression to model the correlations definitely separates us from "black boxes" so users are confident in what they do. The well-known and trivial sub-regression found comfort users in that if a structure does exist, CoMPASS will find it so when a new sub-regression, or a new main regression is given they are more likely to look further and try it. The automated aspect shows the power of statistics without a priori so users begin to understand that statistics are not only descriptive or predictive but based on {\it a priori} models. This method has a positive impact on the way users looks at the statistics.
			It is good to see that sequential methods (predictive model) and automation can produce good results. Probabilistic models are efficient even without human expertise and let the experts improve the results by adding their expertise in the model (coercing some sub-regression for example).
		
		
	\section{Perspective}
		\subsection{Non-linear regression}
			Polynomial regression, logistic regression, {\it etc.} could be improved by a method like this.
		\subsection{Pretreatment not only for regression}
			Classification and Regression Tree, and any other method could benefit of the variable selection pretreatment implied by our marginal model.
		\subsection{Improved programming}
			Even if it is written in C++, the algorithm could be optimized by a better usage of sparse matrices, memory usage optimization, and other small things that could reduce computational cost to be faster and allow to work with larger datasets (already works with thousands of covariates).
		\subsection{Missing values in classical methods}
			The full generative approach could be used to manage missing values without imputation for many classical methods.
		\subsection{Interpretation improvements}
			Ergonomy of the software could be improved to better fit industrial needs.
\cleardoublepage

\addcontentsline{toc}{chapter}{References}
\bibliographystyle{apalike}
\bibliography{biblio}
\addcontentsline{toc}{chapter}{Appendices}
\appendix
	\chapter{Graphs and CorReg}
		\section{Matricial notations}
		\section{Properties}
	\chapter{Mixture models}
		\section{Linear combination}
			
		\section{Industrial examples}	
\end{document}
