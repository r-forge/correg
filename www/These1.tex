\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Clément THERY}
\title{\textsc{CorReg}: Linear regression with highly correlated covariates}
\begin{document}
\maketitle
\newpage
\itshape To my sons, \upshape
\tableofcontents
\chapter{Abstracts}
\chapter{Acknowledgments}
\chapter{The industrial context}
	This work takes place in a steel industry context. The main objective is to be able to solve quality crisis when they occur. In such a case, a new type of unknown quality issue is observed and we have no idea of its origin. The defects, even generated at the beginning of the process, are often detected in its last part. The steel-making process includes several sub-process, each implying a whole manufactory. Thus we have many covariates and no a priori on the relevant ones. Moreover, the values of each covariates essentially depends on the characteristics of the final product, and many physical laws and tuning models are implied in the process. Therefore the covariates are highly correlated.
	We have several constraints :
	\begin{itemize}
		\item To be able to predict the defect and stop the process as early as possible to gain time (and money)
		\item To be able to understand the origin of the defect to try to optimize the process
		\item To be able to find parameters that can be changed because the objective is not only to understand but to correct the problematic part of the process.
		\item It also must be fast and automatic (without any a priori).
	\end{itemize}
	We will see in the state of the art that correlations are a real issue and that the number of variables increases the problem.	
	The stakes are very high because of the high productivity of the steel plants but also because steel making is now well-known and optimized thus new defects only appears on innovative steels with high value. Any improvement on such crisis can have important impact on the market shares and when the customer is implied, each day won by the automation of the data mining process can lead to a gain of hundreds of thousands of euros, sometimes more. So we really need a kind of automatic method, able to manage the correlations without any a priori and giving an easily understandable and flexible model.
	
	
\chapter{State of the art}
In the following we note classical norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$, $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^p|\beta_i| $ and $\parallel\boldsymbol{\beta} \parallel_{\infty}=\operatorname{max}(|\beta_1|,\dots,|\beta_p|)$.
	\section{Ordinary least squares and associated problems}
	Linear regression is defined by this simple equation:
	\begin{equation}
		\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}_Y
	\end{equation}
	where $\boldsymbol{Y}\in \mathbf{R}^n$ is the response variable vector observed on $n$ individuals. 
	\section{Penalized models}
		\subsection{Ridge regression}
			%\cite{hoerl1970ridge}
			%\cite{marquardt1975ridge}
Ridge regression \cite{marquardt1975ridge} proposes a biased estimator that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \lambda \textrm{ with } \lambda>0
	\end{equation}
	But this penalty is not guided by the correlations. It is the same for each covariates and will be too large for independent covariates and/or too small for correlated ones. So the efficiency of such a method is limited. 
	Moreover, coefficients tend to 0 but don't reach 0 so it gives difficult interpretations for large values of $p$. 
				
			
		\subsection{LASSO: Least Absolute Shrinkage and Selection Operator }
			\cite{tibshiranilasso}  
			\cite{tibshirani1996regression} 
			\cite{efron2004least} %LAR
			\cite{Zhao2006MSC}%problèmes du lasso/lars en correlations
			\cite{SAM10088}%lars necessite OLS en surcouche
The Least Absolute Shrinkage and Selection Operator (\textsc{LASSO} \cite{tibshirani1996regression}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0
		\end{equation}	
	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) algorithm offers a very efficient way to obtain the whole LASSO path and is very attractive. It requires only the same order of magnitude of computational effort as \textsc{OLS} applied to the full set of covariates. But like the ridge regression, the penalty does not distinguish correlated and independent covariates so there is no guarantee to have less correlated covariates.



		\subsection{Adaptative LASSO and Random LASSO}
			\cite{zou2006adaptive}% adaptative lasso
			\cite{wang2011random}%random lasso
			 Some recent variants of the \textsc{LASSO} do exist for the choice of the penalization coefficient like the adaptative \textsc{LASSO} \cite{zou2006adaptive} or the random \textsc{LASSO} \cite{wang2011random}.  But \textsc{LASSO} also faces consistency problems \cite{Zhao2006MSC} when confronted with correlated covariates.
		\subsection{Elasticnet}
			\cite{zou2005regularization}
			Elastic net \cite{zou2005regularization} is a method developed to be a compromise between Ridge regression and the \textsc{LASSO}: 
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	%Elastic net can be written:
	\begin{equation}
		\boldsymbol{\hat{\beta}}=(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta} \parallel_2^2 \right\rbrace, \textrm{ subject to } (1-\alpha)\parallel\boldsymbol{\beta}\parallel_1+\alpha\parallel\boldsymbol{\beta}\parallel_2^2\leq t \textrm{ for some } t
	\end{equation}
	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	%It seems to give good predictions. 
	But it is based on the grouping effect so correlated covariates get similar coefficients and are selected together whereas LASSO will choose between one of them and will then obtain same predictions with a more parsimonious model. Once again, nothing specifically aims to reduce the correlations. %Hence, when comparing the two models, interpretations are not the same and nothing explicitly explains why. So it can be very confusing. 
		\subsection{OSCAR: Octogonal Shrinkage and Clustering Algorithm for Regression }
			%\cite{bondell2008simultaneous}%Oscar
			Like elasticnet, \textsc{OSCAR} \cite{bondell2008simultaneous} uses combination of two norms for its penalty. Here the objective is to group covariates with the same effect (by a pairwise $L_\infty$ norm) and give them exactly the same coefficient (reducing the dimension) with a simultaneous variable selection (implied by the $L_1$ norm).
			\begin{equation}
				\hat{\boldsymbol{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \parallel\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta} \parallel^2_2 \textrm{ subject to } \sum_{j=1}^p|\beta_j|+c\sum_{j<k}\operatorname{max}(|\beta_j|,|\beta_k|) \leq \lambda		
			\end{equation}						
			But \textsc{OSCAR} depends on two tuning parameters: $c$ anf $\lambda$. For a fixed $c$ the $\lambda$ can be found by the \textsc{LAR} algorithm but $c$ still has to be found "by hand" comparing final models for many values of $c$.
			Correlations are only implicitely taken into account and only pairwise. So it lacks of an efficient algorithm and need a supplementary study to interpret the groups found.
	\section{Modeling the parameters}	
		\subsection{CLERE: CLusterwise Effect REgression}
			\cite{yengo2012variable}%clere
			The CLusterwise Effect REgression (\textsc{CLERE} \cite{yengo2012variable}) describes the $\beta_j$ no longer as fixed effect parameters but as unobserved independant random variables with grouped $\beta_j$ following a Gaussian Mixture distribution. The idea is to hope that the model have a small number of groups of covariates and that the mixture will have few enough components to have a number of parameters to estimate significantly lower than $p$. In such a case, it improves interpretability and ability to yeld reliable prediction with a smaller variance on $\boldsymbol{\hat{\beta}}$. 
	
		\subsection{Spike and Slab}	
			\cite{ishwaran2005spike}%spike and slab
			Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the $\beta_j$ and gives a subset of covariates (not grouped) on which to compute \textsc{OLS} but has no specific protection against correlations issues.
	\section{Multiple Equations}
		\subsection{SEM and Path Analysis}
		\subsection{SUR: Seemingly Unrelated Regression}
			\cite{SURzellner}
		\subsection{SPRING: Structured selection of Primordial Relationships IN the General linear model}
			\cite{chiquetconf}			
			
		\subsection{Selvarclust: Linear regression within covariates for clustering}
			\cite{maugis2009variable}
			The idea is to allow covariates to have different roles : $(S,R,U,W)$.
			But:
			\begin{itemize}
				\item It is about clustering and not regression (not the same application field)
				\item No sub-regression allowed between relevant variables (in the True model)
				\item Using stepwise-like algorithm without protection against correlations \cite{raftery2006variable} even it is known to be often unstable \cite{miller2002subset}
			\end{itemize}	
			We provide an specific MCMC algorithm with the ability to have redundant covariates in the true model.		 
\part{\textsc{CorReg} : the concept}
\chapter{Decorrelating covariates by a generative model}
	\section{Generative model}
	\section{Properties}
		\subsection{general properties}
		\subsection{Identifiability}
	\section{About grouping effect}
\chapter{Estimation of the Structure of subregression by MCMC}
	\section{How to compare structures ?}
		\subsection{Bayesian criterion for quality}
		\subsection{Some indicators for proximity}
	\section{Neighbourhood}
		\subsection{Classical}
		\subsection{Active relaxation of the constraints}
	\section{The walk}
	\section{Numerical results}
\part{Further usage of the structure}	
\chapter{Taking back the residuals}
	\section{The model}
	\section{Properties}
	\section{Consistency}
		\subsection{Consistency Issues}\label{consistency}
		Consistency issues of the LASSO are well known and Zhao \cite{Zhao2006MSC} gives a very simple example to illustrate it.
		We have taken the same example to show how our method is more consistent.
		Here $p=3$ and $n=1000$.We define $\boldsymbol{X}_f,\boldsymbol{X}_r,\boldsymbol{\varepsilon}_Y,\boldsymbol{\varepsilon}_{X} i.i.d. \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}_n)$ and then $X_3=\frac{2}{3}X_1+\frac{2}{3}X_2+\frac{1}{3}\varepsilon_X$ and $Y=2X_1+3X_2+\varepsilon_Y$.
		We compare consistencies of complete,explicative and predictive model with LASSO (and LAR) for selection.
		It happens that the algorithm don't find the true structure but a permuted one so we also look at the results obtained with the true $S$ (but $\hat{B}$ is used) and with the structure found by the Markov chain after a few seconds.
		
		True $S$ is found $340$ times on $1000$ tries.
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & \textsc{CorReg} Explicative & \textsc{CorReg} Predictive \\ 
		\hline 
		True $S$ &  1.006479 & \textbf{1.005468} & \textbf{1.006093} \\ 
		\hline 
		$\hat{Z}$ & \textbf{1.006479} & 1.884175 & 1.006517 \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observer on a validation sample (1000 individuals)}
		\end{table}

		We observe as we hoped that explicative model is better when using true $S$ (coercing real zeros) and that explicative with $\hat{S}$ is penalized (coercing wrong coefficients to be zeros).
		But the main point is that the predictive model stay better than the classical one whith the true $S$ and corrects enough the explicative model to follow the classical LASSO closely when using $\hat{S}$. 
		And when we look at the consistency :
		\begin{table}[h!]	
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  0 & 1000 & 830 \\ 
		\hline 
		$\hat{S}$ & 0 & 340 & \textbf{621} \\ 
		\hline 
		\end{tabular} 
		\caption{number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}
		\end{table}				
		
		$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		
		We also made the same experiment but with $X_1,X_2$ (and consequently $X_3$) following gaussian mixtures (to improve identifiability) randomly generated by our \textsc{CorReg} package for R. 
		True $S$ is now found $714$ times on $1000$ tries \label{testidentifiable}. So it confirms that non-gaussian models are easier to identify.
		
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  1.571029 & \textbf{1.569559} & \textbf{1.570801} \\ 
		\hline 
		$\hat{S}$ & 1.005402 & 1.465768 & \textbf{1.005066} \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observed on a validation sample (1000 individuals)}
		\end{table}

		And when we look at the consistency :
		\begin{table}[h!]
		\centering	
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  0 & 1000 & 789 \\ 
		\hline 
		$\hat{S}$ & 0 & 714 & \textbf{608} \\ 
		\hline 
		\end{tabular} 
		\caption{number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}
		\end{table}				
				
		
		$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		

	\section{Numerical results}

\chapter{Missing values}
	Missing values are a very recurrent issue in industry. We note $\boldsymbol{M}$ the $n\times p$ binary matrix indicating whereas a value is missing (1) or not (0) in $\boldsymbol{X}$.
	We note $\boldsymbol{X}_M$ the missing values. $\Theta$ stands for the parameters of the Gaussian mixture followed by $\boldsymbol{X}$.
	$\boldsymbol{\alpha}$ is the matrix of the sub-regression coefficients with $\alpha_{i,j}$ the coefficients associated to $\boldsymbol{X}^i$ in the sub-regression explaining $\boldsymbol{X}^j$
	We have 
	\begin{equation}
		g(\boldsymbol{X}|\Theta)=\int_{\boldsymbol{X}_M}f(\boldsymbol{X}|\Theta)d \boldsymbol{X}
	\end{equation}
	\section{How to manage missing values in the MCMC ?}
			\subsection{Position of the missing value}

		In the MCMC we need to compute the likelihood of the structure. When missing values occurs, we restrict the likelihood to the known values. So we look at each non-missing value separately. Each value is seen as different random variable noted $x_{i,j}$ for the $i^{th}$ value of $\boldsymbol{X}^j$.  But the structure itself makes things more complicated because known values are not all $iid$ (if $j \in I_r$ ).
			For a given structure $S$, missing values can imply different consequences according to their position in the dataset.
			We decompose the joint distribution $g(\boldsymbol{X}|\Theta)=g(\boldsymbol{X}_r|\Theta,\boldsymbol{X}_f)g(\boldsymbol{X}_f|\Theta)$.
			To compute the likelihood of a value $x_{i,j}$ in the dataset, if $\boldsymbol{M}_{i,j}=1$ : $x_{i,j}$ is not considered because we restrict the likelihood to known values (integrated likelihood).
				 else if $j \in I_f$ : like in previous method, we use the density estimated  ($e.g.$ a Gaussian Mixture model estimated by \textsc{Mixmod}) for $\boldsymbol{X}^j$. Values of $\boldsymbol{X}^j$ are $iid$: 
				 \begin{equation}
				 	g(\boldsymbol{x}_{i,j}|\Theta)=f(\boldsymbol{x}_{i,j}|\Theta)=\sum_{k=1}^{k_j}\pi_{j,k}\Phi(\boldsymbol{x}_{i,j}|\mu_{j,k},\Sigma_{j,k}) 
				 \end{equation} with $\Theta$ estimated by Mixmod and
				 else, $\forall j \in I_r$:
			\begin{eqnarray}
				g(x_{i,j}| \boldsymbol{X}_{i,f},\Theta)=g(x_{i,j}| \boldsymbol{X}_{i,I_f^j},\Theta) &=& \sum_{k=1}^{k_{ij}}\pi_{ij,k}\Phi(x_{i,j}|\mu_{ij,k},\Sigma_{ij,k}) \textrm{ where }  \label{Missingdensity}\\
				\boldsymbol{\pi}_{ij} &=& \bigotimes_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1 } } \boldsymbol{\pi}_l \textrm{ and  }k_{ij}=|\pi_{ij}| ,\\
				\boldsymbol{\mu}_{ij}&=& \sum_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=0  }}\alpha_{l,j}x_{i,l} + \bigoplus_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1  }} \alpha_{l,j} \boldsymbol{\mu}_l \\
				\boldsymbol{\Sigma}_{ij} &=& \sigma_j^2 + \bigoplus_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1 }}\alpha_{i,l}^2 \boldsymbol{\Sigma}_l
			\end{eqnarray}
			 
					
			
		\subsection{Weighted penalty}
			Now we have defined the way to compute the likelihood, other questions remain : how to define the number of parameters in the structure ?		How to take into account missingness (structures relying on highly missing covariates should be penalized) ?
			We have seen that for a same covariate $X^j$ with $ j \in I_r$, the number of parameters is not the same for each individual depending whether or not $M_{i,j}=0$. But the penalty (for $\psi=BIC$) can't be added at the individual level (because $\log(1)=0$ so it would be annihilated). 
			
			To penalize models that suppose dependencies based only on a few individuals, we propose to use the mean of the complexities obtained for a given covariate. Thus if a structure is only touched by one missing value the penalty will be smaller than another same shaped structure but with more missing values implied.
			Another way would be to use $\psi=RIC$ (see \cite{foster1994risk}) so the complexity is associated with $\log(p)$ and can be added individually. Another idea would be to make a compromise and penalize by $\frac{k_i\log(p)}{\log(n)}$.
		
%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	
		
		\subsection{Estimation of the coefficients in each regression}
			Estimating the $\boldsymbol{\alpha}_j$ and $\boldsymbol{\beta}$ with missing values is just estimating independent regressions with missing values. We have seen in equation (\ref{Missingdensity}) that we know the expression of this density for a given the $\boldsymbol{\alpha}_j$. So it's just about maximizing the likelihood of this density on the $\boldsymbol{\alpha}_j$. This can be done with an Expectation-Maximization (EM) algorithm \cite{dempster1977maximum} or one of its extensions \cite{mclachlan2007algorithm}.
			But estimation of the $\boldsymbol{\alpha}_j$ is the most critical part of the MCMC in terms of computational time so it could be a bad idea to put there another iterative algorithm. 
			Alternatives does exist :
			\begin{itemize}
				\item Because sub-regression are supposed to be parsimonious, we could imagine to estimate each column of $\boldsymbol{\alpha}$ with full sub-matrices of $\boldsymbol{X}_f$. When relying on too much missing values, $\hat{\boldsymbol{\alpha}}$ would be a bad candidate and then penalized directly by the likelihood (and it could be a good thing). Computational cost would be reduced significantly.
				\item To estimate the $\boldsymbol{\alpha}_j$ (and not for the global likelihood) we could use data imputation (by the mean) and then obtain a full matrix but still ignoring missing values when estimating the likelihood. Imputation only concerns the estimation of the sub-regression coefficients and because null coefficients in sub-regression are coerced at each step, imputation only concerns a few covariates each time.
			\end{itemize}
			The EM algorithm can be written here :
			
			
			
	\section{Missing values in the main regression}
		\subsection{explicative}
			The reduced model (explicative one) is just a linear regression without structure so estimating $\boldsymbol{\beta}$ is like estimating the $\boldsymbol{\alpha}_j$ and the same methods can be used. An EM algorithm would be prefered because this estimation is out of the MCMC, will be computed only one time and is the final objective where we want to minimize the error.
		\subsection{predictive}
			If there are missing values in $\boldsymbol{X}^j \in \boldsymbol{X}_r$ a new possibility appears. Knowing $S$ and $\boldsymbol{\alpha}_j$ we are able to try a conditional imputation based on the corresponding sub-regression, like every time someone use linear regression for prediction. 
			%si manquant à gauche : EM peut s'appuyer sur la loi conditionnelle sachant ce qui est à droite
\chapter{CorReg: the package and its application in steel industry}	
	\section{\textsc{CorReg} package for R}
	\textsc{CorReg} is already downloadable on the CRAN under CeCILL Licensing. This package permits to generate datasets according to our generative model, to estimate the structure (C++ code) of regression within a given dataset and to estimate both explicative and predictive model with many regression tools (OLS,stepwise,LASSO,elasticnet,clere,spike and slab, adaptative lasso and every models in the \textsc{lars} package). So every simulation presented above can be done with \textsc{CorReg}.
	\textsc{CorReg} also provides tools to interpreat found structures and visualize the dataset (missing values and correlations). More informations can be found on the website www.correg.org which is dedicated to \textsc{CorReg}.
	\section{Application in steel industry}
		\subsection{The dataset}
		\subsection{Found Structure}
		\subsection{Results}
\chapter{Conclusion and perspectives}
\chapter{References}
\bibliography{biblio}{ }
\bibliographystyle{plain}
\chapter{Appendices}
	\section{Graphs and CorReg}
		\subsection{Matricial notations}
		\subsection{Properties}
	\section{Mixture models}
		\subsection{Linear combination}
			
		\subsection{Industrial examples}	
\end{document}