\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Clément TH\'ERY}
\title{Model-based pretreatment for correlated datasets, Application to linear regression and missing values. \\ Real datasets from steel industry}
\begin{document}
%\centering
	Université Lille 1 \\
	\'Ecole Doctorale Sciences Pour l'Ingénieur Université Lille Nord-de-France
	
\maketitle

\newpage
\itshape To my sons
\upshape


\chapter*{Résumé}
	Les travaux effectués durant cette thèse ont pour but de pouvoir pallier le problème des corrélations au sein des bases de données, particulièrement fréquentes dans le cadre industriel. Une modélisation explicite des corrélations par un système de sous-régressions entre covariables permet de pointer les sources des corrélations et d'isoler certaines variables redondantes. 
	\\
	
	Il en découle une pré-sélection de variables nettement moins corrélées sans perte significative d'information et avec un fort potentiel explicatif (la pré-selection elle-même est expliquée par la structure de sous-régression qui est simple à comprendre car uniquement constituée de modèles linéaires). \\
	
	Un algorithme de recherche de structure de sous-régressions est proposé, basé sur un modèle génératif complet sur les données et utilisant une chaîne MCMC (Monte-Carlo Markov Chain). Ce prétraitement est utilisé pour la régression linéaire à des fins illustratives mais ne dépend pas de la variable réponse et peut donc être utilisé de manière générale pour toute problématique de corrélations.\\
	
	Par suite, le modèle génératif complet peut être utilisé pour gérer d'éventuelles valeurs manquantes dans les données, tant pour la recherche de structure que pour de l'imputation multiple préalable à l'utilisation de méthodes classiques incompatibles avec la présence de valeurs manquantes. Cela permet également d'estimer les valeurs manquantes et à terme  de fournir un estimateur de la variance de leur estimation.
	Encore une fois, la régression linéaire vient illustrer l'apport de la méthode qui reste cependant générique et applicable à d'autres contextes tels que le clustering.
	\\
	
	Enfin, un estimateur plug-in pour la régression linéaire est proposé pour ré-injecter les variables redondantes de manière séquentielle et donc utiliser toute l'information sans souffrir des corrélations entre covariables.
	\\
	
	Tout au long de ces travaux, l'accent est mis principalement sur l'interprétabilité des résultats en raison du caractère industriel du financement de cette thèse. 
\\	

	Le package R intitulé {\tt CorReg}, disponible sur le CRAN\footnote{http://cran.r-project.org} sous licence CeCILL\footnote{http://www.cecill.info}, implémente les méthodes développées durant cette thèse.
	
\paragraph{Mots clés:}Prétraitement, Régression, Corrélations, Valeurs manquantes, MCMC, modèle génératif, Critère Bayésien, sélection de variable, méthode séquentielle, graphs.
\chapter*{Abstract}
	This thesis was motivated by correlation issues in real datasets, in particular industrial datasets. The main idea stands in explicit modeling of the correlations between covariates by a structure of sub-regression, that simply is a system of linear regression between the covariates. It points out redundant covariates that can be deleted in a pre-selection step to improve matrix conditioning without significant loss of information and with strong explicative potential because this pre-selection is explained by the structure of sub-regression, itself easy to interpret.
	\\
	
	An algorithm to find the sub-regression structure inherent to the dataset is provided, based on full generative model and using Monte-Carlo Markov Chain (MCMC) method. This pretreatment is then illustrated on linear regression to show its efficiency but does not depend on a response variable and thus can be used in a more general way with any correlated datasets.
	\\
	
	The generative model defined here allows to manage missing values both during the MCMC and then for imputation (for example multiple imputation) to be able to use classical methods that are not compatible with missing datasets. Missing values can be imputed with a confidence interval to show estimation accuracy. Once again, linear regression is used to illustrate the benefits of this method but it remains a pretreatment that can be used in other contexts, like clustering and so on.
	\\ 
	
	Finally a plug-in estimator is defined to get back the redundant covariates sequentially. Then all the covariates are used but the sequential approach act as a protection against correlations.
\\

	The industrial motivation of this work define interpretation as a stronghold at each step. 	
	\\
	The R package {\tt CorReg}, is on CRAN\footnote{http://cran.r-project.org} now under CeCILL\footnote{http://www.cecill.info} license. It implements the methods created during this thesis.
	
	 	
\paragraph{Keywords:} Pretreatment, Regression, Correlations, Missing values, MCMC, generative model, Bayesian Criterion, variable selection, plug-in method,\dots
	

\chapter*{Acknowledgments}
	I want to thank ArcelorMittal for the funding of this thesis, the opportunity to make this thesis with real datasets and the confidence in this work that has lead me to be recruited since may but let me work in Villeneuve d'Ascq this last year to reduce road time.\\
	
	But this work would not have been possible without the help of Gaétan LORIDANT, my hierarchic superior and friend who has convinced ArcelorMittal to fund this thesis and helped me in this work by a strong moral support and spending a great amount of time with me to find the good direction between academic and industrial needs with some technical help when he could. I would not have made all this work without him.\\
	
	I also want to thank Christophe BIERNACKI, my academic director who accepted to lead this work even if the subject was not coming from the university and even if I won't pursue a research career. He also has spent a lot of time on this thesis with patience and has trust in the new method enough to share it with others, and it really means a lot to me. \\
	
	The last year I have worked mostly in the INRIA center with M$\Theta$dal team, especially those from the "bureau 106 " who helped me to put {\tt CorReg} on CRAN (in particular Quentin GRIMONPREZ) and those who had already submitted something on CRAN know that it is not always fun to achieve this goal. They also help me in this last and rude year just by their presence, giving me the courage to go further. \\
	
	I finally want to thank my family. My wife who had to support most of the charge of the family on top of her work and to let me work during the holidays and my three sons, Nathan, Louis and Thibault who have been kind with her even if they rarely saw their father during the week. I love them and am thankful for their comprehension.




\tableofcontents
\chapter{Résumé substantiel en français}
	\section{Position du problème}
	La régression linéaire est l'outil de modélisation le plus classique et se résume à une équation bien connue :
	\begin{equation}
		\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}_Y
	\end{equation}
	où $\boldsymbol{Y}$ est la variable réponse de taille $n\times 1$ que l'on souhaite décrire à l'aide de $p$ variables explicatives observées sur $n$ individus et dont les valeurs sont stockées dans la matrice $\boldsymbol{X}$ de taille $n\times p$. Le vecteur $\boldsymbol{\beta}$ est le vecteur des coefficients de régression qui permet de décrire le lien linéaire entre $\boldsymbol{Y}$ et $\boldsymbol{X}$. Le vecteur $\boldsymbol{\varepsilon}_Y$  est un bruit blanc gaussien $\mathcal{N}(0,\sigma_Y^2\boldsymbol{I}_n)$ qui représente l'inexactitude du modèle de régression.\\
	
	
	On connaît l'estimateur sans biais de variance minimale (ESBVM) de $\boldsymbol{\beta}$ qui est obtenu par Moindres Carrés Ordinaires (MCO) selon la formule  :
	\begin{equation}
		\hat{\boldsymbol{\beta}}=(\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\boldsymbol{Y}
	\end{equation}
	Cet estimateur nécessite l'inversion de la matrice $(\boldsymbol{X}'\boldsymbol{X})$ qui est mal conditionnée si les variables explicatives sont corrélées entre elles. Ce mauvais conditionnement nuit à la qualité de l'estimation et vient impacter la variance de l'estimateur comme le montre la formule :
	\begin{equation}
		\operatorname{Var}_{\boldsymbol{X}}(\hat{\boldsymbol{\beta}})=\sigma_Y^2(\boldsymbol{X}'\boldsymbol{X})^{-1}
	\end{equation}
	
	C'est cette situation problèmatique que nous nous proposons d'améliorer.
	\section{Modélisation explicite des corrélations}
	Le mauvais conditionnement de la matrice provient de la quasi-singularité (parfois singularité numérique) de celle-ci quand les colonnes de $\boldsymbol{X}$ sont presque linéairement dépendantes.	Cette quasi dépendance linéaire peut être elle aussi modélisée par régression lineaire. On se propose donc de considérer notre problématique comme l'existence d'un modèle de sous-régressions au sein des variables explicatives avec certaines des variables expliquées par d'autres, formant ainsi une partition des $p$ variables en 2 blocs:
	\begin{equation}
		\boldsymbol{X}^{I_r}=\boldsymbol{X}^{I_f}\boldsymbol{\alpha}+\boldsymbol{\varepsilon} \textrm{ \ \ \ (regression multiple multivariée)}
	\end{equation}
	Avec $\boldsymbol{X}^{I_r}$ sous-matrice de $p_r$ variables redondantes dans $\boldsymbol{X}$. Nous supposons alors l'indépendance entre les variables non redondantes qui définissent $\boldsymbol{X}^{I_f}$, les variables libres ("free" en anglais). La matrice $\boldsymbol{\alpha}$ de taille $(p-p_r)\times (p_r)$ a pour colonnes les vecteurs $\boldsymbol{\alpha}_j$ qui contiennent les coefficients de régression associées à la $j^{ieme}$ colonne de $\boldsymbol{X}$.
	
	La figure \ref{MQEOLS1} illustre la déterioration de l'estimation quand les sous-régressions deviennent trop fortes ($R^2$ proche de 1) pour différentes valeurs de $n$ sur un modèle composé de 5 variables dont 4 gaussiennes centrées réduites {\it i.i.d. } $\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_4,\boldsymbol{x}_5$ et une variable redondante $\boldsymbol{x}_3=\boldsymbol{x}_1+\boldsymbol{x}_2+\boldsymbol{\varepsilon}_3$ avec $\boldsymbol{\varepsilon}_3\sim{\mathcal{N}(\boldsymbol{0},\sigma_3^2\boldsymbol{I}_n)}$. Deux régressions principales en $\boldsymbol{Y}$ ont été testées avec $\boldsymbol{\beta}=(1,1,1,1,1)$ and $\sigma_Y \in \{10,20\}$. Le conditionnement de $(\boldsymbol{X}'\boldsymbol{X})$ se déteriore donc quand $\sigma_3$ diminue. \\
	
	\section{Modèle marginal}
	Le fait de modéliser explicitement les corrélations entre les covariables nous permet de réécrire le modèle de régression principal. On peut en effet substituer les variables redondantes par leur sous-régression, ce qui revient à intégrer la régression sur $\boldsymbol{X}^{I_r}$ sachant la structure de sous-régressions.
\begin{eqnarray}
		P(\boldsymbol{Y}|\boldsymbol{X}^{I_f})&=& \int_{\boldsymbol{X}^{I_r}}P(\boldsymbol{Y}|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f})P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f}) d \boldsymbol{X} \\
	\boldsymbol{Y}_{|\boldsymbol{X}^{I_f},S}&=&\boldsymbol{X}^{I_f} (\boldsymbol{\beta}_{I_f}+ \sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j)+  \sum_{j \in I_r}\beta_{j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y  \\
	&=&\boldsymbol{X}^{I_f}\boldsymbol{\beta}_{I_f}^*+\boldsymbol{\varepsilon}_Y^*
\end{eqnarray}
on se retrouve donc avec un modèle marginal plus parsimonieux, sans biais mais avec une variance accrue. Cet accroissement de la variance est proportionnel à $\boldsymbol{\varepsilon}$ qui est la matrice des résidus des sous-régressions. Plus les sous-régressions sont fortes plus cette variance sera donc faible.
Tout le principe du modèle marginal repose sur le compromis entre l'amélioration du conditionnement de $(\boldsymbol{X}'\boldsymbol{X})$ par suppression des variables redondantes et aussi la réduction de la dimension (le modèle marginal ne nécessite que l'inversion de $(\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f})$) face au léger accroissement de la variance issu de la marginalisation. \\
On va donc comparer 
	\begin{eqnarray}
		\hat{\boldsymbol{\beta}}&=& (\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\boldsymbol{Y} \textrm{ au modèle marginal} \\
		\hat{\boldsymbol{\beta}}^*&=& (\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f})^{-1}\boldsymbol{X}^{I_f'}\boldsymbol{Y}
	\end{eqnarray}
	Les deux modèles sont sans biais et de dimension différente, on compare donc leurs erreurs moyennes quadratiques respectives (MSE):
	\begin{eqnarray}
		E[MSE(\hat{\boldsymbol{\beta}}|\boldsymbol{X})]&=& \sigma^2_Y \operatorname{Tr}(\boldsymbol{X}'\boldsymbol{X})^{-1}  \\
		E[MSE(\hat{\boldsymbol{\beta}}^*|\boldsymbol{X}^*)]&=& (\sigma^2_Y+\sum_{j \in I_r}\sigma^2_{j}\beta_{j}^2)\operatorname{Tr}(\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f})^{-1}
	\end{eqnarray}
		La figure \ref{MQEexplOLSp5col} compare ces deux erreurs pour différentes valeurs des paramètres, montrant la nette amélioration rendue possible par la marginalisation.
	
	\section{Notion de prétraitement}
	Le modèle marginal peut être interprété comme étant un prétraitement par sélection de variable puisqu'on se ramène à un modèle de régression linéaire classique pour lequel n'importe quel estimateur peut être utilisé. Cela fait de ce modèle un outil générique. La préselection permet de cibler des variables qui n'interviendront pas dans le modèle final sans pour autant être indépendante de $\boldsymbol{Y}$ donc le modèle final est parsimonieux mais ne fausse pas l'interprétation. L'estimation de $\boldsymbol{\beta}^*$ peut ensuite se faire en utilisant une quelconque méthode de sélection de variables pour éliminer les variables qui, elles, sont indépendantes de $\boldsymbol{Y}$.\\
	 On obtient donc deux types de 0 : ceux de la marginalisation qui pointent les variables redondantes et ceux de sélection qui viennent dans un second temps et point les variables indépendantes. L'interprétation est donc enrichie par rapport à une méthode de sélection classique qui fournirait le même modèle final. Or, le contexte industriel de ces travaux rend indispensable d'avoir une bonne qualité d'interprétation. L'objectif est donc atteint pour ce point précis. Notre modèle marginal est un outil de décorrélation de variables par préselection.
	 
	 \section{Estimation de la structure}
		La raison d'être de notre modèle marginal est la fragilité des méthodes de régression face à des covariables fortement corrélées. Il serait donc vain d'essayer les sous-régression en estimant les modèles de régression de chaque variable en fonction de toutes les autres. Pour cette raison, nous avons établi un algorithme MCMC pour trouver le meilleur modèle de sous-régression. L'idée consiste à voir la structure de sous-régression comme un paramètre binaire, une matrice binaire creuse pour être plus précis. Cette matrice $\boldsymbol{G}$ de taille $p\times p$ correspond à une matrice d'adjacence qui indique les liaisons entre covariables de la manière suivante : $\boldsymbol{G}_{i,j}=1$ si, et seulement si $\boldsymbol{X}^j$ est expliqué par $\boldsymbol{X}^i$. \\
		
		Chaque étape $(q+1)$ de l'algorithme propose de garder la structure $\boldsymbol{G}^{(q)}$ en cours ou bien de bouger vers une structure candidate qui diffère de $\boldsymbol{G}^{(q)}$ en un unique point. Ainsi, selon les cas, les candidats vont allonger ou réduire des sous-régression, les supprimer ou les créer. \\
		Pour pouvoir trancher entre plusieurs candidats, nous avons besoin d'une fonction coût qui soit capable de comparer des modèles avec des nombres distincts de sous-régressions. Nous définissons alors un modèle génératif complet sur $\boldsymbol{X}$ qui complète le modèle de sous-régressions en établissant des modèles de mélanges gaussiens indépendants pour les variables de $\boldsymbol{X}^{I_f}$. Une fois ce modèle génératif établi, nous pouvons utiliser le critère $BIC$ pour comparer les différents modèles et conduire chaque étape de la chaîne MCMC par un tirage aléatoire pondéré par les écarts entre les $BIC$ des différents modèles proposés (dont le modèle en cours). L'algorithme continue ainsi sa marche et fournit à l'utilisateur le modèle rencontré (qu'il ait été choisi ou non) qui a le $BIC$ le plus faible. \\
		La chaîne MCMC est conditionnée par le critère de partitionnement : les variables expliquées ne doivent en expliquer aucune autre ($\boldsymbol{X}^{I_r}\cap \boldsymbol{X}^{I_f}=\emptyset$). Chaque modèle réalisable peut être entièrement construit ou déconstruit pendant la marche aléatoire donc l'algorithme suit une chaîne de Markov régulière. Ainsi, il est certain que, asymptotiquement l'algorithme trouve le modèle ayant le meilleur $BIC$.
	\section{Relaxation des contraintes et nouveau critère}
		Pour améliorer la mélangeance de l'algorithme et donc sa vitesse de convergence, on peut jouer avec la contrainte de partitionnement par une méthode de relxation semblable à un recuit simulé. Quand une structure candidate n'est pas réalisable (ne produit pas de partition), on peut la modifier en d'autres endroits pour la rendre réalisable. Il suffit de suivre les formules suivantes pour une modification en $(i,j)$:
		\begin{eqnarray}
			\boldsymbol{G}_{i,j}^{(q+1)}&=&1-\boldsymbol{G}_{i,j}^{(q)} \\
			\boldsymbol{G}_{.,i}^{(q+1)}&=& (\boldsymbol{G}_{i,j}^{(q)})\boldsymbol{G}_{.,i}^{(q)} \textrm{ Si $i$ explique $j$ alors personne ne peut plus expliquer $i$} \\
			\boldsymbol{G}_{j,.}^{(q+1)}&=&(\boldsymbol{G}_{i,j}^{(q)})\boldsymbol{G}_{j,.}^{(q)} \textrm{ Si $i$ explique $j$ alors $j$ ne peut plus expliquer personne}
		\end{eqnarray}
		Cette méthode de relaxation permet de sortir rapidement des extrema locaux et améliore donc significativement l'efficacité de l'algorithme.\\
		 
		Mais il reste un problème. Le nombre de modèles envisageable est considérable et le critère $BIC$ ne tient pas compte de cette quantité, menant à des modèles trop complexes. On lui ajoute donc une pénalité qui tient compte du nombre de modèles réalisables pour pénaliser plus lourdement les modèles complexes.
	De manière générale quand on estime la vraisemblance d'une structure $S$ dans une base de données $\boldsymbol{X}$, $BIC$ est utilisé comme approximation pour $P(S|\boldsymbol{X})\propto  P(\boldsymbol{X}|S)P(S)$  	car $P(S)$ est considéré comme suivant une loi uniforme. On s'appuie donc sur une loi uniforme hiérarchique sur $P(S)=P(I_f | \boldsymbol{p}_f,I_r,p_r)P(\boldsymbol{p}_f|I_r,p_r)P(I_r|p_r)P(p_r)$ pour ajouter une pénalité supplémentaire aux structures complexes (même probabilité globale pour un plus grand nombre de structures donc chaque structure devient moins probable). Avec $I_r$ l'ensemble des variables redondantes, $p_r$ le nombre de sous-régressions, $\boldsymbol{p}_f$ le vecteur des complexités des sous-régressions et enfin $I_f$ l'ensemble des ensembles des variables explicatives dans les sous-régressions.
On note $BIC_+$ ce nouveau critère car il ne modifie pas $BIC$ (on conserve ainsi ses propriétés) mais ajoute simplement une pénalisation.\\
	Ces deux outils viennent améliorer l'efficacité de l'algorithme sans paramètre utilisateur supplémentaire à optimiser. Tout reste naturel et intuitif pour une meilleur automatisation.

	\section{Résultats}
		La méthode a été testée sur données simulées puis réelles, montrant l'efficacité du modèle marginal s'appuyant sur la vraie structure de sous-régressions (section \ref{MSEvraiS}), l'efficacité de l'algorithme de recherche de structure (section \ref{compZ}), et l'efficacité du modèle marginal s'appuyant sur la structure estimée (section \ref{compY} et chapitre \ref{sectionrealcase}). Le bilan est très positif comme le montrent les graphiques de ces différentes sections.
	\section{Au-delà du marginal avec le plug-in}
		Une première perspective a été développée pour compléter le modèle marginal. Il s'agit d'un modèle plug-in permettant de ré-injecter les variables redondantes après estimation du modèle marginal pour obtenir un modèle utilisant toutes les variables mais protégé des corrélations par l'estimation séquentielle.
		
\begin{eqnarray}
			%\boldsymbol{Y}&=& \boldsymbol{X}^{I_r}\boldsymbol{\beta}_{I_r}+\boldsymbol{X}^{I_f}\boldsymbol{\beta}_{I_f}+\boldsymbol{\varepsilon}_Y \\
			\boldsymbol{X}^{I_r}&=&\boldsymbol{X}^{I_f}\boldsymbol{\alpha}+\boldsymbol{\varepsilon} \\
			\boldsymbol{Y}&=& \boldsymbol{X}^{I_r}\underbrace{(\boldsymbol{\beta}_{I_r}+\boldsymbol{\alpha}\boldsymbol{\beta}_{I_f})}_{\boldsymbol{\beta}^*}+\boldsymbol{\varepsilon}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y 
					\end{eqnarray}		 
Ce qui donne donc
		\begin{eqnarray}
			\boldsymbol{Y}- \boldsymbol{X}^{I_r}\boldsymbol{\beta}^*&=&\boldsymbol{\varepsilon}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y \textrm{ avec}\\
			\boldsymbol{\varepsilon}&=&\boldsymbol{X}^{I_r}-\boldsymbol{X}^{I_f}\boldsymbol{\alpha}
		\end{eqnarray}		 
D'où le modèle plugin 
		\begin{eqnarray}
			\underbrace{\boldsymbol{Y}- \boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}^*}}_{\tilde{\boldsymbol{Y}}}&=&\underbrace{(\boldsymbol{X}^{I_r}-\boldsymbol{X}^{I_f}\hat{\boldsymbol{\alpha}})}_{\tilde{\boldsymbol{X}}}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y \\
		\end{eqnarray}
		qui permet d'estimer dans un second temps $\boldsymbol{\beta}_{I_r}$ par un modèle classique de régression linéaire en s'appuyant sur l'estimation de $\boldsymbol{\beta}^*$ issue du modèle marginal et sur $\hat{\boldsymbol{\alpha}}$.
		Le modèle plug-in réduit le bruit de la régression
		\begin{equation}
			\boldsymbol{Y}= \boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}}^* + \hat{\boldsymbol{\varepsilon}}\hat{\boldsymbol{\beta}}_{I_r}+\boldsymbol{\varepsilon}_Y 
		\end{equation}
		and simple étape d'identification (sans estimation supplémentaire) permet de retouver le vrai modèle:
		\begin{eqnarray}
			\boldsymbol{Y}&=& \boldsymbol{X}^{I_r}(\hat{\boldsymbol{\beta}}^*-\hat{\boldsymbol{\alpha}}\hat{\boldsymbol{\beta}}_{I_r}) + \boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}}_{I_r}+\boldsymbol{\varepsilon}_Y \\
			&=&\boldsymbol{X}^{I_f}\hat{\boldsymbol{\beta}}_{I_f}+\boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}}_{I_r}+\boldsymbol{\varepsilon}_Y
		\end{eqnarray}
	
	La figure \ref{MQE2} montre l'efficacié du modèloe plug-in et son champ d'application : les cas avec assez de correlations pour que les méthodes classiques appliquées à $\boldsymbol{X}$ soient handicapées mais pas assez de corrélations pour que le retrait des variables redondantes se fasse sans perte siginificative d'information.
		
	\section{Valeurs manquantes}
		Une seconde perspective a été entamée concernant les valeurs manquantes. Le fait de disposer d'un modèle génératif complet sur $\boldsymbol{X}$ avec modélisation explicite des dépendances permet en effet de gérer les valeurs manquantes. Tout d'abord, l'estimation de $\boldsymbol{\alpha}$ peut se faire sur les données observées en intégrant sur les données manquantes. On peut alors utiliser un algorithme de type EM (expectation Maximization) pour estimer $\hat{\boldsymbol{\alpha}}$.\\
		 En pratique, l'étape $E$ n'est pas systématiquement explicite et peut nécessiter de faire appel à une variante de EM : l'aglorithme SEM (pour Stochastic EM) qui remplace l'étape $E$ par une étape stochastique d'imputation des valeurs manquantes, par exemple en utilisant un échantillonneur de Gibbs. \\
		 Cet algorithme de Gibbs peut alors être utilisé pour faire de l'imputation multiple sur les valeurs manquantes en s'appuyant sur le $\hat{\boldsymbol{\alpha}}$ issu du SEM. Comme cette imputation tient compte des corrélations entre les variables, elle est plus précise qu'une simple imputation par la moyenne. Un avantage de l'imputation multiple est que l'on peut avoir une idée de la robustesse des imputations en regardant simplement la variance des valeurs imputées, ce qui donne une information de fiabilité sur les imputations. Encore une fois, on y gagne en qualité d'interprétation.  
\chapter{The industrial context}
	\paragraph{Abstract:} Ce chapître explique les contraintes industrielles qui ont orienté les travaux pour répondre aux demandes d'ArcelorMittal qui est le commanditaire de ces travaux de recherche.
\section{Steelmaking process}
	This work takes place in a steel industry context, fund by ArcelorMittal, the world leading company in steelmaking.
	Steelmaking starts from raw materials to give highly specific products found.
	
	We first melt down a mix of iron ore and cock to obtain cast iron that is then transformed in steel by addiction of pure oxygen to reduce the quantity of carbon. Liquid steel is then refreshed in a mold (continuous casting) to obtain steel slabs (nearly 20 tons each). Cold slabs are then warmed to be pressed in a hat rolling mill to obtain coils of steel. If the final product requires a thinner steel, coils pass through a cold rolling mill. Each step of this process involves a whole manufactory and the whole process can take several weeks. The most sensitive products are the thinner and sometimes defects are generated by small inclusion in the steel down to the dozen of microns. So even if quality is evaluated at each step of the process, some defects are only found when the whole process is finished even if the origin comes from the first part of this process. So we have hundreds of parameters to analyse.
	\begin{center}
          \begin{tabular}{ccc}
         \includegraphics[width=130px,height=130px]{figures/liquid.jpg} & \includegraphics[width=130px,height=130px]{figures/Brame1.jpg} & \includegraphics[width=130px,height=130px]{figures/Brame.jpg} \\
          	\includegraphics[width=130px,height=130px]{figures/ecras_moy.jpg} &\includegraphics[width=130px,height=130px]{figures/tcc2.jpg} & \includegraphics[width=130px,height=130px]{figures/bobines.jpg}
          \end{tabular}
        \end{center}
Steelmaking is always evolving and we are now able to produce steel that is thinner and stronger at the same time. Steel is completely recyclable so we will always be able to produce it. This quickly evolving industry is associated to a lot of research in metallurgy but also need adapted statistical tools. That is why this thesis has been made.
	
	\section{Impact of the industrial context}
	 The main objective is to be able to solve quality crisis when they occur. In such a case, a new type of unknown quality issue is observed and we may have no idea of its origin. The defects, even generated at the beginning of the process, are often detected in its last part. The steel-making process includes several sub-process, each implying a whole plant. Thus we have many covariates and no a priori on the relevant ones. Moreover, the values of each covariates essentially depends on the characteristics of the final product, and many physical laws and tuning models are implied in the process. Therefore the covariates are highly correlated.
	We have several constraints :
	\begin{itemize}
		\item To be able to predict the defect and stop the process as early as possible to gain time (and money)
		\item To be able to understand the origin of the defect to try to optimize the process
		\item To be able to find parameters that can be changed because the objective is not only to understand but to correct the problematic part of the process.
		\item It also must be fast and automatic (without any a priori).
	\end{itemize}
	We will see in the state of the art that correlations are a real issue and that the number of variables increases the problem.	
	The stakes are very high because of the high productivity of the steel plants but also because steel making is now well-known and optimized thus new defects only appears on innovative steels with high value. Any improvement on such crisis can have important impact on the market shares and when the customer is implied, each day won by the automation of the data mining process can lead to a gain of hundreds of thousands of euros, sometimes more. So we really need a kind of automatic method, able to manage the correlations without any a priori and giving an easily understandable and flexible model.
	
\chapter{State of the art}
\paragraph{Abstract:} Rapide aperçu de ce qui existe déjà pour tenter de répondre à notre problématique. La plupart des outils présentés le sont plus en détail dans le livre de Hastie, Tibshirani et Friedman : {\it "The Elements of Statistical Learning: Data Mining, Inference, and Prediction" } accessible gratuitement sur le web\footnote{ \url{http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf}}.
	\section{Linear regression}
%		\subsection{Historic interest}
%			méthode ancienne et reconnue, remonte aux origines des statistiques, méthode pionnière en prédiction.
%		\subsection{Simplicity}
%			Facile à mettre en oeuvre théoriquement, rapide en pratique et présent partout (même dans Excel)
%			Très simple à interpréter, principe intuitif.
%			donne tout de suite l'impact des variables (positif ou négatif) sur la réponse et leur poids (si scaled dataset) 
%			
%			C'est ici qu'on peut mettre le principe de la régression linéaire (image d'un nuage de point et d'une droite qui le traverse)
		\subsection{Industrial context}
		
			Industrial context is often poor in statistical background and the stakes are frequently very high in terms of financial impact. 
		These two points give strong constraints because methods used has to be accessible for non-statistician in a minimum amount of time and results obtained have to be clearly interpreted (no black-box) because if industrial experts don't understand the result, they will not trust it and then they will not use it. So a powerful tool without interpretation becomes kind of useless in such a context.
		
		Every engineer, even non-statistician use frequently linear regression to seek relationship between some covariates. It is easy to understand, fast to do, it can be done directly in Microsoft Excel that remains the most used software in industry and the software used by engineers to open most of the datasets.
		 
\begin{figure}
\centering
	\includegraphics[width=350px]{figures/simpleOLS.png} 
	\caption{An example of simple linear regression}
\end{figure}		 
		 
		 Regression appears to be the basis of industrial statistics so we have chosen to work in this way. As of 2014 Google Scholar proposes more than $3.8$ millions of papers related to regression and many of them were cited several thousands times. It is an old strategy well known and with many derivative (as we will see in the followings) and can be generalized 		 \cite{kiebel2003general,wickens2004general,nelder1972generalized,mccullagh1989generalized} 
. 
		 It's simplicity facilitates a wide spread usage in industry and other fields of application. It is also a powerful tool for interpretation allowing to know with precision the positive or negative impact of each covariate on the response variable. 
	
	More complex situations can be described by evolved forms of linear regression like the hierarchical linear model \cite{raudenbush2002hierarchical,woltman2012introduction}  or  multilevel regression \cite{moerbeek2003comparison,maas2004robustness,hox1998multilevel} that allows to consider effects of the covariates on nested sub-populations in the dataset. It is like using interactions but with a proper modeling that improves interpretation. It is not really a linear model because it is not linear in $\boldsymbol{X}$ but can be seen as a basis expansion using new covariates (the interactions) composed by the product of some of the original covariates.
%			
%		\subsection{Flexibility and future of regression}
%			Richesse des types de régression et des méthodes d'estimation
%			On peut faire des choses plus pointues en adaptant un peu le modèle mais en conservant la simplicité : Multilevel Regression \cite{moerbeek2003comparison,maas2004robustness,hox1998multilevel}
%



\paragraph{Notations:}	
In the following we note classical (respectively $L_2,L_1,L_{\infty}$) norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$, $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^p|\beta_i| $ and $\parallel\boldsymbol{\beta} \parallel_{\infty}=\operatorname{max}(|\beta_1|,\dots,|\beta_p|)$. Vectors are in bold characters.
	\section{Ordinary least squares and associated problems}\label{sectionOLS}		% ne pas oublier de mentionner les packages existants

We note the linear regression model:
\begin{equation}
		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \label{regressionsimple}
	\end{equation}
	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables,% (that is a sub-matrix of $\tilde{\boldsymbol{X}}$ the $n\times \tilde{p}$ matrix of provided covariates)
	 $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $p\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{OLS}): %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaOLS}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1} \label{eqOLS}
	\end{equation}
	and without any bias \cite{saporta2006probabilites,dodge2004analyse}. In fact it is the Best Linear Unbiased Estimator (BLUE).
	The theoretical MSE is given by
	\begin{equation}
	E[\textsc{MSE}(\hat{\boldsymbol{\beta}}_{OLS}|\boldsymbol{X})]= 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1})
	\end{equation}
	\\
	Equation \ref{regressionsimple}	has no intercept but can be generalized by adding to $\boldsymbol{X}$ a first column full of 1. So we don't consider the intercept to simplify notations. In practice, an intercept is added by default.
	
	Ordinary Least Squares find a $p-$dimensional hyperplane that minimizes the distance with each individual $(\boldsymbol{X}_i,Y_i)$. It can be written
	\begin{equation}
		\boldsymbol{\hat{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace
	\end{equation}
	So estimation of $\boldsymbol{Y}$ by OLS can be viewed as a projection onto the linear space spanned by the regressors $\boldsymbol{X}$ as shown in figure \ref{geomOLS}.% that is in public domain\footnote{"OLS geometric interpretation" by Stpasha - Own work. Licensed under Public domain via Wikimedia Commons - http://commons.wikimedia.org/wiki/File:OLS\_ geometric\_ interpretation.svg\#mediaviewer/File:OLS\_geometric\_interpretation.svg}.
	\begin{figure}[h!]
	\centering
	\includegraphics[width=250px]{figures/OLS_geometric_interpretation.png}
	\caption{Multiple linear regression with Ordinary Least Squares. Public domain image.} \label{geomOLS}
	\end{figure}
	
	%problème
	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
For a fixed number $n$ of individuals, conditioning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects: 
\begin{itemize}
	\item The dimension $p$ (number of covariates) of the model (the more covariates you have the greater variance you get)
	\item The correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .
\end{itemize}
	When correlations between covariates are strong, the matrix to invert is ill-conditioned and the variance increases, giving unstable and unusable estimator \cite{hoerl1970ridge}.
	Another problem is that matrix inversion requires to have more individuals than covariates ($n\geq p$).
	When matrices are not invertible, classical packages like the function lm of R base package \cite{packagebase} use the Moore-Penrose pseudoinverse \cite{PSP:2043984} to generalize OLS.
		
		
	Last but not least, Ordinary Least Squares is unbiased but if some $\beta_i$ are null (irrelevant covariates) the corresponding $\hat{\beta}_i$ will only asymptotically tend to 0 so the number of covariates in the estimated model remains $p$. This is a major issue because we are searching for a statistical tool able to work without a priori on a big dataset containing many irrelevant datasets. Pointing out some relevant covariate and how they impact the response really is the main goal here. We will need a variable selection method one moment or another. It could be as a pretreatment, during coefficient estimation or by post-treatment. \\
	
		

	
\paragraph{Running example:} we look at a simple case with $p=5$ variables defined by four independent scaled Gaussian $\mathcal{N}(0,1)$ named $\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_4,\boldsymbol{x}_5$ and $\boldsymbol{x}_3=\boldsymbol{x}_1+\boldsymbol{x}_2+\boldsymbol{\varepsilon}_3$ where $\boldsymbol{\varepsilon}_3\sim{\mathcal{N}(\boldsymbol{0},\sigma_3^2\boldsymbol{I}_n)}$. We also define two {\it scenarii} for $\boldsymbol{Y}$ with $\boldsymbol{\beta}=(1,1,1,1,1)$ and $\sigma_Y \in \{10,20\}$. So there is no intercept (can be seen as a null intercept).
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_3$ gets smaller.
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEOLS.png}
	  \caption{Evolution of theoretical Mean Squared error on $\hat{\boldsymbol{\beta}}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEOLS1}
	\end{figure}
	
	Figure \ref{MQEOLS1} shows the theoretical MSE obtained on $\hat{\boldsymbol{\beta}}$ with OLS. These results are based on equation \ref{eqOLS}, we show the mean obtained after 100 experiences computed on our running example.
	
	The $R^2$ stands for:
	\begin{equation}\label{defR2}
	R^2=1-\frac{\sum_i (y_i - x_i\beta)^2}{\sum_i (y_i-\bar{y})^2}
	\end{equation}
	where $\bar{\boldsymbol{Y}}=\frac{1}{n}\sum_{i=1}^n y_i $
	
	
	Many other estimation methods were created to obtain better estimations by playing on the bias/variance tradeoff or by making additionnal hypothesis.
	To have an easier comparison, we also look at the empiric MSE obtained on $\hat{\boldsymbol{\beta}}$.
		
	
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/OLScompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEOLScompl}
	\end{figure}	
	We first observe that real results are better from expected (Figure \ref{MQEOLScompl}). This comes from usage of $QR$ decomposition to inverse matrices, that are less impacted by ill-conditioned matrices \cite{bulirsch2002introduction}. But the correlations issue remains and so do the impact of $n$ and $\sigma_Y$.  Our package {\tt CorReg} also uses this decomposition.
	\\
		 \FloatBarrier
		

	\section{Penalized models}
	
	We have seen that OLS is the Best linear Unbiased Estimator for $\hat{\boldsymbol{\beta}}$, meaning that it has the minimum variance. But it remains possible to play with the bias/variance tradeoff to reduce the variance by adding some bias. The underlying idea is that a small bias and a small variance could be preferred to a huge variance without bias. Many methods do this by a penalization on  $\hat{\boldsymbol{\beta}}$.  Some of them propose an effective variable selection.
		\subsection{Ridge regression}		% ne pas oublier de mentionner les packages existants

			%\cite{hoerl1970ridge}
			%\cite{marquardt1975ridge}
Ridge regression \cite{hoerl1970ridge,marquardt1975ridge} proposes a biased estimator for $\boldsymbol{\beta}$ that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \lambda \textrm{ with } \lambda>0
	\end{equation}
	But this penalty is not guided by the correlations. It introduce an additional parameter $\lambda$ to choose for the whole dataset  whereas correlations may concern only some of the covariates with several intensities.
	
	The solution of the ridge regression is given by
	\begin{equation}
		 \hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} -\lambda\boldsymbol{I}_n\right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaridge}
	\end{equation}
	and we see in this equation that a global modification of $\boldsymbol{X}'\boldsymbol{X}$ is done for a given $\lambda$. Methods does exist to automatically choose a good value for $\lambda$ \cite{cule2013ridge,er2013systematic} and a R package called {\tt ridge} is on CRAN \cite{packageridge}. 
	We have computed the same experiment as in previous figure but with the {\tt ridge} package instead of OLS. It is clear that the ridge regression is efficient in variance reduction (it is what it is built for).
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/ridgecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEridgecompl}
	\end{figure}
	
	
	%It is the same for each covariates and will be too large for independent covariates and/or too small for correlated ones. So the efficiency of such a method is limited. 
	Moreover, like OLS, coefficients tend to 0 but don't reach 0 so it gives difficult interpretations for large values of $p$. Ridge regression is efficient to improve conditioning of the estimator but gives no clue to the origin of ill-conditioning and keep irrelevant covariates. It remains a good candidate for prediction-only studies. But our industrial context makes necessary to have a variable selection method.
	
		 \FloatBarrier
		
		\subsection{LASSO: Least Absolute Shrinkage and Selection Operator }		% ne pas oublier de mentionner les packages existants

			%\cite{tibshiranilasso}  
			%\cite{tibshirani1996regression} 
			%\cite{efron2004least} %LAR
			%\cite{Zhao2006MSC}%problèmes du lasso/lars en correlations
			%\cite{SAM10088}%lars necessite OLS en surcouche
The Least Absolute Shrinkage and Selection Operator (\textsc{LASSO} \cite{tibshirani1996regression,tibshiranilasso}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$ instead of the $L_2$ penalty of the ridge regression:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0
		\end{equation}	
		
		\begin{figure}[h!]
			\centering
			\includegraphics[width=200px]{figures/lasso.png} 
			\caption{Geometric view of the Penalty for the LASSO (left) compared to ridge regression (right) as shown in the book from Hastie \cite{hastie2009elements}} \label{lassogeom}
		\end{figure}
		Figure \ref{lassogeom} show the contour of error (red) and constraint function (blue) for both lasso (left) and ridge regression (right). We see that the optimum will be found on an axis for the lasso because its constraint zone is a polyhedron whose vertices are on the axis but not for the ridge regression. Here the axis stands for the regression coefficients.
		
		Here again we have to choose a value for $\lambda$.
	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) algorithm offers a very efficient way to obtain the whole LASSO path.  But like the ridge regression, the penalty does not distinguish correlated and independent covariates so there is no guarantee to have less correlated covariates. In practice, we know that the LASSO faces consistency issues when confronted to correlated covariates \cite{Zhao2006MSC}. When two covariates are correlated, it tends to keep only one of them. For example, if two covariates are equal and have the same effect, the LASSO will keep only one of them. As explained earlier, variable selection is a real stake for us but is necessary to have a good interpretation. The LASSO does not distinguish a covariate not selected because it is totally redundant with another that was selected from an irrelevant covariate. And that is a problem. This consistency issue is illustrated in section \ref{consistency}.


		% ne pas oublier de mentionner les packages existants

			%\cite{zou2006adaptive}% adaptive lasso
			%\cite{wang2011random}%random lasso
			 Some recent variants of the \textsc{LASSO} do exist for the choice of the penalization coefficient like the adaptive \textsc{LASSO} \cite{zou2006adaptive} or the random \textsc{LASSO} \cite{wang2011random}.  But the consistency issues remains because it is still the same model. Only the choices of $\lambda$ differ.
			 
			 It is notable that the main goal of the LASSO is to select some covariate, thus the penalization is just a mean to achieve selection. But estimation of $\hat{\boldsymbol{\beta}}$ can be improved by a second estimation with OLS based only on selected covariates \cite{SAM10088}.
		\subsection{Least Angle Regression}
		The least Angle Regression algorithm solves the LASSO problem.
		It requires only the same order of magnitude of computational effort as \textsc{OLS} applied to the full set of covariates.
		The idea is to start with all coefficients to zero and then grow them beginning with the most correlated with the response variable until another variable is equally correlated with the residual. So it is a progressive growth of the coefficient leading to reduce the residual. It finishes with the Ordinary Least Squares solution (on the right in figure \ref{plotlarstoy}). We then have a list of models with several numbers of non-zero coefficients and can choose between them with cross-validation for example.
\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/LARtheory.png}
		\caption{The geometry of Least Angle Regression}
	\end{figure}
				
\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/plotlarstoy.png}
		\caption{The LASSO path computed by lars}\label{plotlarstoy}
	\end{figure}			
	
		 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/larcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{lar}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQElarcompl}
	\end{figure}
	Results obtained with the package {\tt lars} for R, included in {\tt CorReg}. Figure \ref{MQElarcompl} shows that strong correlations make the MSE explode even with lar. 
 	 
	 \FloatBarrier
		\subsection{Elasticnet}		% ne pas oublier de mentionner les packages existants

			Elastic net \cite{zou2005regularization} is a method developed to be a compromise between Ridge regression and the \textsc{LASSO} by mixing both $L_1$ and $L_2$ penalties: 
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	%Elastic net can be writt"en:
	\begin{eqnarray}
		\boldsymbol{\hat{\beta}}&=&(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta} \parallel_2^2 \right\rbrace \textrm{ subject to} \nonumber \\
			 & &(1-\alpha)\parallel\boldsymbol{\beta}\parallel_1+\alpha\parallel\boldsymbol{\beta}\parallel_2^2\leq t \textrm{ for some } t
	\end{eqnarray}
	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	
	
	\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/elasticnetcircles.png} 
			\caption{Geometric view of the Penalty for elasticnet}
		\end{figure}	
	
	%It seems to give good predictions. 
	But it is based on the grouping effect so correlated covariates get similar coefficients and are selected together whereas LASSO will choose between one of them and will then obtain same predictions with a more parsimonious model. Once again, nothing specifically aims to reduce the correlations. 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/elasticnetcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{elasticnet}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEelasticnetcompl}
	\end{figure}
	Results obtained with the package {\tt elasticnet} for R.
		
		 \FloatBarrier

		\subsection{OSCAR: Octogonal Shrinkage and Clustering Algorithm for Regression}		% ne pas oublier de mentionner les packages existants

			%\cite{bondell2008simultaneous}%Oscar
			Like elasticnet, \textsc{OSCAR} \cite{bondell2008simultaneous} uses combination of two norms for its penalty. Here the objective is to group covariates with the same effect (by a pairwise $L_\infty$ norm) and give them exactly the same coefficient (reducing the dimension) with a simultaneous variable selection (implied by the $L_1$ norm).
			\begin{equation}
				\hat{\boldsymbol{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \parallel\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta} \parallel^2_2 \textrm{ subject to } \sum_{j=1}^p|\beta_j|+c\sum_{j<k}\operatorname{max}(|\beta_j|,|\beta_k|) \leq \lambda		
			\end{equation}						
			But \textsc{OSCAR} depends on two tuning parameters: $c$ anf $\lambda$. For a fixed $c$ the $\lambda$ can be found by the \textsc{LAR} algorithm but $c$ still has to be found "by hand" comparing final models for many values of $c$.
			
\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/oscarcircles.png} 
			\caption{Geometric view of the Penalty for OSCAR}\label{oscarcircles}
		\end{figure} 
Figure 	\ref{oscarcircles} show the geometric interpretation of the penalty. It follows the same principle as the LASSO with supplementary vertices in the four quarters to obtain equal values for the $\beta_j$. So the estimator will give both zero coefficients and equal coefficients that can be grouped for interpretation and correspond to a dimension reduction. So two covariates with a similar effect may obtain the same estimated coefficient. But correlations are only implicitly taken into account and only pairwise. It lacks of an efficient algorithm  (to find $c$) and need a supplementary study to interpret the groups found.
		
		
	\section{Modeling the parameters}			% ne pas oublier de mentionner les packages existants

		\subsection{CLERE: CLusterwise Effect REgression}		% ne pas oublier de mentionner les packages existants

			The CLusterwise Effect REgression (\textsc{CLERE} \cite{yengo2012variable}) describes the $\beta_j$ no longer as fixed effect parameters but as unobserved independent random variables with $\beta_j$ following a Gaussian Mixture distribution, allowing to group them into by their component membership. \\
			
			 The idea is to hope that the model have a small number of groups of covariates so that the mixture will have few enough components to have a number of parameters to estimate significantly lower than $p$. In such a case, it improves interpretation and ability to yeld reliable prediction with a smaller variance on $\boldsymbol{\hat{\beta}}$. A package {\tt clere} for R does exist on CRAN and is included in {\tt CorReg}. \\
			 But We have to choose the maximum number of components $g$ and have no method to choose this value. Yengo recommends to use $g=5$ in our case. It could be interpreted as the possibility to have a group of irrelevant covariates and groups with small or big values (both positives or negatives). The package is able to choose automatically the best number of components between 1 and $g$ based on a $BIC$ criterion but setting $g=p$ gives over-fitting. \\
	 Here again, it has no specific protection against or specific model for correlations. 
		\subsection{Spike and Slab}			% ne pas oublier de mentionner les packages existants

			Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the $\beta_j$ and gives a subset of covariates (not grouped) on which to compute \textsc{OLS} but has no specific protection against correlations issues.  The $\beta_j$ are supposed to come from a mixture distribution as shown in figure \ref{spikeslab}. It allows to have some coefficients set exactly to zero after some draws. 
			
\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/spikeslab.png} 
	\caption{}\label{spikeslab}
\end{figure}		
		The package {\tt spikeslab} for R on CRAN is also included in {\tt CorReg}. \\
		
		Modeling the parameters implies to have no exact value to give to the coefficient and it is not really user-friendly, especially in our industrial context. However, these two methods can be used for variable selection in the {\tt correg} function using the parameter {\tt select="clere"} or {\tt select="spikeslab"}.
		
		\FloatBarrier
	\section{Miscellaneous}	
	\subsection{Principal Component Regression and Partial Least Squares Regression}
	Principal Component Regression \cite{jackson2005user} consists in using the axis from the Principal Component Analysis (PCA) of $\boldsymbol{X}$ instead of $\boldsymbol{X}$ itself. Then we have orthogonal covariates. Dimension reduction is done by keeping only the $M \leq p$ first components of the PCA. Because the axis are linear combination of the original covariates we can then express the model in terms of coefficients of the $\boldsymbol{X}^j$.
	
	Principal Component Regression requires to choose $M$ the number of axis to keep. Finally, even if dimension reduction is effective when $M<p$ each axis depends on all original covariates so it does not select any covariates and that is also a problem for interpretation. We have to choose arbitrary how to interpret each axis and how many covariates really explain each of them. So it is not really satisfying in our industrial context. Principal Component method can be seen as a truncation method whereas the ridge regression is a shrinkage method. 
	
	Partial Least Square Regression \cite{abdi2003partial,geladi1986partial} also relies on a combination of the columns of  $\boldsymbol{X}$ but this combination depends on $\boldsymbol{Y}$.
		It uses scalar products of the covariates with the response variable
		
	It seems to be influenced more by the variance of the covariates than by their correlation with the response variable so results tends to be similar to Principal Component Regression.
	\\
	The R package {\tt pls} on CRAN computes both Principal Component Regression and Partial Least Squares Regression.
		
	\subsection{Sliced Inverse Regression}
		Sliced Inverse Regression is a semi-parametric approach that could be seen as easier to interpret than general non-parametric regression \cite{eubank1999nonparametric,hardle1990applied} that are too complex to interpret for our industrial context. \\
		
		The main idea is to compute the inverse regression $P(\boldsymbol{X}|\boldsymbol{Y})$ instead of $P(\boldsymbol{Y}|\boldsymbol{X})$ to perform a weighted principal component analysis, with which we can identify the effective dimension reducing directions \cite{li1991sliced}. But it needs strong hypothesis on the distribution of $\boldsymbol{X}$ (elliptic hypothesis), even if it is possible to neglect this hypothesis \cite{saracco1999regression} and see what happens. This method has been rejected because it was not sufficient in terms of ease of use (during and after estimation) for non-statisticians due to the semi-parametric aspect and the elliptic hypothesis is compromised on datasets with hundreds of covariates. It is part of the dimension reduction package {\tt dr} on CRAN.
			
		
			
		
\subsection{Classification and Regression Trees (CART)}
		%\cite{quinlan1986induction} à vérifier

	
		Classification And Regression Trees (CART) \cite{breiman1984classification} are extremely simple to use and interpret, can work simultaneously with quantitative and qualitative covariates and are very fast to compute. They consist in recursive partitioning of the sample according to binary rules on the covariate (only one at a time) to obtain a hierarchy defined by simple rules and containing pure leaves (same value). It is followed by a pruning method to obtain leaves that are quite homogeneous and described with simple rules.
		
		\begin{figure}
			\centering
				\includegraphics[width=350px]{figures/arbretoy.png} 
			\caption{Regression tree obtain with the package {\tt CorReg}
			 (graphical layer on top of  the {\tt rpart} package) on the running example.}\label{arbretoy}
		\end{figure}
		CART is implemented in the package {\tt rpart} 
		for R. Our {\tt CorReg} 
		package offers a function to compute and plot the tree in one command with a subtitle to explain how to read the tree and global statistics on the dataset.
		But it is not convenient for linear regression problems as we see in figure \ref{arbretrivial} because a same variable will be used several times and the tree will fail to give a simple interpretation as " $\boldsymbol{Y}$ and $\boldsymbol{X}_1$ are proportional ". Trivial case : $\boldsymbol{Y}=\boldsymbol{X}_1 + \boldsymbol{\varepsilon}$ where $\boldsymbol{\varepsilon} \sim \mathcal{N}(0, \sigma^2 \boldsymbol{I}_n)$ with $\sigma=0.5$. \\
		So CART will be used as a complementary tool for datasets with both quantitative and qualitative covariates or when the dependence between $\boldsymbol{Y}$ and $\boldsymbol{X}$ is not linear. We will focus our research on linear models with quantitative only variables.
	
\begin{figure}[h!]
	\subfigure[Tree found for the trivial case]{
			\includegraphics[height=180px,width=245px]{figures/arbretrivial.png} 
	} \quad
   	\subfigure[True linear regression and splits obtained by the tree.]{
			\includegraphics[height=180px,width=240px]{figures/arbretrivialcoupure.png} 
	}
	\caption{Predictive model associated to the tree (red) and true model (black)}\label{arbretrivial}
\end{figure}

More details in the book from Hastie \cite{hastie2009elements}. The main issues are the lack of smoothness (prediction function with jumps) and especially instability because of the hierarchical partitioning. Modifying only one value in the dataset can impact a split and then change the range of possible splits in the resulting sub-samples so if a top split is modified the tree can be widely changed. Random Forests are a way to solve this problem and can be seen as a cross-validation method for regression trees.
	
	\subsection{Neural networks}	
		Neural networks \cite{fausett1994fundamentals} are a statistical model designed to mimic the brain and its neuronal organization. It seems to be really powerful but it has a predictive only goal and the model obtained can't be interpreted easily (and can't be interpreted at all if too complex). So it does not correspond to our needs. Interpretation remains our first goal and prediction comes far behind.
		
	\subsection{Bayesian networks}
		
		Bayesian networks \cite{heckerman1995learning,jensen2007bayesian,friedman2000using} model the covariates and their conditional dependencies via a Directed Acyclic Graph (DAG). Such an orientation is very user-friendly because it is similar to the way we imagine causality. But it is only about conditional dependencies. Conditional dependencies allow to use information from independent covariates. The usual example is the case of wet grass in a garden. You don't remember if the sprinkler was on or off, you don't know if it has rain and these two facts are independent. Then you look at the grass in your neighbour's garden and it is not wet \dots \\
		You will deduce that your sprinkler was on. Such conditionals dependencies are used in chapter \ref{chapmiss} when confronted to missing values. \\
		Bayesian networks are quite good in terms of interpretation because of that graphical and oriented representation of conditional probabilities but suffer from great dimension (combinatory issue) and require to transform the dataset arbitrary (discretisation), that imply a loss of information and usage of a priori (that is explicitly not suitable in our industrial context). The choice of the way you discretise the dataset has a great impact on the results and nothing can help if you have no a priori on the result you want to obtain. Computation relies on a table that describes all the possible combinations for each covariate. Hence it is extremely combinatory if the graph has too much edges or is not sparse enough. Moreover, you need to define the graph before computing the bayesian network and without a priori it can be challenging and time consuming.\\
The concept of representing dependencies with directed acyclic graph is good an we keep it in our model.		
			
	

		\section{Choice of model}
		% ne pas oublier de mentionner les packages existants
			\subsection{Cross validation}
				
				Time reveals quality of a model. To have an idea of the stability of a model, it is recommended to test it on a validation sample. The model parameters are estimated with a learning sample and then the model is evaluated (by its predictive MSE for example) on a validation sample to avoid over-fitting. But it is not always possible to have a validation sample and over-fitting is a real problem. A solution is to use Cross-Validation \cite{kohavi1995study,arlot2010survey}. It consists in splitting the dataset in $k$ sub-sample ($k$-fold cross-validation) and then each of the $k$ sub-samples is successively used as validation sample for the model learn with the $k-1$ remaining sub-samples. Each time a quality criterion  is computed (predictive MSE or other) and then the mean of this criterion is taken as the global criterion. The global estimator is also the mean of the estimators. The two main issues are:
				\begin{itemize}
					\item How to choose $k$ the number of sub-samples ?
					\item It can be time consuming as the model is estimated $k$ times.
				\end{itemize}
				If $k=n$ we call this method the "leave-one-out" cross-validation.
				Cross-validation allows to learn the model using all individual exactly once for validation.
				Cross-validation is computed on each model we want to compare and just allows to avoid over-fitting when computing the comparison criterion.
				It is often used with the Mean Squared Error (MSE), for example on the prediction:
				\begin{equation}
					MSE_{\hat{Y}}=\parallel \boldsymbol{Y}-\hat{\boldsymbol{Y}} \parallel^2_2
				\end{equation}
			\subsection{ Bayesian Information Criterion}
			Cross-validation depends on a criterion to be used as a method to choose a model. The mean square error is not the only criterion. Probabilist criteria can also be used when we have an hypothesis on the distribution of the model studied. Such criteria can also be used without cross-validation.
			The Bayesian Information Criterion \cite{BIChuard} is a widely used criterion that relies on the likelihood of the dataset knowing the model the estimated parameters. The advantage of $BIC$ over simple usage of the likelihood is the penalty added to take into account the numbers of parameters to estimate (complexity is then penalized) and the number of individuals in the dataset. The Akaike Information Criterion \cite{akaike1974new} known as $AIC$ or the Risk Inflation Criterion \cite{foster1994risk} can also be used.
			In this work we decided to start with the $BIC$ that is approximated by:
			\begin{equation}
				BIC=-2\ln(\hat{L})+k\ln(n)
			\end{equation}
			where $\hat{L}$ is the estimated likelihood, $k$ the number of free parameters to estimate and $n$ the number of individuals in the dataset.
			\subsection{Stepwise}
			
			Stepwise \cite{seber2012linear} is an algorithm to choose a subset of covariates to use in the final regression model. It is a variable selection method using OLS for estimation. It is proposed in the R package {\tt stats} with the function {\tt step}. The main idea is to start with first model (than can be either void or using the whole dataset or using any subset of covariates) and then to add and remove covariates step by step to improve the chosen criterion. \\
			\begin{itemize}
				\item 	Starting with a void model and having only adding steps is called Forward Selection. Covariates are added by choosing first the one that improves the most the criterion and the algorithm stops when all the covariates are in or when remaining covariates does not improve the model.
				\item Backward Elimination is the same as Forward selection but starting with the full model and deleting at each step the covariates that improves the most the criterion once deleted.
				\item Bidirectional elimination is more flexible and allows to start from any model. Each proposes to add a covariate and then to delete another so it is not a hierarchical construction any more because succesives models are not necessarily nested into each other.
			\end{itemize}
		A critical value can be defined to stop the algorithm when improvement become too small, to try to avoid over-fitting. \\
		Stepwise regression is subject to over-fitting and the algorithm is in trouble when confronted to correlated covariates \cite{miller2002subset} giving unstable results, especially for nested strategies, just like regression trees that are unstable because of their discrete nested nature.
\begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/stepwisecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{stepwise}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEstepwisecompl}
	\end{figure}
	Figure \ref{MQEstepwisecompl} illustrate the consequences of correlations in the dataset.
	
%			\subsection{bootstrap}
%			\cite{efron1979bootstrap,efron1994introduction}
		\FloatBarrier
%		\section{MCMC}
		
		\section{Gibbs}
		Gibbs sampling \cite{casella1992explaining} is a special case of Markov Chain Monte Carlo algorithm \cite{gilks1996markov,chib1995understanding,roberts2001optimal} that allows to sample from a complex $p-$multivariate distribution when direct sampling is difficult. It is a randomized algorithm so each run may give distinct results. It generates a Markov Chain that follows the desired distribution with nearby draws. It starts from an initial value $\boldsymbol{X}^{(0)}$ and then for each iteration $(q)$ and successively each variable $x_j^{(q+1)}$ to draw, it draws from $P(x_j|x_1^{(q+1)}, \dots,x_{j-1}^{(q+1)},x_{j+1}^{(q)},\dots,x_p^{(q)})$ using the most recent drawn values each time. \\
		It will be used in this work for imputation of missing values in chapter \ref{chapmiss}.
		
		\section{EM}
			Sometimes the maximization of a likelihood is too complex to be done analytically. In such a case, we can use Expectation-Maximization algorithms \cite{mclachlan2007algorithm} that maximize a likelihood by the optimization of its parameters and estimation of latent variables $Z$. This kind of algorithm allows to manage missing values \cite{dempster1977maximum}. \\
EM is an alternate optimization of the parameters $\boldsymbol{\theta}$ and the latent variables $Z$ with two steps at each iteration $(q)$:
\begin{itemize}
	\item The Expectation step in which the expected value of the loglikelihood is computed for the current estimate of the parameters $\boldsymbol{\theta}^{(q)}$ . It finds the best value of $Z$ given $\boldsymbol{\theta}^{(q)}$.
	\begin{equation}
		Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(q)})=E_{Z|\boldsymbol{X},\boldsymbol{\theta}^{(q)}} [\log L(\boldsymbol{\theta};\boldsymbol{X},Z)]
	\end{equation}
	\item The Maximization step that maximizes:
	\begin{equation}
		\boldsymbol{\theta}^{(q+1)}=\arg \max_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(q)})
	\end{equation}
\end{itemize}
	So it alternates between estimation of $Z$ and $\boldsymbol{\theta}$ until convergence. This algorithm can be used to choose the best mixture model associated to the distribution of an observed variable for example.		
	It requires to choose initial values for $\boldsymbol{\theta}^{(0)}$ and faces local extrema problems so it is recommended to make multiple initializations and run of the algorithm.
			
			Some variants were developed like the Stochastic EM \cite{diebolt1996stochastic,celeux1986algorithme} or the Classification EM \cite{celeux1992classification}.
			
			
		\section{Industrial tools}
			%Linear regression, decision trees, bayesian networks, neural network but without confidence
			Our engineers are frequently contacted by software resellers. The concerned software are sold as non-statistical methods, based on rules (derivatives of decision trees but without underlying theory). Statistics are often described as too theoretical, without ergonomy and not compatible with the real life. Our goal was to demonstrate that statistics can provide efficient methods for real datasets, easy to use and understand.	This was a battle against correlations but also against scepticism. \\
			Figure \ref{Regle2D} shows the kind of results proposed by rule-based softwares. It is just a partition of the sample by binary rules (like regression trees). Some blur is added to the plot to help interpretation. The algorithm used here is somewhere between exhaustive research and decision trees. It is extremely slow (research with $p>10$ would take years) and is less efficient than regression trees. Morever, it requires to discretize the response variable to obtain "good" and "bad" values. The green rectangle is very far from the true green zone even for this toy example provided by the reseller.\\
			But some engineers are seduced by this kind of tool. Ergonomy of use and quality of interpretation are stakes for us to make engineers use efficient methods instead of this kind of stuff.
	
		\begin{figure}[h!]
	\subfigure[Without blur]{
			\includegraphics[height=180px,width=245px]{figures/Regle2Dbis.png} 
	} \quad
   	\subfigure[With some blur proportional to the density of points]{
			\includegraphics[height=180px,width=240px]{figures/Regle2D.png} 
	}
	\caption{Result on a toy example provided by a non-statistical software, similar to decision trees but less efficient and extremely slower. Colors are part of the learning set.}\label{Regle2D}
\end{figure}	
			
	\section{Multiple Equations}		% ne pas oublier de mentionner les packages existants
		
		\subsection{Simultaneous Equation Model (SEM) and Path Analysis}		% ne pas oublier de mentionner les packages existants
		Applied statistics in for non statisticians are well developed in sociology where interpretation  stakes are fare beyond prediction. Sociologists use simple models like linear regression (often with $R^2<0.2$) and describe complex situations with systems of linear regressions. Such systems are called Structural Equation Model or Simultaneaous Equation Model, better known as SEM \cite{davidson1993estimation}. Several softwares from the open-source GRETL \cite{CottrellLucchetti2007gretlmanual} to proprietary STATA does implement the SEM. The systems allow to describe which covariate influence others with an orientation that users can interpret as causality \cite{pearl2000causality,pearl1998graphs}. \\
		SEM are easy to understand for non-statisticians and can be resumed by Directed Acyclic Graphs (DAG) as the Bayesian networks do. But the problem is that the structure of regression between the covariates is defined by hand. SEM are used to confirm sociological theories, not to create new theories. \\
		Moreover, estimation of recursive SEM, without instrumental variables is exactly a succession of independent OLS (confirmed with both GRETL and STATA) so the structure is only used for interpretation, not for estimation \cite{brito2006graphical}. Last but not least, there is no specific status for a response variable, each regression has the same status. We want to be able to model complex dependencies within the covariates and use this knowledge to estimate understand a specific response variable.
			%mcdonald2002principles
		\subsection{SUR: Seemingly Unrelated Regression}		% ne pas oublier de mentionner les packages existants
		Seemingly Unrelated Regression \cite{SURzellner} is an estimation method for Multiples equations with correlated error terms. It does not take into account correlations between the covariates but starts to estimate the system jointly instead of independent estimations. This estimation relies on Feasible Generalized Least Squares (FGLS) that depends on the variance-covariance matrix of the error terms. But when the error terms are independent or the subset of covariates are the same it is equivalent to successive independent OLS. The R package {\tt systemfit} on CRAN computes SUR. 
			
		\subsection{SPRING: Structured selection of Primordial Relationships IN the General linear model}		% ne pas oublier de mentionner les packages existants

\textsc{SPRING} (Structured selection of Primordial Relationships IN the General linear model \cite{chiquetconf}) takes into account correlations between endogenous covariates but without explicit expression of the correlations and thus a reduced interpretation potential. It is similar to SUR in that dependencies are modeled with the variance-covariance matrix of the error terms, but with sparsity constraints in this matrix.  It does not distinguish a response variable from internal structure in a dataset. Every equation in the system has a distinct response variable and the explicative covariates are the same for all equations.		
		\subsection{Selvarclust: Linear regression within covariates for clustering}		% ne pas oublier de mentionner les packages existants

			Selvarclust is a software written in C++ modeling dependencies within a dataset			 \cite{maugis2009variable} in a clustering context.
			The idea is to allow covariates to have different roles $(S,R,U,W)$ for the clustering:
			\begin{itemize}
				\item $S$ stands for the subset of the relevant covariates for clustering
				\item $U$ and $W$ form a partition of the complementary subset of $S$, with $U$ the subset of irrelevant covariates depending linearly from relevant covariates and $W$ the subset of totally irrelevant and independent covariates
				\item $R$ is the subset of $S$ that contain the covariates explaining those in $U$, with $R\cap U=\emptyset$.
\end{itemize}			 
So we have a system of linear regression:
\begin{equation}
	\boldsymbol{X}^{U}=\boldsymbol{X}^R \boldsymbol{\alpha}+ \boldsymbol{\varepsilon}
\end{equation}
where $\boldsymbol{\alpha}$ is the $\operatorname{Card}(R)\times \operatorname{Card}(U)$ matrix of the regression coefficients and $\boldsymbol{\varepsilon}$ the matrix of the error terms.\\
Selvarclust is one step beyond SEM with an algorithm to find the structure.		

			But:
			\begin{itemize}
				\item It is about clustering and not regression (not the same application field) so we don't have here any response variable neither method to use this structure for estimation of another regression. 
				\item It uses stepwise-like algorithm without protection against correlations \cite{raftery2006variable} even it is known to be often unstable \cite{miller2002subset} in such a context.
			\end{itemize}
			
			In this work we propose to adapt this model for linear regression and to use it as a pretreatment on correlated covariates. We will see that, as a pretreatment it can be used then for a wide range of statistical tools and not only linear regression.\\
			We provide a specific MCMC algorithm to find the structure between the covariates and propose two distinct models to use the structure for prediction: a marginal model and a plug-in model.	\\
			
	Our first intention was to make something different from SelvarClust but our model has evolved during this thesis and finally became more comparable to SelvarClust than expected.		
			
\part{Pretreatment for correlations}
\chapter{Decorrelating covariates by a generative model}
\paragraph{Abstract:} Nous modélisons explicitement les corrélations entre covariables par un système de régressions linéaires entre covariables. Cela permet une meilleure compréhension des données mais aussi une préselection de variables mettant de côté les variables redondantes pour réduire fortement les corrélations tout en ne perdant que peu d'information. La préselection prend un sens particulier grâce à la structure de sous-régression qui permet de distinguer par suite les variables indépendantes de la variable réponse de celles qui sont juste redondantes mais potentiellement liées à la variable réponse. 
\\

\section{Our proposal: modelisation of the correlations}
	Let $\boldsymbol{X}$ be a $n \times p$ matrix of observed covariates and $\boldsymbol{Y}$ be the $n \times 1$ matrix of the observed response variable. In the following, we note $\boldsymbol{X}^j$ the $j^{th}$ column of $\boldsymbol{X}$ and $\boldsymbol{X}^{J}$ where $J=\{j_1,\dots,j_k\}$ the $n\times k$ sub-matrix of $\boldsymbol{X}$ composed by the columns of $\boldsymbol{X}$ whose indices are in the set $J$. 
\\

We make the hypothesis that $\boldsymbol{X}$ can be described by a partition $\boldsymbol{X}=(\boldsymbol{X}^{I_f},\boldsymbol{X}^{I_r}) $ given by an explicit structure $S$ where variables in the $n\times p_r$ sub-matrix $\boldsymbol{X}^{I_r}$ are redundant endogenous covariates resulting from linear sub-regressions based on $\boldsymbol{X}^{I_f}$, the  $n\times (p-p_r)$ sub-matrix of free (mutually independent) exogenous covariates.
So we model the correlations by $P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f}) $ with $\boldsymbol{X}^{I_f}$ orthogonal covariates.
 
 

The structure $S$ of $p_r$ sub-regressions within correlated covariates in $\boldsymbol{X}$ is described by:
	\begin{equation}
		\boldsymbol{X}^{I_r}_{|\boldsymbol{X}^{I_f},S} \textrm{ defined by }\forall j \in I_r: \boldsymbol{X}^j_{|\boldsymbol{X}^{I_f},S}=\boldsymbol{X}^{I_f}\boldsymbol{\alpha}^j+\boldsymbol{\varepsilon}^j \textrm{ with } \boldsymbol{\varepsilon}^j \sim\mathcal{N}(\boldsymbol{0},\sigma^2_j\boldsymbol{I}_n) \label{SR}
	\end{equation}
		where $\boldsymbol{\alpha}^j \in \mathcal{R}^{(p-p_r)}$ are the sparse vectors of the regression coefficients between the covariates (each sub-regression freely implies different covariates). 
We also define $I_f=\{I_f^1,\dots,I_f^p \}$ the set of the sets of indices of exogenous covariates with
\begin{eqnarray}
	\forall j \in I_r, I_f^j&=&\{i|\boldsymbol{\alpha}^j_i\neq 0 \} \\
	\forall j \notin I_r, I_f^j&=&\emptyset .
\end{eqnarray}
%We see that $I_f$ defines the non-null coefficients in $\boldsymbol{\alpha}_j$ (each sub-regression can be very parsimonious).
Then we have the explicit structure characterized by $S=\{I_f,I_r,\boldsymbol{p}_f,p_r\}$ where $p_r=|I_r|$, $\boldsymbol{p}_f=(p_f^1,\dots,p_f^{p_r})$ is the vector of the number of covariates in each sub-regression  and $p_f^j=|I_f^j|$, with $|.|$ the cardinal of an ensemble. 
\\
\\


The partition of $\boldsymbol{X}$ implies the uncrossing rule  $\boldsymbol{X}^{I_r} \cap \boldsymbol{X}^{I_f}$ 
{\it i.e.} endogenous variables don't explain other covariates. This hypothesis ensures that $S$ contains no cycle and is straightforward readable (no need to order the sub-regressions). It is not so restrictive because cyclic structures have no sense and any non-cyclic structure can be associated with a structure that verifies the uncrossing constraint by just successively replacing endogenous covariates by their sub-regression when they are also exogenous in some other sub-regressions.

	
	  We make the choice to distinguish the response variable from the other endogenous variables (that are on the left of a sub-regression). Thus we have one regression on the response variable ($P(\boldsymbol{Y}|\boldsymbol{X}))$ and a system of sub-regressions (without the response variable: $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$). Then we consider correlations between the explicative covariates of the main regression, not between the residuals. We see that the $S$ does not depend on $\boldsymbol{Y}$ so it can be learnt independently, even with a larger dataset (if missing values in $\boldsymbol{Y}$).
	 
The structure obtained gives a system of linear regression that can be viewed as a recursive Simultaneous Equation Model (\textsc{SEM})\cite{davidson1993estimation} \cite{TIMM}.
\\
We note $\boldsymbol{\alpha}$ the $(p-p_r)\times p_r$ matrix of the  $\boldsymbol{\alpha}^j$.
  	Here we suppose the $\boldsymbol{\varepsilon}_j$ independent but in other cases \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) takes into account correlations between residuals and could be used to estimate $\boldsymbol{\alpha}$. 
		 
	 
\paragraph{In the running example:}$\boldsymbol{X}_r=\boldsymbol{x}_3$, $\boldsymbol{X}_f=\{\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_4,\boldsymbol{x}_5 \}$, $p_r=1$ and $\boldsymbol{\alpha}_3=(1,1,0,0)'$ and we have $S= \left( \{ \{1,2\}\},\{3\},(2),(1)\right)$


\section{Graph theory}\label{sectiongraph}
	We can model $S$ by a Directed Acyclic Graph (DAG) whose vertices are the $p$ covariates and arcs are the link between them described by the adjacency matrix $\boldsymbol{G}$ \cite{bondy1976graph}. This adjacency matrix is a binary $p\times p$ matrix with $\boldsymbol{G}_{i,j}=1$ if and only if $i \in I_f^j$ that is $\boldsymbol{X}^j$ is explained by $\boldsymbol{X}^i$ and can also be seen as $\boldsymbol{\alpha}_i^j\neq 0$.
	
	Graphical representation of $S$ helps to understand it and can be compared to the bayesian network representation. It helps to interpret the structure has also been used to construct the algorithm to find $S$ (chapter \ref{chapterMCMC}).


	
	The partition of $\boldsymbol{X}$ mean that the associated graph is bipartite (vertices follow a partition $(\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f})$) with arcs only going from $\boldsymbol{X}^{I_f}$ to $\boldsymbol{X}^{I_r}$.
	
	We know (\cite{biggs1993algebraic})as a classical result of graph theory that the power of adjacency matrices give the paths in the graph: $\boldsymbol{G}^k_{i,j}\neq 0$ means that there is a path of length $k$ going from $\boldsymbol{X}^i$ to $\boldsymbol{X}^j$. Because the graph is bipartite and arcs are only going from $\boldsymbol{X}^{I_f}$ to $\boldsymbol{X}^{I_r}$ we can deduce that $\boldsymbol{G}$ is nilpotent: $\boldsymbol{G}^2=0$. And we have the following result: every binary nilpotent matrix of order 2 can be seen as an adjacency matrix of a structure that respects the uncrossing rule. proof by contradiction: if there exist a path of length 2 between some vertices $i$ and $j$ then $\boldsymbol{G}^2_{i,j}\neq 0$ so the matrix is not nilpotent of order 2. We can deduce that the number of feasible structure with $p$ covariates is the number of binary nilpotent matrix of order 2.
	
	We see that $\boldsymbol{G}$ completely describe $S$ and that the sparse storage of $G$ gives $I_f$ which is sufficient to obtain $S$ by doing $\forall 1\leq j\leq p :  p_f^j=|I_f^j|$, $I_r=\{j |p_f^j>0 \}$ and $p_r=|I_r|$.
	This decomposition helps us to enumerate all the feasible structure (and thus all the binary nilpotent matrix of order 2).\\
	We note $\mathcal{S}_p$ the set of the feasible structure with $p$ covariates. If we consider all the structure with equiprobability:
	\begin{eqnarray}
		S&=&\{I_f,I_r,\boldsymbol{p}_f,p_r\} \\
		P(S|p_r)&=&P(I_f , \boldsymbol{p}_f|I_r,p_r)P(I_r|p_r) \\
		&=&P(\boldsymbol{p}_f|I_f, I_r,p_r)P(I_f |I_r,p_r)P(I_r|p_r) \\
		&=&P(I_f |I_r,p_r)P(I_r|p_r) \\
		%&=&\sum_{p_r=0}^{p-1}P(I_f |I_r,p_r)P(I_r|p_r)\\
		P(I_r|p_r)&=& \frac{1}{{p \choose p_r}} \\
		P(I_f|I_r,p_r)&=& \frac{1}{(2^{p-p_r}-1)^{p_r}}\\
		|\mathcal{S}_p|&=&\sum_{p_r=0}^{p-1}|\mathcal{S}_{p|p_r}|= \sum_{p_r=0}^{p-1}\frac{1}{P(S|p_r)} =\sum_{p_r=0}^{p-1}{p \choose p_r}(2^{p-p_r}-1)^{p_r}
	\end{eqnarray}
	where \begin{equation}
	{n \choose k}=\frac{n!}{k!(n-k)!}
	\end{equation} is the binomial coefficient.\\
	We have then $|\mathcal{S}_2| =3 $,$|\mathcal{S}_3| =13 $ and $|\mathcal{S}_{10}| >13.26\times10^9 $ so the number of feasible structures really explodes when $p$ is growing.

\paragraph{In the running example:} $|\mathcal{S}_5| =841 $

\begin{figure}[h!]
	\centering
	\includegraphics[width=200px]{figures/graphetoy.png} 
	\caption{The bipartite graph associated to the running example}
	\end{figure}	
and the adjacency matrix is:
\begin{displaymath}G=\left(	\begin{array}{ccccc}
	0 & 0 & 1 & 0 & 0 \\ 
	0 & 0 & 1 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0
	\end{array} \right)
\end{displaymath}



\section{A by-product model: marginal regression with decorrelated covariates}
Now we know $P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},S)$ by the structure of sub-regressions, we are able to define a marginal regression model $P(\boldsymbol{Y}|\boldsymbol{X}^{I_f},S)$ based on the reduced set of independent covariates $\hat{\boldsymbol{\beta}}_f$ without significant information loss. We use the information of the correlations structure to rewrite the true model without bias in the marginal space defined by the independent covariates.
 	\\
Using the partition $\boldsymbol{X}=[\boldsymbol{X}^{I_f},\boldsymbol{X}^{I_f}]$ we can rewrite (\ref{regressionsimple}):
	\begin{equation}
			\boldsymbol{Y}_{|\boldsymbol{X}^{I_f},\boldsymbol{X}^{I_r},S}=\boldsymbol{X}^{I_f}\boldsymbol{\beta}_{I_f}+\boldsymbol{X}^{I_r}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon_Y} \label{MainR}
		\end{equation}
		where $\boldsymbol{\beta}=(\boldsymbol{\beta}_{I_f},\boldsymbol{\beta}_{I_r}) \in  \mathcal{R}^p$ is the vector of the regression coefficients associated respectively to $\boldsymbol{X}^{I_f}$ and $\boldsymbol{I}_n$ the identity matrix. 
We note that (\ref{SR}) and (\ref{MainR}) give also by simple integration on $\boldsymbol{X}^{I_r}$ a marginal regression model on $\boldsymbol{Y}$ {\it depending only on uncorrelated covariates $\boldsymbol{X}^{I_f}$}:
\begin{eqnarray}
		P(\boldsymbol{Y}|\boldsymbol{X}^{I_f})&=& \int_{\boldsymbol{X}^{I_r}}P(\boldsymbol{Y}|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f})P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f}) d \boldsymbol{X} \\
	\boldsymbol{Y}_{|\boldsymbol{X}^{I_f},S}&=&\boldsymbol{X}^{I_f} (\boldsymbol{\beta}_{I_f}+ \sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j)+  \sum_{j \in I_r}\beta_{j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{Trueexpl} \\
	&=&\boldsymbol{X}^{I_f}\boldsymbol{\beta}_{I_f}^*+\boldsymbol{\varepsilon}_Y^*\label{modexpl}
\end{eqnarray}
 This model is still the true model and OLS estimator will still give an unbiased estimator, but its variance will be reduced by both dimension reduction and decorrelation (variables in $\boldsymbol{X}^{I_f}$ are independent so the matrix $\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f}$ will be well-conditioned). So the information given by the structure $S$ allows to reduce the variance without adding bias, by simple marginalization.
\\
Nevertheless, to be able to compare the bias-variance tradeoff, we can see this model as a variable pre-selection independent of the response in $\boldsymbol{Y}_{|\boldsymbol{X}}$.
We note that it is simply a linear regression on some of the original covariates so we only made a pre-treatment on the dataset by selecting $\boldsymbol{X}^{I_f}$ because of the correlations given by $S$. So we also get the model
\begin{equation}
\boldsymbol{Y}_{|\boldsymbol{X},S}=\boldsymbol{X}\boldsymbol{\beta}^*+\boldsymbol{\varepsilon}_Y^* \textrm{ where }\boldsymbol{\beta}^*=(\boldsymbol{\beta}_{I_f}^*,\boldsymbol{\beta}_{I_r}^*) \textrm{ and } \boldsymbol{\beta}_{I_r}^*=\boldsymbol{0}
\end{equation}
	for which OLS estimator of the coefficients may be biased.  

\paragraph{Running example:} $\boldsymbol{Y}_{|\boldsymbol{X}^{I_f}}= 2\boldsymbol{x}_1+2\boldsymbol{x}_2+\boldsymbol{x}_4+\boldsymbol{x}_5+\boldsymbol{\varepsilon}_3 +\boldsymbol{\varepsilon}_Y$




\section{Strategy of use: pre-treatment before classical estimation/selection methods}\label{interpretation}

As a pre-treatment, the model allows usage of any method in a second time to estimate $\boldsymbol{\beta}_{I_f}^*$, even with variable selection methods like LASSO or a best subset algorithm like stepwise \cite{seber2012linear}. However, we always have $\boldsymbol{X}^{I_r}=\boldsymbol{0}$

After selection and estimation we will obtain a model with { \it two steps of variable selection}: the decorrelation step by marginalization(coerced selection associated to redundant information defined in $S$) and the classical selection step, with different meanings for obtained zeros in $\hat{\boldsymbol{\beta}}^*_{I_f}$ (irrelevant covariates) and for $\hat{\boldsymbol{\beta}}^*_{I_r}=0$ (redundant information). 
 Thus we are able to distinguish the reasons of selection and consistency issues don't mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.


The explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset and to take better actions. Expert knowledge can even be added to the structure, physical models for example.

Moreover, the uncrossing constraint (partition of $\boldsymbol{X}$) guarantee to keep a simple structure easily interpretable (no cycles and no chain-effect) and straightforward readable.

	
			There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypothesis. We just play on the traditional bias-variance tradeoff. 
\chapter{Numerical results with a known structure}	
\paragraph{Abstract:} Premiers résultats numériques pour une structure (hors coefficients de sous-régression) connue. On constate un net apport de la méthode de préselection.
		 
	\section{Illustration of the tradeoff conveyed by the pre-treatment}	
	We compare the OLS estimator on $\boldsymbol{X}$ defined in section \ref{sectionOLS} with the estimator obtained by the pre-treatment that is $\boldsymbol{X}^{I_f}$ selection.
  
For the marginal regression model defined in (\ref{modexpl})
%	\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}_f}= \boldsymbol{X}_f\boldsymbol{\beta}_f^*+ \boldsymbol{\varepsilon}_Y^*
%	\end{equation}			
%		So 
we have the \textsc{OLS} unbiased estimator of $\boldsymbol{\beta}^*$: 
		\begin{equation}
			\hat{\boldsymbol{\beta}}_{I_f}^* = (\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f})^{-1}\boldsymbol{X}^{I_f'}\boldsymbol{Y}  \textrm{ and }\boldsymbol{\hat\beta}_{I_r}^* = \boldsymbol{0}
		\end{equation}
		We see in (\ref{Trueexpl}) that it gives an unbiased estimation of $\boldsymbol{Y}$ and $\boldsymbol{\beta^*}$
		but in terms of $\boldsymbol{\beta}$ this estimator is biased:
		\begin{equation}
			\operatorname{E}[\hat{\boldsymbol{\beta}}_{I_f}^*|\boldsymbol{X}^{I_f}]=\boldsymbol{\beta}_{I_f}+\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \textrm{ and }\operatorname{E}[\hat{\boldsymbol{\beta}}_{I_r}^*|\boldsymbol{X}^{I_f}]=\boldsymbol{0}
		\end{equation}
		with variance:
		\begin{equation}
			\operatorname{Var}[\hat{\boldsymbol{\beta}}_{I_f}^*|\boldsymbol{X}^{I_f}]= (\sigma^2_Y+\sum_{j \in I_r}\sigma^2_{j}\beta_{j}^2 )(\boldsymbol{X}^{I_f'} \boldsymbol{X}^{I_f})^{-1}  \textrm{ and }\operatorname{Var}[\hat{\boldsymbol{\beta}}_{I_r}^*|\boldsymbol{X}^{I_f}]= \boldsymbol{0} 
		\end{equation}
		We see that the variance is reduced compared to OLS described in equation (\ref{eqOLS})(no correlations and smaller matrix give better conditioning ) for small values of $\sigma_j$ $i.e.$ strong correlations. So we play on the bias-variance tradeoff, reducing the variance by adding a bias. 				  
		  
		  
	 The theoretical Mean Squared Error (\textsc{MSE}) on $\hat{\boldsymbol{\beta}}$ is:
	\begin{eqnarray}
		E[\textsc{MSE}(\hat{\boldsymbol{\beta}}|\boldsymbol{X})]&=&\parallel \operatorname{Bias}\parallel_2^2+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}})) \\
			E[\textsc{MSE}(\hat{\boldsymbol{\beta}}_{OLS}|\boldsymbol{X})]&=& 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}) %\textrm{ for OLS, and then for the marginal model:}
			 \\
			E[\textsc{MSE}(\hat{\boldsymbol{\beta}}^*_{OLS}|\boldsymbol{X})]&=& \parallel\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \parallel_2^2 +\parallel \boldsymbol{\beta}_{I_r}\parallel^2_2 + (\sigma^2_Y+\sum_{j \in I_r}\sigma^2_{j}\beta_{j}^2 ) \operatorname{Tr}((\boldsymbol{X}^{I_f'} \boldsymbol{X}^{I_f})^{-1})
	\end{eqnarray}	 
	To better illustrate the bias-variance tradeoff, we look at the running example. We observe the theoretical Mean Squared Error (MSE) of the estimator of both OLS and \textsc{CorReg}'s marginal  model for several values of $\sigma_3$ (strength of the sub-regression) and $n$. Figure \ref{MQEexplOLSp5col} shows the theoretical MSE evolution with the strength of the sub-regression. In this section, all experiences have been made 100 times to obtain smooth curves. So we have generated 100 times $\boldsymbol{X}$ and $\boldsymbol{Y}$.
%	\begin{equation}
%		1-\mathcal{R}^2=\frac{\operatorname{Var}(\boldsymbol{\varepsilon)_3}}{\operatorname{Var}(\boldsymbol{x}_3)}=\frac{\sigma_3^2}{\sigma_3^2+2}
%	\end{equation}
\begin{figure}[h!]
%	\begin{minipage}[l]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn15sigmaY10.png} 
%			\caption{For $n=15$. Dotted: \textsc{Correg}, plain: OLS}\label{MQE1}
%	\end{minipage} \hfill
%	\begin{minipage}[c]{.32\linewidth}
%			\includegraphics[ width=170px]{figures/MQEn100sigmaY10.png} 
%			\caption{For $n=100$. Dotted: \textsc{Correg}, plain: OLS}
%	\end{minipage} \hfill
%   \begin{minipage}[r]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn1000sigmaY10.png} 
%			\caption{For $n=1000$. Dotted: \textsc{Correg}, plain: OLS.} \label{MQE3}
%   \end{minipage} 
	\includegraphics[width=500px]{figures/MQEexplOLSp5col.png}\label{MQEexplOLSp5col}
	\caption{Theoretical MSE on $\hat{\boldsymbol{\beta}}$ of OLS (red) and {\tt CorReg}'s marginal model (blue) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$.}
\end{figure} 
It is clear in Figure \ref{MQEexplOLSp5col} that the marginal model is more robust than \textsc{OLS} on $\boldsymbol{X}$. And when sub-regression get weaker ($\mathcal{R}^2$ tends to 0) it remains stable until extreme values (sub-regression nearly fully explained by the noise). We also see that the error implied by strong correlations shrinks with the rise of $n$. 
We know that $\sigma_Y$ multiplies $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}))=\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{I_f}))+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{I_r}))$ for both models but for the marginal model $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{I_r}))=0$.
 Thus, when $\sigma_Y^2$ rises it increases the advantage of {\tt CorReg} versus \textsc{OLS}. It illustrates the importance of dimension reduction when the main model has a strong noise (very usual case on real datasets where true model is not even exactly linear). 
 
But it is only the theoretical MSE and we want to know what happens in the real life. 
\section{Observed MSE comparison}\label{MSEvraiS}
We look at the empirical MSE on both $\hat{\boldsymbol{\beta}}$ and $\hat{\boldsymbol{Y}}$. Here again, each configuration is computed 100 times and we take the mean to smooth the curves. The MSE on $\hat{\boldsymbol{Y}}$ is computed on a validation sample with 1 000 individuals. Our marginal model remains better for strong correlations.\\

	We also look at the observed MSE on both $\boldsymbol{\beta}$ and $\boldsymbol{Y}$ for some of the methods depicted above.
	\subsection{On the running example}
 \begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/OLSexpl.png}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}$ of OLS (red) and {\tt CorReg}'s marginal model (blue) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}\label{MSEOLSexpl}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/OLSYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEOLSYexpl}
	\end{figure}
	We see in figures \ref{MSEOLSexpl} and \ref{MQEOLSYexpl} that MSE on $\hat{\boldsymbol{Y}}_{OLS}$ give the same results as those on $\hat{\boldsymbol{\beta}}_{OLS}$: the marginal model is better for stronger sub-regressions, smaller samples and weaker main regression. But we notice that when the MSE on $\hat{\boldsymbol{\beta}}_{OLS}$ explodes, the MSE on $\hat{\boldsymbol{Y}}_{OLS}$ does not grow so much. This is a good illustration of the problem generated by the correlations. The model seems to be good in prediction but coefficients are very far from the real value and interpretation can be extremely misleading.\\
	Once again, usage of QR decomposition to invert matrices does improve the results significantly compared to the theoretical MSE. Mathematics offer a wide range of tools and this is a good example of how linear algebra can be used in statistical fields. \\
	
	
 \begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/larexpl.png}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}$ of LASSO with LAR on both $\boldsymbol{X}$ (red) and {\tt CorReg}'s marginal $\boldsymbol{X}^{I_f}$ (blue) for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}\label{MSElarexpl}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/larYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{LASSO}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQElarYexpl}
	\end{figure}
	Figure \ref{MSElarexpl} shows that variable selection done the LASSO gives a biased $\hat{\boldsymbol{\beta}}$ by setting some coefficients to 0 but strong correlations makes this bias neutral for prediction (figure \ref{MQElarYexpl}). Here the LASSO tends to propose the same model as we do  with our marginal model, but without explanation. We will see  later in section \ref{compY} that it is not sufficient in higher dimension. \\
%ridge
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/ridgeexpl.png}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}_{ridge}$ on both $\boldsymbol{X}$ (red) and {\tt CorReg}'s marginal $\boldsymbol{X}^{I_f}$ (blue) for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}\label{MSEridgexpl}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/ridgeYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEridgeYexpl}
	\end{figure}	
Here again (figures \ref{MSEridgexpl} and \ref{MQEridgeYexpl}), ridge regression provides good results for this running example. But we will see later in section \ref{compY} that high dimension reduces the efficiency of the ridge regression when some covariates begin to be irrelevant or not enough relevant because ridge regression is not able to select relevant covariates. \\

Elasticnet and stepwise give results quite similar to the LASSO (figures \ref{MSEelasticnetexpl} to \ref{MQEstepwiseYexpl}).
 %elasticnet
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/elasticnetexpl.png}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}_{elasticnet}$ on both $\boldsymbol{X}$ (red) and {\tt CorReg}'s marginal $\boldsymbol{X}^{I_f}$ (blue) for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}\label{MSEelasticnetexpl}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/elasticnetYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{elasticnet}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEelasticnetYexpl}
	\end{figure}
  %stepwise
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/stepwiseexpl.png}\label{MSEstepwiseexpl}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}_{stepwise}$ on both $\boldsymbol{X}$ (red) and {\tt CorReg}'s marginal $\boldsymbol{X}^{I_f}$ (blue) for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/stepwiseYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{stepwise}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEstepwiseYexpl}
	\end{figure}
	
 	\FloatBarrier
	\subsection{On more complex datasets}\label{thedatasets}
		The {\tt CorReg} package has been tested on simulated datasets. 
 For each simulation,  $p=40$, the $R^2$ of the main regression is $0.4$, variables in $\boldsymbol{X}_f$ follow Gaussian mixture models of $\lambda=5$ classes which means follow Poisson's law of parameter $\lambda=5$ and which standard deviation is $\lambda$. The $\beta_j$ and the coefficients of the $\boldsymbol{\alpha}_j$ are generated according to the same Poisson law but with a random sign. $\forall j \in I_r, p_1^j=2$ (sub-regressions of length 2) and we have $p_r=16$ sub-regressions. The datasets were then scaled so that covariates $\boldsymbol{X}^{I_r}$ don't have a greater variance or mean.
	Results are based on the true $S$ used to generate the dataset.
	When $n<p$, a frequently used method is the Moore-Penrose generalized inverse \cite{katsikis2008fast}, thus OLS can obtain some results even with $n<p$. %(see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
	When using penalized estimators for selection, a last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}) and is applied for both classical and marginal model.
	We compare different methods with and without {\tt CorReg} as a pretreatment. All the results are provided by the {\tt CorReg} package. Here $\boldsymbol{Y}$ depends on all the covariate and the MSE provided were computed on a validation sample of 1 000 individuals each time. \\
	
	Figures \ref{MSEexplOLSY_zone} to \ref{MSEexplOLSbeta_zone} illustrate what happens for OLS. 
Our marginal model is better than OLS for $n<p$ and the complete model (that is the true model) only starts to win for values of $n$ many times larger than $p$ and weak correlations. We still have the phenomenon of MSE on $\hat{\boldsymbol{\beta}}$ more impacted by correlations than the MSE on $\hat{\boldsymbol{\beta}}$, and our main goal is interpretation so our marginal model really is efficient in our context. \\

Figures \ref{MSEexpllarY_zone} to \ref{MSEexpllarbeta_zone} show that even if the LASSO is able to select a subset of covariate and even if we have seen with OLS that taking a subset can give better results, the LASSO does not do so and give more complex models than our marginal model until correlations are extremely strong. We also observe that our marginal model combined with the LASSO has varying complexities so our pretreatment by selection is just a pretreatment and not competitor against the LASSO. Such combination improves the results in a significant way when compared to the LASSO on the complete dataset or OLS on the marginal model. We see that the complexity rises with $n$ but the lasso never keeps all the covariates even with $n=400=10p$ when used on the whole dataset but keep all the covariates in $\boldsymbol{X}^{I_f}$ when used on the marginal model. The main result here is that the LASSO can be improved by pretreatment selection both with $n<p$ and $n>>p$ with strong correlations so this well known variable selection method really suffers from correlations. \\

Ridge regression (Figures \ref{MSEexplridgeY_zone} to \ref{MSEexplridgebeta_zone}) is not improved by the marginal model. Ridge regression is protected against correlations but we see that ridge regression applied on $\boldsymbol{X}^{If}$ (even if it is not the true model) give predictions quite similar to those from ridge regression but with less covariates. Ridge regression will only be damaged when variable selection is needed. \\


Elasticnet and stepwise (Figures \ref{MSEexplelasticnetY_zone} to \ref{MSEexplstepwisebeta_zone})give results mostly equivalent to the LASSO but stepwise seems to be a bit less efficient (higher MSE values). This last point illustrates why we need a specific algorithm to find the structure $S$ and not only variable selection by stepwise like in the method from Maugis \cite{maugis2009variable}.


Further results are provided in sections \ref{sectionsimul} and \ref{sectionrealcase}.
	\FloatBarrier

\newpage
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $S$ known}

%OLS	
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexplOLSY_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSY_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/cplexplOLS_zone.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplOLS_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexplOLSbeta_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSbeta_zone}
	\end{figure}
	\FloatBarrier
\newpage
%LASSO
\subsubsection{LASSO when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $S$ known}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexpllarY_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexpllarY_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/cplexpllar_zone.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexpllar_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexpllarbeta_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexpllarbeta_zone}
	\end{figure}
	\FloatBarrier
\newpage
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $S$ known}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexplelasticnetY_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetY_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/cplexplelasticnet_zone.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplelasticnet_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexplelasticnetbeta_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetbeta_zone}
	\end{figure}
	\FloatBarrier
\newpage
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $S$ known}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexplstepwiseY_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplstepwiseY_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/cplexplstepwise_zone.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplstepwise_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexplstepwisebeta_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplstepwisebeta_zone}
	\end{figure}
	\FloatBarrier
\newpage
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $S$ known}

\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexplridgeY_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplridgeY_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/cplexplridge_zone.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplridge_zone}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explvraiS/MSEexplridgebeta_zone.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplridgebeta_zone}
	\end{figure}
	\FloatBarrier

%	\section{Predictive efficiency}
%		Résultats comme dans l'article	mais sur vrai S	
%		modèles avec beaucoup de variables (car rapide vu que pas de MCMC).
%		Faire avec vrai modèle= modèle complet puis X1 uniquement puis X2 uniquement puis aussi Amax=15 si p largement supérieur à 100
\chapter{Estimation of the Structure of subregression by MCMC}\label{chapterMCMC}
\paragraph{Abstract:} La structure de sous-régression est supposée inconnue. Il nous faut donc la trouver. Un algorithme de type MCMC est proposé pour résoudre cette problématique. La mise en oeuvre de celui-ci passe par une modélisation complète du jeu de données et nous pousse à introduire un nouveau critère de choix de modèle tenant compte du nombre de modèles testés.
\\
\\

\section{Bayesian approach}
Structural equations models are often used in social sciences and economy where a structure is supposed "by hand" but here we want to find it automatically. Graphical LASSO \cite{friedman2008sparse} offers a method to obtain a structure based on the precision matrix (inverse of the variance-covariance matrix), setting some coefficients of the precision matrix to zero (see section \ref{sectionGlasso}). But the resulting matrix is symmetric and we need an oriented structure for $S$ to avoid cycles.

Cross-validation is very time-consuming and thus not friendly with combinatory problematics. Moreover, we need a criterion compatible with structures of different sizes (varying $p_r$) and not related with $\boldsymbol{Y}$ because the structure is inherent to $\boldsymbol{X}$ only. Thus it must be a global criterion. 	
Because it is about model selection, we decide to follow a Bayesian approach (\cite{raftery1995bayesian}, \cite{andrieu1999joint},\cite{chipman2001practical}).  
	
We want to find the most probable structure $S$ knowing the dataset, so we search for the structure that maximizes $P(S|\boldsymbol{X})$ and we have:	
	\begin{eqnarray}
	 \label{approxBIC} P(S|\boldsymbol{X})&\propto & P(\boldsymbol{X}|S)P(S)
	=P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},S)P(\boldsymbol{X}^{I_f}|S)P(S)
	\end{eqnarray}
So we will try to maximize $\psi(\boldsymbol{X},S)=P(\boldsymbol{X}|S)P(S)$. It will be done with a Markov chain Monte-Carlo algorithm (MCMC).
	

\section{Sub-regression structure in details}
	\subsection{Modeling the uncorrelated covariates: a full generative approach on $P(\boldsymbol{X})$} \label{sectionfullgen}
	To be able to compare structures with $P(S|\boldsymbol{X})$, we need a full generative model on $\boldsymbol{X}$. Sub-regressions give $P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},S) $ but $P(\boldsymbol{X}^{I_f}|S)$ is still undefined. We suppose that variables in $\boldsymbol{X}^{I_f}$ follow Gaussian mixtures of $K_j >0$ components: 
	\begin{equation}
			\forall \boldsymbol{X}^j \notin \boldsymbol{X}^{I_r} : \boldsymbol{X}^j_{|S} \sim f(\boldsymbol{\theta}_j)=\mathcal{GM}(\boldsymbol{\pi}_j;\boldsymbol{\mu}_j;\boldsymbol{\sigma}^2_j) \textrm{ with } \boldsymbol{\pi}_j,\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j \textrm{ vectors of size } K_j. \label{mixtureX1}
		\end{equation}
		The great flexibility \cite{mclachlan2004finite} of such models makes our model more robust. Gaussian case is just a special case ($K_j=1$) of Gaussian mixture so it is included in our hypothesis.
		
	%Thus if one have some hypothesis on the distribution of some variables (exponentially distributed for example) it is possible to use it without impacting the model in other ways. %compute corresponding $\psi$ according to it. %and then improve the walk (will keep a structure only if it is really relevant).%and give it as an input of \textsc{CorReg} and then improve the efficiency of the algorithm (it will find a structure only if it is really relevant).
	We now have a full generative model on $\boldsymbol{X}$.
	
\subsection{Identifiability of the structure} \label{preuveident}
	The model presented above relies on a discrete structure $S$ between the covariates. But to find it we need identifiability property to insure that we will asymptotically find the true model. Identifiability of the structure is asked in following terms: Is it possible to find another structure $\tilde{S}$ of linear regression between the covariates leading to the same joint distribution and marginal distributions? 
	
		If there are exact sub-regressions ($\exists j, \sigma^2_j=0$), the structure won't be identifiable. But it only means that several structure will have the same likelihood and they will have the same interpretation. So it's not really a problem. Moreover, when an exact sub-regression is found, we can delete one of the implied variables without any loss of information and the structure will define a list of variable from which to delete.{\tt CorReg} (Our R package) prints a warning to point out exact regressions when found.
	In the followings we suppose $\forall j, \sigma^2_j\neq 0$, then $\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f}$ and $\boldsymbol{X}'\boldsymbol{X}$ are of full rank (but the later is ill-conditioned for small values of $\sigma^2_j$).
	\\
	
Our full generative model is a $p$-sized Gaussian mixture model of $K$ distinct components and 
%	\begin{equation}
%	f(\boldsymbol{X}|K,S)=f(\boldsymbol{X}_f|K,S)f(\boldsymbol{X}_r|\boldsymbol{X}_f,S)
%	\end{equation}	
%	
	can be seen as a $\mathbf{SR}$ model defined by Maugis \cite{maugis2009variable}. In this section, $S$ will denote the set of variable as in the paper from Maugis and we call Gaussian mixtures the Gaussian mixtures with at least two distinct components. The equivalence with Maugis's model is defined by:
	$\boldsymbol{X}^{I_r}=\boldsymbol{y}^{S^c}$ and $\boldsymbol{X}^{I_f}=\boldsymbol{y}^R$. We have supposed independence between variables in $\boldsymbol{X}^{I_f}$ so the identifiability theorem from Maugis tells that our model is identifiable if variables in $\boldsymbol{X}^{I_f}$ are Gaussian mixtures.% (what we supposed in section \ref{sectionfullgen}).
	\\
	
	
%First, we observe that if each variable in $\boldsymbol{X}_r$ is a Gaussian mixture, then there must be at least one Gaussian mixture on the right of each sub-regression. 
We define $\boldsymbol{X}^G \subsetneq \boldsymbol{X}^{I_f}$ containing Gaussian variables and we note the Gaussian mixtures $\boldsymbol{X}^{G^c}\neq \emptyset$ its complement in $\boldsymbol{X}^{I_f}$.
We suppose that variables in $\boldsymbol{X}^{I_r}$ are all Gaussian mixtures. It implies that $\forall j  \in I_r,\exists i \in I_f^j $ so that $\boldsymbol{X}^i \subset \boldsymbol{X}^{G^c} $ since any linear combination of Gaussian variable would only give a Gaussian (so each sub-regression contain at least one Gaussian mixture as a regressor).
\\
	We introduce the matricial notation
		$\boldsymbol{X}^{I_r}=\boldsymbol{X}^{I_f}\boldsymbol{\alpha} + \boldsymbol{\varepsilon}$ where
		 $\boldsymbol{\alpha}$ is the $(p-p_r)\times p_r$ matrix whose columns are the $\boldsymbol{\alpha}_j$ and $\boldsymbol{\varepsilon}$ is the $n\times p_r$ matrix whose columns are the $\boldsymbol{\varepsilon}_j$
		%\\We note $\Theta$ the parameter of the model.We want to know if $P(\boldsymbol{X}|S,\Theta)=P(\boldsymbol{X}|\tilde{S},\tilde{\Theta})$ does imply $(S,\Theta)=(\tilde{S},\tilde{\Theta})$.
		
The theorem from Maugis guarantee that a sub-regression between Gaussian mixtures is identifiable in terms of which one is regressed by others.
		\begin{eqnarray}
%			\forall j \in I_r, \boldsymbol{X}^j_{|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_j^G+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c}+ \boldsymbol{\varepsilon}_j \\
%			\boldsymbol{X}^j_{|\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c} + \tilde{\boldsymbol{\varepsilon}}_j \textrm{ is identifiable where} \\
%			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_j^{G}+ \boldsymbol{\varepsilon}_j \textrm{ is Gaussian. 
		 \boldsymbol{X}_{r|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_G+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_{G^c}+ \boldsymbol{\varepsilon} \\
			\boldsymbol{X}_{r|\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_{G^c} + \tilde{\boldsymbol{\varepsilon}} \textrm{ is identifiable where} \\
			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_j^{G}+ \boldsymbol{\varepsilon}_j \textrm{ is Gaussian.}  
		\end{eqnarray}
		So a sufficient condition for identifiability is to have at least one Gaussian mixture in each sub-regression.	
		It implies then that: $\forall j \in I_r, \boldsymbol{X}^{j} $ is a Gaussian mixture and $\exists i \in I_f^j,\boldsymbol{X}^{i} \subset \boldsymbol{X}^G $.  \\
		
	%Tables \ref{testidentifiableG}	and \ref{testidentifiableGM} illustrates this property.
	
\paragraph{Remark:} Identifiability of $S$ is not necessary to use a given structure but helps to find it.
In the followings, true $S$ is supposed to be identifiable (at least one Gaussian mixture in each sub-regression).
		
	\subsection{Impact of the structure itself}
		
	
	Correlations can lead to serious problems in industrial context. Let's imagine a dataset with two highly correlated covariates. Estimators may give them the same coefficient (grouping effect  when using elasticnet) or only keep one of them (like the LASSO does). \\
	
	If only one is kept without explicitly pointing the correlations we will think that the deleted covariate is irrelevant even if it is not and even if it could be more relevant than the kept covariate in terms of possible actions or physical meanings.\\
	
	 On the other hand, keeping both without pointing correlations will lead to modify one of the covariates to impact the value of the response as we want and the other covariate will automatically move so we won't obtain the expected result.\\
	 
	  Choosing whereas we have to use or not the marginal model as a pretreatment is independent of the utility of the structure. Knowing explicitly the complex correlations that hold the dataset is a real stake when times come to interpret the model and to  decide actions. This is a strength of our method. We don't have to make a choice between grouping effect or variable selection because our explicit structure describes in details the complexity of the situation so we can then act knowing what we do, without having to blindly follow one of the two heuristics.

\section{Sub-regression model selection}	

			\subsection{Bayesian criterion for quality}
			
			
		Our full generative generative model allows us to compare structures with criterions like the Bayesian Information Criterion ($BIC$) which penalize the log-likelihood of the joint law on $\boldsymbol{X}$ according to the complexity of the structure~\cite{BIChuard}. \\
		
		
			 We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context.
		In the followings we use the $BIC$, that is more classical.
		
		
		\subsection{Penalization of the integrated likelihood by $P(S)$} \label{compstruct}
		
		
When considering (\ref{approxBIC}) we see that uniform law on $P(S)$ gives $\psi(\boldsymbol{X},S)\propto P(\boldsymbol{X}|S)$ so it is equivalent to a minimization of the $BIC$.
	We note $\boldsymbol{\Theta}$ the set of the parameters of the generative model.
	\begin{eqnarray}
		-2\log P(\boldsymbol{X}|S)&\approx & BIC=-2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+|\boldsymbol{\Theta}|\log(n)  
	\end{eqnarray}
	
	But $BIC$ tends to give too complex structures because we test a great range of models and the number of model compared is not taken into account\cite{massart2007concentration}. Thus we choose to penalise the complexity a bit more. We don't want to modify the $BIC$ to keep its properties.
	
We have the explicit structure characterized by $S=\{I_f,I_r,p_f,p_r\}$, then we suppose a hierarchical uniform {\it a priori} distribution $P(S)=P(I_f | \boldsymbol{p}_f,I_r,p_r)P(\boldsymbol{p}_f|I_r,p_r)P(I_r|p_r)P(p_r)$  instead of the simple uniform law on $S$ that is generally used and provides no penalty.
It goes against the fact that the number of models with $p_r$ sub-regressions and the number of possible combination for each sub-regression depends on $p_r$ and thus provides distinct penalties according to the complexity. 
	 Thus we have :
		\begin{eqnarray}
		BIC_+(X|S)&=&BIC(X|S) -\ln(P(S)) \label{Bicstar}
	\end{eqnarray}		
	The hierarchical uniform hypothesis gives:
	\begin{eqnarray}
	P(S)&=&P(I_f | \boldsymbol{p}_f,I_r,p_r)P(\boldsymbol{p}_f|I_r,p_r)P(I_r|p_r)P(p_r) \textrm{ with} \\
	P(p_r)&=&\frac{1}{p} \\
	P(I_r|p_r)&=& \frac{1}{ { p \choose p_r} }\\
	P(\boldsymbol{p}_f|I_r,p_r)&=& \frac{1}{p_r \times \frac{1}{p-p_r}}=\frac{p-p_r}{p_r}\\
	P(I_f | \boldsymbol{p}_f,I_r,p_r)&=&\frac{1}{\prod_{j \in I_r}{ p-p_r \choose p_f^j }}  
	\end{eqnarray}
	instead of $P(S)=\frac{1}{|\mathcal{S}_p|}$ as defined in section \ref{sectiongraph} for the classical uniform hypothesis.
	It increases penalty on complexity for $p_r\leq\frac{p}{2}$ and $p_f^j\leq\frac{p}{2}$ because probability of a complex model is under-estimated. Hence %when using $BIC*$ 
	this constraint on $\hat{p}_r$ and $\hat{p}_f^j$ is given in the research algorithm when the Hierarchical Uniform hypothesis is made instead of Uniform one in numerical experiments (section \ref{sectionsimul} and \ref{sectionrealcase}).
		$BIC_+$ does not change $BIC$ but only $P(S)$ so the properties of $BIC_+$ are the same as classical $BIC$ but we obtain better results when the constraints on the complexity are verified.  %With the Hierarchical Uniform hypothesis we maximize $\psi(\boldsymbol{X},S)\approx BIC + P(S)$.


%	 
%	
		\subsection{Some indicators for proximity}
		The first criterion is $\psi(\boldsymbol{X},S)$ which is maximized in the MCMC. But in our case, it is estimated by the likelihood (see (\ref{approxBIC}))whose value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $S$ we define some indicators to compare the true model $S$ and the found one $\hat{S}$.
			Global indicators :
			\begin{itemize}
				\item $TL$ (True left) : the number of found dependent variables that really are dependent\\ $TL=|I_r\cap \hat{I}_r|$ 
				\item $WL$ (Wrong left) : the number of found dependent variables that are not dependent \\ $WL=|\hat{I}_r|-TL$
				\item $ML$ (Missing left) : the number of really dependent variables not found\\ $ML=|I_r|-TL$
				\item $\Delta p_r$ : the gap between the number of sub-regression in both model: \\ $\Delta p_r=|I_r|-|\hat{I}_r|$. The sign defines if $\hat{S}$ is too complex or too simple compared to the true model
				\item $\Delta compl$ : the difference in complexity between both model: \\$\Delta compl=\sum_{j \in p_r}p_f^j-\sum_{j \in \hat{p}_r}\hat{p}_f^j$
			\end{itemize}
			
			
			
	\section{Neighbourhood}
	We note $\mathcal{S}_p$ the ensemble of feasible structures of size $p$ (those uncrossed, {\it i.e.} with $I_f\cap I_r=\emptyset$).
	\\
	For each step $q$, starting from $S \in \mathcal{S}_p$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}_{S}&=& \{S \}\cup \{ S^{(i,j)} |(i,j) \in \mathcal{A}_q\}\cap{\mathcal{S}_p} 
	\end{eqnarray}	
	where $\mathcal{A}_q$ is a set of arcs to modify (add or remove) in the associated graph, defined according to a strategy.
	And we have for  given $S$ and $(i,j)$:
	\begin{itemize}
		\item if $i \in I_f^j$ then we remove $i$ from $I_f^j$ (arc removal)
		\item else we had $i$ in $I_f^j$
	\end{itemize} 
	Then coherence between $I_f$ and others parts of $S$ is done by $\forall 1\leq j\leq p :  p_f^j=|I_f^j|$, $I_r=\{j |p_f^j>0 \}$ and $p_r= |I_r|$.
	
In the adjacency matrix we just do: $G_{i,j}=1-G_{i,j}$. 
The main advantage of such a neighbourhood is that increasing and decreasing complexities are tested at each step without arbitrary ratio. If we just look at the sub-regression system, we have to choose for each sub-regression if we add, remove or keep covariates and we also have to choose if we had or delete some sub-regression. Adjacency matrix makes the neighbourhood extremely natural with just the modification of a value in a binary matrix.

		\subsection{Strategy}
			Many strategies can be imagined. First, we can decide to keep the local $\hat{S}$ in the neighbourhood or not, that is allowing or not stationarity. Here the MCMC is not used for sampling or density estimation. We just want to find the structure with the best value of $\psi(\boldsymbol{X},S)$ so it is not an evidence to allow or not stationarity. Our package {\tt CorReg} give the user the choice with stationarity,  included in the neighbourhood by default.
			

		The only constraint on $\mathcal{A}_q$ is that $\forall (i,j) \in \mathcal{A}_q, i\neq j$			
			
		We propose, for step $q$ to draw $j$ from $\mathcal{U}(\{1,\dots,p\})$ and then 
		\begin{equation}
			\mathcal{A}_{q|j}=\{ (i,j)|i \neq j \}
		\end{equation}
			Such a strategy can be interpreted as the uniform choice of a sub-regression to modify followed by the proposal of each possible unary change.
			Our package {\tt CorReg} let the user choose many other strategies like a fixed number of random couples $(i,j)$, or the union of the $j^{th}$ line and column of $G$.
		\subsection{Active relaxation of the constraints}
		We have defined the neighbourhood with an intersection with $\mathcal{S}_p$. 
		In practice, for some of the $(i,j) \in \mathcal{A}_q$, we have $S^{(i,j)}\notin \mathcal{S}_p$. Such candidates are basically rejected so the number of candidates is not constant at each step. Moreover, complex structures reduce the size of the potential neighbourhood because of the uncrossing rule. 
		Thus we propose a relaxation method by a new definition of $S^{(i,j)}$:
	\begin{itemize}
		\item if $i \notin I_f^j$ (add): 
			\begin{itemize}
				\item $I_f^j=I_f^j\cup \{i\}$
				\item $I_f^i=\emptyset$ (explicative variables can't depend on others : column-wise relaxation)
				\item $I_f=I_f \setminus \{j\}$ (dependent variables can't explain others : row-wise relaxation) 
			\end{itemize}			 
		\item else (remove): $I_f^j=I_f^j\setminus \{i\}$
	\end{itemize}
	It can be seen as forcing the modification by deleting what would make the structure not feasible. So in one step we can test a model that remove completely a sub-regression, remove the explicative role of a covariate in all sub-regression and create a new pairwise sub-regression. It drastically increases the scope of the neighbourhood and guarantee to always have the same number of candidates during the MCMC. It can be compared to simulated annealing that sometimes proposes exotic candidates to avoid local extrema, but here without any temperature to set. Here again, the neighbourhood remains natural, without arbitrary parameters to tune. 
	Another advantage of the relaxation method is that it reduces complexity very quickly without having to deconstruct a sub-regression (Figure \ref{comparecomplrelax}), so it helps to have simpler models in a small amount of time (asymptotical results are the same because the chain is regular thus ergodic).
			
	\section{The walk}
		Once we have a neighbourhood, we have to choose a candidate for the next step.
The walk follows a time-homogeneous Markov Chain whose transition matrix $\mathcal{P}$ has $|\mathcal{S}_p|$ rows and columns (combinatory so we just compute the probabilities when we need them).
	At each step the Markov chain moves with probability:
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=&\sum_{j=1}^p \mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S,j}\} }\frac{\exp(-\frac{1}{2}\psi(\boldsymbol{X},\tilde{S}))}{\sum_{S_l\in \mathcal{V}_{S,j}}\exp(-\frac{1}{2}\psi(\boldsymbol{X},S_l))} 
	\end{eqnarray}
	And $\mathcal{S}_p$ is a finite state space.%la relaxation rend P non symétrique mais ne remets  pas en cause l'homogénéité	
	 
Because the walk follows a regular and thus ergodic markov chain with a finite state space, it has exactly one stationary distribution \cite{grinstead1997introduction} %: $\pi$ and every rows of $\operatorname{lim}_{k\rightarrow \infty}\mathcal{P}^k=W$ equals $\pi$.
%	
%	
%	With $\forall S \in  \mathcal{S}$ :
%	\begin{eqnarray}		
%		0 \leq &\pi (S)& \leq 1 \nonumber \\
%		\sum_{S \in \mathcal{S}}\pi(S) &=&1 \nonumber \\
%		\pi (S) &=&\sum_{\tilde{S}\in \mathcal{S}} \pi(\tilde{S})\mathcal{P}(\tilde{S},S) \\%définition de la lois stationnaire
%	\end{eqnarray}
%		
and the output will be the best structure in terms of $P(S|\boldsymbol{X})$ which weights each candidate. Practically speaking, {\tt CorReg} returns the best structure seen during the walk (even if the corresponding candidate has never been chosen). The package also give the local structure when the walk stops so user can relaunch the algorithm from the same point if he wants to go further.
The main criterion to stop the walk is a maximum number of iteration but {\tt CorReg} can also stop the walk after a given number of step on the best model found.
Numerical results (Section 4) illustrates the efficiency of the walk when the true model contains structures with various strength (section \ref{compZ}) and an example with a non-linear structure (Figure \ref{resnonlin}).
		
		
		
	\section{Initialization}
		\subsection{Correlation-based initialization}
		 If we have some knowledge about some sub-regressions (physical models for example) we can add them in the found and/or initial structure. So the model is really expert-friendly.
The initial structure can be based on a first warming algorithm taking the correlations into account. Coefficients are randomly placed into $I_f$, according to a Bernoulli draw weighted by the absolute value of the correlations and with respect to the uncrossing constraint. Uncrossing constraint will not allow some strong correlation to be taken into account according to the order of the Bernoulli drawing so we can draw with a random order or by ordering by descending correlations. 
		
		We note that the $BIC$ associated to initial model is often worse than the $BIC$ of the void structure, so we compare several chains in Figures \ref{pourcini} and \ref{Bicini}:			
			
	\begin{center}
		\begin{figure}[h!]
		\centering
		\includegraphics[width=350px]{figures/pourc_meilleur_marelaxini.png} 
		\caption{Amount of time each method is better for the 100 first steps of the MCMC. } \label{pourcini}
		\end{figure}		
	\end{center}
	
	\begin{center}
		\begin{figure}[h!]
		\centering
		\includegraphics[width=350px]{figures/typeinit.png} %mars 2013 analyse plan ini marelax
		\caption{Evolution of the $BIC$ (criterion to minimize in the MCMC) for each method.}\label{Bicini}
		\end{figure}
	\end{center}
	We see that correlation-based initialization quickly beat the void structure. This can be explained by local extrema.
	\FloatBarrier
		\subsection{Multiple intialization}	
		Local extrema are a known issue for most of optimization methods, and one would rather test multiple short chains than lose time in initialisation or long chains \cite{gilks1996markov}. 
		We also compare the results obtained with several number of chains. Figure \ref{nbini} shows the evolution of the $BIC$ of the best chain with a number of chains varying from 1 to 10, so the model with 10 chains contain the others and is almost as good as they are. We see that multiple initialization is efficient but the gain seems to be logarithmic in the number of tries so it is recommended to use multiple chains but not too much (time consuming). Important remark: multiple chains can be computed in parallel so it is not really time consuming.		
			
	\begin{center}
	\begin{figure}[h!]
	\centering
		\includegraphics[width=400px]{figures/courbes_BICmoyen_marelax_nbinibis.png} 
		\caption{Comparison of distinct number of correlation-based initialisations for the MCMC.}\label{nbini}
	\end{figure}
	\end{center}
		In the followings, the chain was launched with twenty initialisations each time, based on the correlation matrix.

	\section{Pruning}
		If the complexity of $S$ is too high, pruning methods can be used. We note that, for each of the following pruning methods, the final complexity may stay the same (for example if the MCMC had time to find a good model).
		\subsubsection{Variable selection}
			We can use variable selection methods like the LASSO on $\boldsymbol{X}^{I_f^j}$ to estimate the coefficients $\boldsymbol{\alpha}_j$ and obtain some supplementary zeros. Working on $\boldsymbol{X}^{I_f^j}$ protects the LASSO against dimension and correlations issues.
		\subsubsection{$R^2$ thresholding }
			We can also define a minimal value for the $R^2$ of the sub-regression to maintain them in the final structure. But this minimal value would be totally arbitrary and we know that it is frequent to use linear regression with real datasets that only show a $R^2$ between $0.1$ and $0.2$. It is particularly true in social sciences.
		\subsubsection{Test of hypothesis}
			Another pruning method would be to delete sub-regression that offer a F-statistic under a minimal value.
		\subsubsection{Additional cleaning steps}
			Because the walk is not exhaustive, it does make sense to let the walk continue a few steps with neighbourhood containing only suppressions in the structure. Every sub-graph of a bipartite graph is bipartite thus every sub-graph can be reached. It is just an heuristic change in the strategy with:
			\begin{equation}
				\mathcal{A}_q=\{(i,j)| i \in I_f^j \}
			\end{equation}
			It is not based on any arbitrary parameter and change the result only if it founds a better structure in terms of the criterion $\psi$ used in the walk.
			For these reasons, it is our recommended pruning method. The package {\tt CorReg} allows to use this method automatically after the MCMC with the parameter {\tt clean=TRUE }.
	\section{The Graphical LASSO}\label{sectionGlasso}
		Graphical LASSO \cite{friedman2008sparse} \cite{witten2011new} \cite{tibshiranilasso} \cite{friedman2010applications} is set to give undirectionnal (thus symmetric) graphs by selection in the precision matrix (the inverse of the variance-covariance matrix). It does make sense for exponential family because in these cases, zeros in the precision matrix $\Sigma^{-1}$ can be interpreted in terms of conditional independence between covariates \cite{dempster1972covariance}. But we have supposed Gaussian mixture on $\boldsymbol{X}$ and we search an oriented graph.
	However, we can still use it for initialization, for example by a Hadamard product with $G_0$ the adjacency matrix of the initial structure. We can also try to give the graph a bipartite orientation. We first have to obtain a bipartite graph, that mean to have no even cycles. A particular case would be the minimum spanning tree \cite{graham1985history,moret1991empirical,gower1969minimum} because trees have no cycles. But it is time consuming and has no theoretical properties relied to our problematic of minimizing $\psi$, so the idea was left behind after some tries.		
	
\section{CorReg}	
	The {\tt CorReg} package is now on CRAN and provides many parameters for the walk. If wanted it can return some curves associated to the walk to have an idea of what happens with distinct strategies. 		
	
We define the complexity of a structure $S$ as the number of elements in the adjacency matrix, that is the number of links between covariates and is obtained by:
\begin{equation}
	\textrm{Complexity}(S)=\sum_{j \in I_r}p_f^j
\end{equation}	
	
		We compare some walks with each time the same dataset and the same seed for the random generator. We have $p=100$ and $n=50$.
		
		For Figures \ref{comparecomplrelax} and \ref{compareBICrelax} we start from an arbitrary structure with a complexity of $62$. We see that relaxation helps to delete these false sub-regressions and avoid to be stuck in it, improving the $BIC$ much faster. We also observe that final complexities are comparable. Here the MCMC was launched only once (with the totally arbitrary initial structure based on nothing), the true structure had a complexity of $120$.
\begin{center}
	\begin{figure}[h!]
	\centering
\includegraphics[width=400px]{figures/complexitycompareMCMC.png} 		
\caption{Comparison of complexity evolution with or without constraint relaxation.}\label{comparecomplrelax}
	\end{figure}
	\end{center}
			
\begin{center}
	\begin{figure}[h!]
	\centering
\includegraphics[width=400px]{figures/BICcompareMCMCrelax.png} 		
\caption{Comparison of BIC evolution with or without constraint relaxation.}\label{compareBICrelax}
	\end{figure}
	\end{center}
			
		
	\chapter{Numerical results on simulated datasets} \label{sectionsimul}


	\section{The datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if {\tt CorReg} 	keeps its promises.
	The {\tt CorReg} package has been tested on the simulated datasets from section \ref{thedatasets}.
Section \ref{compZ} shows the results obtained in terms of $\hat{S}$. Section \ref{compY} shows the results obtained using only {\tt CorReg}, or {\tt CorReg} combined with other methods. The graph in section \ref{compY} give both mean, first and third quartiles of the chosen indicator. The MSE on $\hat{\boldsymbol{\beta}}$ and $\hat{\boldsymbol{Y}}$ were computed on a validation sample of $1 000$ individuals. Several pattern for $\boldsymbol{Y}$ were tested to evaluate the impact of irrelevant covariates.\\

	We used \textsc{Rmixmod} to estimate the densities of each covariate. For each configuration, the MCMC walk was launched on $10$ initial structures with a maximum of 1 000 steps each time.
	When $n<p$, a frequently used method is the Moore-Penrose generalized inverse \cite{katsikis2008fast}, thus OLS can obtain some results even with $n<p$. %(see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
	We compare different methods with and without {\tt CorReg} as a pretreatment. All the results are provided by the {\tt CorReg} package.
	
		\section{Results on $\hat S$}	\label{compZ}

Figure \ref{compZBIC} illustrates the impact of large samples. For $n>>p$ the MCMC founds most of the truly redundant covariates and only few wrong redundant covariates. We also observe that strong correlations ($R^2\geq 0.7$) get more wrong sub-regressions for a same total number of sub-regressions. It comes from induced correlations. If two covariates are explained by the same others, they may have a strong induced pairwise correlation and if the walk tries to combine them in a single sub-regression we can have a local extremum. The walk is ergodic but in a finite number of steps it can keep such a wrong sub-regression, that's why we launch the walk several times with distinct initial structures. Such a wrong sub-regressions is not totally wrong in that it describes real correlations. So interpretation is not compromise and neither is the predictive efficiency as shown in section \ref{compY}. \\
For smaller values of $n$ we observe that the number of true sub-regressions found increases with their strength (growing $R^2$).\\
When comparing $BIC$ to $BIC_+$ it becomes evident that $BIC_+$ is less confident to keep sub-regressions (it is what it was made for). Weak sub-regressions are kept only if the sample is large enough to be confident and when the $R^2$ rises, the number of kept true sub-regressions grows quickly whereas wrong sub-regressions remain exceptional. Induced pairwise correlations give weaker sub-regressions so the walk is less attracted by them. We can then conclude that $BIC_+$ does achieve its main purpose that was to reduce the complexity of the structure by keeping only strong sub-regressions. In these simulated datasets, the $R^2$ were equal for each sub-regression. We can see several reasons to explain why the sub-regression are not all kept or all missing. 
\begin{itemize}
	\item The walk has only walked a finite number of steps so only a subset of all the feasible structures has been tested.
	\item Some true sub-regressions are polluted by over-fitting and the non-crossing rule can then make other true sub-regressions not compatible (the walk has to clean the previous sub-regression first).
	\item $\psi$ relies on the likelihood and if marginal laws are well-estimated by Mixmod, the gap between the marginal and dependent likelihood might be small and thus the walk can be slowed whereas we use a finite number of steps.
\end{itemize}

\begin{figure}[h!]
	\subfigure[With classical $BIC$ criterion]{
			\includegraphics[height=180px,width=245px]{figures/explvraiS/BIC.png} 
	} \quad
   	\subfigure[With our $BIC_+$ criterion]{
			\includegraphics[height=180px,width=240px]{figures/explvraiS/BICplus.png} 
	}
	\caption{Quality of the subregressions found by the MCMC. True left (plain blue) and Wrong left (dotted red) for n varying in $(30,50,100,400)$, the thicker the greater n.}\label{compZBIC}
\end{figure}




\clearpage
\section{Results on prediction}\label{compY}
\subsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	 \label{explsimtout}	
We try the method with a response variable depending on all covariates to compare the results with those from section \ref{thedatasets} (same $\boldsymbol{X}$ and $\boldsymbol{Y}$). ({\tt CorReg} reduces the dimension and can't give the true model if there is a structure). %The datasets used here were those from table \ref{compZvrai}. \\

 We see that {\tt CorReg} tends to give more parsimonious models and better predictions, even if the true model is not parsimonious. We logically observe that when $n$ rises, all the models get better and the correlations cease to be a problem so the complete model starts to be better ({\tt CorReg} does not allow the true model to be chosen). The main result here is that results based on $\hat{S}$ are still good so the MCMC is efficient enough to be useful for the study of the response variable $\boldsymbol{Y}$. \\
 
 Results for OLS (Figures \ref{MSEexplOLSY_zonetout} to \ref{MSEexplOLSbeta_zonetout} ) are similar to those from section \ref{thedatasets} excepted for small correlations because the MCMC using $BIC_+$ does not find the true structure for small correlations and a void structure gives a marginal model equal to the complete one.  \\
 This phenomenon is not observed with variable selection method (Figures \ref{MSEexpllarY_zonetout} to \ref{MSEexplstepwisebeta_zonetout}) where covariates not deleted by the structure are deleted by the variable selection. \\
 Ridge regression results (Figures \ref{MSEexplridgeY_zonetout} to \ref{MSEexplridgebeta_zonetout} ) are also very similar to the previous (Figures \ref{MSEexplridgeY_zone} to \ref{MSEexplridgebeta_zone}).\\
 
 Having simpler structure is important for interpretation so we keep this choice, but user can use classical $BIC$ with single boolean parameter change.

 
\FloatBarrier

\newpage
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

%OLS	
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplOLSY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/cplexplOLS_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplOLS_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplOLSbeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSbeta_zonetout}
	\end{figure}
	\FloatBarrier
\newpage
%LASSO
\subsubsection{LASSO when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexpllarY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexpllarY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/cplexpllar_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexpllar_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexpllarbeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexpllarbeta_zonetout}
	\end{figure}
	\FloatBarrier
\newpage
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplelasticnetY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/cplexplelasticnet_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplelasticnet_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplelasticnetbeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetbeta_zonetout}
	\end{figure}
	\FloatBarrier
\newpage
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplstepwiseY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplstepwiseY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/cplexplstepwise_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplstepwise_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplstepwisebeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplstepwisebeta_zonetout}
	\end{figure}
	\FloatBarrier
\newpage
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplridgeY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplridgeY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/cplexplridge_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplridge_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplridgebeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplridgebeta_zonetout}
	\end{figure}
	\FloatBarrier
 
 
 
 
% 
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_tout_MSE_NB.png} 
%			\caption{Comparison of the MSE between OLS and CorReg+OLS}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_tout_compl_NB.png} 
%			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
%   \end{minipage}
%\end{figure}
% 
%%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/lar_tout_MSE_NB.png} 
%			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/lar_tout_compl_NB.png} 
%			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_tout_MSE_NB.png} 
%			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_tout_compl_NB.png} 
%			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_tout_MSE_NB.png} 
%			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_tout_compl_NB.png} 
%			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
%   \end{minipage}
%\end{figure}





\clearpage
\newpage
\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$ (best case for us)}	 \label{tableMSEsimdroite}
\FloatBarrier
 We want to see what happens in real life, when some covariates are irrelevant to describe $\boldsymbol{Y}$ according to the given dataset. We could generate $\boldsymbol{Y}$ with a random subset of $\boldsymbol{X}$ but in such a case, it would be impossible to say whether results come from sparsity or from the ratio of covariates in the subset that are in $\boldsymbol{X}^{I_r}$. Moreover, we will study real datasets in the next chapter so the only pattern to test here are those with some irrelevant covariates and relevant covariates only in one part of the partition on $\boldsymbol{X}$. \\
 	We start with $\boldsymbol{Y}$ depending only on covariates in $\boldsymbol{X^{I_f}}$. It is the best case for us because our marginal model is then the true model and the complete model will need variable selection to reach the truth. Here $\boldsymbol{Y}$ depends on the 24 covariates in $\boldsymbol{X}^{I_f}$ with an intercept.\\
 	Smaller dimension makes the coefficients easier to learn and we observe that MSE are smaller for both model with any method compared to those from section \ref{explsimtout}.\\
 	For OLS (Figures \ref{MSEexplOLSY_zoneX1} to \ref{MSEexplOLSbeta_zoneX1}) we note the global improvement of the MSE but also a specific improvement for large values of $n$ where our marginal model resists to the complete model. It is logical because the complete model tends to reduce the coefficients associated to irrelevant covariates whereas our marginal delete them. \\
 	
 	When looking at variable selection methods we also have this improvement so it confirm the already observed fact that variable selection method are theoretically able to find the true model but efficiency is not really great when confronted to correlated covariates. There is no surprise here after the results for $\boldsymbol{Y}$ depending on the whole dataset $\boldsymbol{X}$. \\
 	
 	Ridge regression (figures \ref{MSEexplOLSY_zoneX1} to \ref{MSEexplOLSbeta_zoneX1}) is finally  improved here by our pretreatment by selection, like if we had added variable selection feature to the ridge regression. It is the method that provides the best results, but only because $\boldsymbol{Y}$ depends on all covariates in $\boldsymbol{X}^{I_f}$. Our pretreatment is limited in terms of variable selection.
 	
 	
 

\newpage
%OLS	
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexplOLSY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/cplexplOLS_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplOLS_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexplOLSbeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSbeta_zoneX1}
	\end{figure}
	\FloatBarrier
\newpage
%LASSO
\subsubsection{LASSO when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexpllarY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexpllarY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/cplexpllar_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexpllar_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexpllarbeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexpllarbeta_zoneX1}
	\end{figure}
	\FloatBarrier
\newpage
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexplelasticnetY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/cplexplelasticnet_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplelasticnet_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexplelasticnetbeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetbeta_zoneX1}
	\end{figure}
	\FloatBarrier
\newpage
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexplstepwiseY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplstepwiseY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/cplexplstepwise_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplstepwise_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexplstepwisebeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplstepwisebeta_zoneX1}
	\end{figure}
	\FloatBarrier
\newpage
%ridge
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}
	
\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexplridgeY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplridgeY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/cplexplridge_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplridge_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X1/MSEexplridgebeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplridgebeta_zoneX1}
	\end{figure}
	\FloatBarrier

%\begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X1_MSE_NB.png} 
%			\caption{Comparison of the MSE between OLS and CorReg+OLS}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X1_compl_NB.png} 
%			\caption{Comparison of the complexities between OLS and CorReg+OLS} 
%   \end{minipage}
%\end{figure}
% 
%%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X1_MSE_NB.png} 
%			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X1_compl_NB.png} 
%			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X1_MSE_NB.png} 
%			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X1_compl_NB.png} 
%			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X1_MSE_NB.png} 
%			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X1_compl_NB.png} 
%			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
%   \end{minipage}
%\end{figure}

\clearpage
	
	\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $\boldsymbol{X}^{I_r}$. The datasets used here were still the same.
Depending only on $\boldsymbol{X}_r$ implies sparsity and impossibility to obtain the true model when using the true structure.  We get unbiased models but with an increase in the variance as described in equation \ref{Trueexpl}. \\
	In such a case, usage of $BIC_+$ instead of $BIC$ is more obvious because each additional sub-regression deletes a covariate in our marginal model and reduces the probability to find the true model. \\
	
	We first look at OLS (Figures \ref{MSEexplOLSY_zoneX2} to \ref{MSEexplOLSbeta_zoneX2}) and see that we still obtain better results for small values of $n$ or strong correlations. In real studies we will never know the true model but we can be confident that if correlations are strong or if sample is small, using our marginal model can helps whatever the true model is. This is a really powerful result. Improvement for small correlations but $n<p$ comes from dimension reduction. When you don't have enough individual it becomes better to use a small model that does not contain the true one but only covariates correlated to the relevant one instead of trying to work with all the covariates. Let's remember that OLS confronted to $n<p$ only delete covariates to have $n=p$ (or $p+1$ when there is an intercept). QR decomposition leads to delete the last  covariates in the dataset but in our simulations, covariates in $\boldsymbol{X}^{I_r}$ are placed randomly in the dataset so deletion by QR can be seen as random deletion. The gain implied by dimension reduction remains for $n>p$ if correlations are high enough because the matrix to invert is ill-conditioned and OLS needs a lot of individuals to reduce the variance of the estimator. Correlations really put OLS in trouble and our marginal model seems to be a good solution. \\
	
	Variable selection methods still are impacted by correlations but not enough to be improved by our marginal model. Neither is the ridge regression.\\
	 Real datasets will provide $\boldsymbol{Y}$ depending on a mix of covariates from both $\boldsymbol{X}^{I_f}$ and $\boldsymbol{X}^{I_r}$ so our marginal could help. We also have to recall that the structure $S$ is useful by itself to have a better comprehension of the dataset and help the final client to be confident in statistical tools because he sees small models that are known to be true and were found automatically by the method. {\tt CorReg } also has a psychological impact on a study that should not be overlooked. Once $\hat{S}$ is found, trying the marginal model has no cost and should be tested.


\FloatBarrier

\newpage
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

%OLS	
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexplOLSY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/cplexplOLS_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplOLS_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexplOLSbeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSbeta_zoneX2}
	\end{figure}
	\FloatBarrier
\newpage
%LASSO
\subsubsection{LASSO when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexpllarY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexpllarY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/cplexpllar_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexpllar_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexpllarbeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexpllarbeta_zoneX2}
	\end{figure}
	\FloatBarrier
\newpage
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexplelasticnetY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/cplexplelasticnet_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplelasticnet_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexplelasticnetbeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetbeta_zoneX2}
	\end{figure}
	\FloatBarrier
\newpage
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexplstepwiseY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplstepwiseY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/cplexplstepwise_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplstepwise_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexplstepwisebeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplstepwisebeta_zoneX2}
	\end{figure}
	\FloatBarrier
\newpage
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexplridgeY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplridgeY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/cplexplridge_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplridge_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/explhatS/X2/MSEexplridgebeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplridgebeta_zoneX2}
	\end{figure}
	\FloatBarrier


% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X2_MSE_NB.png} 
%			\caption{Comparison of the MSE between OLS and CorReg+OLS}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X2_compl_NB.png} 
%			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
%   \end{minipage}
%\end{figure}
%\textsc{CorReg} is still better than OLS for strong correlations and limited values of $n$. 
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X2_MSE_NB.png} 
%			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X2_compl_NB.png} 
%			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X2_MSE_NB.png} 
%			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X2_compl_NB.png} 
%			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X2_MSE_NB.png} 
%			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X2_compl_NB.png} 
%			\caption{Comparison of the compexities between stepwise and CorReg+stepwise}
%   \end{minipage}
%\end{figure}
\FloatBarrier
\subsection{Robustness with non-linear case}

We have generated a non-linear structure to test the robustess of the model. $\boldsymbol{X}^{I_f}$ is a set of 6 independent Gaussian mixtures defined as previously but with random signs for the components means. $\boldsymbol{X}^{I_r}=\boldsymbol{X}_7=a\boldsymbol{X}_1^2+\boldsymbol{X}_2+\boldsymbol{X}_3+ \varepsilon$. The matrix $\boldsymbol{X}$ is then scaled before doing $$\boldsymbol{Y}=\sum_{i=1}^7\boldsymbol{X}_i+\varepsilon_Y.$$ We let $a$ vary between $0$ and $10$ to increase progressively the non-linear part of the sub-regression. Once again, simulations has been made 100 times and the MSE were computed with 1 000 individuals validation samples.

 \begin{figure}[h!] 
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/robust_S.png} 
			\caption{Evolution of the quality of $\hat{S}$ when the paramater $a$ increases}\label{resnonlin}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/robustY.png} 
			\caption{MSE on the main regression for OLS(thick) and LASSO (thin) used both with (plain) or without {\tt CorReg} (dotted).}\label{MSEnonlin}
   \end{minipage}
\end{figure}
Figure \ref{MSEnonlin} illustrates the advantage of using {\tt CorReg} even with non-linear structures. Figure \ref{resnonlin} shows that the MCMC have more difficulties to find a linear structure as the non-linear part of the sub-regression increases but the model is quite robust (efficient for small values of $a$).

	\FloatBarrier	
\chapter{Numerical results on real datasets} \label{sectionrealcase}
	\section{Quality case study} \label{sectionexfos}
This work takes place in steel industry context, with quality oriented objective : to understand and prevent quality problems on finished product, knowing the whole process. The correlations are strong here (many parameters of the whole process without any a priori and highly correlated because of physical laws, process rules, {\it etc.}). 
		
We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item 205 variables from the whole process to explain it.
		\end{itemize}

\begin{figure}[h!]
	\begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/res_article/nb_comp_X_exfo.png}%{figures/mixmod.png} 
			\caption{Distribution of the number of components found for each covariate.}\label{graphMixmod}
	\end{minipage} \hfill
	\begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/res_article/gaussianmixture_exfo.png}%{figures/histR2exfos.png} $R^2_{adj}$ of the 76 sub-regressions.
			\caption{Example of non-Gaussian real variable easily modeled by a Gaussian mixture.}
	\end{minipage} \hfill
   \begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/correlexfoshist.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEexfos}
   \end{minipage}
\end{figure}   			
	We get a training set of $n=3 000$ products described by $p=205$ variables from the industrial process and a validation sample of $847$ products.
	Let's note $\rho$ the absolute value of correlations between two covariates. Industrial variables are naturally highly correlated as the width and the weight of a steel slab ($\rho=0.905$), the temperature before and after some tool ($\rho=0.983$), the  roughness of both faces of the product ($\rho= 0.919$), a mean and a max ($\rho=0.911$). 
	
	The objective here is not only to predict non-quality but to understand and then avoid it. {\tt CorReg} provides an automatic method without any a priori and is combined with variable selection methods. So it allows to obtain in a small amount of time some indications on the source of the problem, and to use human resources efficiently. When quality crises occurs, time is extremely precious so automation is a real stake. The combinatorial aspect of the sub-regression models makes it impossible to do manually.
		
		
		\begin{figure}[h!]
		\centering
			\includegraphics[height=150px, width=150px]{figures/histR2exfos.png} 
			\caption{$R^2_{adj}$ of the 76 sub-regressions.}
\end{figure} 
		
		
	{\tt CorReg} found the above correlations but it also found more complex structures describing physical models, like   Width = f (Mean flow , Mean speed) even if the true Physcial model is not linear : Width = flow / (speed * thickness) (here thickness is constant). Non-linear regulation models used to optimize the process were also found (but are confidential). These first results are easily understandable and meet metallurgists expertise.  Sub-regressions with small values of $R^2$ are associated with non-linear model (chemical kinetics for example).
			The algorithm gives a structure of $p_r=76$ subregressions with a mean of $\bar{\boldsymbol{p}}_f=5.17$ regressors. In $\boldsymbol{X}^{I_f}$ the number of $\rho>0.7$ is $\textbf{79.33\%}$ smaller than in $\boldsymbol{X}$.		
	\\
	
	
			It is now time to look at the predictive results (Table \ref{Res_exfos}). We see that {\tt CorReg} improves the results for each method tested in terms of prediction.
			We get parsimonious models automatically in a small amount of time (several hours but able to work during the night or the week-end)
%				The best model found when not using \textsc{CorReg} is given by the LASSO. But when using \textsc{CorReg} elasticnet produces a better model in terms of prediction. LASSO gives a model with 21 non-zero coefficients and elasticnet with \textsc{CorReg} gives a model with 40 non-zero parameters but $6.40\%$ better in prediction on the validation sample (847 products). $14$ non-zero coefficients are common between the two models.
%				Elasticnet alone get a model with 78 parameters that is improved by $9.75\%$ in prediction when used with \textsc{CorReg}. When using LASSO with \textsc{CorReg} we obtain a model with 24 non-zero coefficients that is $4.11\%$ better than LASSO alone. We also computed the OLS model (without selection) and the naive one (estimating the response by the mean of the learning set). All the MSE were modified here to obtain a value of 100 for the best (to preserve confidentiality). Elasticnet with \textsc{CorReg} is $13.51\%$ better than OLS.
%%		\begin{figure}[h]
%			\centering
%				\label{barplotMSEexfos}
%				\includegraphics[width=430px]{figures/MSEfinal.png}
%			\caption{MSE comparison on industrial dataset. Learning set : 3 000 products, validation set : 847 products}
%		\end{figure}		

		\begin{table}[h!]
\centering
\begin{tabular}{|c c|c|c|}
	\hline 
	Method& indicator& With {\tt CorReg} & without {\tt CorReg} \\ 
	\hline 
	OLS & MSE & 13.30 & 14.03 \\
		& (complexity)& (130) & (206) \\
	\hline
	LASSO & MSE & 12.77 & 12.96 \\
		& (complexity)& (24) & (21) \\
	\hline
	Elasticnet & MSE & \textbf{12.15} & 13.52 \\
		& (complexity)& (40) & (78) \\
	\hline
	Ridge & MSE & 12.69 & 13.09 \\
		& (complexity)& (130) & (206) \\
	\hline
\end{tabular} 
\caption{Results obtained on a validation sample (847 individuals).}\label{Res_exfos}
\end{table}


		In terms of interpretation, the main regression comes with the family of regression so it gives a better understanding of the consequences of corrective actions on the whole process. It typically permits to determine the \textit{tuning parameters} whereas variable selection alone would point variables we can't directly act on.	So it becomes easier to take corrective actions on the process to reach the goal. The stakes are so important that even a little improvement leads to consequent benefits, and we don't even talk about the impact on the market shares that is even more important.
		\FloatBarrier
		\section{Production case study}\label{sectionBV}
This second example is about a phenomenon that impacts the productivity of a steel plant.
We have:
		\begin{itemize}
			\item a (confidential)  response variable,
			\item $p=145$ variables from the whole process to explain it but only $n=100$ individuals.
			\item The stakes : $20\%$ of productivity to gain on a specific product with high added value.
		\end{itemize}
		
		
		\begin{figure}[h!]
	\begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/nbcompBV.png}%{figures/mixmod.png} 
			\caption{Distribution of the number of components found for each covariate.}\label{graphMixmodcompnumber}
	\end{minipage} \hfill
	\begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/GMcriseBV.png}%{figures/histR2exfos.png} $R^2_{adj}$ of the 76 sub-regressions.
			\caption{Another example of non-Gaussian real variable easily modeled by a Gaussian mixture.}
	\end{minipage} \hfill
   \begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/histcorrelBVBI.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEBV}
   \end{minipage}
\end{figure} 
  		
  			
{\tt CorReg} found 55 sub-regressions as shown in Figure \ref{R2bv}. One of them seems to be weak $R^2=0.17$ but is not linear (points out a link between diameter of a coil and some shape indicator).	

\begin{figure}[h!]
\centering
	\includegraphics[height=150px, width=150px]{figures/R2corregBVBI.png} 
			\caption{$R^2_{adj}$ of the 55 sub-regressions.}\label{R2bv}
\end{figure}
The response variable was binary but $n$ was too small compared to $p$ to use logistic regression so we have considered $\boldsymbol{Y}$ as a continuous variable and then made imputation by $1$ when $\hat{\boldsymbol{Y}}>0.5$ and by $0$ else.


\begin{table}[h!]
\centering
\begin{tabular}{|c c|c|c|}
	\hline 
	Method& indicator& With {\tt CorReg} & without {\tt CorReg} \\ 
	\hline
	OLS & well-classified & 100& 56 \\
		& MSE (leave-one-out)& 1.95& 51 810\\
		& complexity & 91& 100 \\
	\hline 
		LASSO & well-classified &93 &93 \\
		& MSE (leave-one-out)& 0.106 & 0.120\\
		& complexity & 27&34\\
	\hline 
		Elasticnet & well-classified &84 &87 \\
		& MSE (leave-one-out)&0.140 &0.148\\
		& complexity &10 &13\\
	\hline 
		Ridge & well-classified &88 &85 \\
		& MSE (leave-one-out)& 0.179 & 0.177\\
		& complexity &91 &146\\
	\hline 
\end{tabular} 
\caption{Results obtained with leave-one out cross-validation. $n=100, p=145$.}	
\end{table}

In this precise case, {\tt CorReg} found a structure that helped to decorrelate covariates in interpretation and to find the relevant part of the process to optimize. This product is made by a long process that requires several steel plants so it was necessary to point out the steel plant where the problem occurred.


\part{Further usage of the structure and perspectives}	
			
		
\chapter{Taking back the residuals}
	We have seen that eviction of redundant covariates improves the results by a good trade-off between dimension reduction and better conditioning versus keeping all the information. But The fact is that we lost some information and we want to get it back.
	\section{The model}
		After the estimation of the marginal model, we know both $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}^*}$.
		\begin{eqnarray}
			\boldsymbol{Y}&=& \boldsymbol{X}^{I_r}\boldsymbol{\beta}_{I_r}+\boldsymbol{X}^{I_f}\boldsymbol{\beta}_{I_f}+\boldsymbol{\varepsilon}_Y \\
			\boldsymbol{X}^{I_r}&=&\boldsymbol{X}^{I_f}\boldsymbol{\alpha}+\boldsymbol{\varepsilon} \\
			\boldsymbol{Y}&=& \boldsymbol{X}^{I_r}\underbrace{(\boldsymbol{\beta}_{I_r}+\boldsymbol{\alpha}\boldsymbol{\beta}_{I_f})}_{\boldsymbol{\beta}^*}+\boldsymbol{\varepsilon}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y 
					\end{eqnarray}		 
		Thus we have
		\begin{eqnarray}
			\boldsymbol{Y}- \boldsymbol{X}^{I_r}\boldsymbol{\beta}^*&=&\boldsymbol{\varepsilon}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y \\
			\boldsymbol{\varepsilon}&=&\boldsymbol{X}^{I_r}-\boldsymbol{X}^{I_f}\boldsymbol{\alpha}
		\end{eqnarray}		 
		So we introduce a plug-in model
		\begin{eqnarray}
			\underbrace{\boldsymbol{Y}- \boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}^*}}_{\tilde{\boldsymbol{Y}}}&=&\underbrace{(\boldsymbol{X}^{I_r}-\boldsymbol{X}^{I_f}\hat{\boldsymbol{\alpha}})}_{\tilde{\boldsymbol{X}}}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y \\
		\end{eqnarray}
		That allows us to estimate $\boldsymbol{\beta}_{I_r}$ with a classical linear model based on previous estimations of $\boldsymbol{\beta}^*$ and $\boldsymbol{\alpha}$.
		Then we have a model with a smaller noise
		\begin{equation}
			\boldsymbol{Y}= \boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}}^* + \hat{\boldsymbol{\varepsilon}}\hat{\boldsymbol{\beta}}_{I_r}+\boldsymbol{\varepsilon}_Y 
		\end{equation}
		and we can even find the original model by doing an identification step:
		\begin{eqnarray}
			\boldsymbol{Y}&=& \boldsymbol{X}^{I_r}(\hat{\boldsymbol{\beta}}^*-\hat{\boldsymbol{\alpha}}\hat{\boldsymbol{\beta}}_{I_r}) + \boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}}_{I_r}+\boldsymbol{\varepsilon}_Y \\
			&=&\boldsymbol{X}^{I_f}\hat{\boldsymbol{\beta}}_{I_f}+\boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}}_{I_r}+\boldsymbol{\varepsilon}_Y
		\end{eqnarray}
	
	Figure \ref{MQE2} shows the target of this plug-in model: the cases with enough correlations to have problem when using $\boldsymbol{X}$	but not enough correlations to have truly redundant covariates and to be able to delete some of them without significant information loss.
\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQE_toutOLSp5col.png}
	\caption{MSE of OLS (plain red) and {\tt CorReg} marginal(blue dashed) and {\tt CorReg} plug-in (green dotted) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$.}\label{MQE2}
\end{figure}	
			
		
	\section{Interpretation and latent variables}
			$\hat{\boldsymbol{\beta}}_{I_r}$ can be interpreted as the proper effect of $\boldsymbol{X}^{I_r}$ on $\boldsymbol{Y}$ in that it is the effect of the part of $\boldsymbol{X}^{I_r}$ that is independent from other covariates. Then if $\boldsymbol{X}^{I_r}$ is correlated to $\boldsymbol{Y}$ only through its correlation with $\boldsymbol{X}^{I_f}$ this sequential estimation will point it out and give a parsimonious model ($\hat{\boldsymbol{\beta}}_{I_r}=0$) but the real stake is greater. We can see $\boldsymbol{\varepsilon}$ as a latent variable instead of the noise of a sub-regression. This latent variable is known to be independent of $\boldsymbol{X}^{I_f}$ and dependent of $\boldsymbol{X}^{I_r}$ so we can appreciate its meaning and we also know its value by $\hat{\boldsymbol{\varepsilon}}=\boldsymbol{X}^{I_r}-\boldsymbol{X}^{I_f}\hat{\boldsymbol{\alpha}}$. Thus, the plug-in model can reveal some kinds of latent variables.
			
	

	\section{Consistency}\label{consistency}
		Consistency issues of the LASSO are well known and Zhao \cite{Zhao2006MSC} gives a very simple example to illustrate it.
		We have taken the same example to show how our method is more consistent.
		Here $p=3$ and $n=1000$.\\
		We define $\boldsymbol{X}^{I_f}, \boldsymbol{X}^{I_r}, \boldsymbol{\varepsilon}_Y, \boldsymbol{\varepsilon}_{X} i.i.d. \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}_n)$ and then \\
		$\boldsymbol{X}_3=\frac{2}{3}\boldsymbol{X}_1+\frac{2}{3}\boldsymbol{X}_2+\frac{1}{3}\boldsymbol{\varepsilon}_X$ and \\
		$\boldsymbol{Y}=2\boldsymbol{X}_1+3\boldsymbol{X}_2+\boldsymbol{\varepsilon}_Y$.\\
		We compare consistencies of complete, marginal and full plug-in model with LASSO (and LAR) for selection.
		It happens that our MCMC algorithm don't find the true structure but a permuted one so we also look at the results obtained with the true $S$ (but $\hat{\boldsymbol{\alpha}}$ is used) and with the structure found by the Markov chain after a few seconds.
		
		True $S$ was found $340$ times on $1000$ tries (model is not identifiable because $\boldsymbol{X}^j$ are all Gaussian).
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & {\tt CorReg} + marginal LASSO& {\tt CorReg} + full plug-in LASSO\\ 
		\hline 
		True $S$ &  1.006479 & \textbf{1.005468} & \textbf{1.006093} \\ 
		\hline 
		$\hat{Z}$ & \textbf{1.006479} & 1.884175 & 1.006517 \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observed on a validation sample (1000 individuals)}
		\end{table}

		We observe as we hoped that our marginal model is better when using true $S$ (coercing real zeros) and that marginal with $\hat{S}$ is penalized (coercing wrong coefficients to be zeros when true $S$ is not found).
		But the main point is that the plug-in model stays better than the classical one with the true $S$ and corrects enough the marginal model to follow the classical LASSO closely when using $\hat{S}$. 
		And when we look at the consistency :
		\begin{table}[h!]	
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & {\tt CorReg} + marginal LASSO& {\tt CorReg} + full plug-in LASSO \\ 
		\hline 
		True $S$ &  0 & 1000 & 830 \\ 
		\hline 
		$\hat{S}$ & 0 & 340 & \textbf{621} \\ 
		\hline 
		\end{tabular} 
		\caption{Number of consistent model found ($\boldsymbol{Y}$ depending on $\boldsymbol{X}_1,\boldsymbol{X}_2$ and only them) on $1000$ tries}\label{testidentifiableG}
		\end{table}				
		
	%	$299$ times on $1000$ tries, the plug-in model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		
		We also made the same experiment but with $\boldsymbol{X}_1,\boldsymbol{X}_2$ (and consequently $\boldsymbol{X}_3$) following Gaussian mixtures (to improve identifiability) randomly generated by our {\tt CorReg} package for R. 
		True $S$ is now found $714$ times on $1000$ tries . So it confirms that Gaussian mixture models are easier to identify.
		
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & {\tt CorReg} + marginal LASSO& {\tt CorReg} + full plug-in LASSO \\ 
		\hline 
		True $S$ &  1.571029 & \textbf{1.569559} & \textbf{1.570801} \\ 
		\hline 
		$\hat{S}$ & 1.005402 & 1.465768 & \textbf{1.005066} \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observed on a validation sample (1000 individuals)}
		\end{table}

		And when we look at the consistency :
		\begin{table}[h!]
		\centering	
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & {\tt CorReg} + marginal LASSO& {\tt CorReg} + full plug-in LASSO \\ 
		\hline 
		True $S$ &  0 & 1000 & 789 \\ 
		\hline 
		$\hat{S}$ & 0 & 714 & \textbf{608} \\ 
		\hline 
		\end{tabular} 
		\caption{Number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}\label{testidentifiableGM}
		\end{table}				
				
		
	%	$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		

	\section{Numerical results}
		We test the plug-in model with datasets generated the same way as for section \ref{compY}.
	\subsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	 	
We try the method with a response depending on all covariates ({\tt CorReg} reduces the dimension and can't give the true model if there is a structure). We observe for OLS (Figures \ref{MSEpredOLSY_zonetout} to \ref{MSEpredOLSbeta_zonetout}) that the plug-in model gives results similar in efficiency to the marginal model, but remains better than the complete model for smaller  correlations even for $n=400$. We also observe that we can found a model with more than $n$ coefficients when each estimation step computes less than $n$ coefficients. It means that  we estimate more coefficients than the classical OLS and keep a smaller variance so the plug-in model can also be an alternative to the complete model. It is interesting to see that OLS combined with the plug-in model is a sort of sequential estimation that allows to estimate more than $n$ coefficients.
 
\FloatBarrier

\newpage
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}
%OLS	
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredOLSY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredOLSY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/cplpredOLS_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredOLS_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredOLSbeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredOLSbeta_zonetout}
	\end{figure}
	\FloatBarrier
	
\newpage
\subsubsection{LASSO when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

%LASSO
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredlarY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredlarY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/cplpredlar_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredlar_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredlarbeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredlarbeta_zonetout}
	\end{figure}
	\FloatBarrier
\newpage
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

%elasticnet
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredelasticnetY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredelasticnetY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/cplpredelasticnet_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredelasticnet_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredelasticnetbeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredelasticnetbeta_zonetout}
	\end{figure}
	\FloatBarrier
\newpage
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredstepwiseY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredstepwiseY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/cplpredstepwise_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredstepwise_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredstepwisebeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredstepwisebeta_zonetout}
	\end{figure}
	\FloatBarrier
\newpage
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredridgeY_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredridgeY_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/cplpredridge_zonetout.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredridge_zonetout}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/tout/MSEpredridgebeta_zonetout.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredridgebeta_zonetout}
	\end{figure}
	\FloatBarrier

\clearpage

Combined with variable selection methods (Figures \ref{MSEpredlarY_zonetout} to \ref{MSEpredstepwisebeta_zonetout}) it does converge to the complete model results for large values of $n$ so it improves the marginal model for weak correlations (it is what it was built for) has no significant interest compared to the complete model. Ridge regression (Figures \ref{MSEpredridgeY_zonetout} to \ref{MSEpredridgebeta_zonetout}) leads to the same conclusion.

\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}	 \label{tableMSEsimdroitepred}
This case shows the robustness of selection methods, that rejects covariates in the plug-in model.\\ 

	
	\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$ }	 \label{tableMSEsimgauchepred}
We now try the method with a response depending only on variables in $\boldsymbol{X}^{I_r}$. The datasets used here were still the same.
Depending only on $\boldsymbol{X}_r$ implies sparsity and impossibility for the marginal model to obtain the true model when using the true structure, so we hope to see an improvement with the plug-in method. This is the reason why we have developed the plug-in model. \\

For OLS (figures \ref{MSEpredOLSY_zoneX2} to \ref{MSEpredOLSbeta_zoneX2}) we note that the plug-in model improves results of the marginal model for large values of $n$. This is still the case with variable selection methods even if it is not sufficient to improve the complete model. \\
This phenomenon is still observed with the ridge regression with more efficiency.

\subsection{About the plug-in model}
	The plug-in model sounds good when described theoretically and figure \ref{MQE2} makes us hope to obtain good results with it. But the reality is that the plug-in model (by definition) relies on a first estimation (the marginal model) thus it does reach the true model but asymptotically, and with a slow convergence speed. Moreover, if $\hat{S}$ is not exactly the true $S$ but the partition is good, the marginal model is not impacted whereas the plug-in model uses both $\hat{\boldsymbol{\beta}}^*_{I_f}$ and $\hat{\boldsymbol{\alpha}}$ so it is does not rely only on one estimation. $\hat{\boldsymbol{\alpha}}$ is another source of bias for finite values of $n$ and it depends itself on $\hat{S}$. Ordinary Least Squares really are in great trouble when confronted to correlated datasets so the plug-in model improves OLS any way but other methods are a bit less sensitive to correlations so it is difficult to improve them with a plug-in model relying on so many estimators. 
\newpage
%OLS	
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredOLSY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredOLSY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/cplpredOLS_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredOLS_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredOLSbeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredOLSbeta_zoneX1}
	\end{figure}
	\FloatBarrier
\newpage
%LASSO
\subsubsection{LASSO when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredlarY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredlarY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/cplpredlar_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredlar_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredlarbeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredlarbeta_zoneX1}
	\end{figure}
	\FloatBarrier
\newpage
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredelasticnetY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredelasticnetY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/cplpredelasticnet_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredelasticnet_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredelasticnetbeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredelasticnetbeta_zoneX1}
	\end{figure}
	\FloatBarrier
\newpage
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredstepwiseY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredstepwiseY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/cplpredstepwise_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredstepwise_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredstepwisebeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredstepwisebeta_zoneX1}
	\end{figure}
	\FloatBarrier
\newpage
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$}

\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredridgeY_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredridgeY_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/cplpredridge_zoneX1.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredridge_zoneX1}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X1/MSEpredridgebeta_zoneX1.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredridgebeta_zoneX1}
	\end{figure}
	\FloatBarrier


\clearpage


\FloatBarrier

\newpage

%OLS	
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredOLSY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredOLSY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/cplpredOLS_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredOLS_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredOLSbeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredOLSbeta_zoneX2}
	\end{figure}
	\FloatBarrier
\newpage
%LASSO
\subsubsection{LASSO when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredlarY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredlarY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/cplpredlar_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredlar_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredlarbeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredlarbeta_zoneX2}
	\end{figure}
	\FloatBarrier
\newpage
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredelasticnetY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredelasticnetY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/cplpredelasticnet_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredelasticnet_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredelasticnetbeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredelasticnetbeta_zoneX2}
	\end{figure}
	\FloatBarrier
\newpage
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredstepwiseY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredstepwiseY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/cplpredstepwise_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredstepwise_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredstepwisebeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredstepwisebeta_zoneX2}
	\end{figure}
	\FloatBarrier
\newpage
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$}

\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredridgeY_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredridgeY_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/cplpredridge_zoneX2.png}
		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{cplpredridge_zoneX2}
	\end{figure}
	\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/predhatS/X2/MSEpredridgebeta_zoneX2.png}
		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model, green=plug-in model}\label{MSEpredridgebeta_zoneX2}
	\end{figure}
	\FloatBarrier

\FloatBarrier

\chapter{Missing values} \label{chapmiss}
	\paragraph{Résumé:} Le modèle génératif complet sur les données nous permet d'obtenir la loi des valeurs manquantes. Mais nous pouvons aller encore plus loin car la modélisation explicite des corrélations nous permet d'obtenir les lois conditionnelles de chaque valeur manquante sachant les valeurs observées. Ce chapitre présente comment nous pouvons par un simple algorithme SEM estimer les paramètres des sous-régressions dans la chaîne MCMC de recherche de structure. Enfin, nous pouvons imputer les valeurs manquantes à l'aide d'un Gibbs qui procède par imputations multiples, fournissant au passage un indicateur de précision sur l'imputation proposée. 

\section{Introduction}
	Real datasets often have missing values and it is a very recurrent issue in industry. We note $\boldsymbol{M}$ the $n\times p$ binary matrix indicating whereas a value is missing (1) or not (0) in $\boldsymbol{X}$.
	We note $\boldsymbol{X}_M$ the missing values and $\boldsymbol{X}_{O}$ the observed values. $\Theta=\{\boldsymbol{\mu}_X,\boldsymbol{\Sigma}_X \}$ stands for the parameters of the Gaussian mixture followed by $\boldsymbol{X}$.
	$\boldsymbol{\alpha}$ is the matrix of the sub-regression coefficients with $\alpha_{i,j}$ the coefficients associated to $\boldsymbol{X}^i$ in the sub-regression explaining $\boldsymbol{X}^j$.\\ 
			Here we suppose that missing values are Missing Completely At Random (MCAR). 
	 Many methods does exist to manage such problems \cite{little1992regression} but they make approximation , add noise (imputation methods) or delete information (cutting methods).	
	 
%Détailler les différents pattern de valeurs manquantes
%	 
%Detailler Les 6 types de méthodes	 
%	 
	We have a full generative model on $\boldsymbol{X}$ with explicit dependencies within the covariates. So when a value is missing, we know its distribution but more than that, we know its conditional distribution based on observed values for the same individual. Thus we are able to make imputation and to describe the missing values with their conditional distribution. This is a positive side-effect of the explicit generative model on $\boldsymbol{X}$. 
\section{Estimation of the sub-regressions with missing values}
\subsection{The integrated likelihood}
The first thing we do with $\boldsymbol{X}$ is to estimate S.
	 During the MCMC, for each candidate we have to compute the likelihood of the candidate, depending on $\boldsymbol{\alpha}$ the matrix of the sub-regression coefficients.
We start with the complete likelihood of $\boldsymbol{X}$
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})&=& \prod_{i=1}^n f(\boldsymbol{X}_i)= \prod_{i=1}^n\left[f(\boldsymbol{X}_i^{I_r}|\boldsymbol{X}_i^{I_f};\boldsymbol{\alpha},\Theta,S)f(\boldsymbol{X}_i^{I_f};\boldsymbol{\alpha},\Theta,S) \right] \\
	&=&\prod_{i=1}^n\left[\prod_{j \in I_r}f(x_{i,j}|\boldsymbol{X}_i^{I_f};\boldsymbol{\alpha},\Theta,S)\prod_{j \notin I_r} f(x_{i,j};\boldsymbol{\alpha},\Theta,S) \right] \\
	&=&\prod_{i=1}^n\left[\prod_{j \in I_r}f(x_{i,j}|\boldsymbol{X}_i^{I_f^j};\boldsymbol{\alpha},\Theta,S)\prod_{j \notin I_r} f(x_{i,j};\boldsymbol{\alpha},\Theta,S) \right] \\
%	&=&\prod_{i=1}^n\left[\prod_{\substack{j \in I_r \\ \boldsymbol{M}_{i,j}=1}}P(x_{i,j}|\boldsymbol{X}_i^{I_f^j})\prod_{\substack{j \in I_r \\ \boldsymbol{M}_{i,j}=0}}P(x_{i,j}|\boldsymbol{X}_i^{I_f^j})
%			\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=1}} P(x_{i,j})\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}} P(x_{i,j}) \right] \\
	\mathcal{L}(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})&=&\sum_{i=1}^n\left[\sum_{j \in I_r}\log \left(f(x_{i,j}|\boldsymbol{X}_i^{I_f^j};\boldsymbol{\alpha},\Theta,S)\right)+\sum_{j \notin I_r} \log \left(f(x_{i,j};\boldsymbol{\alpha},\Theta,S)\right) \right] \label{loglikmiss}
\end{eqnarray}
		In the MCMC we need to compute the likelihood of the dataset knowing the structure. When missing values occurs, we restrict the likelihood to the known values by integration on $\boldsymbol{X}_M$.
%		We have 
%	\begin{equation}
%		g(\boldsymbol{X}|\Theta)=\int_{\boldsymbol{X}_M}f(\boldsymbol{X}|\Theta)d \boldsymbol{X} \label{integralmiss}
%	\end{equation}
%For the covariates in $\boldsymbol{X}_f$, we use the density estimated  ($e.g.$ a Gaussian Mixture model estimated by \textsc{Mixmod}) or given as hypothesis. All individuals are supposed $iid$ so $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \notin I_r $: 
%				 \begin{equation}
%				 	g(x_{i,j}|\Theta)=f(x_{i,j}|\Theta)=\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k}) 
%				 \end{equation} with $k_j,\pi_{j,k}, \mu_{j,k}$ and $\Sigma_{j,k}$ estimated by Mixmod (for example). 
%\\				 		
				 		
%		Then we have
%		\begin{eqnarray}
%			g(\boldsymbol{X}|\Theta)&=& g(\boldsymbol{X}_r|\boldsymbol{X}_f,\Theta)g(\boldsymbol{X}_f|\Theta) \\
%			&=&\prod_{i=1}^n\left[ g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}} g(x_{i,j}|\Theta) \right]\\
%			&=&\prod_{i=1}^n\left[ g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)
%							\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}}\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k}) \right] \label{decomplikelimiss}
%		\end{eqnarray}
%		reminding that covariates in $\boldsymbol{X}_f$ are orthogonal. \\
%		
%	 Residuals of the sub-regressions are orthogonal but missing values can make the residuals dependent. We have to decompose more precisely $g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)$. To have a better view on the dependencies implied, we first write the marginal distributions. \\

We know that $\boldsymbol{X}$ is a Gaussian mixture ({\it iid} individuals, vectors of orthogonal Gaussian mixtures $\boldsymbol{X}^{I_f}$ and linear combinations of these Gaussian mixtures and some Gaussian for $\boldsymbol{X}^{I_r}$) with $K$ the number of its components.
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)&=&\int_{\boldsymbol{X}_M}L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})d\boldsymbol{X} 
	=\int_{\boldsymbol{X}_M}\sum_{k=1}^K \pi_k \phi_k(\boldsymbol{X};\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} \\
	&=&\sum_{k=1}^K \pi_k \int_{\boldsymbol{X}_M}\phi_k(\boldsymbol{X};\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} 
	=\sum_{k=1}^K \pi_k \int_{\boldsymbol{X}_M}\prod_{i=1}^n\phi_k(\boldsymbol{X}_i;\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} \\
	&=&\sum_{k=1}^K \pi_k \prod_{i=1}^n\int_{\boldsymbol{X}_{i,M}}\phi_k(\boldsymbol{X}_i;\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X}_i 
	=\sum_{k=1}^K \pi_k \prod_{i=1}^n\phi_k(\boldsymbol{X}_{i,O};\boldsymbol{\alpha},\Theta,S)\\
	&=&\sum_{k=1}^K \pi_k \phi_k(\boldsymbol{X}_{O};\boldsymbol{\alpha},\Theta,S)=f(\boldsymbol{X}_{O},\boldsymbol{\alpha},\Theta,S)
\end{eqnarray}






% $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \notin I_r $:
%	 \begin{equation}
%	 \int_{\boldsymbol{X}_M}f(x_{i,j}|\boldsymbol{\alpha},\Theta,S) d\boldsymbol{X}=1
%	 \end{equation}	
To compute this likelihood, we will use the decomposition
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)&=&f(\boldsymbol{X}_{O};\boldsymbol{\alpha},\Theta,S)=\prod_{i=1}^nf(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)f(\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S) \\
	&=&\prod_{i=1}^nf(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)\prod_{\substack{j \in I_f \\ M_{i,j}=0}}f(x_{i,j};\boldsymbol{\alpha},\Theta,S)
\end{eqnarray}

with	  $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}= 0 \textrm{ and } j \notin I_r $:
	 \begin{equation}
	 f(x_{i,j};\boldsymbol{\alpha},\Theta,S)=\sum_{k=1}^{K_j}\pi_{j,k}\Phi_k(x_{i,j};\mu_{j,k},\Sigma_{j,k}) \label{likmissdroite}
	 \end{equation} with $K_j,\pi_{j,k}, \mu_{j,k}$, $\Sigma_{j,k}$ and the likelihood estimated once (for example by RMixmod \cite{packageRmixmod}) before the MCMC starts. 
%	 $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \in I_r $:
%	 \begin{equation}
%	 \int_{\boldsymbol{X}_M}f(x_{i,j}|\boldsymbol{X}_i^{I_f^j},\boldsymbol{\alpha},\Theta,S) d\boldsymbol{X}=1
%	 \end{equation}
\\
	 And, $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}= 0 \textrm{ and } j \in I_r $:
		\begin{eqnarray}
 f(x_{i,j}|\boldsymbol{X}_{i,O}^{I_f^j};\boldsymbol{\alpha},\Theta,S)&=& \sum_{k=1}^{K_{ij}}\pi_{ij,k}\Phi(x_{i,j};\mu_{ij,k},\Sigma_{ij,k}) \textrm{ where }  \label{Missingdensity}\\
				\boldsymbol{\pi}_{ij} &=& \bigotimes_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1 } } \boldsymbol{\pi}_l \textrm{ and  }K_{ij}=|\boldsymbol{\pi}_{ij}| ,\\
				\boldsymbol{\mu}_{ij}&=& \sum_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=0  }}\alpha_{l,j}x_{i,l} + \bigoplus_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1  }} \alpha_{l,j} \boldsymbol{\mu}_l \\
				\boldsymbol{\Sigma}_{ij} &=& \sigma_j^2 + \bigoplus_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1 }}\alpha_{i,l}^2 \boldsymbol{\Sigma}_l		
		\end{eqnarray}		 
		This could be easily used for imputation of the missing values in $\boldsymbol{X}^{I_r}$ knowing the parameters $\boldsymbol{\alpha}, \Theta$ and $S$. We note that we obtain a Gaussian when there is no missing value in $I_f^j$.
		But we see that	$f(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)$ is not the product of the $f(x_{i,j}|\boldsymbol{X}_{i,O}^{I_f^j};\boldsymbol{\alpha},\Theta,S) $	if a same missing value occurs in distinct sub-regressions. Thus if every sub-regression are distinct connex component then we can use (\ref{Missingdensity}) and we have
		\begin{equation}
		L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)=\prod_{i=1}^n\prod_{\substack{j \in I_r \\ M_{i,j}=0}}f(x_{i,j}|\boldsymbol{X}^{I_f^j}_{i,O};\boldsymbol{\alpha},\Theta,S)\prod_{\substack{j \in I_f \\ M_{i,j}=0}}f(x_{i,j};\boldsymbol{\alpha},\Theta,S) \label{simplemisslik}
\end{equation}		 
		But for the general case we need to manage the dependencies implied by missing values in common covariates in the $I_f^j$.
%		Because $\forall 1\leq i \leq n, \boldsymbol{X}_i$ is a Gaussian Mixture, $\boldsymbol{X}_{i,O}$ and then $(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}_{i,O}^{I_f})$ are Gaussian mixtures too. \\
		We note $f(\boldsymbol{X})=\sum_{k=1}^K\pi_k \mathcal{N}(\boldsymbol{\mu}_{X,k};\boldsymbol{\Sigma}_{X,k})$.
		
\begin{eqnarray}
L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O};\boldsymbol{\alpha},\Theta,S)\\
&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O}^{I_r}|\boldsymbol{X}_{i,O}^{I_f};\boldsymbol{\alpha},\Theta,S)\Phi_k(\boldsymbol{X}_{i,O}^{I_f};\boldsymbol{\alpha},\Theta,S)\\
&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O}^{I_r}|\boldsymbol{X}_{i,O}^{I_f};\boldsymbol{\alpha},\Theta,S)\prod_{\substack{j \in I_f \\ M_{i,j}=0}}\Phi_k(x_{i,j};\mu_{j,k},\Sigma_{j,k}) \label{liklihoodmissglobal}
\end{eqnarray}		

%définintion des objets génériques
Where	
\begin{eqnarray}
	\boldsymbol{\pi}&=&\bigotimes_{\substack{j \in I_f }} \boldsymbol{\pi}_j \textrm{ (Kronecker product)}\\
	K&=& |\boldsymbol{\pi}| \\
	\boldsymbol{\mu}_{X^{I_f}}&=&  \prod_{\substack{j \in I_f}}\boldsymbol{\mu}_{j} \textrm{ (Cartesian product) } \\	
	\boldsymbol{\sigma}_{X}&=&\prod_{\substack{j \in I_f}}\boldsymbol{\sigma}_{j} \textrm{ (Cartesian product) }
\end{eqnarray}
		with $ \boldsymbol{\pi}_j, \mu_{j,k},\Sigma_{j,k}$ are estimated once before the MCMC starts (by Mixmod for example).
		
		
		$\forall 1\leq i \leq n, \forall 1\leq k \leq K$ we have
\begin{eqnarray}
		\Phi_k(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)&=&\Phi_k(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k},\boldsymbol{\Sigma}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k})\\
				P(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)&=&\Phi_k(\boldsymbol{X}^{I_r}_{i,O};\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k},\boldsymbol{\Sigma}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k})\\
		\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k}&=& 
				\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O},k}+
				\boldsymbol{\Sigma}_{X_{i,O}^{I_r},X_{i,O}^{I_f},k}(\boldsymbol{\Sigma}_{X_{i,O}^{I_f},X_{i,O}^{I_f},k})^{-1}
				( ^t\boldsymbol{X}_{i,O}^{I_f}-\boldsymbol{\mu}_{X^{I_f}_{i,O},k})\\
		\boldsymbol{\Sigma}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k}&=&\boldsymbol{\Sigma}_{X_{i,O}^{I_r},X_{i,O}^{I_r},k}-\boldsymbol{\Sigma}_{X_{i,O}^{I_r},X_{i,O}^{I_f},k}
		(\boldsymbol{\Sigma}_{X_{i,O}^{I_f},X_{i,O}^{I_f},k})^{-1} \boldsymbol{\Sigma}_{X_{i,O}^{I_f},X_{i,O}^{I_r},k} \\
		\forall j \in I_r: \ \  \boldsymbol{\mu}_{X_{i,O}^{j}}&=&\sum_{l \in I_f^j}\alpha_{l,j}\mu_{l,k} 
\end{eqnarray}		
		$\forall j \in I_r \textrm{ with } M_{i,j}=0$ 
\begin{equation}
	\operatorname{var}_{k}(x_{i,j})=\sigma_{j}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{X^l,k}^2
\end{equation}			
	$\forall j \notin I_r \textrm{ with } M_{i,j}=0$
\begin{equation}
	\operatorname{var}_{k}(x_{i,j})=\sigma_{X^j,k}^2
\end{equation}			
	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}\neq \emptyset \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})=\sum_{l\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{l,j_1}\alpha_{l,j_2}\operatorname{var}_{k}(x_{i,l}) =\sum_{l\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{l,j_1}\alpha_{l,j_2}\sigma_{X^l,k}^2
\end{equation}
	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}= \emptyset \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})=0
\end{equation}
	$\forall j_1 \in I_f, j_2 \in I_f \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})=0
\end{equation}
	$\forall j_1 \in I_r, j_2 \in I_f^{j_1} \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})= \alpha_{j_2,j_1}\sigma_{X^{j_2},k}^2
\end{equation}
$\forall j_1 \in I_r, j_2 \notin I_f^{j_1}\cup I_r \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})= 0
\end{equation}
	\subsection{Likelihood computation optimized}
	The main problem with the likelihood in its global form (\ref{liklihoodmissglobal}) is that the number of components explodes so we can't use it in practice. But in many case, it can be simplified.
We see that the $0$ in the variance-covariance matrix does not depend on the component $k$ so the structure of sparsity of $\boldsymbol{\Sigma}$ can be stored and used back in each iteration for a given structure $S$ to reduce computing time.  Another strictly technical tip would be to use sparse matrix storage to avoid null value storage and useless zero multiplications.
		Moreover, we can look if there are missing values shared by several sub-regression. 
		Connex component detection could be done to reduce the dimension down to strictly dependent covariates and use equation (\ref{Missingdensity}) elsewhere.
		We just need to compute the  row-sums of the adjacency matrix $G$ or to search for redundancy in $I_f$ and then if there is no redundancy or if $\forall j$ redundant we have $\sum_{i=1}^nM_{i,j}=0$ then we can use the simplified form of the likelihood given in (\ref{simplemisslik}). For faster computation we can stock the vector of covariates that have missing values.
		So the true value of the likelihood can be computed efficiently in most of cases but in the MCMC, it remains the possibility to have a structure with explosive likelihood expression when combined with the missing values and we need to compute the likelihood for a great number of candidates. Then it is possible to use directly the simplified form of the likelihood, that can be seen as an approximation of the likelihood, not taking into account some of the dependencies but it would offer no guarantee in terms of efficiency for the MCMC. %Numerical results on simulated datasets will show if this approximation is effective.

%		In first approximation we can suppose independence between the sub-regression:
%		\begin{equation}
%		\forall (j,j') \in I_r \times I_r, g(x_{i,j}| \boldsymbol{X}_{i}^{I_f},\Theta) \perp g(x_{i,j'}| \boldsymbol{X}_{i}^{I_f},\Theta)
%\end{equation}		 
%then we have the complete expression of the likelihood with \ref{decomplikelimiss} and \ref{Missingdensity}.
% Such approximation can be costless according to the position of the missing values ({\it e.g.} if they are all in $\boldsymbol{X}^{I_r}$). It is closer to the real model than the orthogonal hypothesis made by classical imputation by the mean. Moreover, sub-regressions are used only locally and errors don't cumulate whereas the true general decomposition combine many sub-regressions with cumulated noise of approximation. Thus, a general model would be better asymptotically but may not be efficient with finite dataset if the structure is complex. This first approximation is a good candidate to compare to the naive model (not taking into account the structure of sub-regression but making imputations by the mean for each covariate individually). 
%		
%		However, we write the real generalized expression for the log-likelihood.
%		Let $\mathcal{I}_r$ be a permutation of $I_r$ (arbitrary chosen, so the package will use identity). We define the general decomposition:
%		\begin{eqnarray}
%			g(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},\Theta)&=& \prod_{i=1}^n \left[g(x_{i,\mathcal{I}_r(p_r)}|\boldsymbol{X}_i^{I_f},\Theta)\prod_{j=1}^{p_r-1}g(x_{i,\mathcal{I}_r(j)}|\boldsymbol{X}_{i}^{\mathcal{I}_r(j+1)},\dots ,\boldsymbol{X}_{i}^{\mathcal{I}_r(p_r)}, \boldsymbol{X}_i^{I_f},\Theta)\right]
%		\end{eqnarray}
%		where we don't know the expression of $ g(x_{i,\mathcal{I}_r(j)}|\boldsymbol{X}_{i}^{\mathcal{I}_r(j+1)},\dots ,\boldsymbol{X}_{i}^{\mathcal{I}_r(p_r)}, \boldsymbol{X}_i^{I_f},\Theta)$ so the previous approximation stands still. 
%			
%	
%To estimate $\boldsymbol{\alpha}$ we use an EM algorithm. We start with an arbitrary value $\boldsymbol{\alpha}^{(0)}$, then:
%For the iteration $h$ of the algorithm at the E step we want 
%\begin{equation}
%	E_{\boldsymbol{X}_{\boldsymbol{M}}|\boldsymbol{X}_O,\boldsymbol{\alpha},\Theta,S}\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha}^{(h)},S,\Theta)\right]
%\end{equation}
%So we get $\boldsymbol{X}_M^{(h)}$ the  imputation for $\boldsymbol{X}_M$ and then the M step simply is
%\begin{equation}
%	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}E\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha},S,\Theta) \right]
%\end{equation}
%and we can use the same method as the one for classical case without missing values (OLS, SUR, {\it etc.}).
%		And we continue until convergence ($\parallel \boldsymbol{\alpha}^{(h+1)} - \boldsymbol{\alpha}^{(h)}\parallel < tol $ where $tol$ is the tolerance.\\
%		
%Imputation in $\boldsymbol{X}^{I_r}$ is made according to (\ref{Missingdensity}) and we have orthogonality in $\boldsymbol{X}^{I_f}$:
%\begin{displaymath}
%\forall j \notin I_r, P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f}\setminus \boldsymbol{X}^j)=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r})
%\end{displaymath}
%Moreover, $\forall j \notin I_r, \forall l \in I_r $ with $j \notin I_f^l, \boldsymbol{X}^j\perp \boldsymbol{X}^j$ so
%\begin{displaymath}
%\forall j \notin I_r, P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f}\setminus \boldsymbol{X}^j)=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r})=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r^j})
%\end{displaymath}
%where $I_r^j=\{i \in I_r| j \in I_f^j \}=\{i \in I_r|\alpha_{j,i}\neq 0 \}$ So we will make imputations (for E step only) according to $P(\boldsymbol{X}^j|\boldsymbol{X}_O^{I_r^j})$
%		\\
%$\forall 1 \leq i \leq n$	
%To use formulas on conditional distribution for Gaussian multivariate distribution we first write $P(\boldsymbol{X}_i^j,\boldsymbol{X}_i^{I_r^j})$ which is a Gaussian mixture with $K_{ij}$ components.
%\\
%$\forall j \notin I_r, \forall (l_1,l_2) \in I_r^j, P(x_{i,l_1}|x_{i,j},x_{i,l_2})=P(x_{i,l_1}|x_{i,j})$
%\begin{eqnarray}
%	P(x_{i,j},\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\alpha})&=& P(x_{i,j})\prod_{\substack{ l \in I_r^j \\M_{i,l=0} } }P(x_{i,l}|x_{i,j}) \\
%	&=&\sum_{k =1}^{ K_{ij}} \pi_{ij,k} \phi(x_{i,j},\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\mu}_{ij,k},\boldsymbol{\Sigma}_{ij,k}) \textrm{ where } \\
%	\boldsymbol{\pi}_{ij}&=&\boldsymbol{\pi}_j\otimes \left[ \bigotimes_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 } } \boldsymbol{\pi}_{ijl} \right]=\boldsymbol{\pi}_j\otimes\bigotimes_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 } } \left(\bigotimes_{\substack{h \in I_f^l \\ h \neq j} }\boldsymbol{\pi}_h \right)
%	 \textrm{ and  }K_{ij}=|\boldsymbol{\pi}_{ij}| ,\\
%	\boldsymbol{\mu}_{ij}&=&\boldsymbol{\mu}_j \times \left[\prod_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 }}\boldsymbol{\mu}_{ijl} \right]
%		=\boldsymbol{\mu}_j \times \prod_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 }}\left[\alpha_{j,l}x_{i,j}+\bigoplus_{\substack{h \in I_f^l}}\alpha_{h,l}\boldsymbol{\mu}_{h} \right] \\	
%		\boldsymbol{\Sigma}_{ij}&& \textrm{is the associated variance-covariance matrix}
%\end{eqnarray}
%		Cartesian product and power in the expression of the mean.
%		
%
%Then we have 
%\begin{eqnarray}
%	P(x_{i,j}|\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\alpha})&= &\sum_{k=1}^{K_{ij}}\pi_{ij,k}\phi(x_{i,j}|\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\mu}_{ij,k},\boldsymbol{\Sigma}_{ij,k}) \\
%	&=&\sum_{k=1}^{K_{ij}}\pi_{ij,k}\phi \left(x_{i,j},\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}},\Sigma_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}\right) \textrm{ with} \\
%	\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}&=& \mu_{j,k} + \operatorname{cov}(x_{i,j,k},\boldsymbol{X}_{i,O,k}^{I_r^j})\operatorname{var}(\boldsymbol{X}_{i,O,k}^{I_r^j})^{-1}(\boldsymbol{X}_{i,O}^{I_r^j}-\boldsymbol{\mu}_{\boldsymbol{X}_{i,O}^{I_r^j},k})\textrm{ and} \\
%	\Sigma_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}&=&\operatorname{var}(x_{i,j,k})-\operatorname{cov}(x_{i,j},\boldsymbol{X}_{i,O,k}^{I_r^j})\operatorname{var}(\boldsymbol{X}_{i,O,k}^{I_r^j})^{-1}\operatorname{cov}(\boldsymbol{X}_{i,O,k}^{I_r^j},x_{i,j,k})
%\end{eqnarray}
% 
%But we do not need to compute the variance because we only want $E_{\boldsymbol{X}_{\boldsymbol{M}}|\boldsymbol{X}_O,\boldsymbol{\alpha},\Theta,S}\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha}^{(h)},S,\Theta)\right]$ so the mean is sufficient, we impute  
%\begin{equation}
%	\hat{x}_{i,j}=\frac{1}{K_{ij}}\sum_{k=1}^{K_{ij}}\pi_{ij,k}\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}
%\end{equation}
%
%We have 
%\begin{eqnarray}
%	\forall l \in I_r^j,\forall 1\leq k \leq K_{ij}, \operatorname{cov}(x_{i,j,k},\boldsymbol{X}_{i,O,k}^{l})&=&\operatorname{cov}(x_{i,j,k},\sum_{h\in I_f^l}x_{i,h}\alpha_{h,l}+\varepsilon_{i,l})
%	=\alpha_{j,l}\sigma_{j,k}^2 \\
%	\forall l \in I_r^j,\forall 1\leq k \leq K_{ij}, \operatorname{var}(x_{i,l,k})&=&\sigma_l^2+\sum_{h \in I_f^l}\alpha_{h,l}^2\sigma_{h,k}^2 
%	\end{eqnarray}
%	$\forall l_1\neq l_2 \in I_r^j,\forall 1\leq k \leq K_{ij},$
%	\begin{eqnarray}
%	 \operatorname{cov}(x_{i,l_1,k},x_{i,l_2,k})&=&\operatorname{cov}(\sum_{h\in I_f^{l_1}}x_{i,h,k}\alpha_{h,l_1}+\varepsilon_{i,l_1},\sum_{h\in I_f^{l_2}}x_{i,h,k}\alpha_{h,l_2}+\varepsilon_{i,l_2})\\
%	&=& \operatorname{cov}(\sum_{h\in I_f^{l_1}}x_{i,h,k}\alpha_{h,l_1},\sum_{h\in I_f^{l_2}}x_{i,h,k}\alpha_{h,l_2}) \\
%	&=& \sum_{h \in I_f^{l_1}\cap I_f^{l_2}}\sigma_{h,k}^2\alpha_{h,l_1}\alpha_{h,l_2}
%\end{eqnarray}
%
%
	\subsection{Weighted penalty}
			Now we have defined the way to compute the likelihood, other questions remain : how to define the number of parameters in the structure ?		How to take into account missingness (structures relying on highly missing covariates should be penalized) ?
			We have seen that for a same covariate $X^j$ with $ j \in I_r$, the number of parameters is not the same for each individual depending whether or not $M_{i,j}=0$. But the penalty (for $\psi=BIC$) can't be added at the individual level (because $\log(1)=0$ so it would be annihilated). 
			
			To penalize models that suppose dependencies based only on a few individuals, we propose to use the mean of the complexities obtained for a given covariate.
			\begin{equation}
			k_j=\frac{1}{n}\sum_{i=1}^nk_{i,j}
\end{equation}						where $k_{i,j}$ is the number of parameter to estimate in $P(x_{i,j}|\boldsymbol{X}_i\setminus \boldsymbol{X}_i^j)$.
			\begin{eqnarray}
		-2\log P(\boldsymbol{X}|S)&\approx & BIC=-2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+|\boldsymbol{\Theta}|\log(n) \\
		&=& -2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+(\sum_{j=1}^pk_j)\log(n)
	\end{eqnarray}
			 Thus if a structure is only touched by one missing value the penalty will be smaller than another same shaped structure but with more missing values implied.
			Another way would be to use $\psi=RIC$ (see \cite{foster1994risk}) so the complexity is associated with $\log(p)$ and can be added individually. Or to make a compromise and penalize by $\frac{k_i\log(p)}{\log(n)}$.
		
%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	
			
%			In fact we have the same number of parameters to estimate with or without missing values but the problem is that some of the parameters are estimated based only on a portion of the individuals so each parameter has a different weight.
%			The penalty in $k\log(n)$ stands for $k$ parameters each $n-estimated$ so our proposed penalty term is an intuitive way to use a weighted penalty.
%	
\section{SEM}
	The integrated likelihood depicted above depends on $\boldsymbol{\alpha}$ which was formerly estimated by OLS when there was no missing values. But when missing values occurs in a sub-regression we need another solution.
	
	We use a  Stochastic Expectation Maximization (SEM) algorithm \cite{celeux1986algorithme} to estimate $\boldsymbol{\alpha}$ because missing values do not allow to use OLS and  the log-likelihood (\ref{loglikmiss}) is not linear so a simple Expectation-Maximization (EM) would be difficult to compute.
		
	\subsection{Our implementation of SEM}
	\paragraph{initialization:} We start with some imputation (for example by the mean) for each missing value (done only once for the MCMC). $\boldsymbol{\alpha}^{(0)}$ can be initialized by cutting method	(sparse structure) or using imputed values in $\boldsymbol{X}$.
	At iteration $h$,
	\paragraph{SE step:}
		We generate the missing values according to $P(\boldsymbol{X}_M|\boldsymbol{X}_O; \alpha^{(h)},\Theta,S)$, that is stochastic imputation.
	\paragraph{M step:}
		We estimate 
		\begin{equation}
	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}E\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha},S,\Theta) \right]
\end{equation}
and we can use the same method as the one for classical case without missing values (OLS, SUR, {\it etc.}).
		We continue until convergence ($\parallel \boldsymbol{\alpha}^{(h+1)} - \boldsymbol{\alpha}^{(h)}\parallel < tol $ where $tol$ is the tolerance). Then we make $m$ iterations and take $\hat{\boldsymbol{\alpha}}$ as the mean of these $m$ last iterations.
		
		
Another (faster but not optimal) way would be to only use the structure for $\boldsymbol{X}^{I_r}$ and use the distribution given by Mixmod for $\boldsymbol{X}^{I_f}$ along the MCMC. The full SEM would then be used only once with the final structure to make imputation in $\boldsymbol{X}$ before using variable selection methods like the LASSO.


	\subsection{Stochastic imputation by Gibbs sampling}
		We use a Gibbs sampling method to generate the missing values at the SE step. $\boldsymbol{X}$ follows a multivariate Gaussian mixture with $K$ component and we note $Z$ the set of the $Z_{i,j}$ indicating the component from which $x_{i,j}$ is generated.
		\paragraph{Initialisation:} all the $z_{i,j}$ are set to the first component (such an initialisation does not depend on $K$) and $\boldsymbol{X}_M$ are imputed by the marginal means.
		\paragraph{Iteration:} At each iteration of the Gibbs sampler: \\
			$\forall x_{i,j} \in \boldsymbol{X}_M^{I_r}$:  $x_{i,j}$ is generated according to 
			\begin{eqnarray}
			P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}},Z;\boldsymbol{\alpha}^{(h)},\Theta,S)&=&P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}};\alpha^{(h)},\Theta,S) \\
			&=&P(x_{i,j}|\boldsymbol{X}_i^{I_f^j};\boldsymbol{\alpha}^{(h)},\Theta,S)=\mathcal{N}(\boldsymbol{X}_i^{I_f^j}\boldsymbol{\alpha}^{(h)}_{I_f^j,j};\sigma_j^2 )
			\end{eqnarray}		
			We have $P(\boldsymbol{X}|Z)=\mathcal{N}(\boldsymbol{\mu}_{|Z},\boldsymbol{\Sigma}_{|Z})$. \\
			$\forall x_{i,j} \in \boldsymbol{X}_M^{I_f}$:  $x_{i,j}$ is generated according to 
			\begin{eqnarray}
			P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}},Z;\boldsymbol{\alpha}^{(h)},\Theta,S)&=&P(x_{i,j}|\boldsymbol{X}_{i,\bar{j}},Z_i;\boldsymbol{\alpha}^{(h)},\Theta,S)			
			\end{eqnarray}			
			\begin{eqnarray}
			=\mathcal{N}(\mu_{j|Z_{i,j}} + \boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}^{-1}_{X_{\bar{ij}},X_{\bar{ij}}|Z_i}(X_{\bar{ij}}-\boldsymbol{\mu}_{X_{\bar{ij}}|Z_i}) ;  \sigma_{j|Z_{i,j}}^2-\boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}^{-1}_{X_{\bar{ij}},X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}')
			\end{eqnarray}		
			Where all the values needed here were described above for the likelihood computation.
%With $\forall j \in I_r$ 
%\begin{equation}
%	\operatorname{var}_{|Z_{i}}(x_{i,j})=\sigma_{j|Z_{i,j}}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{l|Z_i,l}^2=\sigma_{j}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{l|Z_i,l}^2
%\end{equation}			
%	$\forall j \notin I_r $
%\begin{equation}
%	\operatorname{var}_{|Z_{i}}(x_{i,j})=\sigma_{j|Z_{i,j}}^2
%\end{equation}			
%	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}\neq \emptyset $
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=\sum_{k\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{k,j_1}\alpha_{k,j_2}\operatorname{var}_{|Z_i}(x_{k|Z_i,k}) =\sum_{k\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{k,j_1}\alpha_{k,j_2}\sigma_{k|Z_{i,k}}^2
%\end{equation}
%	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}= \emptyset $
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=0
%\end{equation}
%	$\forall j_1 \in I_f, j_2 \in I_f$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=0
%\end{equation}
%	$\forall j_1 \in I_r, j_2 \in I_f^{j_1}$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})= \alpha_{j_2,j_1}\sigma^2_{j_2|Z_{i,j_2}}
%\end{equation}
%$\forall j_1 \in I_r, j_2 \notin I_f^{j_1}\cup I_r$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})= 0
%\end{equation}
%We see that the $0$ in the variance-covariance matrix does not depend on $Z$ so the structure of sparsity of $\boldsymbol{\Sigma}$ can be stored and used back in each iteration for a given structure $S$ to reduce computing time.

	Then, $\forall 1\leq i \leq n, \forall j \in I_f$ we draw new values for $Z_{i,j}$ according to
	\begin{eqnarray}
		P(Z_{i,j}|\boldsymbol{X},Z_{\bar{i,j}};\Theta,\boldsymbol{\alpha},S)&=&P(Z_{i,j}|\boldsymbol{X}_i,Z_{i,\bar{j}};\Theta,\boldsymbol{\alpha},S)=\mathcal{M}(t_{i,j,1},\dots ,t_{i,j,K_j}) \\
		\textrm{where } t_{i,j,k}&=&\frac{\pi_{j,k}\Phi(x_{i,j};\mu_{j,k},\sigma_{j,k}^2)}{\sum_{l=1}^{K_j}\pi_{j,l}\Phi(x_{i,j};\mu_{j,l},\sigma_{j,l}^2) }
	\end{eqnarray}
		
	
	
	We see that $Z_{i,j}$ are not used if there is no missing values in $\boldsymbol{X}_i$ and others are not all needed so we can also optimize computation time by  computing only the $Z_{i,j}$ that are needed in the Gibbs.
	For the last iteration of the Gibbs, in the last iteration of the SEM, we do not need to draw $Z$.	
	
	Instead of using long chain for each Gibbs, we can use small chains because SEM iteration will simulate longer chains so it remains efficient with a smaller computation cost.
	
	Computation cost will be the main purpose here because we need an iterative algorithm (Gibbs sampler) at each iteration of another iterative algorithm (SEM) for each candidate of the MCMC.
	So alternative method should be preferred for large datasets with many missing values and only a small amount of time.
	
	Because $K$ can be very large we search a way to compute the likelihood.
	We can use a Gibbs algorithm to estimate the likelihood:
	\begin{eqnarray}
	P(\boldsymbol{X}_O;\Theta, S, \boldsymbol{\alpha})&=& 
		\sum_{Z\in \mathcal{Z}}\int_{\boldsymbol{X}_M}\frac{P(\boldsymbol{X}_M,Z,\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},S)}{P(\boldsymbol{X}_M,Z|\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},S)}P(\boldsymbol{X}_M,Z|\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},S) dX \\
		&\approx &\frac{1}{Q} \sum_{q=1}^Q\frac{P(\boldsymbol{X}_M^{(q)},\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha},S)}{P(\boldsymbol{X}_M^{(q)}|\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha},S)} \textrm{ by the law of large numbers}
	\end{eqnarray}		
	where $Q$ is the number of iterations of the Gibbs sampler.
	But to be faster, we use the previous Gibbs algorithm with:
	\begin{eqnarray}
	P(\boldsymbol{X}_O;\Theta, S, \boldsymbol{\alpha})&\approx & \frac{1}{Q} \sum_{q=1}^Q\frac{P(\boldsymbol{X}_M^{(q)},\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha}^{(q)},S)}{P(\boldsymbol{X}_M^{(q)}|\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha}^{(q)},S)}
	\end{eqnarray}	
		
\subsection{Alternative E step}
	If we can't (or don't want to) compute the SE step described above, then we can use alternative imputation step for missing data based on $\boldsymbol{\alpha}$ (and keep the alternate optimisation to find the best $\boldsymbol{\alpha}$). 
	
	$\forall x_{i,j} \in \boldsymbol{X}_M $ we have:
	\\
if $j\in I_r$, Equation(\ref{Missingdensity}) gives: 
	\begin{eqnarray}
	E[x_{i,j}|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S]&=&E[\sum_{k=1}^{k_{ij}}\pi_{ij,k}\Phi(x_{i,j}|\mu_{ij,k},\Sigma_{ij,k})|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S] 
	\end{eqnarray}
	  
	Let $r_{i,j}=\{l \in I_r| \boldsymbol{\alpha}_{j,l}\neq 0, \boldsymbol{M}_{i,j}=0 \}$ the set of observed covariates for individual $i$ that are explained by $x_{i,j}$ according to $S$.
	\\
	If $j\notin I_r$ we can do:
	\begin{eqnarray}
	E[x_{i,j}|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S]&=&\frac{1}{|r_{i,j}|}\sum_{k \in r_{i,j}}E_{|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S}\left[\frac{1}{\alpha_{j,k}}\left(x_{i,k}-\varepsilon_{k}(i)-\sum_{l \in I_f^k} x_{i,l}\alpha_{l,k}\right)\right] \\
	&=& \frac{1}{|r_{i,j}|}\sum_{k \in r_{i,j}}E_{|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S}\left[\frac{1}{\alpha_{j,k}}\left(x_{i,k}- \sum_{l \in I_f^k} x_{i,l}\alpha_{l,k}\right)\right]
	\end{eqnarray}
	that is the mean of the expectations of the inverse sub-regressions implying $x_i,j$ with value in $\boldsymbol{X}^{I_r}_i$ not missing.



%		\subsection{Estimation of the coefficients in each regression}
%			Estimating the $\boldsymbol{\alpha}_j$  with missing values is just estimating independent regressions with missing values. We have seen in equation (\ref{Missingdensity}) that we know the expression of this density for a given the $\boldsymbol{\alpha}_j$. So it's just about maximizing the likelihood of this density on the $\boldsymbol{\alpha}_j$. This can be done with an Expectation-Maximization (EM) algorithm \cite{dempster1977maximum} or one of its extensions \cite{mclachlan2007algorithm}.
%			
%step E: ($\Theta$ stands for the parameters of the gaussian mixtures for the marginal distributions, estimated once by Mixmod):
%\begin{equation}
%	\boldsymbol{X}^{(h)}=E[\boldsymbol{X}|\boldsymbol{X}_{O},\boldsymbol{\alpha}^{(h)},\boldsymbol{\varepsilon},\Theta,S]
%\end{equation}			
%	
%	step M:	
%\begin{equation}
%	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}(\mathcal{L}(\boldsymbol{X}^{(h)},\boldsymbol{\alpha},\boldsymbol{\varepsilon},\Theta,S)) \textrm{ by OLS}
%\end{equation}	
%	
%		
%			But estimation of the $\boldsymbol{\alpha}_j$ is the most critical part of the MCMC in terms of computational time so it could be a bad idea to put there another iterative algorithm. 
%			Alternatives does exist :
%			\begin{itemize}
%				\item Because sub-regression are supposed to be parsimonious, we could imagine to estimate each column of $\boldsymbol{\alpha}$ with full sub-matrices of $\boldsymbol{X}_f$. When relying on too much missing values, $\hat{\boldsymbol{\alpha}}$ would be a bad candidate and then penalized directly by the likelihood (and it could be a good thing). Computational cost would be reduced significantly.
%				\item To estimate the $\boldsymbol{\alpha}_j$ (and not for the global likelihood) we could use data imputation (by the mean) and then obtain a full matrix but still ignoring missing values when estimating the likelihood. Imputation only concerns the estimation of the sub-regression coefficients and because null coefficients in sub-regression are coerced at each step, imputation only concerns a few covariates each time.
%			\end{itemize}
%			
%			
%			 $\forall j \in I_r$, estimation of $\boldsymbol{\alpha}^j$ only depends on individuals not missing in $\boldsymbol{X}^j$ (individuals are {\it iid}).
%			 So we work with a restriction of $\boldsymbol{X}$ for each $\boldsymbol{\alpha}^j$. Thus in this section, to simplify the notation, we will consider no missing values in $\boldsymbol{X}_r$ but in fact we work with restrictions.
%			
%			The EM algorithm can be written here: we start with some $\Theta^{(0)}=(\boldsymbol{\alpha},\boldsymbol{\varepsilon}) $ initial value for $\Theta$. The $\pi_{ij,k}$ are estimated once for each covariate (for example by Mixmod) and stay the same during the EM algorithm.
%			Naive E step : estimation of 
%			\begin{equation}
%				\boldsymbol{X}^{(h)}=E(\boldsymbol{X}|\boldsymbol{X}_{\bar M},\Theta^{(h)}) \textrm{ so it simply is}
%			\end{equation}
%			$\forall (i,j), \boldsymbol{M}_{i,j}=1, j\neq I_r$, 			
%			\begin{equation}
%				x^{(h)}_{i,j}=E(x_{i,j}|\boldsymbol{X}_{\bar M},\Theta^{(h)}) =\sum_{k=1}^{k_{ij}}\pi_{ij,k}\mu_{ij,k}^{(h)} \label{Estep}
%			\end{equation}
%			where, $\forall j \in I_f, k_{ij}=k_j, \pi_{ij,k}=\pi_{j,k}, \mu_{ij,k}=\mu_{j,k}$ \\
%			M-step : we determine $\Theta^{(h+1)}$ as the solution of the equation
%			\begin{equation}
%				E(\boldsymbol{X}|\Theta)=\boldsymbol{X}^{(h)} \textrm{ done by OLS}
%			\end{equation}
%			So the M step is just computing linear regressions on the filled dataset.
%			
%			
%		real E step : individuals are $iid$ so we just look at the expression for one individual, and use it for all
%		$\forall 1\leq n \leq n , \forall j \notin I_r$, we note $\bar{\boldsymbol{X}}_{i,j}=(\boldsymbol{X}_{\bar M}\cap \boldsymbol{X}_{i} \setminus \boldsymbol{X}^j)$ 
%			\begin{eqnarray}
%				P(\boldsymbol{X}_{fi}^M,\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M} | \Theta)&=&
%					P(\boldsymbol{X}_{fi}^M|\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M}|\Theta) \\
%				&=&P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M}|\Theta) \\
%				P(\boldsymbol{X}_{fi}^M|\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M},\Theta)&=&\frac{P(\boldsymbol{X}_{ri}^M|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M}|\Theta)}{P(\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M}|\Theta)} \\
%				&=&\frac{P(\boldsymbol{X}_{ri}^{O}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M}|\Theta)P(\boldsymbol{X}_{fi}^{\bar M}|\Theta)
%				}{
%				P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{\bar M}|\Theta)
%				} \\
%				&=&\frac{P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M}|\Theta)}{P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{\bar M},\Theta)}
%			\end{eqnarray}
%			
%			No imputation for missing left. Imputations for missing right are just used to obtain $\hat{\boldsymbol{\alpha}}$ but not when computing the $BIC$ or $BIC_+$.
%			
			
	\section{Missing values in the main regression}
		The easier way would be to draw missing values with the SEM described above and then use classical methods on the completed dataset, with the possibility to repeat this procedure a few times and then take the mean. We should for example try multiple draw and LASSO for variable selection like variable selection by random forest. One great advantage of multiple draw procedures is that it gives an idea of the precision of the imputations with the variance of these imputed values among the multiple draws. So we know whether it is reliable or not. 
		
		But another way would be to consider classical estimation methods as likelihood optimizer and then adapt them to the integrated likelihood of our model. Thus we can imagine to use LASSO without imputation. But the choice of the penalty using the LAR algorithm need also to adapt the LAR that is based on correlations that are computed on vectors with distinct number of individuals (due to missing values). So it requires more work but could be a good perspective for our method.
	\section{Numerical results on simulated datastets}
		\subsection{Estimation of the sub-regression coefficients}
			We take datasets from the experiments in part I and then we compare the MSE obtained on $\boldsymbol{\alpha}$ with our SEM to those obtain by classical OLS after imputation of the missing values by the marginal empirical means. Here $p=40$ and $n=30$, missing values position are generated randomly for each of the 100 datasets to obtain $10 \%$ of missing values each time. Thus we have 120 missing values and none of the datasets contain a full individual without missing values.
Both methods were tested with the true structure $S$. Initial value of $\boldsymbol{\alpha}$ for the SEM was the result of the method using imputation by the empirical mean. Only 10 iterations for the SEM after 2 warming steps with only 1 iteration for the Gibbs at each step.

\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/MSEalphaSEM.png} 
	\caption{MSE on $\boldsymbol{\alpha}$ is significantly lower and more robust with our SEM than with imputation by the mean.}\label{MSEalphaSEM}
\end{figure}

	We see (Figure \ref{MSEalphaSEM}) that our SEM is nearly 13 times more efficient in mean that estimation based on imputation by the mean. Our results are extremely good because each sub-regression is true and we have 30 individuals (even if missing values kind of reduce this number) to estimate 3 coefficients only each time. Although, using imputed values lead to learn a true regression with a factually incorrect dataset. Thus we should prefer to work without imputing the missing values but using the full generative model and the dependencies it implies. Imputation will always introduce some bias.

		
			\subsection{Imputation by the sub-regression}
			We have then imputed missing values in $\boldsymbol{X}^{I_r}$  by using the corresponding sub-regressions after $\boldsymbol{\alpha}$ has been estimated  by the SEM.
			Missing values in $\boldsymbol{X}^{I_f}$ are estimated by the mean of 50 Gibbs iterations after the SEM and 2 warming steps of the Gibbs. Results are shown in figure \ref{MSEXsubreg}.
			\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/MSEXsubreg.png} 
	\caption{MSE on $\hat{\boldsymbol{X}}$ is significantly lower and more robust when using our SEM than with imputation by the mean.}\label{MSEXsubreg}
\end{figure}
\FloatBarrier
		\subsection{Multiple imputation for the main regression}
		We use the previously imputed $\boldsymbol{X}$ to estimate $\boldsymbol{Y}$ with $\boldsymbol{\beta}=\boldsymbol{1}$ and $\sigma_Y=10$.
			\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/MSESEMYtoymean.png} 
	\caption{MSE on $\hat{\boldsymbol{Y}}$ are lower when using our SEM (blue) than with imputation by the mean (red) for the three model (complete, marginal and plug-in) using OLS or LASSO}\label{MSESEMYtoymean}
\end{figure}	

			\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/MSESEMYtoysd.png} 
	\caption{our SEM (blue) provides more robust results than imputation by the mean (red) but the variances are still too wide.}\label{MSESEMYtoysd}
\end{figure}		
We obtain on a validation sample of 1000 individuals a predictive MSE smaller in mean with our method (Figure \ref{MSESEMYtoymean}). But the variance are too important to really conclude (Figure \ref{MSESEMYtoysd}). We can say that imputation by SEM is more robust, but the Gibbs do not give satisfying results. Maybe the increase of the number of steps allowed by a code optimization would help to improve these results. For now, we can just say that our generative model significantly improves estimation of $\boldsymbol{\alpha}$ and make possible to find $S$ based on a dataset with missing values.

		One big advantage with our regression model is that it does not depend on the response variable $\boldsymbol{Y}$ so the structure can be learnt independently. Thus we can imagine to obtain big samples to learn the structure without being annoyed by the missing values. Then when a response variable is chosen, we can keep the same $S$ and use previously computed values of $\boldsymbol{\alpha}$ as initial value for the SEM. 

	\section{Missing values on real datasets}	
		To be able to evaluate the results on a real dataset, we have deleted some values in the production dataset from section \ref{sectionBV} to obtain $10\%$ of missing values. Figure \ref{missingBV} shows the pattern of the missing values (MCAR). It confirms that $10\%$ of missing values is sufficient to have no complete line or column in the dataset. 
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=350px]{figures/missingBV.png} 
		\caption{Graphical representation of the dataset with $10\%$ of missing values}\label{missingBV}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=350px]{figures/MSEXmissBV.png} 
		\caption{MSE on $\hat{\boldsymbol{X}}$ is $1.32$ times lower in mean when using our method (blue) than with imputation by the mean (red).}\label{MSEXmissBV}
	\end{figure}
	We see in figure \ref{MSEXmissBV} that our SEM gives a smaller MSE with a smaller variance than imputation by the mean. There is still a lot of work to do in the field of missing values and we have only walked the firsts steps of this huge perspective, but these first results are encouraging and make us feel like this orientation is worthy to be followed.

\chapter{CorReg: the concept}	
	
		The {\tt CorReg} package is already downloadable on the CRAN under CeCILL Licensing. This package permits to generate datasets according to our generative model, to estimate the structure (C++ code) of regression within a given dataset and to estimate both explicative and predictive model with many regression tools (OLS,stepwise,LASSO,elasticnet,clere,spike and slab, adaptive lasso and every models in the \textsc{lars} package). So every simulation presented above can be done with {\tt CorReg}.
	{\tt CorReg} also provides tools to interpret found structures and visualize the dataset (missing values and correlations). \\%More informations can be found on the website www.correg.org which is dedicated to \textsc{CorReg}. 
	The objective of {\tt CorReg} is also to bring recent statistical tools to engineers. Thus it will be made  available in Microsoft Excel for the end of the year 2014, probably using Basic Excel R Toolkit(BERT\footnote{https://github.com/StructuredDataLLC/Basic-Excel-R-Toolkit}). \\
	It also provides some small scripts put in functions to obtain graphical representations and basic statistics with legends for non-statistician with only one command line (or macro button in Excel).\\
	 One example of graphical tool is the {\tt matplot\_zone } function that allows to compare several curves according to a given function (input parameter) and was widely used to compare the MSE and complexities in this document. Another example is the {\tt recursive\_tree} function to plot classification and regression trees with basic statistics and legend but also to successively compute trees removing some correlated covariates or covariates that cannot be changed in the process to see if they are replaced by others more useful (this recursive aspect has given its name to the function).\\
	More features will be added as statistics will continue to be taught to engineers to provide ergonomic and powerful statistical tools to non-statisticians. 
\chapter{Conclusion and perspectives}
	\section{Conclusion}
		It is well-known that no model is the better in every situation. Here we propose two additional models (marginal and plug-in) but the best idea is to compare the full, marginal and plug-in and then choose the best for the study concerned. Our goal was not to replace any model but to enlarge the scope of statisticians in the real life. It is important to note that our model can be useful for interpretation even if the full model is chosen for interpretation, because we explicitly describe the correlations between the covariates. Moreover, it is only a pretreatment so it could easily be used with future statistical tools. \\
		Our model is easy to understand and to use. Usage of linear regression to model the correlations definitely separates us from "black boxes" so users are confident in what they do. The well-known and trivial sub-regression found comfort users in that if a structure does exist, {\tt CorReg} will find it so when a new sub-regression, or a new main regression is given they are more likely to look further and try it. The automated aspect shows the power of statistics without a priori so users begin to understand that statistics are not only descriptive or predictive but based on {\it a priori} models. This method seems to have a positive impact on the way users looks at the statistics (according to them). \\
			It is good to see that sequential methods (plug-in model) and automation can produce good results. Probabilistic models are efficient even without human expertise and let the experts improve the results by adding their expertise in the model (coercing some sub-regression for example). So we hope that statistics will continue to be a central tool for engineers.
		
		
	\section{Perspective}
		\subsection{Non-linear regression}
			Polynomial regression, logistic regression \cite{hosmer2000applied}, {\it etc.} might be improved by a method like this.
		\subsection{Pretreatment not only for regression}
			Classification and Regression Tree, and any other method could benefit of the variable selection pretreatment implied by our marginal model.
		\subsection{Improved programming}
			Even if it is written in C++, the algorithm could be optimized by a better usage of sparse matrices, memory usage optimization, and other small things that could reduce computational cost to be faster and allow to work with larger datasets (already works with thousands of covariates).
		\subsection{Missing values in classical methods}
			The full generative approach could be used to manage missing values without imputation for many classical methods.
			It can notably be used for clustering and not only in response variable prediction context.
		\subsection{Interpretation improvements}
			Ergonomy of the software could be improved to better fit industrial needs. This work is in progress and further work will be provided just after the redaction of this thesis to reach this goal.
\cleardoublepage

\addcontentsline{toc}{chapter}{References}
\bibliographystyle{apalike}
%\bibliographystyle{plain}
\nocite{*}
\bibliography{biblio}
%\addcontentsline{toc}{chapter}{Appendices}
%\appendix
%	\chapter{Graphs and CorReg}
%		\section{Matricial notations}
%		\section{Properties}
%		Toute matrice binaire est associable à une matrice d'adjacence d'un graphe orienté (DAG)
%		
%		Matrice nilpotente = carré nul
%		
%		carré d'une matrice d'adjacence => chemins de longueur 2
%		
%		donc matrices binaires nilpotentes = graphes orientés sans chemins de longueur 2 donc celui qui reçoit n'emet pas donc graphe bi-partie
%		
%		dénombrement : liste systématique des possibilités (par décompostion comme dans la hiérarchie)		
%		
%	\chapter{Mixture models}
%		\section{Linear combination}
%			
%		\section{Industrial examples}	
\end{document}
