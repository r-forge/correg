\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Clément THERY}
\title{\textsc{CorReg}: Linear regression with highly correlated covariates}
\begin{document}
\maketitle
\newpage
\itshape To my sons, \upshape
\tableofcontents
\chapter{Abstracts}
\chapter{Acknowledgments}
\chapter{The industrial context}
	This work takes place in a steel industry context. The main objective is to be able to solve quality crisis when they occur. In such a case, a new type of unknown quality issue is observed and we have no idea of its origin. The defects, even generated at the beginning of the process, are often detected in its last part. The steel-making process includes several sub-process, each implying a whole manufactory. Thus we have many covariates and no a priori on the relevant ones. Moreover, the values of each covariates essentially depends on the characteristics of the final product, and many physical laws and tuning models are implied in the process. Therefore the covariates are highly correlated.
	We have several constraints :
	\begin{itemize}
		\item To be able to predict the defect and stop the process as early as possible to gain time (and money)
		\item To be able to understand the origin of the defect to try to optimize the process
		\item To be able to find parameters that can be changed because the objective is not only to understand but to correct the problematic part of the process.
		\item It also must be fast and automatic (without any a priori).
	\end{itemize}
	We will see in the state of the art that correlations are a real issue and that the number of variables increases the problem.	
	The stakes are very high because of the high productivity of the steel plants but also because steel making is now well-known and optimized thus new defects only appears on innovative steels with high value. Any improvement on such crisis can have important impact on the market shares and when the customer is implied, each day won by the automation of the data mining process can lead to a gain of hundreds of thousands of euros, sometimes more. So we really need a kind of automatic method, able to manage the correlations without any a priori and giving an easily understandable and flexible model.
	
	
\chapter{State of the art}
In the following we note classical norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$, $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^p|\beta_i| $ and $\parallel\boldsymbol{\beta} \parallel_{\infty}=\operatorname{max}(|\beta_1|,\dots,|\beta_p|)$.
	\section{Ordinary least squares and associated problems}
	Linear regression is defined by this simple equation:
	\begin{equation}
		\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}_Y
	\end{equation}
	where $\boldsymbol{Y}\in \mathbf{R}^n$ is the response variable vector observed on $n$ individuals. 
	\section{Penalized models}
		\subsection{Ridge regression}
			\cite{hoerl1970ridge}
			\cite{marquardt1975ridge}
		\subsection{LASSO: Least Absolute Shrinkage and Selection Operator }
			\cite{tibshiranilasso}  
			\cite{tibshirani1996regression} 
			\cite{efron2004least} %LAR
			\cite{Zhao2006MSC}%problèmes du lasso/lars en correlations
			\cite{SAM10088}%lars necessite OLS en surcouche

		\subsection{Adaptative LASSO and Random LASSO}
			\cite{zou2006adaptive}% adaptative lasso
			\cite{wang2011random}%random lasso
		\subsection{Elasticnet}
			\cite{zou2005regularization}
		\subsection{OSCAR: Octogonal Shrinkage and Clustering Algorithm for Regression }
			%\cite{bondell2008simultaneous}%Oscar
			Like elasticnet, \textsc{OSCAR} \cite{bondell2008simultaneous} uses combination of two norms for its penalty. Here the objective is to group covariates with the same effect and give them exactly the same coefficient (reducing the dimension) with a simultaneous variable selection (group with null coefficient).
			\begin{equation}
				\hat{\boldsymbol{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \parallel\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta} \parallel^2 \textrm{ subject to } \sum_{j=1}^p|\beta_j|+c\sum_{j<k}\operatorname{max}(|\beta_j|,|\beta_k|) \leq \lambda		
			\end{equation}						
			
	\section{Modeling the parameters}	
		\subsection{CLERE: CLusterwise Effect REgression}
			\cite{yengo2012variable}%clere
		\subsection{Spike and Slab}	
			\cite{ishwaran2005spike}%spike and slab
	\section{Multiple Equations}
		\subsection{SEM and Path Analysis}
		\subsection{SUR: Seemingly Unrelated Regression}
			\cite{SURzellner}
		\subsection{SPRING: Structured selection of Primordial Relationships IN the General linear model}
			\cite{chiquetconf}			
			
		\subsection{Selvarclust: Linear regression within covariates for clustering}
			\cite{maugis2009variable}
			The idea is to allow covariates to have different roles : $(S,R,U,W)$.
			But:
			\begin{itemize}
				\item It is about clustering and not regression (not the same application field)
				\item No sub-regression allowed between relevant variables (in the True model)
				\item Using stepwise-like algorithm without protection against correlations \cite{raftery2006variable} even it is known to be often unstable \cite{miller2002subset}
			\end{itemize}	
			We provide an specific MCMC algorithm with the ability to have redundant covariates in the true model.		 
\part{\textsc{CorReg} : the concept}
\chapter{Decorrelating covariates by a generative model}
	\section{Generative model}
	\section{Properties}
		\subsection{general properties}
		\subsection{Identifiability}
	\section{About grouping effect}
\chapter{Estimation of the Structure of subregression by MCMC}
	\section{How to compare structures ?}
		\subsection{Bayesian criterion for quality}
		\subsection{Some indicators for proximity}
	\section{Neighbourhood}
		\subsection{Classical}
		\subsection{Active relaxation of the constraints}
	\section{The walk}
	\section{Numerical results}
\part{Further usage of the structure}	
\chapter{Taking back the residuals}
	\section{The model}
	\section{Properties}
	\section{Consistency}
		\subsection{Consistency Issues}\label{consistency}
		Consistency issues of the LASSO are well known and Zhao \cite{Zhao2006MSC} gives a very simple example to illustrate it.
		We have taken the same example to show how our method is more consistent.
		Here $p=3$ and $n=1000$.We define $\boldsymbol{X}_f,\boldsymbol{X}_r,\boldsymbol{\varepsilon}_Y,\boldsymbol{\varepsilon}_{X} i.i.d. \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}_n)$ and then $X_3=\frac{2}{3}X_1+\frac{2}{3}X_2+\frac{1}{3}\varepsilon_X$ and $Y=2X_1+3X_2+\varepsilon_Y$.
		We compare consistencies of complete,explicative and predictive model with LASSO (and LAR) for selection.
		It happens that the algorithm don't find the true structure but a permuted one so we also look at the results obtained with the true $S$ (but $\hat{B}$ is used) and with the structure found by the Markov chain after a few seconds.
		
		True $S$ is found $340$ times on $1000$ tries.
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & \textsc{CorReg} Explicative & \textsc{CorReg} Predictive \\ 
		\hline 
		True $S$ &  1.006479 & \textbf{1.005468} & \textbf{1.006093} \\ 
		\hline 
		$\hat{Z}$ & \textbf{1.006479} & 1.884175 & 1.006517 \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observer on a validation sample (1000 individuals)}
		\end{table}

		We observe as we hoped that explicative model is better when using true $S$ (coercing real zeros) and that explicative with $\hat{S}$ is penalized (coercing wrong coefficients to be zeros).
		But the main point is that the predictive model stay better than the classical one whith the true $S$ and corrects enough the explicative model to follow the classical LASSO closely when using $\hat{S}$. 
		And when we look at the consistency :
		\begin{table}[h!]	
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  0 & 1000 & 830 \\ 
		\hline 
		$\hat{S}$ & 0 & 340 & \textbf{621} \\ 
		\hline 
		\end{tabular} 
		\caption{number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}
		\end{table}				
		
		$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		
		We also made the same experiment but with $X_1,X_2$ (and consequently $X_3$) following gaussian mixtures (to improve identifiability) randomly generated by our \textsc{CorReg} package for R. 
		True $S$ is now found $714$ times on $1000$ tries \label{testidentifiable}. So it confirms that non-gaussian models are easier to identify.
		
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  1.571029 & \textbf{1.569559} & \textbf{1.570801} \\ 
		\hline 
		$\hat{S}$ & 1.005402 & 1.465768 & \textbf{1.005066} \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observed on a validation sample (1000 individuals)}
		\end{table}

		And when we look at the consistency :
		\begin{table}[h!]
		\centering	
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & Explicative & Predictive \\ 
		\hline 
		True $S$ &  0 & 1000 & 789 \\ 
		\hline 
		$\hat{S}$ & 0 & 714 & \textbf{608} \\ 
		\hline 
		\end{tabular} 
		\caption{number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}
		\end{table}				
				
		
		$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		

	\section{Numerical results}

\chapter{Missing values}
	Missing values are a very recurrent issue in industry. We note $M$ the binary matrix indicating whereas a value is missing (1) or not (0).
	\section{How to manage missing values in the MCMC ?}
			\subsection{Position of the missing value}

		In the MCMC we need to compute the likelihood of the structure. When missing values occurs, we restrict the likelihood to the known values. So we look at each non-missing value separately. Each value is seen as different random variable noted $x_{i,j}$.  But the structure itself makes things more complicated because known values are not all $iid$ (if $j \in I_2$ ).
			For a given structure $S$, missing values can imply different consequences according to their position in the dataset.
			To compute the likelihood of a value $x_{i,j}$ in the dataset :
			\begin{itemize}
				\item if $M_{i,j}=1$ : $x_{i,j}$ is not considered because we restrict the likelihood to known values.
				\item else if $j \in I_1$ : like in previous method, we use the density estimated  ($e.g.$ a Gaussian Mixture model estimated by \textsc{Mixmod}) for $X^j$. Values in $X^j$ are $iid$.
				\item else :
			\end{itemize}
			\begin{eqnarray}
				x_{i,j}| X_{i,I_1^j} &\sim & \sum_{\substack{1\leq k \leq p \\B_k^j\neq 0 } } x_{i,k}B_k^j  + \mathcal{N}(0;\sigma_j) \\
				&=& \sum_{\substack{1\leq k \leq p \\B_k^j\neq 0 \\ M_{i,k}\neq 0 } } x_{i,k}B_k^j  + \mathcal{N}(\sum_{\substack{1\leq k \leq p \\B_k^j\neq 0 \\ M_{i,k}= 0 } } x_{i,k}B_k^j \ ; \ \sigma_j) \label{Missingdensity}\\ 
			\end{eqnarray}
			
					
			
		\subsection{Weighted penalty}
			Now we have defined the way to compute the likelihood, other questions remain : how to define the number of parameters in the structure ?		How to take into account missingness (structures relying on highly missing covariates should be penalized) ?
			We have seen that for a same covariate $X^j$ with $ j \in I_2$, the number of parameters is not the same for each individual depending whether or not $M_{i,j}=0$. But the penalty (for $\psi=BIC$) can't be added at the individual level (because $\log(1)=0$ so it would be annihilated). 
			
			To penalize models that suppose dependencies based only on a few individuals, we propose to use the mean of the complexities obtained for a given covariate. Thus if a structure is only touched by one missing value the penalty will be smaller than another same shaped structure but with more missing values implied.
			Another way would be to use $\psi=RIC$ (see \cite{foster1994risk}) so the complexity is associated with $\log(p)$ and can be added individually. Another idea would be to make a compromise and penalize by $\frac{k_i\log(p)}{\log(n)}$.
		
%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	
		
		\subsection{Parameters estimation}
			Estimating $B$ with missing values is just estimating independent regressions with missing values. We have seen in equation (\ref{Missingdensity}) that we know the expression of this density for a given $B$. So it's just about maximizing the likelihood of this density on $B$. This can be done with an Expectation-Maximization (EM) \cite{dempster1977maximum} or one of its extensions \cite{mclachlan2007algorithm}.
			But estimation of $B$ is the mast critical part of the MCMC in terms of computational time so it could be dangerous to put there another iterative algorithm. 
			Alternatives does exist :
			\begin{itemize}
				\item Because sub-regression are supposed to be simple, we could imagine to use estimate each column of $B$ with full submatrices of $X$. When relying on too much missing values, $\hat{B}$ would be a bad candidates and then penalized directly by the likelihood (and it could be a good thing). Computational cost would be reduced significantly.
				\item To estimate $B$ (and not for the global likelihood) we could use data imputation (by the mean) and then obtain a full matrix but still ignoring missing values when estimating the likelihood.
			\end{itemize}
	\section{Missing values in the main regression}
		\subsection{explicative}
			The reduced model (explicative one) is just a linear regression without structure so the method used for $\hat{B}$ can also be used here. 
		\subsection{predictive}
			
			%si manquant à gauche : EM peut s'appuyer sur la loi conditionnelle sachant ce qui est à droite
\chapter{CorReg: the package and its application in steel industry}	
	\section{\textsc{CorReg} package for R}
	\textsc{CorReg} is already downloadable on the CRAN under CeCILL Licensing. This package permits to generate datasets according to our generative model, to estimate the structure (C++ code) of regression within a given dataset and to estimate both explicative and predictive model with many regression tools (OLS,stepwise,LASSO,elasticnet,clere,spike and slab, adaptative lasso and every models in the \textsc{lars} package). So every simulation presented above can be done with \textsc{CorReg}.
	\textsc{CorReg} also provides tools to interpreat found structures and visualize the dataset (missing values and correlations). More informations can be found on the website www.correg.org which is dedicated to \textsc{CorReg}.
	\section{Application in steel industry}
		\subsection{The dataset}
		\subsection{Found Structure}
		\subsection{Results}
\chapter{Conclusion and perspectives}
\chapter{References}
\bibliography{biblio}{ }
\bibliographystyle{plain}
\chapter{Appendices}
	\section{Graphs and CorReg}
		\subsection{Matricial notations}
		\subsection{Properties}
	\section{Mixture models}
		\subsection{Linear combination}
		\subsection{Industrial examples}	
\end{document}