\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Clément THERY}
\title{Model-based pretreatment for correlated datasets, Application to linear regression and missing values. \\ Real datasets from steel industry}
\begin{document}
\maketitle
\newpage
\itshape To my sons
\upshape


\chapter*{Résumé}
	Les travaux effectués durant cette thèse ont pour but de pouvoir pallier le problème des corrélations au sein des bases de données, particulièrement fréquentes dans le cadre des données industrielles. Une modélisation explicite des corrélations par un système de sous-régressions entre covariables permet de pointer les sources des corrélations et d'isoler certaines variables redondantes. 
	\\
	
	Il en découle une pré-sélection de variables nettement moins corrélées sans perte significative d'information et avec un fort potentiel explicatif (la préselection elle-même est expliquée par la structure de sous-régression qui est simple à comprendre car uniquement constituée de modèles linéaires). \\
	
	Un algorithme de recherche de structure de sous-régressions est proposé, basé sur un modèle génératif complet sur les données et utilisant une chaîne MCMC (Monte-Carlo Markov Chain). Ce prétraitement est utilisé pour la régression linéaire à des fins illustratives mais ne dépend pas de la variable réponse et peut donc être utilisé de manière générale pour toute problématique de corrélations.\\
	
	Par suite, le modèle génératif complet peut être utilisé pour gérer d'éventuelles valeurs manquantes dans les données, tant pour la recherche de structure que pour de l'imputation multiple préalable à l'utilisation de méthodes classiques incompatibles avec la présence de valeurs manquantes. Cela permet également d'estimer les valeurs manquantes et de fournir un intervalle de confiance sur leur estimation.
	Encore une fois, la régression linéaire vient illustrer l'apport de la méthode qui reste cependant générique et applicable à d'autres contextes tels que le clustering.
	\\
	
	Enfin, un estimateur plug-in pour la régression linéaire est proposé pour ré-injecter les variables redondantes de manière séquentielle et donc utiliser toute l'information sans souffrir des corrélations entre covariables.
	\\
	
	Tout au long de ces travaux, l'accent est mis principalement sur l'interprétabilité des résultats en raison du caractère industriel du financement de cette thèse. 
\\	

	Le package R intitulé CorReg, disponible sur le CRAN\footnote{http://cran.r-project.org} sous licence CeCILL\footnote{http://www.cecill.info}, implémente les méthodes développées durant cette thèse.
	
\paragraph{Mots clés:}Prétraitement, Régression, Corrélations, Valeurs manquantes, MCMC, modèle génératif, Critère Bayésien, sélection de variable, méthode séquentielle.
\chapter*{Abstract}
	This thesis was motivated by correlation issues in real datasets, in particular industrial datasets. The main idea stands in explicit modeling of the correlations between covariates by a structure of sub-regression, that simply is a system of linear regression between the covariates. It points out redundant covariates that can be deleted in a pre-selection step to improve matrix conditonning without significant loss of information and with strong explicative potential because this pre-selection is explained by the structure of sub-regression, itself easy to interpret.
	\\
	
	An algorithm to find the sub-regression structure inherent to the dataset is provided, based on full generative model and using Monte-Carlo Markov Chain (MCMC) method. This pretreatment is then illustrated on linear regression to show its efficiency but does not depend on a response variable and thus can be used in a more general way with any correlated datasets.
	\\
	
	The generative model defined here allows to manage missing values both during the MCMC and then for imputation (for example multiple imputation) to be able to use classical methods that are not compatible with missing datasets. Missing values can be imputed with a confidence interval to show estimation accuracy. Once again, linear regression is used to illustrate the benefits of this method but it remains a pretreatment that can be used in other contexts, like clustering and so on.
	\\ 
	
	Finally a plug-in estimator is defined to get back the redundant covariates sequentially. Then all the covariates are used but the sequential approach act as a protection against correlations.
\\

	The industrial motivation of this work define interpretation as a stronghold at each step. 	
	
	The R package CorReg, is on CRAN\footnote{http://cran.r-project.org} now under CeCILL\footnote{http://www.cecill.info} license. It implements the methods created during this thesis.
	
	 	
\paragraph{Keywords:} Pretreatment, Regression, Correlations, Missing values, MCMC, generative model, Bayesian Criterion, variable selection, plug-in method,\dots
\chapter*{Résumé substantiel en français}
	en gros, un article en français
\chapter*{Acknowledgments}

% ArcelorMittal pour le financement et la confiance

% encadrants soutien moral technique patience

% collègues inria soutien technique package

% famille ?


\tableofcontents

\chapter{The industrial context}
	\paragraph{Abstract:} Ce chapître explique les contraintes industrielles qui ont orienté les travaux pour répondre aux demandes d'ArcelorMittal qui est le commanditaire de ces travaux de recherche.
\subsection{Steelmaking process}
	This work takes place in a steel industry context.
	Steelmaking starts from raw materials to give highly specific products.
	
	
		
	parler du process de manière  linéaire	
	\begin{center}
          \begin{tabular}{ccc}
         \includegraphics[width=130px,height=130px]{figures/liquid.jpg} & \includegraphics[width=130px,height=130px]{figures/Brame1.jpg} & \includegraphics[width=130px,height=130px]{figures/Brame.jpg} \\
          	\includegraphics[width=130px,height=130px]{figures/ecras_moy.jpg} &\includegraphics[width=130px,height=130px]{figures/tcc2.jpg} & \includegraphics[width=130px,height=130px]{figures/bobines.jpg}
          \end{tabular}
        \end{center}

	faire remarquer la longueur du process et le nombre de paramètres.
	Faire remarquer les corrélations
	Conclure avec le côté innovant de la sidérurgie puis transition vers la recherche et donc vers la thèse

	\subsection{Impact of the industrial context}
	 The main objective is to be able to solve quality crisis when they occur. In such a case, a new type of unknown quality issue is observed and we may have no idea of its origin. The defects, even generated at the beginning of the process, are often detected in its last part. The steel-making process includes several sub-process, each implying a whole plant. Thus we have many covariates and no a priori on the relevant ones. Moreover, the values of each covariates essentially depends on the characteristics of the final product, and many physical laws and tuning models are implied in the process. Therefore the covariates are highly correlated.
	We have several constraints :
	\begin{itemize}
		\item To be able to predict the defect and stop the process as early as possible to gain time (and money)
		\item To be able to understand the origin of the defect to try to optimize the process
		\item To be able to find parameters that can be changed because the objective is not only to understand but to correct the problematic part of the process.
		\item It also must be fast and automatic (without any a priori).
	\end{itemize}
	We will see in the state of the art that correlations are a real issue and that the number of variables increases the problem.	
	The stakes are very high because of the high productivity of the steel plants but also because steel making is now well-known and optimized thus new defects only appears on innovative steels with high value. Any improvement on such crisis can have important impact on the market shares and when the customer is implied, each day won by the automation of the data mining process can lead to a gain of hundreds of thousands of euros, sometimes more. So we really need a kind of automatic method, able to manage the correlations without any a priori and giving an easily understandable and flexible model.
	
\chapter{State of the art}
\paragraph{Abstract:} Rapide aperçu de ce qui existe déjà pour tenter de répondre à notre problématique.
	\section{Linear regression}
		\subsection{Historic interest}
			méthode ancienne et reconnue, remonte aux origines des statistiques, méthode pionnière en prédiction.
		\subsection{Simplicity}
			Facile à mettre en oeuvre théoriquement, rapide en pratique et présent partout (même dans Excel)
			Très simple à interpréter, principe intuitif.
			donne tout de suite l'impact des variables (positif ou négatif) sur la réponse et leur poids (si scaled dataset) 
			
			C'est ici qu'on peut mettre le principe de la régression linéaire (image d'un nuage de point et d'une droite qui le traverse)
		\subsection{Industrial context}
			Manque d'arriere plan statistique
			besoin de comprendre pour corriger
			défaut de confiance dans les statistiques d'où besoni accru en interprétation
			La régression s'y prete bien car déjà connue et utilisée par chacun dans Excel (parfois à tort à travers).
			
			
			Industrial context is often poor in statistical background and the stakes are frequently very high in terms of financial impact. 
		These two points give strong constraints because methods used has to be accessible for non-statistician in a minimum amount of time and results obtained have to be clearly interpreated (no black-box) because if industrial experts don't understand the result, they will not trust it and then they will not use it. So a powerfull tool without interpretation becomes kind of useless in such a context.
		
		Every engineer, even non-statistician use frequently linear regression to seek relationship between some covariates. It is easy to understand, fast to do, it can be done directly in Microsoft Excel that remains the most used software in industry and the software used by engineers to open most of the datasets.
		 
		 Regression appears to be the basis of industrial statistics so we have chosen to work in this way. As of 2014 Google Scholar proposes more than $3.8$ millions of papers related to regression and many of them were cited several thousands times. It is an old strategy well known and with many derivative. 
		 It's simplicity facilitates a wide spread usage in industry and other fields of application.
			
			
		\subsection{Flexibility and future of regression}
			Richesse des types de régression et des méthodes d'estimation
			On peut faire des choses plus pointues en adaptant un peu le modèle mais en conservant la simplicité : Multilevel Regression \cite{moerbeek2003comparison,maas2004robustness,hox1998multilevel}




\paragraph{Notations:}	
In the following we note classical (respectively $L_2,L_1,L_{\infty}$) norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^p(\beta_i)^2$, $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^p|\beta_i| $ and $\parallel\boldsymbol{\beta} \parallel_{\infty}=\operatorname{max}(|\beta_1|,\dots,|\beta_p|)$. Vectors are in bold characters.
	\section{Ordinary least squares and associated problems}\label{sectionOLS}		% ne pas oublier de mentionner les packages existants

We note the linear regression model:
\begin{equation}
		\boldsymbol{Y}_{|\boldsymbol{X}}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \label{regressionsimple}
	\end{equation}
	where $\boldsymbol{X}$ is the $n\times p$ matrix of the explicative variables,% (that is a sub-matrix of $\tilde{\boldsymbol{X}}$ the $n\times \tilde{p}$ matrix of provided covariates)
	 $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $p\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{OLS}): %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaOLS}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1} \label{eqOLS}
	\end{equation}
	and without any bias \cite{saporta2006probabilites,dodge2004analyse}. In fact it is the Best Linear Unbiased Estimator (BLUE).
	The theoretical MSE is given by
	\begin{equation}
	E[\textsc{MSE}(\hat{\boldsymbol{\beta}}_{OLS}|\boldsymbol{X})]= 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1})
	\end{equation}
	\\
	Equation \ref{regressionsimple}	has no intercept but can be generalized by adding to $\boldsymbol{X}$ a first column full of 1. So we don't consider the intercept to simplify notations. In practice, an intercept is added by default.
	
	Ordinary Least Squares find a $p-$dimensional hyperplane that minimizes the distance with each individual $(\boldsymbol{X}_i,Y_i)$. It can be written
	\begin{equation}
		\boldsymbol{\hat{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace
	\end{equation}
	So estimation of $\boldsymbol{Y}$ by OLS can be viewed as a projection onto the linear space spanned by the regressors $\boldsymbol{X}$ as shown in figure \ref{geomOLS}.% that is in public domain\footnote{"OLS geometric interpretation" by Stpasha - Own work. Licensed under Public domain via Wikimedia Commons - http://commons.wikimedia.org/wiki/File:OLS\_ geometric\_ interpretation.svg\#mediaviewer/File:OLS\_geometric\_interpretation.svg}.
	\begin{figure}[h!]
	\centering
	\includegraphics[width=250px]{figures/OLS_geometric_interpretation.png}
	\caption{Multiple linear regression with Ordinary Least Squares. Public domain image.} \label{geomOLS}
	\end{figure}
	
	%problème
	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
For a fixed number $n$ of individuals, conditionning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects: 
\begin{itemize}
	\item The dimension $p$ (number of covariates) of the model (the more covariates you have the greater variance you get)
	\item The correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .
\end{itemize}
	When correlations between covariates are strong, the matrix to invert is ill-conditioned and the variance increases, giving unstable and unusable estimator \cite{hoerl1970ridge}.
	Another problem is that matrix inversion requires to have more individuals than covariates ($n\geq p$).
	When matrices are not invertible, classical packages like the function lm of R base package \cite{packagebase} use the Moore-Penrose pseudoinverse \cite{PSP:2043984} to generalize OLS.
		
		
	Last but not least, Ordinary Least Squares is unbiased but if some $\beta_i$ are null (irrelevant covariates) the corresponding $\hat{\beta}_i$ will only asymptotically tend to 0 so the number of covariates in the estimated model remains $p$. This is a major issue because we are searching for a statistical tool able to work without a priori on a big dataset containing many irrelevant datasets. Pointing out some relevant covariate and how they impact the response really is the main goal here. We will need a variable selection method one moment or another. It could be as a pretreatment, during coefficient estimation or by post-treatment.
	
\paragraph{Running example:} we look at a simple case with $p=5$ variables defined by four independent scaled Gaussian $\mathcal{N}(0,1)$ named $\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_4,\boldsymbol{x}_5$ and $\boldsymbol{x}_3=\boldsymbol{x}_1+\boldsymbol{x}_2+\boldsymbol{\varepsilon}_3$ where $\boldsymbol{\varepsilon}_3\sim{\mathcal{N}(\boldsymbol{0},\sigma_3^2\boldsymbol{I}_n)}$. We also define two {\it scenarii} for $\boldsymbol{Y}$ with $\boldsymbol{\beta}=(1,1,1,1,1)$ and $\sigma_Y \in \{10,20\}$. So there is no intercept (can be seen as a null intercept).
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_3$ gets smaller.
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEOLS.png}
	  \caption{Evolution of theoretical Mean Squared error on $\hat{\boldsymbol{\beta}}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEOLS1}
	\end{figure}
	
	Figure \ref{MQEOLS1} shows the theoretical MSE obtained on $\hat{\boldsymbol{\beta}}$ with OLS. These results are based on equation \ref{eqOLS}, we show the mean obtained after 100 experiences computed on our running example.
	
	The $R^2$ stands for:
	\begin{equation}\label{defR2}
	R^2=1-\frac{\sum_i (y_i - x_i\beta)^2}{\sum_i (y_i-\bar{y})^2}
	\end{equation}
	where $\bar{\boldsymbol{Y}}=\frac{1}{n}\sum_{i=1}^n y_i $
	
	
	Many other estimation methods were created to obtain better estimations by playing on the bias/variance tradeoff or by making additionnal hypothesis.
	To have an easier comparison, we also look at the empiric MSE obtained on $\hat{\boldsymbol{\beta}}$.
		
	
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/OLScompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEOLScompl}
	\end{figure}	
	We first observe that real results are better from expected (Figure \ref{MQEOLScompl}). This comes from usage of $QR$ decomposition to inverse matrices, that are less impacted by ill-conditioned matrices. Our package CorReg also uses this decomposition. But the correlations issue remains and so do the impact of $n$ and $\sigma_Y$
	
	\section{Penalized models}
	
	We have seen that OLS is the Best linear Unbiased Estimator for $\hat{\boldsymbol{\beta}}$, meaning that it has the minimum variance. But it remains possible to play with the bias/variance tradeoff to reduce the variance by adding some bias. The underlying idea is that a small bias and a small variance could be preferred to a huge variance without bias. Many methods do this by a penalization on  $\hat{\boldsymbol{\beta}}$.  Some of them propose an effective variable selection.
		\subsection{Ridge regression}		% ne pas oublier de mentionner les packages existants

			%\cite{hoerl1970ridge}
			%\cite{marquardt1975ridge}
Ridge regression \cite{hoerl1970ridge,marquardt1975ridge} proposes a biased estimator for $\boldsymbol{\beta}$ that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \lambda \textrm{ with } \lambda>0
	\end{equation}
	But this penalty is not guided by the correlations. It introduce an additional parameter $\lambda$ to choose for the whole dataset  whereas correlations may concern only some of the covariates with several intensities.
	
	The solution of the ridge regression is given by
	\begin{equation}
		 \hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} -\lambda\boldsymbol{I}_n\right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaridge}
	\end{equation}
	and we see in this equation that a global modification of $\boldsymbol{X}'\boldsymbol{X}$ is done for a given $\lambda$. Methods does exist to automatically choose a good value for $\lambda$ \cite{cule2013ridge,er2013systematic} and a R package called \textsc{ridge} is on CRAN \cite{packageridge}. 
	We have computed the same experiment as in previous figure but with the ridge package instead of OLS. It is clear that the ridge regression is efficient in variance reduction (it is what it is built for).
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/ridgecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEridgecompl}
	\end{figure}
	
	
	%It is the same for each covariates and will be too large for independent covariates and/or too small for correlated ones. So the efficiency of such a method is limited. 
	Moreover, like OLS, coefficients tend to 0 but don't reach 0 so it gives difficult interpretations for large values of $p$. Ridge regression is efficient to improve conditionning of the estimator but gives no clue to the origin of ill-conditioning and keep irrelevant covariates. It remains a good candidate for prediction-only studies. 
	
			
		\subsection{LASSO: Least Absolute Shrinkage and Selection Operator }		% ne pas oublier de mentionner les packages existants

			%\cite{tibshiranilasso}  
			%\cite{tibshirani1996regression} 
			%\cite{efron2004least} %LAR
			%\cite{Zhao2006MSC}%problèmes du lasso/lars en correlations
			%\cite{SAM10088}%lars necessite OLS en surcouche
The Least Absolute Shrinkage and Selection Operator (\textsc{LASSO} \cite{tibshirani1996regression,tibshiranilasso}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$ instead of the $L_2$ penalty of the ridge regression:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0
		\end{equation}	
		
		\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/lassocircles.png} 
			\caption{Geometric view of the Penalty for the LASSO}
		\end{figure}
		
		
détailler le graphique
		
		Here again we have to choose a value for $\lambda$.
	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) algorithm offers a very efficient way to obtain the whole LASSO path and is very attractive. It requires only the same order of magnitude of computational effort as \textsc{OLS} applied to the full set of covariates. But like the ridge regression, the penalty does not distinguish correlated and independent covariates so there is no guarantee to have less correlated covariates. In practice, we know that the LASSO faces consistency issues when confronted to correlated covariates \cite{Zhao2006MSC}. When two covariates are correlated, it tends to keep only one of them. For example, if two covariates are equal and have the same effect, the LASSO will keep only one of them. As explained earlier, variable selection is a real stake for us but is necessary to have a good interpretation. The LASSO does not distinguish a covariate not selected because it is totally redundant with another that was selected from an irrelevant covariate. And that is a problem. This consistency issu is illustrated in section \ref{consistency}.


		% ne pas oublier de mentionner les packages existants

			%\cite{zou2006adaptive}% adaptive lasso
			%\cite{wang2011random}%random lasso
			 Some recent variants of the \textsc{LASSO} do exist for the choice of the penalization coefficient like the adaptive \textsc{LASSO} \cite{zou2006adaptive} or the random \textsc{LASSO} \cite{wang2011random}.  But the consistency issues remains because it is still the same model. Only the choices of $\lambda$ differ.
			 
			 It is notable that the main goal of the LASSO is to select some covariate, thus the penalization is just a mean to achieve selection. But estimation of $\hat{\boldsymbol{\beta}}$ can be improved by a second estimation with OLS based only on selected covariates \cite{SAM10088}.
		\subsection{Least Angle Regression}
			Schéma de la géométrie qui motive le nom de la méthode
			
			sortie graphique du package lars avec les chemins d'évolution des coefficients.
			
			Explication du choix par validation croisée et de la surcouche OLS
			
			Résultats comparatifs en modifiant le A du running example pour voir que validation croisée est mieux que le BIC dans ce cas là (au passage, prendre un cas non consistant).	 
			 	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/larcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{lar}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQElarcompl}
	\end{figure}
	
	
		\subsection{Elasticnet}		% ne pas oublier de mentionner les packages existants

			Elastic net \cite{zou2005regularization} is a method developed to be a compromise between Ridge regression and the \textsc{LASSO} by mixing both $L_1$ and $L_2$ penalties: 
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	%Elastic net can be writt"en:
	\begin{eqnarray}
		\boldsymbol{\hat{\beta}}&=&(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta} \parallel_2^2 \right\rbrace \textrm{ subject to} \nonumber \\
			 & &(1-\alpha)\parallel\boldsymbol{\beta}\parallel_1+\alpha\parallel\boldsymbol{\beta}\parallel_2^2\leq t \textrm{ for some } t
	\end{eqnarray}
	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	
	
	\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/elasticnetcircles.png} 
			\caption{Geometric view of the Penalty for elasticnet}
		\end{figure}	
	
	%It seems to give good predictions. 
	But it is based on the grouping effect so correlated covariates get similar coefficients and are selected together whereas LASSO will choose between one of them and will then obtain same predictions with a more parsimonious model. Once again, nothing specifically aims to reduce the correlations. 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/elasticnetcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{elasticnet}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEelasticnetcompl}
	\end{figure}
	
		
	
		\subsection{OSCAR: Octogonal Shrinkage and Clustering Algorithm for Regression}		% ne pas oublier de mentionner les packages existants

			%\cite{bondell2008simultaneous}%Oscar
			Like elasticnet, \textsc{OSCAR} \cite{bondell2008simultaneous} uses combination of two norms for its penalty. Here the objective is to group covariates with the same effect (by a pairwise $L_\infty$ norm) and give them exactly the same coefficient (reducing the dimension) with a simultaneous variable selection (implied by the $L_1$ norm).
			\begin{equation}
				\hat{\boldsymbol{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \parallel\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta} \parallel^2_2 \textrm{ subject to } \sum_{j=1}^p|\beta_j|+c\sum_{j<k}\operatorname{max}(|\beta_j|,|\beta_k|) \leq \lambda		
			\end{equation}						
			But \textsc{OSCAR} depends on two tuning parameters: $c$ anf $\lambda$. For a fixed $c$ the $\lambda$ can be found by the \textsc{LAR} algorithm but $c$ still has to be found "by hand" comparing final models for many values of $c$.
			
\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/oscarcircles.png} 
			\caption{Geometric view of the Penalty for OSCAR}
		\end{figure} 
		détailler le graphique			
			
			
			Correlations are only implicitly taken into account and only pairwise. So it lacks of an efficient algorithm and need a supplementary study to interpret the groups found.
		
		
	\section{Modeling the parameters}			% ne pas oublier de mentionner les packages existants

		\subsection{CLERE: CLusterwise Effect REgression}		% ne pas oublier de mentionner les packages existants

			The CLusterwise Effect REgression (\textsc{CLERE} \cite{yengo2012variable}) describes the $\beta_j$ no longer as fixed effect parameters but as unobserved independant random variables with grouped $\beta_j$ following a Gaussian Mixture distribution. The idea is to hope that the model have a small number of groups of covariates and that the mixture will have few enough components to have a number of parameters to estimate significantly lower than $p$. In such a case, it improves interpretability and ability to yeld reliable prediction with a smaller variance on $\boldsymbol{\hat{\beta}}$. 
	
		\subsection{Spike and Slab}			% ne pas oublier de mentionner les packages existants

			Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the $\beta_j$ and gives a subset of covariates (not grouped) on which to compute \textsc{OLS} but has no specific protection against correlations issues.
			
Mettre un dessin du spike et du slab			
			
			
	\section{Miscellaneous}	
	\subsection{Principal Component Regression}
	\cite{jackson2005user}
	\subsection{Partial Least Squares (PLS) Regression}
	\cite{abdi2003partial,geladi1986partial}
	\subsection{Non-parametric regression}
	\cite{eubank1999nonparametric,hardle1990applied}
		Non conforme aux exigences d'interprétabilité
	\subsection{Sliced Inverse Regression}
		It is a semi-parametric approach that could be seen as easier to interpret than general non-parametric regression. But it is not sufficient in terms of ease of use (after estimation) for non-statisticians.
		\cite{li1991sliced,saracco1999regression}
	\subsection{General Linear Model (GLM)}
		\cite{kiebel2003general,wickens2004general,nelder1972generalized,mccullagh1989generalized}
		Generalise le modèle linéaire, assez classique dans les outils statistiques, mais pas assez immédiat en interprétation par rapport au modèle linéaire classique ou bien aux arbres de décision (transition vers la section suivante sur CART)
\subsection{Classification and Regression Trees (CART)}
		\cite{breiman1984classification}%\cite{quinlan1986induction} à vérifier
		Bonne méthode pour l'industrie
		Son problème principale est le cas linéaire... donc bon complément à notre modèle.
		CorReg propose un outil de mise en oeuvre rapide et avec légendes ainsi qu'une fonctionnalité de sélection pour mieux analyser les problématiques de corrélations.	
		Gros avantage: les données mixtes	
	
	\subsection{Neural networks}	
		Neural networks \cite{fausett1994fundamentals} seems to be really powerful but it has a predictive only goal and the model obtained can't be interpreted easily (and can't be interpreted at all if too complex). So it does not correspond to our needs. Interpretation remains our first goal and prediction comes far behind.
		
	\subsection{Bayesian networks}
		\cite{heckerman1995learning,jensen2007bayesian,friedman2000using}
		Bayesian networks are quite good in terms of interpretation but suffer from great dimension and require to transform the dataset arbitrary (discretisation), that imply a loss of information and usage of a priori (that is explicitely not suitable in our industrial context. 
		
		
	

		\section{Choice of model}
		% ne pas oublier de mentionner les packages existants
			\subsection{Cross validation}
				\cite{kohavi1995study,arlot2010survey}
			\subsection{Information Criterion}
			
			\cite{BIChuard}
			\subsection{stepwise}
			\cite{seber2012linear,miller2002subset}
			
				 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/stepwisecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{stepwise}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEstepwisecompl}
	\end{figure}
	
	
			\subsection{bootstrap}
			\cite{efron1979bootstrap,efron1994introduction}
		\section{SEM}
		\section{MCMC}
		\cite{gilks1996markov,chib1995understanding,roberts2001optimal}
		\section{Gibbs}
		\cite{casella1992explaining}
		\section{Industrial tools}
			Linear regression, decision trees, bayesian networks, neural network but without confidence
			Presence of softwares sent as non-statistical methods, based on rules (derivatives of decision trees).			
			metre image du flou vert et rouge avec encadré
			
			
	\section{Multiple Equations}		% ne pas oublier de mentionner les packages existants
		
		\subsection{SEM and Path Analysis}		% ne pas oublier de mentionner les packages existants
			\cite{davidson1993estimation,pearl2000causality,pearl1998graphs,brito2006graphical}%mcdonald2002principles
		\subsection{SUR: Seemingly Unrelated Regression}		% ne pas oublier de mentionner les packages existants

			\cite{SURzellner}
		\subsection{SPRING: Structured selection of Primordial Relationships IN the General linear model}		% ne pas oublier de mentionner les packages existants

			\cite{chiquetconf}			
			
		\subsection{Selvarclust: Linear regression within covariates for clustering}		% ne pas oublier de mentionner les packages existants

			\cite{maugis2009variable}
			The idea is to allow covariates to have different roles : $(S,R,U,W)$.
			But:
			\begin{itemize}
				\item It is about clustering and not regression (not the same application field)
				\item No sub-regression allowed between relevant variables (in the True model)
				\item Using stepwise-like algorithm without protection against correlations \cite{raftery2006variable} even it is known to be often unstable \cite{miller2002subset}
			\end{itemize}
			In this work we propose to adapt this model for linear regression and to use it as a pretreatment on correlated covariates.
			We provide a specific MCMC algorithm with the ability to have redundant covariates in the true model.		 
			
			
			
\part{Pretreatment for correlations}
\chapter{Decorrelating covariates by a generative model}
\paragraph{Abstract:} Nous modélisons explicitement les corrélations entre covariables par un système de régressions linéaires entre covariables. Cela permet une meilleure compréhension des données mais aussi une préselection de variables mettant de côté les variables redondantes pour réduire fortement les corrélations tout en ne perdant que peu d'information. La préselection prend un sens particulier grâce à la structure de sous-régression qui permet de distinguer par suite les variables indépendantes de la variable réponse de celles qui sont juste redondantes mais potentiellement liées à la variable réponse.
\\

\section{Our proposal: modelisation of the correlations}
	Let $\boldsymbol{X}$ be a $n \times p$ matrix of observed covariates and $\boldsymbol{Y}$ be the $n \times 1$ matrix of the observed response variable. In the following, we note $\boldsymbol{X}^j$ the $j^{th}$ column of $\boldsymbol{X}$ and $\boldsymbol{X}^{J}$ where $J=\{j_1,\dots,j_k\}$ the $n\times k$ sub-matrix of $\boldsymbol{X}$ composed by the columns of $\boldsymbol{X}$ whose indices are in the set $J$. 
\\

We make the hypothesis that $\boldsymbol{X}$ can be described by a partition $\boldsymbol{X}=(\boldsymbol{X}^{I_f},\boldsymbol{X}^{I_r}) $ given by an explicit structure $S$ where variables in the $n\times p_r$ sub-matrix $\boldsymbol{X}^{I_r}$ are redundant endogenous covariates resulting from linear sub-regressions based on $\boldsymbol{X}^{I_f}$, the  $n\times (p-p_r)$ sub-matrix of free (mutually independent) exogenous covariates.
So we model the correlations by $P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f}) $ with $\boldsymbol{X}^{I_f}$ orthogonal covariates.
 
 

The structure $S$ of $p_r$ sub-regressions within correlated covariates in $\boldsymbol{X}$ is described by:
	\begin{equation}
		\boldsymbol{X}^{I_r}_{|\boldsymbol{X}^{I_f},S} \textrm{ defined by }\forall j \in I_r: \boldsymbol{X}^j_{|\boldsymbol{X}^{I_f},S}=\boldsymbol{X}^{I_f}\boldsymbol{\alpha}^j+\boldsymbol{\varepsilon}^j \textrm{ with } \boldsymbol{\varepsilon}^j \sim\mathcal{N}(\boldsymbol{0},\sigma^2_j\boldsymbol{I}_n) \label{SR}
	\end{equation}
		where $\boldsymbol{\alpha}^j \in \mathcal{R}^{(p-p_r)}$ are the sparse vectors of the regression coefficients between the covariates (each sub-regression freely implies different covariates). 
We also define $I_f=\{I_f^1,\dots,I_f^p \}$ the set of the sets of indices of exogenous covariates with
\begin{eqnarray}
	\forall j \in I_r, I_f^j&=&\{i|\boldsymbol{\alpha}^j_i\neq 0 \} \\
	\forall j \notin I_r, I_f^j&=&\emptyset .
\end{eqnarray}
%We see that $I_f$ defines the non-null coefficients in $\boldsymbol{\alpha}_j$ (each sub-regression can be very parsimonious).
Then we have the explicit structure characterized by $S=\{I_f,I_r,\boldsymbol{p}_f,p_r\}$ where $p_r=|I_r|$, $\boldsymbol{p}_f=(p_f^1,\dots,p_f^{p_r})$ is the vector of the number of covariates in each sub-regression  and $p_f^j=|I_f^j|$, with $|.|$ the cardinal of an ensemble. 
\\
\\


The partition of $\boldsymbol{X}$ implies the uncrossing rule  $\boldsymbol{X}^{I_r} \cap \boldsymbol{X}^{I_f}$ 
{\it i.e.} endogenous variables don't explain other covariates. This hypothesis ensures that $S$ contains no cycle and is straightforward readable (no need to order the sub-regressions). It is not so restrictive because cyclic structures have no sense and any non-cyclic structure can be associated with a structure that verifies the uncrossing constraint by just successively replacing endogenous covariates by their sub-regression when they are also exogenous in some other sub-regressions.

	
	  We make the choice to distinguish the response variable from the other endogenous variables (that are on the left of a sub-regression). Thus we have one regression on the response variable ($P(\boldsymbol{Y}|\boldsymbol{X}))$ and a system of sub-regressions (without the response variable: $P(\boldsymbol{X}_r|\boldsymbol{X}_f,S)$). Then we consider correlations between the explicative covariates of the main regression, not between the residuals. We see that the $S$ does not depend on $\boldsymbol{Y}$ so it can be learnt independently, even with a larger dataset (if missing values in $\boldsymbol{Y}$).
	 
The structure obtained gives a system of linear regression that can be viewed as a recursive Simultaneous Equation Model (\textsc{SEM})\cite{davidson1993estimation} \cite{TIMM}.
\\
We note $\boldsymbol{\alpha}$ the $(p-p_r)\times p_r$ matrix of the  $\boldsymbol{\alpha}^j$.
  	Here we suppose the $\boldsymbol{\varepsilon}_j$ independent but in other cases \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) takes into account correlations between residuals \textsc{SUR} (Seemingly Unrelated Regression \cite{SURzellner}) and could be used to estimate $\boldsymbol{\alpha}$. 
		 
	 
\paragraph{In the running example:}$\boldsymbol{X}_r=\boldsymbol{x}_3$, $\boldsymbol{X}_f=\{\boldsymbol{x}_1,\boldsymbol{x}_2,\boldsymbol{x}_4,\boldsymbol{x}_5 \}$, $p_r=1$ and $\boldsymbol{\alpha}_3=(1,1,0,0)'$ and we have $S= \left( \{ \{1,2\}\},\{3\},(2),(1)\right)$


\section{Graph theory}\label{sectiongraph}
	We can model $S$ by a Directed Acyclic Graph (DAG) whose vertices are the $p$ covariates and arcs are the link between them described by the adjacency matrix $\boldsymbol{G}$ \cite{bondy1976graph}. This adjacency matrix is a binary $p\times p$ matrix with $\boldsymbol{G}_{i,j}=1$ if and only if $i \in I_f^j$ that is $\boldsymbol{X}^j$ is explained by $\boldsymbol{X}^i$ and can also be seen as $\boldsymbol{\alpha}_i^j\neq 0$.
	
	Graphical representation of $S$ helps to understand it and can be compared to the bayesian network representation. It helps to interprete the structure has also been used to construct the algorithm to find $S$ (chapter \ref{chapterMCMC}).


	
	The partition of $\boldsymbol{X}$ mean that the associated graph is bipartite (vertices follow a partition $(\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f})$) with arcs only going from $\boldsymbol{X}^{I_f}$ to $\boldsymbol{X}^{I_r}$.
	
	We know (\cite{biggs1993algebraic})as a classical result of graph theory that the power of adjacency matrices give the paths in the graph: $\boldsymbol{G}^k_{i,j}\neq 0$ means that there is a path of length $k$ going from $\boldsymbol{X}^i$ to $\boldsymbol{X}^j$. Because the graph is bipartite and arcs are only going from $\boldsymbol{X}^{I_f}$ to $\boldsymbol{X}^{I_r}$ we can deduce that $\boldsymbol{G}$ is nilpotent: $\boldsymbol{G}^2=0$. And we have the following result: every binary nilpotent matrix of order 2 can be seen as an adjacency matrix of a structure that respects the uncrossing rule. proof by contradiction: if there exist a path of length 2 between some vertices $i$ and $j$ then $\boldsymbol{G}^2_{i,j}\neq 0$ so the matrix is not nilpotent of order 2. We can deduce that the number of feasible structure with $p$ covariates is the number of binary nilpotent matrix of order 2.
	
	We see that $\boldsymbol{G}$ completely describe $S$ and that the sparse storage of $G$ gives $I_f$ which is sufficient to obtain $S$ by doing $\forall 1\leq j\leq p :  p_f^j=|I_f^j|$, $I_r=\{j |p_f^j>0 \}$ and $p_r=|I_r|$.
	This decomposition helps us to enumerate all the feasible structure (and thus all the binary nilpotent matrix of order 2).\\
	We note $\mathcal{S}_p$ the set of the feasible structure with $p$ covariates. If we consider all the structure with equiprobability:
	\begin{eqnarray}
		S&=&\{I_f,I_r,\boldsymbol{p}_f,p_r\} \\
		P(S|p_r)&=&P(I_f , \boldsymbol{p}_f|I_r,p_r)P(I_r|p_r) \\
		&=&P(\boldsymbol{p}_f|I_f, I_r,p_r)P(I_f |I_r,p_r)P(I_r|p_r) \\
		&=&P(I_f |I_r,p_r)P(I_r|p_r) \\
		%&=&\sum_{p_r=0}^{p-1}P(I_f |I_r,p_r)P(I_r|p_r)\\
		P(I_r|p_r)&=& \frac{1}{{p \choose p_r}} \\
		P(I_f|I_r,p_r)&=& \frac{1}{(2^{p-p_r}-1)^{p_r}}\\
		|\mathcal{S}_p|&=&\sum_{p_r=0}^{p-1}|\mathcal{S}_{p|p_r}|= \sum_{p_r=0}^{p-1}\frac{1}{P(S|p_r)} =\sum_{p_r=0}^{p-1}{p \choose p_r}(2^{p-p_r}-1)^{p_r}
	\end{eqnarray}
	where \begin{equation}
	{n \choose k}=\frac{n!}{k!(n-k)!}
	\end{equation} is the binomial coefficient.\\
	We have then $|\mathcal{S}_2| =3 $,$|\mathcal{S}_3| =13 $ and $|\mathcal{S}_{10}| >13.26\times10^9 $ so the number of feasible structures really explodes when $p$ is growing.

\paragraph{In the running example:} $|\mathcal{S}_5| =841 $

\begin{figure}[h!]
	\centering
	\includegraphics[width=200px]{figures/graphetoy.png} 
	\caption{The bipartite graph associated to the running example}
	\end{figure}	

\begin{displaymath}\left(	\begin{array}{ccccc}
	0 & 0 & 1 & 0 & 0 \\ 
	0 & 0 & 1 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0
	\end{array} \right)
\end{displaymath}



\section{A by-product model: marginal regression with decorrelated covariates}
Now we know $P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},S)$ by the structure of sub-regressions, we are able to define a marginal regression model $P(\boldsymbol{Y}|\boldsymbol{X}^{I_f},S)$ based on the reduced set of independent covariates $\hat{\boldsymbol{\beta}}_f$ without significant information loss. We use the information of the correlations structure to rewrite the true model without bias in the marginal space defined by the independent covariates.
 	\\
Using the partition $\boldsymbol{X}=[\boldsymbol{X}^{I_f},\boldsymbol{X}^{I_f}]$ we can rewrite (\ref{regressionsimple}):
	\begin{equation}
			\boldsymbol{Y}_{|\boldsymbol{X}^{I_f},\boldsymbol{X}^{I_r},S}=\boldsymbol{X}^{I_f}\boldsymbol{\beta}_{I_f}+\boldsymbol{X}^{I_r}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon_Y} \label{MainR}
		\end{equation}
		where $\boldsymbol{\beta}=(\boldsymbol{\beta}_{I_f},\boldsymbol{\beta}_{I_r}) \in  \mathcal{R}^p$ is the vector of the regression coefficients associated respectively to $\boldsymbol{X}^{I_f}$ and $\boldsymbol{I}_n$ the identity matrix. 
We note that (\ref{SR}) and (\ref{MainR}) give also by simple integration on $\boldsymbol{X}^{I_r}$ a marginal regression model on $\boldsymbol{Y}$ {\it depending only on uncorrelated covariates $\boldsymbol{X}^{I_f}$}:
\begin{eqnarray}
		P(\boldsymbol{Y}|\boldsymbol{X}^{I_f})&=& \int_{\boldsymbol{X}^{I_r}}P(\boldsymbol{Y}|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f})P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f}) d \boldsymbol{X} \\
	\boldsymbol{Y}_{|\boldsymbol{X}^{I_f},S}&=&\boldsymbol{X}^{I_f} (\boldsymbol{\beta}_{I_f}+ \sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j)+  \sum_{j \in I_r}\beta_{j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{Trueexpl} \\
	&=&\boldsymbol{X}^{I_f}\boldsymbol{\beta}_{I_f}^*+\boldsymbol{\varepsilon}_Y^*\label{modexpl}
\end{eqnarray}
 This model is still the true model and OLS estimator will still give an unbiased estimator, but its variance will be reduced by both dimension reduction and decorrelation (variables in $\boldsymbol{X}^{I_f}$ are independent so the matrix $\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f}$ will be well-conditioned). So the information given by the structure $S$ allows to reduce the variance without adding bias, by simple marginalization.
\\
Nevertheless, to be able to compare the bias-variance tradeoff, we can see this model as a variable pre-selection independent of the response in $\boldsymbol{Y}_{|\boldsymbol{X}}$.
We note that it is simply a linear regression on some of the original covariates so we only made a pre-treatment on the dataset by selecting $\boldsymbol{X}^{I_f}$ because of the correlations given by $S$. So we also get the model
\begin{equation}
\boldsymbol{Y}_{|\boldsymbol{X},S}=\boldsymbol{X}\boldsymbol{\beta}^*+\boldsymbol{\varepsilon}_Y^* \textrm{ where }\boldsymbol{\beta}^*=(\boldsymbol{\beta}_{I_f}^*,\boldsymbol{\beta}_{I_r}^*) \textrm{ and } \boldsymbol{\beta}_{I_r}^*=\boldsymbol{0}
\end{equation}
	for which OLS estimator of the coefficients may be biased.  

\paragraph{Running example:} $\boldsymbol{Y}_{|\boldsymbol{X}^{I_f}}= 2\boldsymbol{x}_1+2\boldsymbol{x}_2+\boldsymbol{x}_4+\boldsymbol{x}_5+\boldsymbol{\varepsilon}_3 +\boldsymbol{\varepsilon}_Y$
\section{Strategy of use: pre-treatment before classical estimation/selection methods}\label{interpretation}

As a pre-treatment, the model allows usage of any method in a second time to estimate $\boldsymbol{\beta}_{I_f}^*$, even with variable selection methods like LASSO or a best subset algorithm like stepwise \cite{seber2012linear}. However, we always have $\boldsymbol{X}^{I_r}=\boldsymbol{0}$

After selection and estimation we will obtain a model with { \it two steps of variable selection}: the decorrelation step by marginalization(coerced selection associated to redundant information defined in $S$) and the classical selection step, with different meanings for obtained zeros in $\hat{\boldsymbol{\beta}}^*_{I_f}$ (irrelevant covariates) and for $\hat{\boldsymbol{\beta}}^*_{I_r}=0$ (redundant information). 
 Thus we are able to distinguish the reasons of selection and consistency issues don't mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.


The explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset and to take better actions. Expert knowledge can even be added to the structure, physical models for example.

Moreover, the uncrossing constraint (partition of $\boldsymbol{X}$) guarantee to keep a simple structure easily interpretable (no cycles and no chain-effect) and straightforward readable.

	
			There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypothesis. We just play on the traditional bias-variance tradeoff.
\chapter{Numerical results with a known structure}	
\paragraph{Abstract:} Premiers résultats numériques pour une structure (hors coefficients de sous-régression) connue. On constate un net apport de la méthode de préselection.
		 
	\section{Illustration of the tradeoff conveyed by the pre-treatment}	
	We compare the OLS estimator on $\boldsymbol{X}$ defined in section \ref{sectionOLS} with the estimator obtained by the pre-treatment that is $\boldsymbol{X}^{I_f}$ selection.
  
For the marginal regression model defined in (\ref{modexpl})
%	\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}_f}= \boldsymbol{X}_f\boldsymbol{\beta}_f^*+ \boldsymbol{\varepsilon}_Y^*
%	\end{equation}			
%		So 
we have the \textsc{OLS} unbiased estimator of $\boldsymbol{\beta}^*$: 
		\begin{equation}
			\hat{\boldsymbol{\beta}}_{I_f}^* = (\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f})^{-1}\boldsymbol{X}^{I_f'}\boldsymbol{Y}  \textrm{ and }\boldsymbol{\hat\beta}_{I_r}^* = \boldsymbol{0}
		\end{equation}
		We see in (\ref{Trueexpl}) that it gives an unbiased estimation of $\boldsymbol{Y}$ and $\boldsymbol{\beta^*}$
		but in terms of $\boldsymbol{\beta}$ this estimator is biased:
		\begin{equation}
			\operatorname{E}[\hat{\boldsymbol{\beta}}_{I_f}^*|\boldsymbol{X}^{I_f}]=\boldsymbol{\beta}_{I_f}+\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \textrm{ and }\operatorname{E}[\hat{\boldsymbol{\beta}}_{I_r}^*|\boldsymbol{X}^{I_f}]=\boldsymbol{0}
		\end{equation}
		with variance:
		\begin{equation}
			\operatorname{Var}[\hat{\boldsymbol{\beta}}_{I_f}^*|\boldsymbol{X}^{I_f}]= (\sigma^2_Y+\sum_{j \in I_r}\sigma^2_{j}\beta_{j}^2 )(\boldsymbol{X}^{I_f'} \boldsymbol{X}^{I_f})^{-1}  \textrm{ and }\operatorname{Var}[\hat{\boldsymbol{\beta}}_{I_r}^*|\boldsymbol{X}^{I_f}]= \boldsymbol{0} 
		\end{equation}
		We see that the variance is reduced compared to OLS described in equation (\ref{eqOLS})(no correlations and smaller matrix give better conditioning ) for small values of $\sigma_j$ $i.e.$ strong correlations. So we play on the bias-variance tradeoff, reducing the variance by adding a bias. 				  
		  
		  
	 The theoretical Mean Squared Error (\textsc{MSE}) on $\hat{\boldsymbol{\beta}}$ is:
	\begin{eqnarray}
		E[\textsc{MSE}(\hat{\boldsymbol{\beta}}|\boldsymbol{X})]&=&\parallel \operatorname{Bias}\parallel_2^2+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}})) \\
			E[\textsc{MSE}(\hat{\boldsymbol{\beta}}_{OLS}|\boldsymbol{X})]&=& 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}) %\textrm{ for OLS, and then for the marginal model:}
			 \\
			E[\textsc{MSE}(\hat{\boldsymbol{\beta}}^*_{OLS}|\boldsymbol{X})]&=& \parallel\sum_{j \in I_r}\beta_{j}\boldsymbol{\alpha}_j \parallel_2^2 +\parallel \boldsymbol{\beta}_{I_r}\parallel^2_2 + (\sigma^2_Y+\sum_{j \in I_2}\sigma^2_{j}\beta_{j}^2 ) \operatorname{Tr}((\boldsymbol{X}^{I_f'} \boldsymbol{X}^{I_f})^{-1})
	\end{eqnarray}	 
	To better illustrate the bias-variance tradeoff, we look at the running example. We observe the theoretical Mean Squared Error (MSE) of the estimator of both OLS and \textsc{CorReg}'s marginal  model for several values of $\sigma_3$ (strength of the sub-regression) and $n$. Figure \ref{MQEexplOLSp5col} shows the theoretical MSE evolution with the strength of the sub-regression. In this section, all experiences have been made 100 times to obtain smooth curves. So we have generated 100 times $\boldsymbol{X}$ and $\boldsymbol{Y}$
%	\begin{equation}
%		1-\mathcal{R}^2=\frac{\operatorname{Var}(\boldsymbol{\varepsilon)_3}}{\operatorname{Var}(\boldsymbol{x}_3)}=\frac{\sigma_3^2}{\sigma_3^2+2}
%	\end{equation}
\begin{figure}[h!]
%	\begin{minipage}[l]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn15sigmaY10.png} 
%			\caption{For $n=15$. Dotted: \textsc{Correg}, plain: OLS}\label{MQE1}
%	\end{minipage} \hfill
%	\begin{minipage}[c]{.32\linewidth}
%			\includegraphics[ width=170px]{figures/MQEn100sigmaY10.png} 
%			\caption{For $n=100$. Dotted: \textsc{Correg}, plain: OLS}
%	\end{minipage} \hfill
%   \begin{minipage}[r]{.32\linewidth}
%			\includegraphics[width=170px]{figures/MQEn1000sigmaY10.png} 
%			\caption{For $n=1000$. Dotted: \textsc{Correg}, plain: OLS.} \label{MQE3}
%   \end{minipage} 
	\includegraphics[width=500px]{figures/MQEexplOLSp5col.png}\label{MQEexplOLSp5col}
	\caption{Theoretical MSE on $\hat{\boldsymbol{\beta}}$ of OLS (red) and CorReg's marginal model(blue) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$.}
\end{figure} 
It is clear in Figure \ref{MQEexplOLSp5col} that the marginal model is more robust than \textsc{OLS} on $\boldsymbol{X}$. And when sub-regression get weaker ($\mathcal{R}^2$ tends to 0) it remains stable until extreme values (sub-regression nearly fully explained by the noise). We also see that the error implied by strong correlations shrinks with the rise of $n$. 
We see that $\sigma_Y$ multiplies $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}))=\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{I_f}))+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{I_r}))$ for both models but for the marginal model $\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{I_r}))=0$.
 Thus, when $\sigma_Y^2$ rises it increases the advantage of \textsc{CorReg} versus \textsc{OLS}. It illustrates the importance of dimension reduction when the model has a strong noise (very usual case on real datasets where true model is not even exactly linear). 
 
But it is only the theoretical MSE and we want to know what happens in the real life. So we look at the empirical MSE on both $\hat{\boldsymbol{\beta}}$ and $\hat{\boldsymbol{Y}}$. Here again, each configuration is computed 100 times and we take the mean to smooth the curves. The MSE on $\hat{\boldsymbol{Y}}$ is computed on a validation sample with 1 000 individuals. Our marginal model remains better for strong correlations.\\

\section{Observed MSE comparison}
	We also look at the observed MSE on both $\boldsymbol{\beta}$ and $\boldsymbol{Y}$ for some of the methods depicted above.
	\subsection{On the running example}
 \begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/OLSexpl.png}\label{MSEOLSexpl}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}$ of OLS (red) and CorReg's marginal model(blue) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/OLSYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEOLSYexpl}
	\end{figure}
	We see in figures \ref{MSEOLSexpl} and \ref{MQEOLSYexpl} that MSE on $\hat{\boldsymbol{Y}}_{LASSO}$ give the same results as those on $\hat{\boldsymbol{\beta}}$.
	
	
 \begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/larexpl.png}\label{MSElarexpl}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}$ of LASSO with LAR on both $\boldsymbol{X}$ (red) and CorReg's marginal $\boldsymbol{X}^{I_f}$ (blue for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/larYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{LASSO}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQElarYexpl}
	\end{figure}
	Figure \ref{MSElarexpl}  and \ref{} show that variable selection done the LASSO gives a biased $\hat{\boldsymbol{\beta}}$ by setting some coefficients to 0 but strong correlations makes this bias neutral for prediction. Here the LASSO tends to propose the same model as we do  with our marginal model, but without explanation. We will see  later that it is not sufficient in higher dimension.
%ridge
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/ridgeexpl.png}\label{MSEridgexpl}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}_{ridge}$ on both $\boldsymbol{X}$ (red) and CorReg's marginal $\boldsymbol{X}^{I_f}$ (blue for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/ridgeYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEridgeYexpl}
	\end{figure}	
Here again (figures \ref{MSEridgexpl} and \ref{MQEridgeYexpl}), ridge regression provides good results for this running example. But we will see later that high dimension reduces the efficiency of the ridge regression when some covariates begin to be irrelevant or not enough relevant.  
 %elasticnet
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/elasticnetexpl.png}\label{MSEelasticnetexpl}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}_{elasticnet}$ on both $\boldsymbol{X}$ (red) and CorReg's marginal $\boldsymbol{X}^{I_f}$ (blue for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/elasticnetYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{elasticnet}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEelasticnetYexpl}
	\end{figure}
  %stepwise
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/stepwiseexpl.png}\label{MSEstepwiseexpl}
	\caption{Observed MSE on $\hat{\boldsymbol{\beta}}_{stepwise}$ on both $\boldsymbol{X}$ (red) and CorReg's marginal $\boldsymbol{X}^{I_f}$ (blue for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $p=5$ covariates.}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/stepwiseYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{stepwise}$ with the strength of the correlations for various sample sizes and strength of regression. $p=5$ covariates. } \label{MQEstepwiseYexpl}
	\end{figure}
	
 Further results are provided in sections \ref{sectionsimul} and \ref{sectionrealcase}.
	
%	\section{Predictive efficiency}
%		Résultats comme dans l'article	mais sur vrai S	
%		modèles avec beaucoup de variables (car rapide vu que pas de MCMC).
%		Faire avec vrai modèle= modèle complet puis X1 uniquement puis X2 uniquement puis aussi Amax=15 si p largement supérieur à 100
\chapter{Estimation of the Structure of subregression by MCMC}\label{chapterMCMC}
\paragraph{Abstract:} La structure de sous-régression est supposée inconnue. Il nous faut donc la trouver. Un algorithme de type MCMC est proposé pour résoudre cette problématique. La mise en oeuvre de celui-ci passe par une modélisation complète du jeu de données et nous pousse à introduire un nouveau critère de choix de modèle tenant compte du nombre de modèles testés.
\\
\\

\section{Bayesian approach}
Structural equations models like \textsc{SEM} are often used in social sciences and economy where a structure is supposed "by hand" but here we want to find it automatically. Graphical LASSO \cite{friedman2008sparse} offers a method to obtain a structure based on the precision matrix (inverse of the variance-covariance matrix), setting some coefficients of the precision matrix to zero (see section \ref{sectionGlasso}). But the resulting matrix is symmetric and we need an oriented structure for $S$ to avoid cycles.

Cross-validation is very time-consuming and thus not friendly with combinatory problematics. Moreover, we need a criterion compatible with structures of different sizes (varying $p_r$) and not related with $\boldsymbol{Y}$ because the structure is inherent to $\boldsymbol{X}$ only. Thus it must be a global criterion. 	
Because it is about model selection, we decide to follow a Bayesian approach (\cite{raftery1995bayesian}, \cite{andrieu1999joint},\cite{chipman2001practical}).  
	
We want to find the most probable structure $S$ knowing the dataset, so we search for the structure that maximizes $P(S|\boldsymbol{X})$ and we have:	
	\begin{eqnarray}
	 \label{approxBIC} P(S|\boldsymbol{X})&\propto & P(\boldsymbol{X}|S)P(S)
	=P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},S)P(\boldsymbol{X}^{I_f}|S)P(S)
	\end{eqnarray}
So we will try to maximize $\psi(\boldsymbol{X},S)=P(\boldsymbol{X}|S)P(S)$. It will be done with a Markov chain Monte-Carlo algorithm (MCMC).
	

\section{Sub-regression structure in details}
	\subsection{Modeling the uncorrelated covariates: a full generative approach on $P(\boldsymbol{X})$} \label{sectionfullgen}
	To be able to compare structures with $P(S|\boldsymbol{X})$, we need a full generative model on $\boldsymbol{X}$. Sub-regressions give $P(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},S) $ but $P(\boldsymbol{X}^{I_f}|S)$ is still undefined. We suppose that variables in $\boldsymbol{X}^{I_f}$ follow Gaussian mixtures of $K_j >0$ components: 
	\begin{equation}
			\forall \boldsymbol{X}^j \notin \boldsymbol{X}^{I_r} : \boldsymbol{X}^j_{|S} \sim f(\boldsymbol{\theta}_j)=\mathcal{GM}(\boldsymbol{\pi}_j;\boldsymbol{\mu}_j;\boldsymbol{\sigma}^2_j) \textrm{ with } \boldsymbol{\pi}_j,\boldsymbol{\mu}_j,\boldsymbol{\sigma}^2_j \textrm{ vectors of size } K_j. \label{mixtureX1}
		\end{equation}
		The great flexibility \cite{mclachlan2004finite} of such models makes our model more robust. Gaussian case is just a special case ($K_j=1$) of Gaussian mixture so it is included in our hypothesis.
		
	%Thus if one have some hypothesis on the distribution of some variables (exponentially distributed for example) it is possible to use it without impacting the model in other ways. %compute corresponding $\psi$ according to it. %and then improve the walk (will keep a structure only if it is really relevant).%and give it as an input of \textsc{CorReg} and then improve the efficiency of the algorithm (it will find a structure only if it is really relevant).
	We now have a full generative model on $\boldsymbol{X}$.
	
\subsection{Identifiability of the structure} \label{preuveident}
	The model presented above relies on a discrete structure $S$ between the covariates. But to find it we need identifiability property to insure that we will asymptotically find the true model. Identifiability of the structure is asked in following terms: Is it possible to find another structure $\tilde{S}$ of linear regression between the covariates leading to the same joint distribution and marginal distributions? 
	
		If there are exact sub-regressions ($\exists j, \sigma^2_j=0$), the structure won't be identifiable. But it only means that several structure will have the same likelihood and they will have the same interpretation. So it's not really a problem. Moreover, when an exact sub-regression is found, we can delete one of the implied variables without any loss of information and the structure will define a list of variable from which to delete. \textsc{CorReg} (Our R package) prints a warning to point out exact regressions when found.
	In the followings we suppose $\forall j, \sigma^2_j\neq 0$, then $\boldsymbol{X}^{I_f'}\boldsymbol{X}^{I_f}$ and $\boldsymbol{X}'\boldsymbol{X}$ are of full rank (but the later is ill-conditioned for small values of $\sigma^2_j$).
	\\
	
Our full generative model is a $p$-sized Gaussian mixture model of $K$ distinct components and 
%	\begin{equation}
%	f(\boldsymbol{X}|K,S)=f(\boldsymbol{X}_f|K,S)f(\boldsymbol{X}_r|\boldsymbol{X}_f,S)
%	\end{equation}	
%	
	can be seen as a $\mathbf{SR}$ model defined by Maugis \cite{maugis2009variable}. In this section, $S$ will denote the set of variable as in the paper from Maugis and we call Gaussian mixtures the Gaussian mixtures with at least two distinct components. The equivalence with Maugis's model is defined by:
	$\boldsymbol{X}^{I_r}=\boldsymbol{y}^{S^c}$ and $\boldsymbol{X}^{I_f}=\boldsymbol{y}^R$. We have supposed independence between variables in $\boldsymbol{X}^{I_f}$ so the identifiability theorem from Maugis tells that our model is identifiable if variables in $\boldsymbol{X}^{I_f}$ are Gaussian mixtures.% (what we supposed in section \ref{sectionfullgen}).
	\\
	
	
%First, we observe that if each variable in $\boldsymbol{X}_r$ is a Gaussian mixture, then there must be at least one Gaussian mixture on the right of each sub-regression. 
We define $\boldsymbol{X}^G \subsetneq \boldsymbol{X}^{I_f}$ containing Gaussian variables and we note the Gaussian mixtures $\boldsymbol{X}^{G^c}\neq \emptyset$ its complement in $\boldsymbol{X}^{I_f}$.
We suppose that variables in $\boldsymbol{X}^{I_r}$ are all Gaussian mixtures. It implies that $\forall j  \in I_r,\exists i \in I_f^j $ so that $\boldsymbol{X}^i \subset \boldsymbol{X}^{G^c} $ since any linear combination of Gaussian variable would only give a Gaussian (so each sub-regression contain at least one Gaussian mixture as a regressor).
\\
	We introduce the matricial notation
		$\boldsymbol{X}^{I_r}=\boldsymbol{X}^{I_f}\boldsymbol{\alpha} + \boldsymbol{\varepsilon}$ where
		 $\boldsymbol{\alpha}$ is the $(p-p_r)\times p_r$ matrix whose columns are the $\boldsymbol{\alpha}_j$ and $\boldsymbol{\varepsilon}$ is the $n\times p_r$ matrix whose columns are the $\boldsymbol{\varepsilon}_j$
		%\\We note $\Theta$ the parameter of the model.We want to know if $P(\boldsymbol{X}|S,\Theta)=P(\boldsymbol{X}|\tilde{S},\tilde{\Theta})$ does imply $(S,\Theta)=(\tilde{S},\tilde{\Theta})$.
		
The theorem from Maugis guarantee that a sub-regression between Gaussian mixtures is identifiable in terms of which one is regressed by others.
		\begin{eqnarray}
%			\forall j \in I_r, \boldsymbol{X}^j_{|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_j^G+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c}+ \boldsymbol{\varepsilon}_j \\
%			\boldsymbol{X}^j_{|\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c} + \tilde{\boldsymbol{\varepsilon}}_j \textrm{ is identifiable where} \\
%			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_j^{G}+ \boldsymbol{\varepsilon}_j \textrm{ is Gaussian. 
		 \boldsymbol{X}_{r|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_G+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_{G^c}+ \boldsymbol{\varepsilon} \\
			\boldsymbol{X}_{r|\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_{G^c} + \tilde{\boldsymbol{\varepsilon}} \textrm{ is identifiable where} \\
			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_j^{G}+ \boldsymbol{\varepsilon}_j \textrm{ is Gaussian.}  
		\end{eqnarray}
		So a sufficient condition for identifiability is to have at least one Gaussian mixture in each sub-regression.	
		It implies then that: $\forall j \in I_r, \boldsymbol{X}^{j} $ is a Gaussian mixture and $\exists i \in I_f^j,\boldsymbol{X}^{i} \subset \boldsymbol{X}^G $.  
	
\paragraph{Remark:} Identifiability of $S$ is not necessary to use a given structure but helps to find it.
In the followings, true $S$ is supposed to be identifiable (at least one Gaussian mixture in each sub-regression).
		

\section{Sub-regression model selection}	

			\subsection{Bayesian criterion for quality}
		Our full generative generative model allows us to compare structures with criterions like the Bayesian Information Criterion ($BIC$) which penalize the log-likelihood of the joint law on $\boldsymbol{X}$ according to the complexity of the structure~\cite{BIChuard}. \\
			 We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context.
		In the followings we use the $BIC$, that is more classical.
		\subsection{Penalization of the integrated likelihood by $P(S)$} \label{compstruct}
When considering (\ref{approxBIC}) we see that uniform law on $P(S)$ gives $\psi(\boldsymbol{X},S)\propto P(\boldsymbol{X}|S)$ so it is equivalent to a minimization of the $BIC$.
	We note $\boldsymbol{\Theta}$ the set of the parameters of the generative model
	\begin{eqnarray}
		-2\log P(\boldsymbol{X}|S)&\approx & BIC=-2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+|\boldsymbol{\Theta}|\log(n)  
	\end{eqnarray}
	But $BIC$ tends to give too complex structures because we test a great range of models and the number of model compared is not taken into account\cite{massart2007concentration}. Thus we choose to penalise the complexity a bit more. We don't want to modify the $BIC$ to keep its properties.
	
We have the explicit structure characterized by $S=\{I_f,I_r,p_f,p_r\}$, then we suppose a hierarchical uniform {\it a priori} distribution $P(S)=P(I_f | \boldsymbol{p}_f,I_r,p_r)P(\boldsymbol{p}_f|I_r,p_r)P(I_r|p_r)P(p_r)$  instead of the simple uniform law on $S$ that is generally used and provides no penalty.
It goes against the fact that the number of models with $p_r$ sub-regressions and the number of possible combination for each sub-regression depends on $p_r$ and thus provides distinct penalties according to the complexity. 
	 Thus we have :
		\begin{eqnarray}
		BIC_+(X|S)&=&BIC(X|S) -\ln(P(S)) \label{Bicstar}
	\end{eqnarray}		
	The hierarchical uniform hypothesis gives:
	\begin{eqnarray}
	P(S)&=&P(I_f | \boldsymbol{p}_f,I_r,p_r)P(\boldsymbol{p}_f|I_r,p_r)P(I_r|p_r)P(p_r) \textrm{ with} \\
	P(p_r)&=&\frac{1}{p} \\
	P(I_r|p_r)&=& \frac{1}{ { p \choose p_r} }\\
	P(\boldsymbol{p}_f|I_r,p_r)&=& \frac{1}{p_r \times \frac{1}{p-p_r}}=\frac{p-p_r}{p_r}\\
	P(I_f | \boldsymbol{p}_f,I_r,p_r)&=&\frac{1}{\prod_{j \in I_r}{ p-p_r \choose p_f^j }}  
	\end{eqnarray}
	instead of $P(S)=\frac{1}{|\mathcal{S}_p|}$ as defined in section \ref{sectiongraph} for the classical uniform hypothesis.
	It increases penalty on complexity for $p_r\leq\frac{p}{2}$ and $p_f^j\leq\frac{p}{2}$ because probability of a complex model is under-estimated. Hence %when using $BIC*$ 
	this constraint on $\hat{p}_r$ and $\hat{p}_f^j$ is given in the research algorithm when the Hierarchical Uniform hypothesis is made instead of Uniform one in numerical experiments (section \ref{sectionsimul} and \ref{sectionrealcase}).
		$BIC_+$ does not change $BIC$ but only $P(S)$ so the properties of $BIC_+$ are the same as classical $BIC$ but we obtain better results when the constraints on the complexity are verified.  %With the Hierarchical Uniform hypothesis we maximize $\psi(\boldsymbol{X},S)\approx BIC + P(S)$.


%	 
%	
		\subsection{Some indicators for proximity}
		The first criterion is $\psi(\boldsymbol{X},S)$ which is maximized in the MCMC. But in our case, it is estimated by the likelihood (see (\ref{approxBIC}))whose value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $S$ we define some indicators to compare the true model $S$ and the found one $\hat{S}$.
			Global indicators :
			\begin{itemize}
				\item $TL$ (True left) : the number of found dependent variables that really are dependent\\ $TL=|I_r\cap \hat{I}_r|$ 
				\item $WL$ (Wrong left) : the number of found dependent variables that are not dependent \\ $WL=|\hat{I}_r|-TL$
				\item $ML$ (Missing left) : the number of really dependent variables not found\\ $ML=|I_r|-TL$
				\item $\Delta p_r$ : the gap between the number of sub-regression in both model: \\ $\Delta p_r=|I_r|-|\hat{I}_r|$. The sign defines if $\hat{S}$ is too complex or too simple compared to the true model
				\item $\Delta compl$ : the difference in complexity between both model: \\$\Delta compl=\sum_{j \in p_r}p_f^j-\sum_{j \in \hat{p}_r}\hat{p}_f^j$
			\end{itemize}
			
			
			
	\section{Neighbourhood}
	We note $\mathcal{S}_p$ the ensemble of feasible structures of size $p$ (those uncrossed, {\it i.e.} with $I_f\cap I_r=\emptyset$).
	\\
	For each step $q$, starting from $S \in \mathcal{S}_p$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}_{S}&=& \{S \}\cup \{ S^{(i,j)} |(i,j) \in \mathcal{A}_q\}\cap{\mathcal{S}_p} 
	\end{eqnarray}	
	where $\mathcal{A}_q$ is a set of arcs to modify (add or remove) in the associated graph, defined according to a strategy.
	And we have for  given $S$ and $(i,j)$:
	\begin{itemize}
		\item if $i \in I_f^j$ then we remove $i$ from $I_f^j$ (arc removal)
		\item else we had $i$ in $I_f^j$
	\end{itemize} 
	Then coherence between $I_f$ and others parts of $S$ is done by $\forall 1\leq j\leq p :  p_f^j=|I_f^j|$, $I_r=\{j |p_f^j>0 \}$ and $p_r= |I_r|$.
	
In the adjacency matrix we just do: $G_{i,j}=1-G_{i,j}$. 
The main advantage of such a neighbourhood is that increasing and decreasing complexities are tested at each step without arbitrary ratio. If we just look at the sub-regression system, we have to choose for each sub-regression if we add, remove or keep covariates and we also have to choose if we had or delete some sub-regression. Adjacency matrix makes the neighbourhood extremely natural with just the modification of a value in a binary matrix.

		\subsection{Strategy}
			Many strategies can be imagined. First, we can decide to keep the local $\hat{S}$ in the neighbourhood or not, that is allowing or not stationarity. Here the MCMC is not used for sampling or density estimation. We just want to find the structure with the best value of $\psi(\boldsymbol{X},S)$ so it is not an evidence to allow or not stationarity. Our package CorReg give the user the choice with stationarity,  included in the neighbourhood by default.
			

		The only constraint on $\mathcal{A}_q$ is that $\forall (i,j) \in \mathcal{A}_q, i\neq j$			
			
		We propose, for step $q$ to draw $j$ from $\mathcal{U}(\{1,\dots,p\})$ and then 
		\begin{equation}
			\mathcal{A}_{q|j}=\{ (i,j)|i \neq j \}
		\end{equation}
			Such a strategy can be interpreted as the uniform choice of a sub-regression to modify followed by the proposal of each possible unary change.
			Our package CorReg let the user choose many other strategies like a fixed number of random couples $(i,j)$, or the union of the $j^th$ line and column of $G$.
		\subsection{Active relaxation of the constraints}
		We have defined the neighbourhood with an intersection with $\mathcal{S}_p$. 
		In practice, for some of the $(i,j) \in \mathcal{A}_q$, we have $S^{(i,j)}\notin \mathcal{S}_p$. Such candidates are basically rejected so the number of candidates is not constant at each step. Moreover, complex structures reduce the size of the potential neighbourhood because of the uncrossing rule. 
		Thus we propose a relaxation method by a new definition of $S^{(i,j)}$:
	\begin{itemize}
		\item if $i \notin I_f^j$ (add): 
			\begin{itemize}
				\item $I_f^j=I_f^j\cup \{i\}$
				\item $I_f^i=\emptyset$ (explicative variables can't depend on others : column-wise relaxation)
				\item $I_f=I_f \setminus \{j\}$ (dependent variables can't explain others : row-wise relaxation) 
			\end{itemize}			 
		\item else (remove): $I_f^j=I_f^j\setminus \{i\}$
	\end{itemize}
	It can be seen as forcing the modification by deleting what would make the structure not feasible. So in one step we can test a model that remove completely a sub-regression, remove the explicative role of a covariate in all sub-regression and create a new pairwise sub-regression. It drastically increases the scope of the neighbourhood and guarantee to always have the same number of candidates during the MCMC. It can be compared to simulated annealing that sometimes proposes exotic candidates to avoid local extrema, but here without any temperature to set. Here again, the neighbourhood remains natural, without arbitrary parameters to tune. 
	Another advantage of the relaxation method is that it reduce complexity very quickly without having to deconstruct a sub-regression, so it helps to have simpler models in a small amount of time (asymptotical results are the same because the chain is regular thus ergodic).
			
	\section{The walk}
		Once we have a neighbourhood, we have to choose a candidate for the next step.
The walk follows a time-homogeneous Markov Chain whose transition matrix $\mathcal{P}$ has $|\mathcal{S}_p|$ rows and columns (combinatory so we just compute the probabilities when we need them).
	At each step the Markov chain moves with probability:
	\begin{eqnarray}
			\forall (S,\tilde{S}) \in \mathcal{S}^2 : \mathcal{P}(S,\tilde{S})&=&\sum_{j=1}^p \mathbf{1}_{ \{\tilde{S}\in \mathcal{V}_{S,j}\} }\frac{\exp(-\frac{1}{2}\psi(\boldsymbol{X},\tilde{S}))}{\sum_{S_l\in \mathcal{V}_{S,j}}\exp(-\frac{1}{2}\psi(\boldsymbol{X},S_l))} 
	\end{eqnarray}
	And $\mathcal{S}_p$ is a finite state space.%la relaxation rend P non symétrique mais ne remets  pas en cause l'homogénéité	
	 
Because the walk follows a regular and thus ergodic markov chain with a finite state space, it has exactly one stationary distribution \cite{grinstead1997introduction} %: $\pi$ and every rows of $\operatorname{lim}_{k\rightarrow \infty}\mathcal{P}^k=W$ equals $\pi$.
%	
%	
%	With $\forall S \in  \mathcal{S}$ :
%	\begin{eqnarray}		
%		0 \leq &\pi (S)& \leq 1 \nonumber \\
%		\sum_{S \in \mathcal{S}}\pi(S) &=&1 \nonumber \\
%		\pi (S) &=&\sum_{\tilde{S}\in \mathcal{S}} \pi(\tilde{S})\mathcal{P}(\tilde{S},S) \\%définition de la lois stationnaire
%	\end{eqnarray}
%		
and the output will be the best structure in terms of $P(S|\boldsymbol{X})$ which weights each candidate. Practically speaking, \textsc{CorReg} returns the best structure seen during the walk (even if the corresponding candidate has never been chosen). The package also give the local structure when the walk stops so user can relaunch the algorithm from the same point if he wants to go further.
The main criterion to stop the walk is a maximum number of iteration but CorReg can also stop the walk after a given number of step on the best model found.
Numerical results (Section 4) illustrates the efficiency of the walk when the true model contains structures with various strength (Figure \ref{reshatZ}) and an example with a non-linear structure (Figure \ref{resnonlin}).
		
		
		
	\section{Initialization}
		\subsection{Correlation-based initialization}
		 If we have some knowledge about some sub-regressions (physical models for example) we can add them in the found and/or initial structure. So the model is really expert-friendly.
The initial structure can be based on a first warming algorithm taking the correlations into account. Coefficients are randomly placed into $I_f$, according to a Bernoulli draw weighted by the absolute value of the correlations and with respect to the uncrossing constraint. Uncrossing constraint will not allow some strong correlation to be taken into account according to the order of the Bernoulli drawing so we can draw with a random order or by ordering by descending correlations. 
		
		We note that the $BIC$ associated to initial model is often better than the $BIC$ of the void structure, so we compare several chains inf Figures \ref{pourcini} and \ref{Bicini}:			
			
		\begin{center}
		\begin{figure}[h!]
		\centering
		\includegraphics[width=300px]{figures/pourc_meilleur_marelaxini.png} 
		\caption{Amount of time each method is better for the 100 first steps of the MCMC. } \label{pourcini}
		\end{figure}		
		\end{center}
	
	\begin{center}
		\begin{figure}[h!]
		\centering
		\includegraphics[width=300px]{figures/typeinit.png} %mars 2013 analyse plan ini marelax
		\caption{Evolution of the $BIC$ (criterion to minimize in the MCMC) for each method.}\label{Bicini}
		\end{figure}
	\end{center}
	
		\subsection{Multiple intialisation}	
		Local extremas are a known issue for most of optimization methods, and one would rather test multiple short chains than lose time in initialisation or long chains \cite{gilks1996markov}. 
		We also compare the results obtained with several number of chains. Figure \ref{nbini} shows the evolution of the $BIC$ of the best chain with a number of chains varying from 1 to 10, so the model with 10 chains contain the others and is almost as good as they are. We see that multiple initialization is efficient but the gain is logarithmic in the number of tries so it is recommended to use multiple chains but not too much.		
			
	\begin{center}
	\begin{figure}[h!]
	\centering
		\includegraphics[width=400px]{figures/courbes_BICmoyen_marelax_nbinibis.png} 
		\caption{Comparison of distinct number of correlation-based initialisations for the MCMC.}\label{nbini}
	\end{figure}
	\end{center}
			
	\section{Pruning}
		If the complexity of $S$ remains too high, pruning methods can be used.
		\subsubsection{Variable selection}
			We can use variable selection methods like the LASSO to estimate the coefficient $\boldsymbol{\alpha}$ and obtain some supplementary zeros.
			The complexity may be the same so it is not really a pruning method.
		\subsubsection{$R^2$ thresholding }
			We can define a minimal value for the $R^2$ of the sub-regression to maintain them in the final structure. But this minimal value would be totally arbitrary and we know that it is frequent to use linear regression with real datasets that only show a $R^2$ between $0.1$ and $0.2$. It is particularly true in social sciences.
		\subsubsection{Test}
			Another pruning method would be to delete sub-regression that offer a F-statistic under a minimal value.
			 In the followings, the chain was launched with twenty initialisations each time, based on the correlation matrix.
		\subsubsection{Additional cleaning steps}
			Because the walk is not exhaustive, it does make sense to let the walk continue a few steps with neighbourhood containing only suppressions in the structure. The walk follows an ergodic chain thus it is just an heuristic change in the strategy with:
			\begin{equation}
				\mathcal{A}_q=\{(i,j)| i \in I_f^j \}
			\end{equation}
			It is not based on any arbitrary parameter and change the result only if it founds a better structure in terms of the criterion $\psi$ used in the walk.
	\section{The Graphical LASSO}\label{sectionGlasso}
		Graphical LASSO \cite{friedman2008sparse} \cite{witten2011new} \cite{tibshiranilasso} \cite{friedman2010applications} is set to give undirectionnal (thus symmetric) graphs by selection in the precision matrix (the inverse of the variance-covariance matrix). It does make sense for exponential family because in these cases, zeros in the precision matrix $\Sigma ^-1$ can be interpreted in terms of conditionnal independence between covariates \cite{dempster1972covariance}. But we have supposed Gaussian mixture on $\boldsymbol{X}$ and we search an oriented graph.
	But we can still use it for initialization, for example by a hadamard product with $G_0$ the adjacency matrix of the initial structure. We can also try to give the graph a bipartite orientation. We first have to obtain a bipartite graph, that mean to have no even cycles. A particular case would be the minimum spanning tree \cite{graham1985history,moret1991empirical,gower1969minimum}. But it is time consuming and has no theoretical properties relied to our problematic of minimizing $\psi$, so the idea was left behind after some tries.		
	
\section{CorReg}	
	The CorReg package is now on CRAN and provides many parameters for the walk. If wanted it can return some curves associated to the walk to have an idea of what happens with distinct strategies. 		
	
We define the complexity of a structure $S$ as the number of elements in the adjacency matrix, that is the number of links between covariates and is obtained by:
\begin{equation}
	\textrm{Complexity}(S)=\sum_{j \in I_r}p_f^j
\end{equation}	
	
		We compare some walks with each time the same dataset and the same seed for the random generator. We have $p=100$ and $n=50$.
		
		For Figures \ref{comparecomplrelax} and \ref{compareBICrelax} we start from an arbitrary structure with a complexity of $62$. We see that relaxation helps to delete these false sub-regressions and avoid to be stuck in it, improving the $BIC$ much faster. We also observe that final complexities are comparable. Here the MCMC was launched only once (with the totally arbitrary initial structure based on nothing), the true structure had a complexity of $120$.
\begin{center}
	\begin{figure}[h!]
	\centering
\includegraphics[width=400px]{figures/complexitycompareMCMC.png} 		
\caption{Comparison of complexity evolution with or without constraint relaxation.}\label{comparecomplrelax}
	\end{figure}
	\end{center}
			
\begin{center}
	\begin{figure}[h!]
	\centering
\includegraphics[width=400px]{figures/BICcompareMCMCrelax.png} 		
\caption{Comparison of BIC evolution with or without constraint relaxation.}\label{compareBICrelax}
	\end{figure}
	\end{center}
			
		
	\chapter{Numerical results on simulated datasets} \label{sectionsimul}


	\section{The datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if \textsc{CorReg} 	keeps its promises.
	The \textsc{CorReg} package has been tested on simulated datasets. 
Section \ref{compZ} shows the results obtained in terms of $\hat{S}$. Sections \ref{tableMSEsimtout} and \ref{tableMSEsimgauche} show the results obtained using only \textsc{CorReg}, or \textsc{CorReg} combined with other methods. Tables give both mean and standard deviation of the observed Mean Squared Errors (MSE) on a validation sample of $1 000$ individuals. For each simulation,  $p=40$, the $R^2$ of the main regression is $0.4$, variables in $\boldsymbol{X}_f$ follow Gaussian mixture models of $\lambda=5$ classes which means follow Poisson's law of parameter $\lambda=5$ and which standard deviation is $\lambda$. The $\beta_j$ and the coefficients of the $\boldsymbol{\alpha}_j$ are generated according to the same Poisson law but with a random sign. $\forall j \in I_r, p_1^j=2$ (sub-regressions of length 2) and we have $p_r=16$ sub-regressions. The datasets were then scaled so that covariates $X_r$ don't have a greater variance or mean.
	We used \textsc{Rmixmod} to estimate the densities of each covariate. For each configuration, the MCMC walk was launched on $10$ initial structures with a maximum of 1 000 steps each time.
	When $n<p$, a frequently used method is the Moore-Penrose generalized inverse \cite{katsikis2008fast}, thus OLS can obtain some results even with $n<p$. %(see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
	When using penalized estimators for selection, a last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}) and is applied for both classical and marginal model.
	We compare different methods with and without CorReg as a pretreatment. All the results are provided by the CorReg package.
	
		\section{Results on $\hat S$}	\label{compZ}


\begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/BIC_p2.png} 
			\caption{Quality of the subregressions found with classical $BIC$ criterion}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/BICSTAR_P2.png} 
			\caption{Quality of the subregressions found with our $BIC_+$ criterion} 
   \end{minipage}
\end{figure}






\clearpage
\section{Results on prediction}\label{compY}

\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_f}}$ (best case for us)}	 \label{tableMSEsimgauche}
\begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X1_compl_NB.png} 
			\caption{Comparison of the complexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
 
%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X1_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X1_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X1_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X1_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
   \end{minipage}
\end{figure}

\clearpage
	\subsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	 	
We then try the method with a response depending on all covariates (\textsc{CorReg} reduces the dimension and can't give the true model if there is a structure). %The datasets used here were those from table \ref{compZvrai}. 
 
 
 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_tout_compl_NB.png} 
			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
 
%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_tout_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_tout_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_tout_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_tout_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
   \end{minipage}
\end{figure}

We see that CorReg tends to give more parsimonious models and better predictions, even if the true model is not parsomious. We logically observe that when $n$ rises, all the models get better and the correlations cease to be a problem so the complete model starts to be better (CorReg does not allow the true model to be choosen).




\clearpage
	\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X^{I_r}}$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $\boldsymbol{X}^{I_r}$. The datasets used here were still those from \ref{compZvrai}.
Depending only on $\boldsymbol{X}_r$ implies sparsity and impossibility to obtain the true model when using the true structure. 

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between OLS and CorReg+OLS}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X2_compl_NB.png} 
			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
   \end{minipage}
\end{figure}
\textsc{CorReg} is still better than OLS for strong correlations and limited values of $n$. 
 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X2_compl_NB.png} 
			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X2_compl_NB.png} 
			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
   \end{minipage}
\end{figure}

 \begin{figure}[h!]
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X2_MSE_NB.png} 
			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X2_compl_NB.png} 
			\caption{Comparison of the compexities between stepwise and CorReg+stepwise}
   \end{minipage}
\end{figure}

\chapter{Numerical results on real datasets} \label{sectionrealcase}
	\section{Quality case study} \label{sectionexfos}
This work takes place in steel industry context, with quality oriented objective : to understand and prevent quality problems on finished product, knowing the whole process. The correlations are strong here (many parameters of the whole process without any a priori and highly correlated because of physical laws, process rules, {\it etc.}). 
		
We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item 205 variables from the whole process to explain it.
		\end{itemize}

\begin{figure}[h!]
	\begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/res_article/nb_comp_X_exfo.png}%{figures/mixmod.png} 
			\caption{Distribution of the number of components found for each covariate.}\label{graphMixmod}
	\end{minipage} \hfill
	\begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/res_article/gaussianmixture_exfo.png}%{figures/histR2exfos.png} $R^2_{adj}$ of the 76 sub-regressions.
			\caption{Example of non-Gaussian real variable easily modeled by a Gaussian mixture.}
	\end{minipage} \hfill
   \begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/correlexfoshist.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEexfos}
   \end{minipage}
\end{figure}   			
	We get a training set of $n=3 000$ products described by $p=205$ variables from the industrial process and a validation sample of $847$ products.
	Let's note $\rho$ the absolute value of correlations between two covariates. Industrial variables are naturally highly correlated as the width and the weight of a steel slab ($\rho=0.905$), the temperature before and after some tool ($\rho=0.983$), the  roughness of both faces of the product ($\rho= 0.919$), a mean and a max ($\rho=0.911$). 
	
	The objective here is not only to predict non-quality but to understand and then avoid it. CorReg provides an automatic method without any a priori and is combined with variable selection methods. So it allows to obtain in a small amount of time some indications on the source of the problem, and to use human resources efficiently. When quality crises occurs, time is extremely precious so automation is a real stake. The combinatorial aspect of the sub-regression models makes it impossible to do manually.
		
		
		\begin{figure}[h!]
		\centering
			\includegraphics[height=150px, width=150px]{figures/histR2exfos.png} 
			\caption{$R^2_{adj}$ of the 76 sub-regressions.}
\end{figure} 
		
		
	\textsc{CorReg} found the above correlations but it also found more complex structures describing physical models, like   Width = f (Mean flow , Mean speed) even if the true Physcial model is not linear : Width = flow / (speed * thickness) (here thickness is constant). Non-linear regulation models used to optimize the process were also found (but are confidential). These first results are easily understandable and meet metallurgists expertise.  Sub-regressions with small values of $R^2$ are associated with non-linear model (chemical kinetics for example).
			The algorithm gives a structure of $p_r=76$ subregressions with a mean of $\bar{\boldsymbol{p}}_f=5.17$ regressors. In $\boldsymbol{X}^{I_f}$ the number of $\rho>0.7$ is $\textbf{79.33\%}$ smaller than in $\boldsymbol{X}$.		
	\\
	
	
			It is now time to look at the predictive results (Table \ref{Res_exfos}). We see that CorReg improves the results for each method tested in terms of prediction.
			We get parsimonious models automatically in a small amount of time (several hours but able to work during the night or the week-end)
%				The best model found when not using \textsc{CorReg} is given by the LASSO. But when using \textsc{CorReg} elasticnet produces a better model in terms of prediction. LASSO gives a model with 21 non-zero coefficients and elasticnet with \textsc{CorReg} gives a model with 40 non-zero parameters but $6.40\%$ better in prediction on the validation sample (847 products). $14$ non-zero coefficients are common between the two models.
%				Elasticnet alone get a model with 78 parameters that is improved by $9.75\%$ in prediction when used with \textsc{CorReg}. When using LASSO with \textsc{CorReg} we obtain a model with 24 non-zero coefficients that is $4.11\%$ better than LASSO alone. We also computed the OLS model (without selection) and the naive one (estimating the response by the mean of the learning set). All the MSE were modified here to obtain a value of 100 for the best (to preserve confidentiality). Elasticnet with \textsc{CorReg} is $13.51\%$ better than OLS.
%%		\begin{figure}[h]
%			\centering
%				\label{barplotMSEexfos}
%				\includegraphics[width=430px]{figures/MSEfinal.png}
%			\caption{MSE comparison on industrial dataset. Learning set : 3 000 products, validation set : 847 products}
%		\end{figure}		

		\begin{table}[h!]
\centering
\begin{tabular}{|c c|c|c|}
	\hline 
	Method& indicator& With CorReg & without CorReg \\ 
	\hline 
	OLS & MSE & 13.30 & 14.03 \\
		& (complexity)& (130) & (206) \\
	\hline
	LASSO & MSE & 12.77 & 12.96 \\
		& (complexity)& (24) & (21) \\
	\hline
	Elasticnet & MSE & \textbf{12.15} & 13.52 \\
		& (complexity)& (40) & (78) \\
	\hline
	Ridge & MSE & 12.69 & 13.09 \\
		& (complexity)& (130) & (206) \\
	\hline
\end{tabular} 
\caption{Results obtained on a validation sample (847 individuals).}\label{Res_exfos}
\end{table}


		In terms of interpretation, the main regression comes with the family of regression so it gives a better understanding of the consequences of corrective actions on the whole process. It typically permits to determine the \textit{tuning parameters} whereas variable selection alone would point variables we can't directly act on.	So it becomes easier to take corrective actions on the process to reach the goal. The stakes are so important that even a little improvement leads to consequent benefits, and we don't even talk about the impact on the market shares that is even more important.
		\FloatBarrier
		\section{Production case study}
This second example is about a phenomenon that impacts the productivity of a steel plant.
We have:
		\begin{itemize}
			\item a (confidential)  response variable,
			\item $p=145$ variables from the whole process to explain it but only $n=100$ individuals.
			\item The stakes : $20\%$ of productivity to gain on a specific product with high added value.
		\end{itemize}
		
		
		\begin{figure}[h!]
	\begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/nbcompBV.png}%{figures/mixmod.png} 
			\caption{Distribution of the number of components found for each covariate.}\label{graphMixmod}
	\end{minipage} \hfill
	\begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px, width=150px]{figures/GMcriseBV.png}%{figures/histR2exfos.png} $R^2_{adj}$ of the 76 sub-regressions.
			\caption{Another example of non-Gaussian real variable easily modeled by a Gaussian mixture.}
	\end{minipage} \hfill
   \begin{minipage}[t]{.30\linewidth}
			\includegraphics[height=150px,width=150px]{figures/histcorrelBVBI.png} 
			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEexfos}
   \end{minipage}
\end{figure} 
  		
  			
CorReg found 55 sub-regressions as shown in Figure \ref{R2bv}. One of them seems to be weak $R^2=0.17$ but is not linear (points out a link between diameter of a coil and some shape indicator).	

\begin{figure}[h!]
\centering
	\includegraphics[height=150px, width=150px]{figures/R2corregBVBI.png} 
			\caption{$R^2_{adj}$ of the 55 sub-regressions.}\label{R2bv}
\end{figure}
The response variable was binary but $n$ was too small compared to $p$ to use logistic regression so we have considered $\boldsymbol{Y}$ as a continuous variable and then made imputation by $1$ when $\hat{\boldsymbol{Y}}>0.5$ and by $0$ else.


\begin{table}[h!]
\centering
\begin{tabular}{|c c|c|c|}
	\hline 
	Method& indicator& With CorReg & without CorReg \\ 
	\hline
	OLS & well-classified & 100& 56 \\
		& MSE (leave-one-out)& 1.95& 51 810\\
		& complexity & 91& 100 \\
	\hline 
		LASSO & well-classified &93 &93 \\
		& MSE (leave-one-out)& 0.106 & 0.120\\
		& complexity & 27&34\\
	\hline 
		Elasticnet & well-classified &84 &87 \\
		& MSE (leave-one-out)&0.140 &0.148\\
		& complexity &10 &13\\
	\hline 
		Ridge & well-classified &88 &85 \\
		& MSE (leave-one-out)& 0.179 & 0.177\\
		& complexity &91 &146\\
	\hline 
\end{tabular} 
\caption{Results obtained with leave-one out cross-validation. $n=100, p=145$.}	
\end{table}

In this precise case, \textsc{CorReg} found a structure that helped to decorrelate covariates in interpretation and to find the relevant part of the process to optimize. This product is made by a long process that requires several steel plants so it was necessary to point out the steel plant where the problem occurred.


\part{Further usage of the structure}	
\chapter{Missing values}
	\paragraph{Résumé:} Le modèle génératif complet sur les données nous permet d'obtenir la loi des valeurs manquantes. Mais nous pouvons aller encore plus loin car la modélisation explicite des corrélations nous permet d'obtenir les lois conditionnelles de chaque valeur manquante sachant les valeurs observées. Ce chapitre présente comment nous pouvons par un simple algorithme SEM estimer les paramètres des sous-régressions dans la chaîne MCMC de recherche de structure. Enfin, nous pouvons imputer les valeurs manquantes à l'aide d'un Gibbs qui procède par imputations multiples, fournissant au passage un indicateur de précision sur l'imputation proposée. 

\section{Introduction}
	Real datasets often have missing values and it is a very recurrent issue in industry. We note $\boldsymbol{M}$ the $n\times p$ binary matrix indicating whereas a value is missing (1) or not (0) in $\boldsymbol{X}$.
	We note $\boldsymbol{X}_M$ the missing values and $\boldsymbol{X}_{O}$ the observed values. $\Theta=\{\boldsymbol{\mu}_X,\boldsymbol{\Sigma}_X \}$ stands for the parameters of the Gaussian mixture followed by $\boldsymbol{X}$.
	$\boldsymbol{\alpha}$ is the matrix of the sub-regression coefficients with $\alpha_{i,j}$ the coefficients associated to $\boldsymbol{X}^i$ in the sub-regression explaining $\boldsymbol{X}^j$.\\ 
			Here we suppose that missing values are Missing Completely At Random (MCAR). 
	 Many methods does exist to manage such problems \cite{little1992regression} but they make approximation , add noise (imputation methods) or delete information (cutting methods).	
	 
%Détailler les différents pattern de valeurs manquantes
%	 
%Detailler Les 6 types de méthodes	 
%	 
	We have a full generative model on $\boldsymbol{X}$ with explicit dependencies within the covariates. So when a value is missing, we know its distribution but more than that, we know its conditional distribution based on observed values for the same individual. Thus we are able to make imputation and to describe the missing values with their conditional distribution. This is a positive side-effect of the explicit generative model on $\boldsymbol{X}$. 
\section{Estimation of the sub-regressions with missing values}
\subsection{The integrated likelihood}
The first thing we do with $\boldsymbol{X}$ is to estimate S.
	 During the MCMC, for each candidate we have to compute the likelihood of the candidate, depending on $\boldsymbol{\alpha}$ the matrix of the sub-regression coefficients.
We start with the complete likelihood of $\boldsymbol{X}$
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})&=& \prod_{i=1}^n f(\boldsymbol{X}_i)= \prod_{i=1}^n\left[f(\boldsymbol{X}_i^{I_r}|\boldsymbol{X}_i^{I_f};\boldsymbol{\alpha},\Theta,S)f(\boldsymbol{X}_i^{I_f};\boldsymbol{\alpha},\Theta,S) \right] \\
	&=&\prod_{i=1}^n\left[\prod_{j \in I_r}f(x_{i,j}|\boldsymbol{X}_i^{I_f};\boldsymbol{\alpha},\Theta,S)\prod_{j \notin I_r} f(x_{i,j};\boldsymbol{\alpha},\Theta,S) \right] \\
	&=&\prod_{i=1}^n\left[\prod_{j \in I_r}f(x_{i,j}|\boldsymbol{X}_i^{I_f^j};\boldsymbol{\alpha},\Theta,S)\prod_{j \notin I_r} f(x_{i,j};\boldsymbol{\alpha},\Theta,S) \right] \\
%	&=&\prod_{i=1}^n\left[\prod_{\substack{j \in I_r \\ \boldsymbol{M}_{i,j}=1}}P(x_{i,j}|\boldsymbol{X}_i^{I_f^j})\prod_{\substack{j \in I_r \\ \boldsymbol{M}_{i,j}=0}}P(x_{i,j}|\boldsymbol{X}_i^{I_f^j})
%			\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=1}} P(x_{i,j})\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}} P(x_{i,j}) \right] \\
	\mathcal{L}(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})&=&\sum_{i=1}^n\left[\sum_{j \in I_r}\log \left(f(x_{i,j}|\boldsymbol{X}_i^{I_f^j};\boldsymbol{\alpha},\Theta,S)\right)+\sum_{j \notin I_r} \log \left(f(x_{i,j};\boldsymbol{\alpha},\Theta,S)\right) \right] \label{loglikmiss}
\end{eqnarray}
		In the MCMC we need to compute the likelihood of the dataset knowing the structure. When missing values occurs, we restrict the likelihood to the known values by integration on $\boldsymbol{X}_M$.
%		We have 
%	\begin{equation}
%		g(\boldsymbol{X}|\Theta)=\int_{\boldsymbol{X}_M}f(\boldsymbol{X}|\Theta)d \boldsymbol{X} \label{integralmiss}
%	\end{equation}
%For the covariates in $\boldsymbol{X}_f$, we use the density estimated  ($e.g.$ a Gaussian Mixture model estimated by \textsc{Mixmod}) or given as hypothesis. All individuals are supposed $iid$ so $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \notin I_r $: 
%				 \begin{equation}
%				 	g(x_{i,j}|\Theta)=f(x_{i,j}|\Theta)=\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k}) 
%				 \end{equation} with $k_j,\pi_{j,k}, \mu_{j,k}$ and $\Sigma_{j,k}$ estimated by Mixmod (for example). 
%\\				 		
				 		
%		Then we have
%		\begin{eqnarray}
%			g(\boldsymbol{X}|\Theta)&=& g(\boldsymbol{X}_r|\boldsymbol{X}_f,\Theta)g(\boldsymbol{X}_f|\Theta) \\
%			&=&\prod_{i=1}^n\left[ g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}} g(x_{i,j}|\Theta) \right]\\
%			&=&\prod_{i=1}^n\left[ g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)
%							\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}}\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k}) \right] \label{decomplikelimiss}
%		\end{eqnarray}
%		reminding that covariates in $\boldsymbol{X}_f$ are orthogonal. \\
%		
%	 Residuals of the sub-regressions are orthogonal but missing values can make the residuals dependent. We have to decompose more precisely $g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)$. To have a better view on the dependencies implied, we first write the marginal distributions. \\

We know that $\boldsymbol{X}$ is a Gaussian mixture ({\it iid} individuals, vectors of orthogonal Gaussian mixtures $\boldsymbol{X}^{I_f}$ and linear combinations of these Gaussian mixtures and some Gaussian for $\boldsymbol{X}^{I_r}$) with $K$ the number of its components.
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)&=&\int_{\boldsymbol{X}_M}L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X})d\boldsymbol{X} 
	=\int_{\boldsymbol{X}_M}\sum_{k=1}^K \pi_k \phi_k(\boldsymbol{X};\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} \\
	&=&\sum_{k=1}^K \pi_k \int_{\boldsymbol{X}_M}\phi_k(\boldsymbol{X};\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} 
	=\sum_{k=1}^K \pi_k \int_{\boldsymbol{X}_M}\prod_{i=1}^n\phi_k(\boldsymbol{X}_i;\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X} \\
	&=&\sum_{k=1}^K \pi_k \prod_{i=1}^n\int_{\boldsymbol{X}_{i,M}}\phi_k(\boldsymbol{X}_i;\boldsymbol{\alpha},\Theta,S)d\boldsymbol{X}_i 
	=\sum_{k=1}^K \pi_k \prod_{i=1}^n\phi_k(\boldsymbol{X}_{i,O};\boldsymbol{\alpha},\Theta,S)\\
	&=&\sum_{k=1}^K \pi_k \phi_k(\boldsymbol{X}_{O};\boldsymbol{\alpha},\Theta,S)=f(\boldsymbol{X}_{O},\boldsymbol{\alpha},\Theta,S)
\end{eqnarray}






% $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \notin I_r $:
%	 \begin{equation}
%	 \int_{\boldsymbol{X}_M}f(x_{i,j}|\boldsymbol{\alpha},\Theta,S) d\boldsymbol{X}=1
%	 \end{equation}	
To compute this likelihood, we will use the decomposition
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)&=&f(\boldsymbol{X}_{O};\boldsymbol{\alpha},\Theta,S)=\prod_{i=1}^nf(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)f(\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S) \\
	&=&\prod_{i=1}^nf(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)\prod_{\substack{j \in I_f \\ M_{i,j}=0}}f(x_{i,j};\boldsymbol{\alpha},\Theta,S)
\end{eqnarray}

with	  $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}= 0 \textrm{ and } j \notin I_r $:
	 \begin{equation}
	 f(x_{i,j};\boldsymbol{\alpha},\Theta,S)=\sum_{k=1}^{K_j}\pi_{j,k}\Phi_k(x_{i,j};\mu_{j,k},\Sigma_{j,k}) \label{likmissdroite}
	 \end{equation} with $K_j,\pi_{j,k}, \mu_{j,k}$, $\Sigma_{j,k}$ and the likelihood estimated once (for example by RMixmod \cite{packageRmixmod}) before the MCMC starts. 
%	 $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \in I_r $:
%	 \begin{equation}
%	 \int_{\boldsymbol{X}_M}f(x_{i,j}|\boldsymbol{X}_i^{I_f^j},\boldsymbol{\alpha},\Theta,S) d\boldsymbol{X}=1
%	 \end{equation}
\\
	 And, $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}= 0 \textrm{ and } j \in I_r $:
		\begin{eqnarray}
 f(x_{i,j}|\boldsymbol{X}_{i,O}^{I_f^j};\boldsymbol{\alpha},\Theta,S)&=& \sum_{k=1}^{K_{ij}}\pi_{ij,k}\Phi(x_{i,j};\mu_{ij,k},\Sigma_{ij,k}) \textrm{ where }  \label{Missingdensity}\\
				\boldsymbol{\pi}_{ij} &=& \bigotimes_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1 } } \boldsymbol{\pi}_l \textrm{ and  }K_{ij}=|\boldsymbol{\pi}_{ij}| ,\\
				\boldsymbol{\mu}_{ij}&=& \sum_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=0  }}\alpha_{l,j}x_{i,l} + \bigoplus_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1  }} \alpha_{l,j} \boldsymbol{\mu}_l \\
				\boldsymbol{\Sigma}_{ij} &=& \sigma_j^2 + \bigoplus_{\substack{l \in I_f^j \\ \boldsymbol{M}_{i,l}=1 }}\alpha_{i,l}^2 \boldsymbol{\Sigma}_l		
		\end{eqnarray}		 
		This could be easily used for imputation of the missing values in $\boldsymbol{X}^{I_r}$ knowing the parameters $\boldsymbol{\alpha}, \Theta$ and $S$. We note that we obtain a Gaussian when there is no missing value in $I_f^j$.
		But we see that	$f(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)$ is not the product of the $f(x_{i,j}|\boldsymbol{X}_{i,O}^{I_f^j};\boldsymbol{\alpha},\Theta,S) $	if a same missing value occurs in distinct sub-regressions. Thus if every sub-regression are distinct connex component then we can use (\ref{Missingdensity}) and we have
		\begin{equation}
		L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)=\prod_{i=1}^n\prod_{\substack{j \in I_r \\ M_{i,j}=0}}f(x_{i,j}|\boldsymbol{X}^{I_f^j}_{i,O};\boldsymbol{\alpha},\Theta,S)\prod_{\substack{j \in I_f \\ M_{i,j}=0}}f(x_{i,j};\boldsymbol{\alpha},\Theta,S) \label{simplemisslik}
\end{equation}		 
		But for the general case we need to manage the dependencies implied by missing values in common covariates in the $I_f^j$.
%		Because $\forall 1\leq i \leq n, \boldsymbol{X}_i$ is a Gaussian Mixture, $\boldsymbol{X}_{i,O}$ and then $(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}_{i,O}^{I_f})$ are Gaussian mixtures too. \\
		We note $f(\boldsymbol{X})=\sum_{k=1}^K\pi_k \mathcal{N}(\boldsymbol{\mu}_{X,k};\boldsymbol{\Sigma}_{X,k})$.
		
\begin{eqnarray}
L(\boldsymbol{\alpha},\Theta,S;\boldsymbol{X}_0)&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O};\boldsymbol{\alpha},\Theta,S)\\
&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O}^{I_r}|\boldsymbol{X}_{i,O}^{I_f};\boldsymbol{\alpha},\Theta,S)\Phi_k(\boldsymbol{X}_{i,O}^{I_f};\boldsymbol{\alpha},\Theta,S)\\
&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O}^{I_r}|\boldsymbol{X}_{i,O}^{I_f};\boldsymbol{\alpha},\Theta,S)\prod_{\substack{j \in I_f \\ M_{i,j}=0}}\Phi_k(x_{i,j};\mu_{j,k},\Sigma_{j,k}) \label{liklihoodmissglobal}
\end{eqnarray}		

%définintion des objets génériques
Where	
\begin{eqnarray}
	\boldsymbol{\pi}&=&\bigotimes_{\substack{j \in I_f }} \boldsymbol{\pi}_j \textrm{ (Kronecker product)}\\
	K&=& |\boldsymbol{\pi}| \\
	\boldsymbol{\mu}_{X^{I_f}}&=&  \prod_{\substack{j \in I_f}}\boldsymbol{\mu}_{j} \textrm{ (Cartesian product) } \\	
	\boldsymbol{\sigma}_{X}&=&\prod_{\substack{j \in I_f}}\boldsymbol{\sigma}_{j} \textrm{ (Cartesian product) }
\end{eqnarray}
		with $ \boldsymbol{\pi}_j, \mu_{j,k},\Sigma_{j,k}$ are estimated once before the MCMC starts (by Mixmod for example).
		
		
		$\forall 1\leq i \leq n, \forall 1\leq k \leq K$ we have
\begin{eqnarray}
		\Phi_k(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)&=&\Phi_k(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k},\boldsymbol{\Sigma}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k})\\
				P(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)&=&\Phi_k(\boldsymbol{X}^{I_r}_{i,O};\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k},\boldsymbol{\Sigma}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k})\\
		\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k}&=& 
				\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O},k}+
				\boldsymbol{\Sigma}_{X_{i,O}^{I_r},X_{i,O}^{I_f},k}(\boldsymbol{\Sigma}_{X_{i,O}^{I_f},X_{i,O}^{I_f},k})^{-1}
				( ^t\boldsymbol{X}_{i,O}^{I_f}-\boldsymbol{\mu}_{X^{I_f}_{i,O},k})\\
		\boldsymbol{\Sigma}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k}&=&\boldsymbol{\Sigma}_{X_{i,O}^{I_r},X_{i,O}^{I_r},k}-\boldsymbol{\Sigma}_{X_{i,O}^{I_r},X_{i,O}^{I_f},k}
		(\boldsymbol{\Sigma}_{X_{i,O}^{I_f},X_{i,O}^{I_f},k})^{-1} \boldsymbol{\Sigma}_{X_{i,O}^{I_f},X_{i,O}^{I_r},k} \\
		\forall j \in I_r: \ \  \boldsymbol{\mu}_{X_{i,O}^{j}}&=&\sum_{l \in I_f^j}\alpha_{l,j}\mu_{l,k} 
\end{eqnarray}		
		$\forall j \in I_r \textrm{ with } M_{i,j}=0$ 
\begin{equation}
	\operatorname{var}_{k}(x_{i,j})=\sigma_{j}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{X^l,k}^2
\end{equation}			
	$\forall j \notin I_r \textrm{ with } M_{i,j}=0$
\begin{equation}
	\operatorname{var}_{k}(x_{i,j})=\sigma_{X^j,k}^2
\end{equation}			
	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}\neq \emptyset \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})=\sum_{l\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{l,j_1}\alpha_{l,j_2}\operatorname{var}_{k}(x_{i,l}) =\sum_{l\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{l,j_1}\alpha_{l,j_2}\sigma_{X^l,k}^2
\end{equation}
	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}= \emptyset \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})=0
\end{equation}
	$\forall j_1 \in I_f, j_2 \in I_f \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})=0
\end{equation}
	$\forall j_1 \in I_r, j_2 \in I_f^{j_1} \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})= \alpha_{j_2,j_1}\sigma_{X^{j_2},k}^2
\end{equation}
$\forall j_1 \in I_r, j_2 \notin I_f^{j_1}\cup I_r \textrm{ with } M_{i,j_1}=M_{i,j_2}=0$
	\begin{equation}
	\operatorname{cov}_{k}(x_{i,j_1},x_{i,j_2})= 0
\end{equation}
	\subsection{Likelihood computation optimized}
	The main problem with the likelihood in its global form (\ref{liklihoodmissglobal}) is that the number of components explodes so we can't use it in practice. But in many case, it can be simplified.
We see that the $0$ in the variance-covariance matrix does not depend on the component $k$ so the structure of sparsity of $\boldsymbol{\Sigma}$ can be stored and used back in each iteration for a given structure $S$ to reduce computing time.  Another strictly technical tip would be to use sparse matrix storage to avoid null value storage and useless zero multiplications.
		Moreover, we can look if there are missing values shared by several sub-regression. 
		Connex component detection could be done to reduce the dimension down to strictly dependent covariates and use equation (\ref{Missingdensity}) elsewhere.
		We just need to compute the  row-sums of the adjacency matrix $G$ or to search for redundancy in $I_f$ and then if there is no redundancy or if $\forall j$ redundant we have $\sum_{i=1}^nM_{i,j}=0$ then we can use the simplified form of the likelihood given in (\ref{simplemisslik}). For faster computation we can stock the vector of covariates that have missing values.
		So the true value of the likelihood can be computed efficiently in most of cases but in the MCMC, it remains the possibility to have a structure with explosive likelihood expression when combined with the missing values and we need to compute the likelihood for a great number of candidates. Then it is possible to use directly the simplified form of the likelihood, that can be seen as an approximation of the likelihood, not taking into account some of the dependencies but it would offer no guarantee in terms of efficiency for the MCMC. %Numerical results on simulated datasets will show if this approximation is effective.

%		In first approximation we can suppose independence between the sub-regression:
%		\begin{equation}
%		\forall (j,j') \in I_r \times I_r, g(x_{i,j}| \boldsymbol{X}_{i}^{I_f},\Theta) \perp g(x_{i,j'}| \boldsymbol{X}_{i}^{I_f},\Theta)
%\end{equation}		 
%then we have the complete expression of the likelihood with \ref{decomplikelimiss} and \ref{Missingdensity}.
% Such approximation can be costless according to the position of the missing values ({\it e.g.} if they are all in $\boldsymbol{X}^{I_r}$). It is closer to the real model than the orthogonal hypothesis made by classical imputation by the mean. Moreover, sub-regressions are used only locally and errors don't cumulate whereas the true general decomposition combine many sub-regressions with cumulated noise of approximation. Thus, a general model would be better asymptotically but may not be efficient with finite dataset if the structure is complex. This first approximation is a good candidate to compare to the naive model (not taking into account the structure of sub-regression but making imputations by the mean for each covariate individually). 
%		
%		However, we write the real generalized expression for the log-likelihood.
%		Let $\mathcal{I}_r$ be a permutation of $I_r$ (arbitrary chosen, so the package will use identity). We define the general decomposition:
%		\begin{eqnarray}
%			g(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},\Theta)&=& \prod_{i=1}^n \left[g(x_{i,\mathcal{I}_r(p_r)}|\boldsymbol{X}_i^{I_f},\Theta)\prod_{j=1}^{p_r-1}g(x_{i,\mathcal{I}_r(j)}|\boldsymbol{X}_{i}^{\mathcal{I}_r(j+1)},\dots ,\boldsymbol{X}_{i}^{\mathcal{I}_r(p_r)}, \boldsymbol{X}_i^{I_f},\Theta)\right]
%		\end{eqnarray}
%		where we don't know the expression of $ g(x_{i,\mathcal{I}_r(j)}|\boldsymbol{X}_{i}^{\mathcal{I}_r(j+1)},\dots ,\boldsymbol{X}_{i}^{\mathcal{I}_r(p_r)}, \boldsymbol{X}_i^{I_f},\Theta)$ so the previous approximation stands still. 
%			
%	
%To estimate $\boldsymbol{\alpha}$ we use an EM algorithm. We start with an arbitrary value $\boldsymbol{\alpha}^{(0)}$, then:
%For the iteration $h$ of the algorithm at the E step we want 
%\begin{equation}
%	E_{\boldsymbol{X}_{\boldsymbol{M}}|\boldsymbol{X}_O,\boldsymbol{\alpha},\Theta,S}\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha}^{(h)},S,\Theta)\right]
%\end{equation}
%So we get $\boldsymbol{X}_M^{(h)}$ the  imputation for $\boldsymbol{X}_M$ and then the M step simply is
%\begin{equation}
%	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}E\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha},S,\Theta) \right]
%\end{equation}
%and we can use the same method as the one for classical case without missing values (OLS, SUR, {\it etc.}).
%		And we continue until convergence ($\parallel \boldsymbol{\alpha}^{(h+1)} - \boldsymbol{\alpha}^{(h)}\parallel < tol $ where $tol$ is the tolerance.\\
%		
%Imputation in $\boldsymbol{X}^{I_r}$ is made according to (\ref{Missingdensity}) and we have orthogonality in $\boldsymbol{X}^{I_f}$:
%\begin{displaymath}
%\forall j \notin I_r, P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f}\setminus \boldsymbol{X}^j)=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r})
%\end{displaymath}
%Moreover, $\forall j \notin I_r, \forall l \in I_r $ with $j \notin I_f^l, \boldsymbol{X}^j\perp \boldsymbol{X}^j$ so
%\begin{displaymath}
%\forall j \notin I_r, P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f}\setminus \boldsymbol{X}^j)=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r})=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r^j})
%\end{displaymath}
%where $I_r^j=\{i \in I_r| j \in I_f^j \}=\{i \in I_r|\alpha_{j,i}\neq 0 \}$ So we will make imputations (for E step only) according to $P(\boldsymbol{X}^j|\boldsymbol{X}_O^{I_r^j})$
%		\\
%$\forall 1 \leq i \leq n$	
%To use formulas on conditional distribution for Gaussian multivariate distribution we first write $P(\boldsymbol{X}_i^j,\boldsymbol{X}_i^{I_r^j})$ which is a Gaussian mixture with $K_{ij}$ components.
%\\
%$\forall j \notin I_r, \forall (l_1,l_2) \in I_r^j, P(x_{i,l_1}|x_{i,j},x_{i,l_2})=P(x_{i,l_1}|x_{i,j})$
%\begin{eqnarray}
%	P(x_{i,j},\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\alpha})&=& P(x_{i,j})\prod_{\substack{ l \in I_r^j \\M_{i,l=0} } }P(x_{i,l}|x_{i,j}) \\
%	&=&\sum_{k =1}^{ K_{ij}} \pi_{ij,k} \phi(x_{i,j},\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\mu}_{ij,k},\boldsymbol{\Sigma}_{ij,k}) \textrm{ where } \\
%	\boldsymbol{\pi}_{ij}&=&\boldsymbol{\pi}_j\otimes \left[ \bigotimes_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 } } \boldsymbol{\pi}_{ijl} \right]=\boldsymbol{\pi}_j\otimes\bigotimes_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 } } \left(\bigotimes_{\substack{h \in I_f^l \\ h \neq j} }\boldsymbol{\pi}_h \right)
%	 \textrm{ and  }K_{ij}=|\boldsymbol{\pi}_{ij}| ,\\
%	\boldsymbol{\mu}_{ij}&=&\boldsymbol{\mu}_j \times \left[\prod_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 }}\boldsymbol{\mu}_{ijl} \right]
%		=\boldsymbol{\mu}_j \times \prod_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 }}\left[\alpha_{j,l}x_{i,j}+\bigoplus_{\substack{h \in I_f^l}}\alpha_{h,l}\boldsymbol{\mu}_{h} \right] \\	
%		\boldsymbol{\Sigma}_{ij}&& \textrm{is the associated variance-covariance matrix}
%\end{eqnarray}
%		Cartesian product and power in the expression of the mean.
%		
%
%Then we have 
%\begin{eqnarray}
%	P(x_{i,j}|\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\alpha})&= &\sum_{k=1}^{K_{ij}}\pi_{ij,k}\phi(x_{i,j}|\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\mu}_{ij,k},\boldsymbol{\Sigma}_{ij,k}) \\
%	&=&\sum_{k=1}^{K_{ij}}\pi_{ij,k}\phi \left(x_{i,j},\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}},\Sigma_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}\right) \textrm{ with} \\
%	\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}&=& \mu_{j,k} + \operatorname{cov}(x_{i,j,k},\boldsymbol{X}_{i,O,k}^{I_r^j})\operatorname{var}(\boldsymbol{X}_{i,O,k}^{I_r^j})^{-1}(\boldsymbol{X}_{i,O}^{I_r^j}-\boldsymbol{\mu}_{\boldsymbol{X}_{i,O}^{I_r^j},k})\textrm{ and} \\
%	\Sigma_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}&=&\operatorname{var}(x_{i,j,k})-\operatorname{cov}(x_{i,j},\boldsymbol{X}_{i,O,k}^{I_r^j})\operatorname{var}(\boldsymbol{X}_{i,O,k}^{I_r^j})^{-1}\operatorname{cov}(\boldsymbol{X}_{i,O,k}^{I_r^j},x_{i,j,k})
%\end{eqnarray}
% 
%But we do not need to compute the variance because we only want $E_{\boldsymbol{X}_{\boldsymbol{M}}|\boldsymbol{X}_O,\boldsymbol{\alpha},\Theta,S}\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha}^{(h)},S,\Theta)\right]$ so the mean is sufficient, we impute  
%\begin{equation}
%	\hat{x}_{i,j}=\frac{1}{K_{ij}}\sum_{k=1}^{K_{ij}}\pi_{ij,k}\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}
%\end{equation}
%
%We have 
%\begin{eqnarray}
%	\forall l \in I_r^j,\forall 1\leq k \leq K_{ij}, \operatorname{cov}(x_{i,j,k},\boldsymbol{X}_{i,O,k}^{l})&=&\operatorname{cov}(x_{i,j,k},\sum_{h\in I_f^l}x_{i,h}\alpha_{h,l}+\varepsilon_{i,l})
%	=\alpha_{j,l}\sigma_{j,k}^2 \\
%	\forall l \in I_r^j,\forall 1\leq k \leq K_{ij}, \operatorname{var}(x_{i,l,k})&=&\sigma_l^2+\sum_{h \in I_f^l}\alpha_{h,l}^2\sigma_{h,k}^2 
%	\end{eqnarray}
%	$\forall l_1\neq l_2 \in I_r^j,\forall 1\leq k \leq K_{ij},$
%	\begin{eqnarray}
%	 \operatorname{cov}(x_{i,l_1,k},x_{i,l_2,k})&=&\operatorname{cov}(\sum_{h\in I_f^{l_1}}x_{i,h,k}\alpha_{h,l_1}+\varepsilon_{i,l_1},\sum_{h\in I_f^{l_2}}x_{i,h,k}\alpha_{h,l_2}+\varepsilon_{i,l_2})\\
%	&=& \operatorname{cov}(\sum_{h\in I_f^{l_1}}x_{i,h,k}\alpha_{h,l_1},\sum_{h\in I_f^{l_2}}x_{i,h,k}\alpha_{h,l_2}) \\
%	&=& \sum_{h \in I_f^{l_1}\cap I_f^{l_2}}\sigma_{h,k}^2\alpha_{h,l_1}\alpha_{h,l_2}
%\end{eqnarray}
%
%
	\subsection{Weighted penalty}
			Now we have defined the way to compute the likelihood, other questions remain : how to define the number of parameters in the structure ?		How to take into account missingness (structures relying on highly missing covariates should be penalized) ?
			We have seen that for a same covariate $X^j$ with $ j \in I_r$, the number of parameters is not the same for each individual depending whether or not $M_{i,j}=0$. But the penalty (for $\psi=BIC$) can't be added at the individual level (because $\log(1)=0$ so it would be annihilated). 
			
			To penalize models that suppose dependencies based only on a few individuals, we propose to use the mean of the complexities obtained for a given covariate.
			\begin{equation}
			k_j=\frac{1}{n}\sum_{i=1}^nk_{i,j}
\end{equation}						where $k_{i,j}$ is the number of parameter to estimate in $P(x_{i,j}|\boldsymbol{X}_i\setminus \boldsymbol{X}_i^j)$.
			\begin{eqnarray}
		-2\log P(\boldsymbol{X}|S)&\approx & BIC=-2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+|\boldsymbol{\Theta}|\log(n) \\
		&=& -2\mathcal{L}(\boldsymbol{X},S,\boldsymbol{\Theta})+(\sum_{j=1}^pk_j)\log(n)
	\end{eqnarray}
			 Thus if a structure is only touched by one missing value the penalty will be smaller than another same shaped structure but with more missing values implied.
			Another way would be to use $\psi=RIC$ (see \cite{foster1994risk}) so the complexity is associated with $\log(p)$ and can be added individually. Or to make a compromise and penalize by $\frac{k_i\log(p)}{\log(n)}$.
		
%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	
			
			In fact we have the same number of parameters to estimate with or without missing values but the problem is that some of the parameters are estimated based only on a portion of the individuals so each parameter has a different weight.
			The penalty in $k\log(n)$ stands for $k$ parameters each $n-estimated$ so our proposed penalty term is an intuitive way to use a weighted penalty.
	
\section{SEM}
	The integrated likelihood depicted above depends on $\boldsymbol{\alpha}$ which was formerly estimated by OLS when there was no missing values. But when missing values occurs in a sub-regression we need another solution.
	
	We use a  Stochastic Expectation Maximization (SEM) algorithm \cite{celeux1986algorithme} to estimate $\boldsymbol{\alpha}$ because missing values do not allow to use OLS and  the log-likelihood (\ref{loglikmiss}) is not linear so a simple Expectation-Maximization (EM) would be difficult to compute.
		
	\subsection{Our implementation of SEM}
	\paragraph{initialization:} We start with some imputation (for example by the mean) for each missing value (done only once for the MCMC). $\boldsymbol{\alpha}^{(0)}$ can be initialized by cutting method	(sparse structure) or using imputed values in $\boldsymbol{X}$.
	At iteration $h$,
	\paragraph{SE step:}
		We generate the missing values according to $P(\boldsymbol{X}_M|\boldsymbol{X}_O; \alpha^{(h)},\Theta,S)$, that is stochastic imputation.
	\paragraph{M step:}
		We estimate 
		\begin{equation}
	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}E\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha},S,\Theta) \right]
\end{equation}
and we can use the same method as the one for classical case without missing values (OLS, SUR, {\it etc.}).
		We continue until convergence ($\parallel \boldsymbol{\alpha}^{(h+1)} - \boldsymbol{\alpha}^{(h)}\parallel < tol $ where $tol$ is the tolerance). Then we make $m$ iterations and take $\hat{\boldsymbol{\alpha}}$ as the mean of these $m$ last iterations.
		
		
Another (faster but not optimal) way would be to only use the structure for $\boldsymbol{X}^{I_r}$ and use the distribution given by Mixmod for $\boldsymbol{X}^{I_f}$ along the MCMC. The full SEM would then be used only once with the final structure to make imputation in $\boldsymbol{X}$ before using variable selection methods like the LASSO.


	\subsection{Stochastic imputation by Gibbs sampling}
		We use a Gibbs sampling method to generate the missing values at the SE step. $\boldsymbol{X}$ follows a multivariate Gaussian mixture with $K$ component and we note $Z$ the set of the $Z_{i,j}$ indicating the component from which $x_{i,j}$ is generated.
		\paragraph{Initialisation:} all the $z_{i,j}$ are set to the first component (such an initialisation does not depend on $K$) and $\boldsymbol{X}_M$ are imputed by the marginal means.
		\paragraph{Iteration:} At each iteration of the Gibbs sampler: \\
			$\forall x_{i,j} \in \boldsymbol{X}_M^{I_r}$:  $x_{i,j}$ is generated according to 
			\begin{eqnarray}
			P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}},Z;\boldsymbol{\alpha}^{(h)},\Theta,S)&=&P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}};\alpha^{(h)},\Theta,S) \\
			&=&P(x_{i,j}|\boldsymbol{X}_i^{I_f^j};\boldsymbol{\alpha}^{(h)},\Theta,S)=\mathcal{N}(\boldsymbol{X}_i^{I_f^j}\boldsymbol{\alpha}^{(h)}_{I_f^j,j};\sigma_j^2 )
			\end{eqnarray}		
			We have $P(\boldsymbol{X}|Z)=\mathcal{N}(\boldsymbol{\mu}_{|Z},\boldsymbol{\Sigma}_{|Z})$. \\
			$\forall x_{i,j} \in \boldsymbol{X}_M^{I_f}$:  $x_{i,j}$ is generated according to 
			\begin{eqnarray}
			P(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}},Z;\boldsymbol{\alpha}^{(h)},\Theta,S)&=&P(x_{i,j}|\boldsymbol{X}_{i,\bar{j}},Z_i;\boldsymbol{\alpha}^{(h)},\Theta,S)			
			\end{eqnarray}			
			\begin{eqnarray}
			=\mathcal{N}(\mu_{j|Z_{i,j}} + \boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}^{-1}_{X_{\bar{ij}},X_{\bar{ij}}|Z_i}(X_{\bar{ij}}-\boldsymbol{\mu}_{X_{\bar{ij}}|Z_i}) ;  \sigma_{j|Z_{i,j}}^2-\boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}^{-1}_{X_{\bar{ij}},X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}')
			\end{eqnarray}		
			Where all the values needed here were described above for the likelihood computation.
%With $\forall j \in I_r$ 
%\begin{equation}
%	\operatorname{var}_{|Z_{i}}(x_{i,j})=\sigma_{j|Z_{i,j}}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{l|Z_i,l}^2=\sigma_{j}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{l|Z_i,l}^2
%\end{equation}			
%	$\forall j \notin I_r $
%\begin{equation}
%	\operatorname{var}_{|Z_{i}}(x_{i,j})=\sigma_{j|Z_{i,j}}^2
%\end{equation}			
%	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}\neq \emptyset $
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=\sum_{k\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{k,j_1}\alpha_{k,j_2}\operatorname{var}_{|Z_i}(x_{k|Z_i,k}) =\sum_{k\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{k,j_1}\alpha_{k,j_2}\sigma_{k|Z_{i,k}}^2
%\end{equation}
%	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}= \emptyset $
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=0
%\end{equation}
%	$\forall j_1 \in I_f, j_2 \in I_f$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=0
%\end{equation}
%	$\forall j_1 \in I_r, j_2 \in I_f^{j_1}$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})= \alpha_{j_2,j_1}\sigma^2_{j_2|Z_{i,j_2}}
%\end{equation}
%$\forall j_1 \in I_r, j_2 \notin I_f^{j_1}\cup I_r$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})= 0
%\end{equation}
%We see that the $0$ in the variance-covariance matrix does not depend on $Z$ so the structure of sparsity of $\boldsymbol{\Sigma}$ can be stored and used back in each iteration for a given structure $S$ to reduce computing time.

	Then, $\forall 1\leq i \leq n, \forall j \in I_f$ we draw new values for $Z_{i,j}$ according to
	\begin{eqnarray}
		P(Z_{i,j}|\boldsymbol{X},Z_{\bar{i,j}};\Theta,\boldsymbol{\alpha},S)&=&P(Z_{i,j}|\boldsymbol{X}_i,Z_{i,\bar{j}};\Theta,\boldsymbol{\alpha},S)=\mathcal{M}(t_{i,j,1},\dots ,t_{i,j,K_j}) \\
		\textrm{where } t_{i,j,k}&=&\frac{\pi_{j,k}\Phi(x_{i,j};\mu_{j,k},\sigma_{j,k}^2)}{\sum_{l=1}^{K_j}\pi_{j,l}\Phi(x_{i,j};\mu_{j,l},\sigma_{j,l}^2) }
	\end{eqnarray}
		
	
	
	We see that $Z_{i,j}$ are not used if there is no missing values in $\boldsymbol{X}_i$ and others are not all needed so we can also optimize computation time by  computing only the $Z_{i,j}$ that are needed in the Gibbs.
	For the last iteration of the Gibbs, in the last iteration of the SEM, we do not need to draw $Z$.	
	
	Instead of using long chain for each Gibbs, we can use small chains because SEM iteration will simulate longer chains so it remains efficient with a smaller computation cost.
	
	Computation cost will be the main purpose here because we need an iterative algorithm (Gibbs sampler) at each iteration of another iterative algorithm (SEM) for each candidate of the MCMC.
	So alternative method should be preferred for large datasets with many missing values and only a small amount of time.
	
	Because $K$ can be very large we search a way to compute the likelihood.
	We can use a Gibbs algorithm to estimate the likelihood:
	\begin{eqnarray}
	P(\boldsymbol{X}_O;\Theta, S, \boldsymbol{\alpha})&=& 
		\sum_{Z\in \mathcal{Z}}\int_{\boldsymbol{X}_M}\frac{P(\boldsymbol{X}_M,Z,\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},S)}{P(\boldsymbol{X}_M,Z|\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},S)}P(\boldsymbol{X}_M,Z|\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},S) dX \\
		&\approx &\frac{1}{Q} \sum_{q=1}^Q\frac{P(\boldsymbol{X}_M^{(q)},\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha},S)}{P(\boldsymbol{X}_M^{(q)}|\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha},S)} \textrm{ by the law of large numbers}
	\end{eqnarray}		
	where $Q$ is the number of iterations of the Gibbs sampler.
	But to be faster, we use the previous Gibbs algorithm with:
	\begin{eqnarray}
	P(\boldsymbol{X}_O;\Theta, S, \boldsymbol{\alpha})&\approx & \frac{1}{Q} \sum_{q=1}^Q\frac{P(\boldsymbol{X}_M^{(q)},\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha}^{(q)},S)}{P(\boldsymbol{X}_M^{(q)}|\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha}^{(q)},S)}
	\end{eqnarray}	
		
\subsection{Alternative E step}
	If we can't (or don't want to) compute the SE step described above, then we can use alternative imputation step for missing data based on $\boldsymbol{\alpha}$ (and keep the alternate optimisation to find the best $\boldsymbol{\alpha}$). 
	
	$\forall x_{i,j} \in \boldsymbol{X}_M $ we have:
	\\
if $j\in I_r$, Equation(\ref{Missingdensity}) gives: 
	\begin{eqnarray}
	E[x_{i,j}|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S]&=&E[\sum_{k=1}^{k_{ij}}\pi_{ij,k}\Phi(x_{i,j}|\mu_{ij,k},\Sigma_{ij,k})|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S] 
	\end{eqnarray}
	  
	Let $r_{i,j}=\{l \in I_r| \boldsymbol{\alpha}_{j,l}\neq 0, \boldsymbol{M}_{i,j}=0 \}$ the set of observed covariates for individual $i$ that are explained by $x_{i,j}$ according to $S$.
	\\
	If $j\notin I_r$ we can do:
	\begin{eqnarray}
	E[x_{i,j}|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S]&=&\frac{1}{|r_{i,j}|}\sum_{k \in r_{i,j}}E_{|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S}\left[\frac{1}{\alpha_{j,k}}\left(x_{i,k}-\varepsilon_{k}(i)-\sum_{l \in I_f^k} x_{i,l}\alpha_{l,k}\right)\right] \\
	&=& \frac{1}{|r_{i,j}|}\sum_{k \in r_{i,j}}E_{|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S}\left[\frac{1}{\alpha_{j,k}}\left(x_{i,k}- \sum_{l \in I_f^k} x_{i,l}\alpha_{l,k}\right)\right]
	\end{eqnarray}
	that is the mean of the expectations of the inverse sub-regressions implying $x_i,j$ with value in $\boldsymbol{X}^{I_r}_i$ not missing.



%		\subsection{Estimation of the coefficients in each regression}
%			Estimating the $\boldsymbol{\alpha}_j$  with missing values is just estimating independent regressions with missing values. We have seen in equation (\ref{Missingdensity}) that we know the expression of this density for a given the $\boldsymbol{\alpha}_j$. So it's just about maximizing the likelihood of this density on the $\boldsymbol{\alpha}_j$. This can be done with an Expectation-Maximization (EM) algorithm \cite{dempster1977maximum} or one of its extensions \cite{mclachlan2007algorithm}.
%			
%step E: ($\Theta$ stands for the parameters of the gaussian mixtures for the marginal distributions, estimated once by Mixmod):
%\begin{equation}
%	\boldsymbol{X}^{(h)}=E[\boldsymbol{X}|\boldsymbol{X}_{O},\boldsymbol{\alpha}^{(h)},\boldsymbol{\varepsilon},\Theta,S]
%\end{equation}			
%	
%	step M:	
%\begin{equation}
%	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}(\mathcal{L}(\boldsymbol{X}^{(h)},\boldsymbol{\alpha},\boldsymbol{\varepsilon},\Theta,S)) \textrm{ by OLS}
%\end{equation}	
%	
%		
%			But estimation of the $\boldsymbol{\alpha}_j$ is the most critical part of the MCMC in terms of computational time so it could be a bad idea to put there another iterative algorithm. 
%			Alternatives does exist :
%			\begin{itemize}
%				\item Because sub-regression are supposed to be parsimonious, we could imagine to estimate each column of $\boldsymbol{\alpha}$ with full sub-matrices of $\boldsymbol{X}_f$. When relying on too much missing values, $\hat{\boldsymbol{\alpha}}$ would be a bad candidate and then penalized directly by the likelihood (and it could be a good thing). Computational cost would be reduced significantly.
%				\item To estimate the $\boldsymbol{\alpha}_j$ (and not for the global likelihood) we could use data imputation (by the mean) and then obtain a full matrix but still ignoring missing values when estimating the likelihood. Imputation only concerns the estimation of the sub-regression coefficients and because null coefficients in sub-regression are coerced at each step, imputation only concerns a few covariates each time.
%			\end{itemize}
%			
%			
%			 $\forall j \in I_r$, estimation of $\boldsymbol{\alpha}^j$ only depends on individuals not missing in $\boldsymbol{X}^j$ (individuals are {\it iid}).
%			 So we work with a restriction of $\boldsymbol{X}$ for each $\boldsymbol{\alpha}^j$. Thus in this section, to simplify the notation, we will consider no missing values in $\boldsymbol{X}_r$ but in fact we work with restrictions.
%			
%			The EM algorithm can be written here: we start with some $\Theta^{(0)}=(\boldsymbol{\alpha},\boldsymbol{\varepsilon}) $ initial value for $\Theta$. The $\pi_{ij,k}$ are estimated once for each covariate (for example by Mixmod) and stay the same during the EM algorithm.
%			Naive E step : estimation of 
%			\begin{equation}
%				\boldsymbol{X}^{(h)}=E(\boldsymbol{X}|\boldsymbol{X}_{\bar M},\Theta^{(h)}) \textrm{ so it simply is}
%			\end{equation}
%			$\forall (i,j), \boldsymbol{M}_{i,j}=1, j\neq I_r$, 			
%			\begin{equation}
%				x^{(h)}_{i,j}=E(x_{i,j}|\boldsymbol{X}_{\bar M},\Theta^{(h)}) =\sum_{k=1}^{k_{ij}}\pi_{ij,k}\mu_{ij,k}^{(h)} \label{Estep}
%			\end{equation}
%			where, $\forall j \in I_f, k_{ij}=k_j, \pi_{ij,k}=\pi_{j,k}, \mu_{ij,k}=\mu_{j,k}$ \\
%			M-step : we determine $\Theta^{(h+1)}$ as the solution of the equation
%			\begin{equation}
%				E(\boldsymbol{X}|\Theta)=\boldsymbol{X}^{(h)} \textrm{ done by OLS}
%			\end{equation}
%			So the M step is just computing linear regressions on the filled dataset.
%			
%			
%		real E step : individuals are $iid$ so we just look at the expression for one individual, and use it for all
%		$\forall 1\leq n \leq n , \forall j \notin I_r$, we note $\bar{\boldsymbol{X}}_{i,j}=(\boldsymbol{X}_{\bar M}\cap \boldsymbol{X}_{i} \setminus \boldsymbol{X}^j)$ 
%			\begin{eqnarray}
%				P(\boldsymbol{X}_{fi}^M,\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M} | \Theta)&=&
%					P(\boldsymbol{X}_{fi}^M|\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M}|\Theta) \\
%				&=&P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M}|\Theta) \\
%				P(\boldsymbol{X}_{fi}^M|\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M},\Theta)&=&\frac{P(\boldsymbol{X}_{ri}^M|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M}|\Theta)}{P(\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M}|\Theta)} \\
%				&=&\frac{P(\boldsymbol{X}_{ri}^{O}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M}|\Theta)P(\boldsymbol{X}_{fi}^{\bar M}|\Theta)
%				}{
%				P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{\bar M}|\Theta)
%				} \\
%				&=&\frac{P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M}|\Theta)}{P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{\bar M},\Theta)}
%			\end{eqnarray}
%			
%			No imputation for missing left. Imputations for missing right are just used to obtain $\hat{\boldsymbol{\alpha}}$ but not when computing the $BIC$ or $BIC_+$.
%			
			
	\section{Missing values in the main regression}
		The easier way would be to draw missing values with the SEM described above and then use classical methods on the completed dataset, with the possibility to repeat this procedure a few times and then take the mean. We should for example try multiple draw and LASSO for variable selection like variable selection by random forest. One great advantage of multiple draw procedures is that it gives an idea of the precision of the imputations with the variance of these imputed values among the multiple draws. So we know whether it is reliable or not. 
		
		But another way would be to consider classical estimation methods as likelihood optimizer and then adapt them to the integrated likelihood of our model. Thus we can imagine to use LASSO without imputation. But the choice of the penalty using the LAR algorithm need also to adapt the LAR that is based on correlations that are computed on vectors with distinct number of individuals (due to missing values). So it requires more work but could be a good perspective for our method.
	\section{Numerical results on simulated datastets}
		\subsection{Estimation of the sub-regression coefficients}
		\subsection{Multiple imputation for the main regression}
	\section{Missing values in real life}
		One advantage of our regression model is that it does not depend on the response variable $\boldsymbol{Y}$ so the structure can be learnt independently. Thus we can imagine to obtain big samples to learn the structure without being annoyed by the missing values. Then when a response variable is chosen, we can keep the same S and use previously computed values of $\boldsymbol{\alpha}$ as initial value for the SEM. 
		
		To be able to evaluate imputation quality we work with a real dataset but artificial missing values.
				
		
\chapter{Taking back the residuals}
	We have seen that eviction of redundant covariates improves the results by a good trade-off between dimension reduction and better conditioning versus keeping all the information. But The fact is that we lost some information and we want to get it back.
	\section{The model}
		After the estimation of the marginal model, we know both $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\beta}^*}$.
		\begin{eqnarray}
			\boldsymbol{Y}&=& \boldsymbol{X}^{I_r}\boldsymbol{\beta}_{I_r}+\boldsymbol{X}^{I_f}\boldsymbol{\beta}_{I_f}+\boldsymbol{\varepsilon}_Y \\
			\boldsymbol{X}^{I_r}&=&\boldsymbol{X}^{I_f}\boldsymbol{\alpha}+\boldsymbol{\varepsilon} \\
			\boldsymbol{Y}&=& \boldsymbol{X}^{I_r}\underbrace{(\boldsymbol{\beta}_{I_r}+\boldsymbol{\alpha}\boldsymbol{\beta}_{I_f})}_{\boldsymbol{\beta}^*}+\boldsymbol{\varepsilon}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y \\
			\boldsymbol{Y}- \boldsymbol{X}^{I_r}\boldsymbol{\beta}^*&=&\boldsymbol{\varepsilon}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y \\
			\boldsymbol{\varepsilon}&=&\boldsymbol{X}^{I_r}-\boldsymbol{X}^{I_f}\boldsymbol{\alpha}
		\end{eqnarray}		 
		So we introduce a plug-in model
		\begin{eqnarray}
			\underbrace{\boldsymbol{Y}- \boldsymbol{X}^{I_r}\hat{\boldsymbol{\beta}^*}}_{\tilde{\boldsymbol{Y}}}&=&\underbrace{(\boldsymbol{X}^{I_r}-\boldsymbol{X}^{I_f}\hat{\boldsymbol{\alpha}})}_{\tilde{\boldsymbol{X}}}\boldsymbol{\beta}_{I_r}+\boldsymbol{\varepsilon}_Y \\
		\end{eqnarray}
		That allows us to estimate $\boldsymbol{\beta}_{I_r}$ with a classical linear model based on previous estimations of $\boldsymbol{\beta}^*$ and $\boldsymbol{\alpha}$.
		Then we have a model with a smaller noise
		\begin{equation}
			\boldsymbol{Y}= \boldsymbol{X}^{I_r}\boldsymbol{\beta}^* + \boldsymbol{\varepsilon}\hat{\boldsymbol{\beta}}_{I_r}+\boldsymbol{\varepsilon}_Y 
		\end{equation}
	\section{Interpretation and latent variables}
			$\hat{\boldsymbol{\beta}}_{I_r}$ can be interpreted as the proper effect of $\boldsymbol{X}^{I_r}$ on $\boldsymbol{Y}$ in that it is the effect of the part of $\boldsymbol{X}^{I_r}$ that is independent from other covariates. Then if $\boldsymbol{X}^{I_r}$ is correlated to $\boldsymbol{Y}$ only through its correlation with $\boldsymbol{X}^{I_f}$ this sequential estimation will point it out and give a parsimonious model ($\hat{\boldsymbol{\beta}}_{I_r}=0$) but the real stake is greater. We can see $\boldsymbol{\varepsilon}$ as a latent variable instead of the noise of a sub-regression. This latent variable is known to be independent of $\boldsymbol{X}^{I_f}$ and dependent of $\boldsymbol{X}^{I_r}$ so we can appreciate its meaning and we also know its value by $\hat{\boldsymbol{\varepsilon}}=\boldsymbol{X}^{I_r}-\boldsymbol{X}^{I_f}\hat{\boldsymbol{\alpha}}$. Thus, it can reveal some kinds of latent variables.
			
	
\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQE_toutOLSp5col.png}\label{MQE2}
	\caption{MSE of OLS (plain red) and CorReg marginal(blue dashed) and CorReg full (green dotted) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$.}
\end{figure}	
	
	\section{Consistency}\label{consistency}
		Consistency issues of the LASSO are well known and Zhao \cite{Zhao2006MSC} gives a very simple example to illustrate it.
		We have taken the same example to show how our method is more consistent.
		Here $p=3$ and $n=1000$.\\
		We define $\boldsymbol{X}^{I_f}, \boldsymbol{X}^{I_r}, \boldsymbol{\varepsilon}_Y, \boldsymbol{\varepsilon}_{X} i.i.d. \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}_n)$ and then \\
		$\boldsymbol{X}_3=\frac{2}{3}\boldsymbol{X}_1+\frac{2}{3}\boldsymbol{X}_2+\frac{1}{3}\boldsymbol{\varepsilon}_X$ and \\
		$\boldsymbol{Y}=2\boldsymbol{X}_1+3\boldsymbol{X}_2+\boldsymbol{\varepsilon}_Y$.\\
		We compare consistencies of complete, marginal and full plug-in model with LASSO (and LAR) for selection.
		It happens that the algorithm don't find the true structure but a permuted one so we also look at the results obtained with the true $S$ (but $\hat{\boldsymbol{\alpha}}$ is used) and with the structure found by the Markov chain after a few seconds.
		
		True $S$ was found $340$ times on $1000$ tries (model is not identifiable because $\boldsymbol{X}^j$ are all Gaussian).
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & CorReg + marginal LASSO& CorReg + full plug-in LASSO\\ 
		\hline 
		True $S$ &  1.006479 & \textbf{1.005468} & \textbf{1.006093} \\ 
		\hline 
		$\hat{Z}$ & \textbf{1.006479} & 1.884175 & 1.006517 \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observed on a validation sample (1000 individuals)}
		\end{table}

		We observe as we hoped that our marginal model is better when using true $S$ (coercing real zeros) and that marginal with $\hat{S}$ is penalized (coercing wrong coefficients to be zeros when true $S$ is not found).
		But the main point is that the plug-in model stay better than the classical one with the true $S$ and corrects enough the marginal model to follow the classical LASSO closely when using $\hat{S}$. 
		And when we look at the consistency :
		\begin{table}[h!]	
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & CorReg + marginal LASSO& CorReg + full plug-in LASSO \\ 
		\hline 
		True $S$ &  0 & 1000 & 830 \\ 
		\hline 
		$\hat{S}$ & 0 & 340 & \textbf{621} \\ 
		\hline 
		\end{tabular} 
		\caption{Number of consistent model found ($\boldsymbol{Y}$ depending on $\boldsymbol{X}_1,\boldsymbol{X}_2$ and only them) on $1000$ tries}
		\end{table}				
		
		$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		
		We also made the same experiment but with $\boldsymbol{X}_1,\boldsymbol{X}_2$ (and consequently $\boldsymbol{X}_3$) following Gaussian mixtures (to improve identifiability) randomly generated by our CorReg package for R. 
		True $S$ is now found $714$ times on $1000$ tries \label{testidentifiable}. So it confirms that Gaussian mixture models are easier to identify.
		
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & CorReg + marginal LASSO& CorReg + full plug-in LASSO \\ 
		\hline 
		True $S$ &  1.571029 & \textbf{1.569559} & \textbf{1.570801} \\ 
		\hline 
		$\hat{S}$ & 1.005402 & 1.465768 & \textbf{1.005066} \\ 
		\hline 
		\end{tabular} 
		\caption{MSE observed on a validation sample (1000 individuals)}
		\end{table}

		And when we look at the consistency :
		\begin{table}[h!]
		\centering	
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical LASSO & CorReg + marginal LASSO& CorReg + full plug-in LASSO \\ 
		\hline 
		True $S$ &  0 & 1000 & 789 \\ 
		\hline 
		$\hat{S}$ & 0 & 714 & \textbf{608} \\ 
		\hline 
		\end{tabular} 
		\caption{Number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}
		\end{table}				
				
		
		$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		

	\section{Numerical results}

\chapter{CorReg: the concept}	
	
		\textsc{CorReg} is already downloadable on the CRAN under CeCILL Licensing. This package permits to generate datasets according to our generative model, to estimate the structure (C++ code) of regression within a given dataset and to estimate both explicative and predictive model with many regression tools (OLS,stepwise,LASSO,elasticnet,clere,spike and slab, adaptive lasso and every models in the \textsc{lars} package). So every simulation presented above can be done with \textsc{CorReg}.
	\textsc{CorReg} also provides tools to interpreat found structures and visualize the dataset (missing values and correlations). %More informations can be found on the website www.correg.org which is dedicated to \textsc{CorReg}.
	The objective of CorReg is to bring recent statistical tools to engineers. Thus it will be made  available in Microsoft Excel for the end of the year 2014, probably using Basic Excel R Toolkit(BERT\footnote{https://github.com/StructuredDataLLC/Basic-Excel-R-Toolkit}). It also provides some small scripts in functions to obtain graphical representations and basic statistics with legends for non-statistician with only one command line (or macro button in Excel). 
	
\chapter{Conclusion and perspectives}
	\section{Conclusion}
		Our model is easy to understand and to use. Usage of linear regression to model the correlations definitely separates us from "black boxes" so users are confident in what they do. The well-known and trivial sub-regression found comfort users in that if a structure does exist, CoMPASS will find it so when a new sub-regression, or a new main regression is given they are more likely to look further and try it. The automated aspect shows the power of statistics without a priori so users begin to understand that statistics are not only descriptive or predictive but based on {\it a priori} models. This method has a positive impact on the way users looks at the statistics.
			It is good to see that sequential methods (predictive model) and automation can produce good results. Probabilistic models are efficient even without human expertise and let the experts improve the results by adding their expertise in the model (coercing some sub-regression for example).
		
		
	\section{Perspective}
		\subsection{Non-linear regression}
			Polynomial regression, logistic regression \cite{hosmer2000applied}, {\it etc.} could be improved by a method like this.
		\subsection{Pretreatment not only for regression}
			Classification and Regression Tree, and any other method could benefit of the variable selection pretreatment implied by our marginal model.
		\subsection{Improved programming}
			Even if it is written in C++, the algorithm could be optimized by a better usage of sparse matrices, memory usage optimization, and other small things that could reduce computational cost to be faster and allow to work with larger datasets (already works with thousands of covariates).
		\subsection{Missing values in classical methods}
			The full generative approach could be used to manage missing values without imputation for many classical methods.
			It can notably be used for clustering and not only in response variable prediction context.
		\subsection{Interpretation improvements}
			Ergonomy of the software could be improved to better fit industrial needs.
\cleardoublepage

\addcontentsline{toc}{chapter}{References}
\bibliographystyle{apalike}
%\bibliographystyle{plain}
\nocite{*}
\bibliography{biblio}
%\addcontentsline{toc}{chapter}{Appendices}
%\appendix
%	\chapter{Graphs and CorReg}
%		\section{Matricial notations}
%		\section{Properties}
%		Toute matrice binaire est associable à une matrice d'adjacence d'un graphe orienté (DAG)
%		
%		Matrice nilpotente = carré nul
%		
%		carré d'une matrice d'adjacence => chemins de longueur 2
%		
%		donc matrices binaires nilpotentes = graphes orientés sans chemins de longueur 2 donc celui qui reçoit n'emet pas donc graphe bi-partie
%		
%		dénombrement : liste systématique des possibilités (par décompostion comme dans la hiérarchie)		
%		
%	\chapter{Mixture models}
%		\section{Linear combination}
%			
%		\section{Industrial examples}	
\end{document}
