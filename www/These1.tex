\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[toc,page]{appendix}
\usepackage{array}%,multirow,makecell}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{graphicx}
%\usepackage{framed}
%\usepackage[tikz]{bclogo}
\usepackage{hyperref}
\usepackage{url}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%\makegapedcells

\author{Clément TH\'ERY}
\title{Model-based linear regression for correlated  and missing covariates, Application to steel industry datasets.}

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%

\xdefinecolor{vertclair}{named}{PaleGreen}
\xdefinecolor{vertfonce}{named}{ForestGreen}

\newtheorem{hyp}{Hypothesis}
\newtheorem{hypfr}{Hypothèse}


\definecolor{darkgreen}{rgb}{0,0.4,0}
	 \definecolor{darkred}{rgb}{0.75,0,0}
	 \definecolor{darkblue}{rgb}{0,0,0.4}


\begin{document}
%\centering
%	Université Lille 1 \\
%	\'Ecole Doctorale Sciences Pour l'Ingénieur Université Lille Nord-de-France
%	
\maketitle

\newpage
\itshape To my sons
\upshape


\chapter*{Résumé}
	Les travaux effectués durant cette thèse ont pour but de pouvoir pallier le problème des corrélations au sein des bases de données, particulièrement fréquentes dans le cadre industriel. Une modélisation explicite des corrélations par un système de sous-régressions entre covariables permet de pointer les sources des corrélations et d'isoler certaines variables redondantes. 
	\\
	
	Il en découle une pré-sélection de variables nettement moins corrélées sans perte significative d'information et avec un fort potentiel explicatif (la pré-selection elle-même est expliquée par la structure de sous-régression qui est simple à comprendre car uniquement constituée de modèles linéaires). \\
	
	Un algorithme de recherche de structure de sous-régressions est proposé, basé sur un modèle génératif complet sur les données et utilisant une chaîne MCMC (Monte-Carlo Markov Chain). Ce prétraitement est utilisé pour la régression linéaire à des fins illustratives mais ne dépend pas de la variable réponse et peut donc être utilisé de manière générale pour toute problématique de corrélations.\\
	
	Par la suite, un estimateur plug-in pour la régression linéaire est proposé pour ré-injecter les variables redondantes de manière séquentielle et donc utiliser toute l'information sans souffrir des corrélations entre covariables.
	\\
	
	Enfin, le modèle génératif complet offre la perspective de pouvoir être utilisé pour gérer d'éventuelles valeurs manquantes dans les données, tant pour la recherche de structure que pour de l'imputation multiple préalable à l'utilisation de méthodes classiques incompatibles avec la présence de valeurs manquantes. Cela permet également d'estimer les valeurs manquantes et à terme  de fournir un estimateur de la variance de leur estimation.
	Encore une fois, la régression linéaire vient illustrer l'apport de la méthode qui reste cependant générique et pourrait être appliquée à d'autres contextes tels que le clustering.
	\\	
	
	Tout au long de ces travaux, l'accent est mis principalement sur l'interprétabilité des résultats en raison du caractère industriel de cette thèse. 
\\	

	Le package R intitulé {\tt CorReg}, disponible sur le CRAN\footnote{http://cran.r-project.org} sous licence CeCILL\footnote{http://www.cecill.info}, implémente les méthodes développées durant cette thèse.
	
\paragraph{Mots clés:}Prétraitement, Régression, Corrélations, Valeurs manquantes, MCMC, modèle génératif, Critère Bayésien, sélection de variable, méthode séquentielle, graphes.
\chapter*{Abstract}
	This thesis was motivated by correlation issues in real datasets, in particular industrial datasets. The main idea stands in explicit modeling of the correlations between covariates by a structure of sub-regressions, that simply is a system of linear regressions between the covariates. It points out redundant covariates that can be deleted in a pre-selection step to improve matrix conditioning without significant loss of information and with strong explicative potential because this pre-selection is explained by the structure of sub-regressions, itself easy to interpret.
	\\
	
	An algorithm to find the sub-regressions structure inherent to the dataset is provided, based on a full generative model and using Monte-Carlo Markov Chain (MCMC) method. This pretreatment is then applied on linear regression to show its efficiency but does not depend on a response variable and thus can be used in a more general way with any correlated datasets.
	\\
	
	In a second part, a plug-in estimator is defined to get back the redundant covariates sequentially. Then all the covariates are used but the sequential approach acts as a protection against correlations.
\\

	Finally, the generative model defined here allows, as a perspective, to manage missing values both during the MCMC and then for imputation (for example multiple imputation) to be able to use classical methods that are not compatible with missing datasets. Missing values can be imputed with a confidence interval to show estimation accuracy. Once again, linear regression is used to illustrate the benefits of this method but it remains a pretreatment that can be used in other contexts, like clustering and so on.
	\\ 
	
	The industrial motivation of this work defines interpretation as a stronghold at each step. 	
	\\
	The R package {\tt CorReg}, is on CRAN\footnote{http://cran.r-project.org} now under CeCILL\footnote{http://www.cecill.info} license. It implements the methods created during this thesis.
	
	 	
\paragraph{Keywords:} Pretreatment, Regression, Correlations, Missing values, MCMC, generative model, Bayesian Criterion, variable selection, plug-in method,\dots
	

\chapter*{Acknowledgments}
	I want to thank ArcelorMittal for the funding of this thesis, the opportunity to make this thesis with real datasets and the confidence in this work that has led me to be recruited since May.% but let me work in Villeneuve d'Ascq this last year to reduce road time.
	\\
	
	But this work would not have been possible without the help of Gaétan LORIDANT, my hierarchic superior and friend who has convinced ArcelorMittal to fund this thesis and helped me in this work by a strong moral support and spending a great amount of time with me to find the good direction between academic and industrial needs with some technical help when he could. I would not have made all this work without him.\\
	
	I also want to thank Christophe BIERNACKI, my academic director who accepted to lead this work even if the subject was not coming from the university. He also has spent a lot of time on this thesis with patience and has trust in the new method enough to share it with others, and it really means a lot to me. \\
	
	The last year I have worked mostly in the INRIA center with M$\Theta$dal team, especially those from the "bureau 106 " who helped me to put {\tt CorReg} on CRAN (in particular Quentin GRIMONPREZ) and those who had already submitted something on CRAN know that it is not always fun to achieve this goal. They also helped me in this last and tough year just by their presence, giving me the courage to go further. \\
	
	I finally want to thank my family. My wife who had to support most of the charge of the family on top of her work and to let me work during the holidays, and my three sons, Nathan, Louis and Thibault who have been kind with her even if they rarely saw their father during the week. I love them and am thankful for their comprehension.




\tableofcontents
\chapter{Résumé substantiel en français}
	\section{Position du problème}
	La régression linéaire est l'outil de modélisation le plus classique et se résume à une équation bien connue%\footnote{En général la constante est ajoutée comme un des régresseurs avec par exemple $\boldsymbol{X}^1=(1,\ldots,1)'$. Le coefficient correspondant dans $\boldsymbol{\beta}$ est alors la constante de régression $\beta_1$.} :
	\begin{equation}
		\boldsymbol{Y}|\boldsymbol{X};\boldsymbol{\beta},\sigma^2_Y=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}_Y
	\end{equation}
	où $\boldsymbol{Y}$ est la variable réponse de taille $n\times 1$ que l'on souhaite décrire à l'aide de $d$ variables explicatives\footnote{En général on ajoute une constante parmi les régresseurs. Par exemple $\boldsymbol{X}^1=(1,\ldots,1)'$. Le coefficient associé dans $\boldsymbol{\beta}$ est alors la constante de régression $\beta_1$.} observées sur $n$ individus et dont les valeurs sont stockées dans la matrice $\boldsymbol{X}$ de taille $n\times d$. Le vecteur $\boldsymbol{\beta}$ est le vecteur des coefficients de régression qui permet de décrire le lien linéaire entre $\boldsymbol{Y}$ et $\boldsymbol{X}$. Le vecteur $\boldsymbol{\varepsilon}_Y$  est un bruit blanc gaussien $\mathcal{N}(0,\sigma_Y^2\boldsymbol{I}_n)$ qui représente l'inexactitude du modèle de régression.\\
	
	On connaît l'estimateur sans biais de variance minimale (ESBVM) de $\boldsymbol{\beta}$ qui est obtenu par Moindres Carrés Ordinaires (MCO) selon la formule  :
	\begin{equation}
		\hat{\boldsymbol{\beta}}=(\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\boldsymbol{Y}.
	\end{equation}
	Cet estimateur nécessite l'inversion de la matrice $(\boldsymbol{X}'\boldsymbol{X})$ qui est mal conditionnée si les variables explicatives sont corrélées entre elles. Ce mauvais conditionnement nuit à la qualité de l'estimation et vient impacter la variance de l'estimateur comme le montre la formule :
	\begin{equation}
		\operatorname{Var}_{\boldsymbol{X}}(\hat{\boldsymbol{\beta}})=\sigma_Y^2(\boldsymbol{X}'\boldsymbol{X})^{-1}
	\end{equation}
	C'est cette situation problèmatique que nous nous proposons d'améliorer.
	\section{Modélisation explicite des corrélations}
	Le mauvais conditionnement de la matrice provient de la quasi-singularité (parfois singularité numérique) de celle-ci quand les colonnes de $\boldsymbol{X}$ sont presque linéairement dépendantes. Cette quasi dépendance linéaire peut être elle aussi modélisée par régression lineaire. On se propose donc de considérer notre problématique comme l'existence d'un modèle de sous-régressions au sein des variables explicatives avec certaines des variables expliquées par d'autres, formant ainsi une partition des $d$ variables en 2 blocs : les variables réponses (expliquées) et les variables prédictives. Notre modèle repose sur 2 hypothèses fondamentales :
	

\begin{hypfr}\label{H1fr}
Les corrélations entre covariables viennent uniquement de ce que certaines d'entre elles dépendent linéairement d'autres covariables. Plus précisément, il y a $d_{r}\geq 0$  ``sous-regressions'', chaque sous-regression $j=1,\ldots,d_{r}$ ayant la variable $\boldsymbol{X}^{J_{r}^j}$ comme variable {\it réponse} ($J_{r}^j\in\{1,\ldots,p\}$ et $J_{r}^j\neq J_{r}^{j'}$ pour $j\neq j'$) et ayant les $d_p^j>0$ variables $\boldsymbol{X}^{J_{p}^j}$  comme variables {\it predictives}  ($J_{p}^j\subset\{1,\ldots,d\} \backslash J_{r}^j$ et $d_p^j=|J_{p}^j|$ le cardinal de $J_{p}^j$):
\begin{equation}
\boldsymbol{X}^{J_{r}^j}|\boldsymbol{X}^{J_{p}^j};\boldsymbol{\alpha}_j,\sigma^2_j=\boldsymbol{X}^{J_{p}^j}\boldsymbol{\alpha}_j+\boldsymbol{\varepsilon}_j, \label{eq:SR_fr}
\end{equation}
où $\boldsymbol{\alpha}_j\in{\mathbb{R}^{d_r^j}}$ (${\alpha}_j^h\neq 0$ pour tout $j=1,\ldots,d_r$ et $h=1,\ldots,d_p^j$) et $\boldsymbol{\varepsilon}_j \sim\mathcal{N}_n(\boldsymbol{0},\sigma^2_j\boldsymbol{I})$.
\end{hypfr}

\begin{hypfr}\label{H2fr}
Nous supposons également que les variables réponses et les variables prédictives forment deux blocs disjoints dans $\boldsymbol{X}$ :
 pour toute sous-régression $j=1,\ldots,d_{r}$, $J_{p}^j\subset J_f$ où $J_{r}=\{J_{r}^1,\ldots,J_{r}^{d_r}\}$ est l'ensemble de toutes les variables réponses avec $J_f=\{1,\ldots,d\} \backslash J_{r}$ l'ensemble des variables non expliquées de cardinal $d_f=d-d_r=|J_f|$. Cette seconde hypothèse garantit l'obtention d'un système de sous-régression très simple et sans imbrications ni surtout aucun cycle. Cette hypothèse n'est pas trop restrictive dans la mesure où tout système sans cycle peut (par substitutions) être reformulé sous cette forme simplifiée (avec une variance accrue).
\end{hypfr}

\paragraph{Notations} Par la suite nous noterons $\boldsymbol{J}_r=(J_{r}^1,\ldots,J_r^{d_r})$ le $d_r$-uplet des variables réponses (à ne pas confondre avec $J_r$ défini plus haut), $\boldsymbol{J}_p=(J_{p}^1,\ldots,J_p^{d_r})$ le $d_r$-uplet des prédicteurs de toutes les sous-régressions, $\boldsymbol{d}_p=(d_p^1,\ldots,d_p^{d_{r}})$ les nombres correspondants de predicteurs et $\boldsymbol{S}=(\boldsymbol{J}_r,\boldsymbol{J}_p)$ le {\it model} global (structure de sous-régressions). Pour alléger les notations, on définit alors $\boldsymbol{X}_r=\boldsymbol{X}^{J_{r}}$ la matrice des variables réponses et $\boldsymbol{X}_f=\boldsymbol{X}^{J_{f}}$ la matrice de {\it toutes} les autres variables, ainsi considérées comme libres ({\it free} en anglais). Les valeurs des paramètres sont également concaténées : $\boldsymbol{\alpha}=(\boldsymbol{\alpha}_1,\ldots,\boldsymbol{\alpha}_{d_r})$ est la matrice des coefficients de sous-régression et $\boldsymbol{\sigma}^2=(\sigma^2_1,\ldots,\sigma^2_{d_r})$ le vecteur des variances associées :

	\begin{equation}
		\boldsymbol{X}_{r}|\boldsymbol{X}_f;\boldsymbol{\alpha},\boldsymbol{\sigma}^2=\boldsymbol{X}_{f}\boldsymbol{\alpha}+\boldsymbol{\varepsilon} \textrm{ \ \ \ (regression multiple multivariée)}
	\end{equation}
	La matrice $\boldsymbol{\varepsilon}\in \mathbb{R}^{n\times d_r}$ est la matrice des bruits des sous-régressions composée des colonnes \mbox{$\boldsymbol{\varepsilon}_j\sim \mathcal{N}(0,\sigma^2_j\boldsymbol{I}_n)$} que nous supposons indépendantes entre elles. Toutes ces notations sont illustrées dans l'exemple ci-après. \\
	
	

\fcolorbox{vertfonce}{vertclair}{\begin{minipage}{0.95\textwidth}
\paragraph{Données d'exemple :} 
$d=5$ variables dont 4 gaussiennes centrées réduites {\it i.i.d. } $\boldsymbol{X}_f=(\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{X}^4,\boldsymbol{X}^5)$ et une variable redondante $\boldsymbol{X}_r=\boldsymbol{X}^3=\boldsymbol{X}^1+\boldsymbol{X}^2+\boldsymbol{\varepsilon}_1$ avec $\boldsymbol{\varepsilon}_1\sim{\mathcal{N}(\boldsymbol{0},\sigma_1^2\boldsymbol{I}_n)}$. Deux régressions principales en $\boldsymbol{Y}$ sont testées avec $\boldsymbol{\beta}=(1,1,1,1,1)'$ et $\sigma_Y \in \{10,20\}$. Le conditionnement de $(\boldsymbol{X}'\boldsymbol{X})$ se déteriore donc quand $\sigma_1$ diminue. Ici $J_f=\{1,2,4,5\},J_r=\{3\},\boldsymbol{J}_r=(3),\boldsymbol{d}_p=2,\boldsymbol{J}_p=(\{1,2\}),\boldsymbol{\alpha}=(\boldsymbol{\alpha}_1)=((1,1)'),\boldsymbol{X} ^{J_p^1}=(\boldsymbol{X}^1,\boldsymbol{X}^2),\boldsymbol{S}=((3),(\{1,2\}))$. On note $R^2$ est le coefficient de détermination :\\
	\begin{equation}\label{defR2fr}
	R^2=1-\frac{\operatorname{Var}(\boldsymbol{\varepsilon}_1)}{\operatorname{Var}(\boldsymbol{X}^3)}
	\end{equation}
\end{minipage}}	
\\
La figure \ref{MQEOLScompl} illustre la détérioration de l'estimation quand les sous-régressions deviennent trop fortes ($R^2$ proche de 1) pour différentes valeurs de $n$ sur nos données d'exemple. 
	\paragraph{Remarques}
\begin{itemize}
\item Les sous-régressions définies en (\ref{eq:SR_fr}) sont très simples à comprendre pour l'utilisateur et permettent donc d'avoir un aperçu net des corrélations présentes dans les données étudiées.
%\item Les hypotheses~\ref{H1fr}  à ~\ref{H2fr} , rendent les variables dans $\boldsymbol{X}_f$ {\it indépendantes}.
\item $\boldsymbol{S}$ ne dépend pas de $\boldsymbol{Y}$ et peut donc être estimé séparément.
\end{itemize} 
	\section{Modèle marginal}
	Le fait de modéliser explicitement les corrélations entre les covariables nous permet de réécrire le modèle de régression principal. On peut en effet substituer les variables redondantes par leur sous-régression, ce qui revient à intégrer la régression sur $\boldsymbol{X}_{r}$ sachant la structure de sous-régressions $\boldsymbol{S}$. On fait alors une hypothèse supplémentaire :

\begin{hypfr}\label{H3fr}
On suppose l'indépendance 2 à 2 entre les erreurs de régression $\boldsymbol{\varepsilon}_Y$ et $\boldsymbol{\varepsilon}_j (j=1,\dots,d_r)$. En particulier on a l'indépendance conditionnelle entre les variables réponses \\$\{\boldsymbol{X}^{J_r^j}|\boldsymbol{X}^{J_p^j},\boldsymbol{S};\boldsymbol{\alpha}_j,\sigma_j^2\}$ définies en (\ref{eq:SR_fr}).
\end{hypfr}
On a donc 
\begin{eqnarray}
		\mathbb{P}(\boldsymbol{X}_r|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2)&=&\prod_{j=1}^{d_r}\mathbb{P}(\boldsymbol{X}^{J_r^j}|\boldsymbol{X}^{J_p^j},\boldsymbol{S};\boldsymbol{\alpha}_j,\sigma^2_j)	\\
		\mathbb{P}(\boldsymbol{Y}|\boldsymbol{X}_{f},\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma^2_Y ,\boldsymbol{\sigma}^2)&=& \int_{\mathbb{R}^{d_r}}\mathbb{P}(\boldsymbol{Y}|\boldsymbol{X}_{r},\boldsymbol{X}_{f},\boldsymbol{S};\boldsymbol{\beta},\sigma_Y^2)\mathbb{P}(\boldsymbol{X}_r|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2) d \boldsymbol{X}_{r}, \textrm{ \ \ \ \ }\\
		\end{eqnarray}
		ce qui donne
		\begin{eqnarray}
		\boldsymbol{Y}|\boldsymbol{X},\boldsymbol{S};\boldsymbol{\beta},\sigma_Y^2 &=& \boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y\\
	\boldsymbol{Y}|\boldsymbol{X}_{f},\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma^2_Y,\boldsymbol{\sigma}^2&=&\boldsymbol{X}_{f} (\boldsymbol{\beta}_{f}+ \sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\alpha}^*_j)+  \sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y  \label{marginalfr} \\
	&=&\boldsymbol{X}_{f}\boldsymbol{\beta}_{f}^*+\boldsymbol{\varepsilon}_Y^*
\end{eqnarray}
où $\boldsymbol{\alpha}^* \in \mathbb{R}^{d_f\times d_r}$ est la matrice des coefficients des sous-régressions avec $(\boldsymbol{\alpha}_j^*)_{J_p^j}=\boldsymbol{\alpha}_j $ et $(\boldsymbol{\alpha}_j^*)_{J_f\setminus J_p^j}=\bf 0  $, $\boldsymbol{\beta}_r=\boldsymbol{\beta}_{J_r}, \boldsymbol{\beta}_f=\boldsymbol{\beta}_{J_f}, \boldsymbol{\beta}_f^*=\boldsymbol{\beta}^*_{J_f} $, $\boldsymbol{\beta}^*$ étant l'estimateur de $\boldsymbol{\beta}$ par le modèle marginal avec donc $\boldsymbol{\beta}_r^*=\boldsymbol{\beta}^*_{J_r}$.
\\

On se retrouve donc avec un modèle marginal plus parsimonieux, sans biais sur $\boldsymbol{Y}$ (vrai modèle) mais avec une variance potentiellement accrue. Cet accroissement de la variance est proportionnel à $\boldsymbol{\varepsilon}$ qui est la matrice des résidus des sous-régressions. Plus les sous-régressions sont fortes et plus cette variance est faible.
Tout le principe du modèle marginal repose sur le compromis entre l'amélioration du conditionnement de $(\boldsymbol{X}'\boldsymbol{X})$ par suppression des variables redondantes et aussi la réduction de la dimension (le modèle marginal ne nécessite que l'inversion de $(\boldsymbol{X}_{f}'\boldsymbol{X}_{f})$) face au léger accroissement de la variance issu de la marginalisation.
On va donc comparer 
	\begin{eqnarray}
		\hat{\boldsymbol{\beta}}&=& (\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\boldsymbol{Y} \textrm{ au modèle marginal} \\
		\hat{\boldsymbol{\beta}}^*_f&=& (\boldsymbol{X}_{f}'\boldsymbol{X}_{f})^{-1}\boldsymbol{X}_{f}'\boldsymbol{Y}
	\end{eqnarray}
	Les deux modèles sont de dimension différente, on compare donc leurs erreurs moyennes quadratiques respectives (\textsc{mse} en anglais pour {\it Mean Squared Error}):
	\begin{eqnarray}
		\textsc{mse}(\hat{\boldsymbol{\beta}}|\boldsymbol{X})&=& \sigma^2_Y \operatorname{Tr}(\boldsymbol{X}'\boldsymbol{X})^{-1}  \\
		\textsc{mse}(\hat{\boldsymbol{\beta}}^*|\boldsymbol{X})&=&\parallel \sum_{j=1}^{d_r}\beta_{J_r^j}\boldsymbol{\alpha}_j^* \parallel_2^2+\parallel\boldsymbol{\beta}_r  \parallel_2^2 +(\sigma^2_Y+\sum_{j =1}^{d_r}\sigma^2_{j}\beta_{J_r^j}^2)\operatorname{Tr}(\boldsymbol{X}_f'\boldsymbol{X}_{f})^{-1}
	\end{eqnarray}
		Ces deux équations illustrent bien le compromis biais-variance du modèle marginal.
		La figure \ref{MSEOLSexpl} compare les erreurs d'estimation obtenues pour différentes valeurs des paramètres, montrant la nette amélioration rendue possible par la marginalisation.
		\paragraph{Remarque}
\begin{itemize}
%\item Les sous-régressions définies en (\ref{eq:SR_fr}) sont très simples à comprendre pour l'utilisateur et permettent donc d'avoir un aperçu net des corrélations présentes dans les données étudiées.
\item Les hypothèses~\ref{H1fr}  à ~\ref{H3fr}, rendent les variables dans $\boldsymbol{X}_f$ {\it non corrélées} ( covariance nulle : voir Lemme en section \ref{suffcondident}).
%\item $\boldsymbol{S}$ ne dépend pas de $\boldsymbol{Y}$ et peut donc être estimé séparément.
\end{itemize} 
	\section{Notion de prétraitement}
	Le modèle marginal peut être interprété comme étant un prétraitement par sélection de variables puisqu'on se ramène à un modèle de régression linéaire classique pour lequel n'importe quel estimateur peut être utilisé. Cela fait de ce modèle un outil générique. La préselection permet de cibler des variables qui n'interviendront pas dans le modèle final sans pour autant être indépendantes de $\boldsymbol{Y}$ donc le modèle final est parsimonieux mais ne fausse pas l'interprétation. L'estimation de $\boldsymbol{\beta}^*$ peut ensuite se faire en utilisant une quelconque méthode de sélection de variables pour éliminer les variables qui, elles, sont indépendantes de $\boldsymbol{Y}$.\\
	
	 On obtient donc deux types de 0 : ceux de la marginalisation qui pointent les variables redondantes et ceux de sélection qui viennent dans un second temps et pointent les variables indépendantes. L'interprétation est donc enrichie par rapport à une méthode de sélection classique qui fournirait le même modèle final. Or, le contexte industriel de ces travaux rend indispensable d'avoir une bonne qualité d'interprétation. L'objectif est donc atteint pour ce point précis. Notre modèle marginal est un outil de décorrélation de variables par préselection.
	 
	 \section{Estimation de la structure}
		La raison d'être de notre modèle marginal est la fragilité des méthodes de régression face à des covariables fortement corrélées. Il serait donc vain d'essayer les sous-régression en estimant les modèles de régression de chaque variable en fonction de toutes les autres car les corrélations nuisent à l'efficacité de ces modèles. Pour cette raison, nous avons établi un algorithme MCMC pour trouver le meilleur modèle de sous-régressions. L'idée consiste à voir la structure de sous-régression comme un paramètre binaire, une matrice binaire creuse pour être plus précis. Cette matrice $\boldsymbol{G}$ de taille $p\times p$ correspond à une matrice d'adjacence qui indique les liaisons entre covariables de la manière suivante : $\boldsymbol{G}_{i,j}=1$ si, et seulement si $\boldsymbol{X}^j$ est expliqué par $\boldsymbol{X}^i$. \\
		
		Chaque étape $(q+1)$ de l'algorithme propose de garder la structure $\boldsymbol{G}^{(q)}$ en cours ou bien de bouger vers une structure candidate qui diffère de $\boldsymbol{G}^{(q)}$ en un unique point. Ainsi, selon les cas, les candidats vont allonger ou réduire des sous-régressions, les supprimer ou les créer. \\
		Pour pouvoir trancher entre plusieurs candidats, nous avons besoin d'une fonction coût qui soit capable de comparer des modèles avec des nombres distincts de sous-régressions. Nous définissons alors un modèle génératif complet sur $\boldsymbol{X}$ qui complète le modèle de sous-régressions en établissant des modèles de mélanges gaussiens indépendants pour les variables de $\boldsymbol{X}_{f}$. Une fois ce modèle génératif établi, nous pouvons utiliser le critère {\sc bic} pour comparer les différents modèles et conduire chaque étape de la chaîne MCMC par un tirage aléatoire pondéré par les écarts entre les {\sc bic} des différents modèles proposés (dont le modèle en cours). L'algorithme continue ainsi sa marche et fournit à l'utilisateur le modèle rencontré (qu'il ait été choisi ou non) qui a le {\sc bic} le plus faible. \\
		
		La chaîne MCMC est conditionnée par le critère de partitionnement : les variables expliquées ne doivent en expliquer aucune autre (hypothèse \ref{H2fr}). Chaque modèle réalisable peut être entièrement construit ou déconstruit pendant la marche aléatoire donc l'algorithme suit une chaîne de Markov régulière \cite{grinstead1997introduction}. Ainsi il est certain que, asymptotiquement (en nombre d'étapes), l'algorithme trouve le modèle ayant le meilleur {\sc bic}.
	\section{Relaxation des contraintes et nouveau critère}
		Pour améliorer la mélangeance de l'algorithme et donc sa vitesse de convergence, on peut jouer avec la contrainte de partitionnement par une méthode de relaxation semblable à un recuit simulé. Quand une structure candidate n'est pas réalisable (ne produit pas de partition), on peut la modifier en d'autres endroits pour la rendre réalisable. Il suffit de suivre les formules suivantes pour une modification en $(i,j)$ de la  matrice $\boldsymbol{G}$ :
\begin{enumerate}
	\item Modification (suppression/création) de l'arc $(i,j)$ :
	\begin{equation}
		\boldsymbol{G}_{i,j}^{(q+1)}=1-\boldsymbol{G}_{i,j}^{(q)}
	\end{equation}
	\item Si la variable $\boldsymbol{X}^i$ devient un prédicteur elle ne peut plus être une variable réponse :
	\begin{equation}
		\boldsymbol{G}_{.,i}^{(q+1)}= \boldsymbol{G}_{i,j}^{(q)}\boldsymbol{G}_{.,i}^{(q)}
	\end{equation}
	\item Si la variable $\boldsymbol{X}^j$ devient une variable réponse elle ne peut plus être un prédicteur :
	\begin{equation}
		\boldsymbol{G}_{j,.}^{(q+1)}=\boldsymbol{G}_{i,j}^{(q)}\boldsymbol{G}_{j,.}^{(q)}
	\end{equation}
\end{enumerate}
%		\begin{eqnarray}
%			\boldsymbol{G}_{i,j}^{(q+1)}&=&1-\boldsymbol{G}_{i,j}^{(q)} \\			
%			\\
%			\boldsymbol{G}_{.,i}^{(q+1)}&=& \boldsymbol{G}_{i,j}^{(q)}\boldsymbol{G}_{.,i}^{(q)} \textrm{ (Si la variable $\boldsymbol{X}^i$ devient un prédicteur elle ne peut plus être une variable réponse)} \\
%			\boldsymbol{G}_{j,.}^{(q+1)}&=&\boldsymbol{G}_{i,j}^{(q)}\boldsymbol{G}_{j,.}^{(q)} \textrm{ (de même, $\boldsymbol{X}^j$ ne peut plus être un prédicteur.)}
%		\end{eqnarray}
		où $\boldsymbol{G}_{i,j}^{(q+1)}$ est la valeur de la matrice $\boldsymbol{G}^{(q+1)}$ ligne $i$ et colonne $j$, $\boldsymbol{G}_{.,i}^{(q+1)}$ est la $i^{\textrm{ième}}$ colonne de $\boldsymbol{G}^{(q+1)}$ et $\boldsymbol{G}_{j,.}^{(q+1)}$ la $j^{\textrm{ième}}$ ligne de $\boldsymbol{G}^{(q+1)}$.
		Cette méthode de relaxation permet de sortir rapidement des extrema locaux et améliore donc significativement l'efficacité de l'algorithme (Figure \ref{comparecomplrelax}). La méthode est illustrée sur un exemple par les figures \ref{ident1} à \ref{ident6}.\\
		 
		Mais il reste un problème. Le nombre de modèles envisageables est considérable et le critère {\sc bic} ne tient pas compte de cette quantité, menant à des modèles trop complexes. On lui ajoute donc une pénalité qui tient compte du nombre de modèles réalisables pour pénaliser plus lourdement les modèles complexes.
	De manière générale quand on estime la vraisemblance d'une structure $\boldsymbol{S}$ dans une base de données $\boldsymbol{X}$, {\sc bic} est utilisé comme approximation pour $\mathbb{P}(\boldsymbol{S}|\boldsymbol{X})\propto  \mathbb{P}(\boldsymbol{X}|\boldsymbol{S})\mathbb{P}(\boldsymbol{S})$  	car $\mathbb{P}(\boldsymbol{S})$ est considéré comme suivant une loi uniforme. On s'appuie donc sur une loi uniforme hiérarchique $\mathbb{P}_{H}(\boldsymbol{S})=\mathbb{P}_{U}(\boldsymbol{J}_p | \boldsymbol{d}_p,\boldsymbol{J}_r,d_r)\mathbb{P}_{U}(\boldsymbol{d}_p|\boldsymbol{J}_r,d_r)\mathbb{P}_{U}(\boldsymbol{J}_r|d_r)\mathbb{P}_{U}(d_r)$ pour ajouter une pénalité supplémentaire aux structures complexes (même probabilité globale pour un plus grand nombre de structures donc chaque structure devient moins probable). 
On note $\mbox{{\sc bic}}_H$ ce nouveau critère. Il ne modifie pas {\sc bic} (on conserve ainsi ses propriétés) mais ajoute simplement une pénalisation.\\

	Ces deux outils viennent améliorer l'efficacité de l'algorithme sans paramètre utilisateur à optimiser par ailleurs. Tout reste naturel et intuitif pour une meilleure automatisation.

	\section{Résultats}
		La méthode a été testée sur données simulées puis réelles, montrant l'efficacité du modèle marginal s'appuyant sur la vraie structure de sous-régressions (section \ref{MSEvraiS}), l'efficacité de l'algorithme de recherche de structure (section \ref{compZ}), et l'efficacité du modèle marginal s'appuyant sur la structure estimée (section \ref{compY} et chapitre \ref{sectionrealcase}). Le bilan est très positif comme le montrent les graphiques de ces différentes sections.
	\section{Modèle plug-in pour réduire l'erreur}
		Une première perspective a été développée pour compléter le modèle marginal. Il s'agit de mettre à profit la formulation exacte du modèle marginal pour estimer $\boldsymbol{\beta}_r$ et ainsi réduire le bruit de la régression marginale puis d'utiliser ce nouvel estimateur pour identifier $\boldsymbol{\beta}_f$ et ainsi obtenir un nouveau modèle complet s'appuyant sur $\boldsymbol{X}$ entier mais protégé des corrélations par l'estimation séquentielle.\\
		
Après estimation du modèle marginal on s'applique à améliorer l'estimation de $\boldsymbol{\beta}$: 
\begin{itemize}
	\item Estimation de  $\boldsymbol{\varepsilon}_Y^*$ à partir de $\hat{\boldsymbol{\beta}}^*_f$:
	\begin{equation}
		\hat{\boldsymbol{\varepsilon}}_Y^*=\boldsymbol{Y}-\boldsymbol{X}_f\hat{\boldsymbol{\beta}}^*_f.
	\end{equation}
	\item Estimation de $\boldsymbol{\varepsilon}$ à partir de $\hat{\boldsymbol{\alpha}}^*$:
	\begin{equation}
		\hat{\boldsymbol{\varepsilon}}=\boldsymbol{X}_r-\boldsymbol{X}_f\hat{\boldsymbol{\alpha}}^* .\label{epsilonchapeau}
	\end{equation}
\end{itemize}
Ces deux nouveaux estimateurs permettent alors d'obtenir un estimateur de $\boldsymbol{\beta}_r$ plus consistant que le marginal $\hat{\boldsymbol{\beta}}^*_r=\boldsymbol{0}$.
\begin{itemize}
	\item Amélioration de l'estimation de $\boldsymbol{\beta}_r$:
	\begin{equation}
		\hat{\boldsymbol{\beta}}_r^{\varepsilon}=(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{Y}- \boldsymbol{X}_f\hat{\boldsymbol{\beta}^*_f}).
	\end{equation}
\end{itemize}
Cet estimateur est consistant et nous permet de réduire le bruit du modèle marginal, donc d'estimer  $\boldsymbol{Y} $ avec une variance réduite.

\begin{equation}
	\hat{\boldsymbol{Y}}_{plug-in}=\boldsymbol{X}_f\hat{\boldsymbol{\beta}}^*_f + \hat{\boldsymbol{\varepsilon}}\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}.
\end{equation}

		On peut ensuite dans une nouvelle phase d'estimation se rapprocher de $\boldsymbol{\beta}_f$.\\On a $\boldsymbol{\beta}^*_f=\boldsymbol{\beta}_f+\boldsymbol{\alpha}^*\boldsymbol{\beta}_r $.
		\begin{itemize}
			\item Amélioration de l'estimation de $\boldsymbol{\beta}_f $:
			\begin{equation}
			\hat{\boldsymbol{\beta}}_f^{\varepsilon}=\hat{\boldsymbol{\beta}}^*_f-\hat{\boldsymbol{\alpha}}^*\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}.
			\end{equation}
		\end{itemize}
	
	La figure \ref{MQE2} montre l'efficacité du modèle plug-in et son champ d'application : les cas avec assez de correlations pour que les méthodes classiques appliquées à $\boldsymbol{X}$ soient handicapées mais pas assez de corrélations pour que le retrait des variables redondantes (modèle marginal)  se fasse sans perte siginificative d'information.
		
	\section{Valeurs manquantes}
		Une seconde perspective a été entamée concernant les valeurs manquantes. Le fait de disposer d'un modèle génératif complet sur $\boldsymbol{X}$ avec modélisation explicite des dépendances permet en effet de composer avec les valeurs manquantes. Tout d'abord, l'estimation de $\boldsymbol{\alpha}$ peut se faire sur les données observées en intégrant sur les données manquantes. On peut alors utiliser un algorithme de type EM (expectation Maximization) pour estimer $\hat{\boldsymbol{\alpha}}$.\\
		 
		 En pratique, l'étape $E$ n'est pas systématiquement explicite et peut nécessiter de faire appel à une variante de EM : l'algorithme Stochastic EM qui remplace l'étape $E$ par une étape stochastique d'imputation des valeurs manquantes, par exemple en utilisant un échantillonneur de Gibbs. Cet algorithme de Gibbs peut alors être utilisé pour faire de l'imputation multiple sur les valeurs manquantes en s'appuyant sur le $\hat{\boldsymbol{\alpha}}$ issu du Stochastic EM. Comme cette imputation tient compte des corrélations entre les variables, elle est plus précise qu'une simple imputation par la moyenne. Un avantage de l'imputation multiple est que l'on peut avoir une idée de la robustesse des imputations en regardant simplement la variance des valeurs imputées. Encore une fois, on y gagne en qualité d'interprétation.  
\chapter{The industrial context}
	\paragraph{Abstract:} This chapter describes the industrial constraints that has led this work. This work takes place in a steel industry context and was funded by ArcelorMittal, the world leading company in steelmaking. 
\section{Steelmaking process}
	Steelmaking starts from raw materials and aims to give highly specific products used in automotive, beverage cans, {\it etc}.
	We first melt down a mix of iron ore and cock to obtain cast iron that is then transformed in steel by addiction of pure oxygen to remove carbon. Liquid steel is then refreshed in a mold (continuous casting) to obtain steel slabs (nearly 20 tons each, 23 cm thick). \\
	
	Cold slabs are then warmed to be pressed in a hot rolling mill to obtain coils (nearly half a  millimeter thick or less). This warming phase also allows to adjust mechanical properties of the final product. It is the process that gave its name to the simulated annealing algorithm that allows to escape from local extrema thanks to a parameter called "temperature" in allusion to the steelmaking process.  \\
	 
	 If the final product requires a thinner strip of steel, coils pass through a cold rolling mill. Each step of this process involves a whole manufactory and the whole process can take several weeks. The most sensitive products are the thinner ones and sometimes defects are generated by small inclusions in the steel down to the dozen of microns. So even if quality is evaluated at each step of the process, some defects are only found when the whole process is finished even if the origin comes from the first part of this process. So we have hundreds of parameters to analyse.
	\begin{center}
          \begin{tabular}{ccc}
         \includegraphics[width=130px,height=130px]{figures/liquid.jpg} & \includegraphics[width=130px,height=130px]{figures/Brame1.jpg} & \includegraphics[width=130px,height=130px]{figures/Brame.jpg} \\
          	\includegraphics[width=130px,height=130px]{figures/ecras_moy.jpg} &\includegraphics[width=130px,height=130px]{figures/tcc2.jpg} & \includegraphics[width=130px,height=130px]{figures/bobines.jpg}
          \end{tabular}
        \end{center}
Steelmaking is continuously improving and we are now able to produce steel that is both thinner and stronger. Steel is 100$\%$ recyclable unlike petroleum so we will continue to use it widely in the future. This quickly evolving industry is associated to a lot of research in metallurgy but also need adapted statistical tools. That is why this thesis has been made.
	
	\section{Impact of the industrial context}
	 The main objective is to be able to solve quality crisis when they occur. In such a case, a new type of unknown quality issue is observed and we may have no idea of its origin. Defects, even generated at the beginning of the process, are often detected in its last part. The steel-making process does include several sub-process. Thus we have many covariates and no a priori on the relevant ones. Moreover, the values of each covariate are linked to the characteristics of the final product, and many physical laws and tuning models are implied in the process. Therefore the covariates are highly correlated.
	We have several constraints :
	\begin{itemize}
		\item Being able to predict the defect and stop the process as early as possible to gain time (and money)
		\item Being able to find parameters that can be changed and to understand the origin of the defect because the objective is not only to understand but to adapt the problematic part of the process.
		\item It also must be fast and automatic (without any a priori) to manage totally new phenomenon on new products.
	\end{itemize}
	We will see in the state of the art that correlations are a real issue and that the number of variables increases the problem.	
	The stakes are very high because of the high productivity of the steel plants and the extreme competition between steelmakers but also because steel making is now well-known and optimized thus new defects only appears on innovative steels with high added value. Any improvement on such crisis can have important impact on market shares and when the customer is impacted, each day won by the automation of the data mining process can lead to substantial savings. So we really need a kind of automatic method, able to manage correlations without any a priori and giving an easily understandable and flexible model.
	
\chapter{State of the art in linear regression}
\paragraph{Abstract:} Brief state of the art to have a glimpse of existing method we could try to solve our problematic.
Most of the tools described here are explained with more details in the book from  Hastie, Tibshirani et Friedman : {\it "The Elements of Statistical Learning: Data Mining, Inference, and Prediction" } accessible on web\footnote{ \url{http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf}} for free.
	

	
	\section{Linear regression}
%		\subsection{Historic interest}
%			méthode ancienne et reconnue, remonte aux origines des statistiques, méthode pionnière en prédiction.
%		\subsection{Simplicity}
%			Facile à mettre en oeuvre théoriquement, rapide en pratique et présent partout (même dans Excel)
%			Très simple à interpréter, principe intuitif.
%			donne tout de suite l'impact des variables (positif ou négatif) sur la réponse et leur poids (si scaled dataset) 
%			
%			C'est ici qu'on peut mettre le principe de la régression linéaire (image d'un nuage de point et d'une droite qui le traverse)
		\subsection{Industrial context}
		
			Industrial context necessitates easily understandable models and the stakes are frequently very high in terms of financial impact. 
		These two points give strong constraints because used methods has to be accessible for non-statistician in a minimum amount of time and results obtained have to be clearly interpretable (no black-box). So a powerful tool without interpretation becomes kind of useless in such a context.\\
		
		Every engineer, even non-statistician uses frequently linear regression to seek relationship between some covariates. It is easy to understand, fast to do, and is used in nearly all the fields where statistics are made \cite{montgomery2012introduction} : Astronomy \cite{isobe1990linear}, Sociology \cite{longford2012revision}, and so on. It can be done directly in Microsoft Excel which is well known and often used by engineers to open and analyse most of their datasets. Thus we have chosen to work in this way.\\
		 
\begin{figure}
\centering
	\includegraphics[width=350px]{figures/simpleOLS.png} 
	\caption{An example of simple linear regression}
\end{figure}		 
		 
		 As of 2014 Google Scholar proposes more than $3.8$ millions of papers related to regression and many of them were cited several thousands times. It is an old strategy well known and with many derivatives (as we will see in the followings) and can be generalized 		 \cite{kiebel2003general,wickens2004general,nelder1972generalized,mccullagh1989generalized}. 
	It's simplicity facilitates a wide spread usage in industry and other fields of application. It is also a good tool for interpretation with the sign of the coefficients indicating whether the associated covariate has a positive or negative impact on the response variable. \\
	
	More complex situations can be described by evolved forms of linear regression like the hierarchical linear model \cite{raudenbush2002hierarchical,woltman2012introduction}  or  multilevel regression \cite{moerbeek2003comparison,maas2004robustness,hox1998multilevel} that allows to consider effects of the covariates on nested sub-populations in the dataset. It is like using interactions but with a proper modeling that improves interpretation. It is not really a linear model because it is not linear in $\boldsymbol{X}$ but can be seen as a basis expansion using new covariates (the interactions) composed by the product of some of the original covariates.
%			
%		\subsection{Flexibility and future of regression}
%			Richesse des types de régression et des méthodes d'estimation
%			On peut faire des choses plus pointues en adaptant un peu le modèle mais en conservant la simplicité : Multilevel Regression \cite{moerbeek2003comparison,maas2004robustness,hox1998multilevel}
%



\paragraph{Notations:}	
In the following we note classical (respectively $L_2,L_1,L_{\infty}$) norms: $\parallel\boldsymbol{\beta}\parallel_2^2=\sum_{i=1}^d(\beta_i)^2$, $\parallel\boldsymbol{\beta} \parallel_1=\sum_{i=1}^d|\beta_i| $ and $\parallel\boldsymbol{\beta} \parallel_{\infty}=\operatorname{max}(|\beta_1|,\dots,|\beta_d|)$. Vectors, matrices and tuples are in bold characters.
	\subsection{Ordinary least squares and associated problems}\label{sectionOLS}		% ne pas oublier de mentionner les packages existants

We note the linear regression model:
\begin{equation}
		\boldsymbol{Y}|\boldsymbol{X};\boldsymbol{\beta},\sigma^2_Y=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}_Y \label{regressionsimple}
	\end{equation}
	where $\boldsymbol{X}$ is the $n\times d$ matrix of the explicative variables, % (that is a sub-matrix of $\tilde{\boldsymbol{X}}$ the $n\times \tilde{p}$ matrix of provided covariates)
	 $\boldsymbol{Y}$ the  $n\times 1$ response vector and $\boldsymbol{\varepsilon}_Y \sim \mathcal{N}(\boldsymbol{0},\sigma_Y^2\boldsymbol{I}_n)$ the noise of the regression, with $\boldsymbol{I}_n$ the $n$-sized identity matrix and $\sigma_Y >0$. The $d\times 1$ vector $\boldsymbol{\beta}$ is the vector of the coefficients of the regression, that can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{ols}): %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}_{OLS}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaOLS}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1} \label{eq:varOLS}
	\end{equation}
	and without any bias \cite{saporta2006probabilites,dodge2004analyse}. In fact it is the Best Linear Unbiased Estimator (BLUE).
	The theoretical \textsc{mse} is given by
	\begin{equation}
	\textsc{mse}(\hat{\boldsymbol{\beta}}_{OLS})= 0 + \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}).
	\end{equation}
	\\
	Equation (\ref{regressionsimple}) has no intercept but usually a constant is included as one of the regressors. For example we can take $\boldsymbol{X}^1=(1,\ldots,1)'$. The corresponding element of $\boldsymbol{\beta}$ is then the intercept $\beta_1$. In the followings we do not consider the intercept to simplify notations. In practice, an intercept is added by default.\\
	
	Ordinary Least Squares find a $d-$dimensional hyperplane that minimises the distance with each individual $(\boldsymbol{X}_i,Y_i)$. It can be written
	\begin{equation}
		\boldsymbol{\hat{\beta}}_{OLS}=\operatorname{argmin}_{\boldsymbol{\beta}}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace
	\end{equation}
	So estimation of $\boldsymbol{Y}$ by \textsc{ols} can be viewed as a projection onto the linear space spanned by the regressors $\boldsymbol{X}$ as shown in figure \ref{geomOLS}.% that is in public domain\footnote{"OLS geometric interpretation" by Stpasha - Own work. Licensed under Public domain via Wikimedia Commons - http://commons.wikimedia.org/wiki/File:OLS\_ geometric\_ interpretation.svg\#mediaviewer/File:OLS\_geometric\_interpretation.svg}.
	\begin{figure}[h!]
	\centering
	\includegraphics[width=250px]{figures/OLS_geometric_interpretation.png}
	\caption{Multiple linear regression with Ordinary Least Squares. Public domain image.} \label{geomOLS}
	\end{figure}
	
	%problème
	Estimation of $\boldsymbol{\beta}$ requires the inversion of $\boldsymbol{X}'\boldsymbol{X}$ which will be ill-conditioned or even singular if some covariates depend linearly from each other. 
For a given number $n$ of individuals, conditioning of $\boldsymbol{X}'\boldsymbol{X}$ get worse based on two aspects: 
\begin{itemize}
	\item The dimension $d$ (number of covariates) of the model (the more covariates you have the greater variance you get)
	\item The correlations within the covariates: strongly correlated covariates give bad-conditioning and increase variance of the estimators .
\end{itemize}
	When correlations between covariates are strong, the matrix to invert is ill-conditioned and the variance increases, giving unstable and unusable estimator \cite{hoerl1970ridge}.
	Another problem is that matrix inversion requires to have more individuals than covariates ($n\geq d$).
	When matrices are not invertible, classical packages like the function lm of R base package \cite{packagebase} use the Moore-Penrose pseudoinverse \cite{PSP:2043984} to generalize \textsc{ols}.	\\	
		
	Last but not least, Ordinary Least Squares is unbiased but if some $\beta_i$ are null (irrelevant covariates) the corresponding $\hat{\beta}_i$ will only asymptotically tend to 0 so the number of covariates in the estimated model remains $d$. This is a major issue because we are searching for a statistical tool able to work without a priori on a big dataset containing many irrelevant covariates. Pointing out some relevant covariate and how they really impact the response is the main goal here. We will need a variable selection method one moment or another. It could be as a pretreatment (to run a first tool to select relevant covariates and then estimate the values of the non-zero coefficients) , during coefficient estimation (some estimators can lead to exact zeros in $\hat{\boldsymbol{\beta}}$) or by post-treatment (by thresholding with tests of hypothesis, {\it etc.}). \\
	
		
\fcolorbox{vertfonce}{vertclair}{\begin{minipage}{0.95\textwidth}
\paragraph{Running example:} We look at a simple case with $d=5$ variables defined by four independent scaled Gaussian $\mathcal{N}(0,1)$ named $\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{X}^4,\boldsymbol{X}^5$ and $\boldsymbol{X}^3=\boldsymbol{X}^1+\boldsymbol{X}^2+\boldsymbol{\varepsilon}_1$ where $\boldsymbol{\varepsilon}_1\sim{\mathcal{N}(\boldsymbol{0},\sigma_1^2\boldsymbol{I}_n)}$. We also define two {\it scenarii} for $\boldsymbol{Y}$ with $\boldsymbol{\beta}=(1,1,1,1,1)'$ and $\sigma_Y \in \{10,20\}$. So there is no intercept (can be seen as a null intercept).
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_1$ gets smaller. In the followings, the $R^2$ stands for the coefficient of determination which is here:
	\begin{equation}\label{defR2}
	R^2=1-\frac{\operatorname{Var}(\boldsymbol{\varepsilon}_1)}{\operatorname{Var}(\boldsymbol{X}^3)}
	\end{equation}
\end{minipage}}
	
%	 \begin{figure}
%	 \centering
%	  \includegraphics[width=500px]{figures/MQEOLS0.png}
%	  \caption{Evolution of theoretical Mean Squared error on $\hat{\boldsymbol{\beta}}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEOLS1}
%	\end{figure}
%	Figure \ref{MQEOLS1} shows the theoretical MSE obtained on $\hat{\boldsymbol{\beta}}$ with OLS. These results are based on equation \ref{eq:varOLS}, 
%	
	
	
	
	Many other estimation methods were created to obtain better estimations by playing on the bias/variance tradeoff or by making additional hypotheses.
	To have an easier comparison, we look at the empiric \textsc{mse} obtained on $\hat{\boldsymbol{\beta}}$.
		\\
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/OLScompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates (running example). } \label{MQEOLScompl}
	\end{figure}	
	Results shown in Figure \ref{MQEOLScompl} were obtained with usage of $QR$ decomposition to inverse matrices, that is less impacted by ill-conditioned matrices \cite{bulirsch2002introduction} and used in the {\tt lm} function from R to compute \textsc{ols}. But the correlations issue remains. Our package {\tt CorReg} also uses this decomposition.
	 We show the mean obtained after 100 experiences computed on our running example with validation sample of 1 000 individuals. The \textsc{mse} does explode with growing values of $R^2$. The results confirm that the situation gets better for large values of $n$ but strong correlations can still make the \textsc{mse} exploding. The variance of $\hat{\boldsymbol{\beta}}$ is proportional to $\sigma_Y^2$ so the \textsc{mse} are bigger when the main regression is weak (as in the real life).
	\\
	
	%where $\bar{\boldsymbol{Y}}=\frac{1}{n}\sum_{i=1}^n y_i $
		 \FloatBarrier
		

	\subsection{Ridge regression: a penalized estimator}
	
	We have seen that \textsc{ols} is the Best linear Unbiased Estimator for $\hat{\boldsymbol{\beta}}$, meaning that it has the minimum variance. But it remains possible to play with the bias/variance tradeoff to reduce the variance by adding some bias. The underlying idea is that a small bias and a small variance could be preferred to a huge variance without bias. Many methods do this by a penalization on  $\hat{\boldsymbol{\beta}}$.  %Some of them propose an effective variable selection.
		%\subsection{Ridge regression}		% ne pas oublier de mentionner les packages existants

			%\cite{hoerl1970ridge}
			%\cite{marquardt1975ridge}
Ridge regression \cite{hoerl1970ridge,marquardt1975ridge} proposes a possibly biased estimator for $\boldsymbol{\beta}$ that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \lambda \textrm{ with } \lambda>0
	\end{equation}
	But this penalty is not guided by correlations. It introduces an additional parameter $\lambda$ to choose for the whole dataset  whereas correlations may concern only some of the covariates with several intensities.
	
	The solution of the ridge regression is given by
	\begin{equation}
		 \hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} -\lambda\boldsymbol{I}_n\right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaridge}
	\end{equation}
	and we see in this equation that a global modification of $\boldsymbol{X}'\boldsymbol{X}$ is done for a given $\lambda$. Methods do exist to automatically choose a good value for $\lambda$ \cite{cule2013ridge,er2013systematic} and a R package called {\tt ridge} is on CRAN \cite{packageridge}. 
	We have computed the same experiment as in previous figure but with the {\tt ridge} package instead of \textsc{ols}. It is clear that the ridge regression is efficient in variance reduction (it is what it is built for). Moreover, ridge allows to have $n<d$.\\
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/ridgecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEridgecompl}
	\end{figure}
	
	
	%It is the same for each covariates and will be too large for independent covariates and/or too small for correlated ones. So the efficiency of such a method is limited. 
	Like \textsc{ols}, coefficients tend to 0 but don't reach 0 so it gives difficult interpretations for large values of $d$. Ridge regression is efficient to improve conditioning of the estimator but gives no clue to the origin of ill-conditioning and keep irrelevant covariates. It remains a good candidate for prediction-only studies. Our industrial context makes necessary to have a variable selection method so we look further.
	
		 \FloatBarrier
	\section{Variable selection method}
		\subsection{Least Absolute Shrinkage and Selection Operator ({\sc lasso})}		% ne pas oublier de mentionner les packages existants

			%\cite{tibshiranilasso}  
			%\cite{tibshirani1996regression} 
			%\cite{efron2004least} %LAR
			%\cite{Zhao2006MSC}%problèmes du lasso/lars en correlations
			%\cite{SAM10088}%lars necessite OLS en surcouche
The Least Absolute Shrinkage and Selection Operator ({\sc lasso} \cite{tibshirani1996regression,tibshiranilasso}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$ instead of the $L_2$ penalty of the ridge regression:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0 .
		\end{equation}	
		
		\begin{figure}[h!]
			\centering
			\includegraphics[width=200px]{figures/lasso.png} 
			\caption{Geometric view of the Penalty for the \textsc{lasso} (left) compared to ridge regression (right) as shown in the book from Hastie \cite{hastie2009elements}} \label{lassogeom}
		\end{figure}
		Figure \ref{lassogeom} shows the contour of error (red) and constraint function (blue) for both \textsc{lasso} (left) and ridge regression (right). We see that the optimum will be found on an axis for the \textsc{lasso} because its constraint zone is a polyhedron whose vertices are on the axis but not for the ridge regression. Here the axis stands for the regression coefficients.\\
		
		Here again we have to choose a value for $\lambda$.
	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) algorithm offers a very efficient way to obtain the whole \textsc{lasso} path.  It can be used through the {\tt lars } package on CRAN (\cite{packagelars}). But like the ridge regression, the penalty does not distinguish correlated and independent covariates so there is no guarantee to have less correlated covariates. In practice, we know that the \textsc{lasso} faces consistency issues when confronted to correlated covariates \cite{Zhao2006MSC}. When two covariates are correlated, it tends to keep only one of them. For example, if two covariates are equal and have the same effect, the \textsc{lasso} will keep only one of them. As explained earlier, variable selection is a real stake for us but is necessary to have a good interpretation. The \textsc{lasso} does not distinguish a covariate not selected because it is totally redundant with another already selected covariate from an irrelevant covariate. And that is a problem. This consistency issue is illustrated in section \ref{consistency}.\\


		% ne pas oublier de mentionner les packages existants

			%\cite{zou2006adaptive}% adaptive lasso
			%\cite{wang2011random}%random lasso
			 Some recent variants of the \textsc{lasso} do exist for the choice of the penalization coefficient like the adaptive \textsc{lasso} \cite{zou2006adaptive} with the {\tt parcor} package on CRAN (\cite{packageparcor}) or the random \textsc{lasso} \cite{wang2011random}.  But the consistency issue remains because it is still the same model. Only the choices of $\lambda$ differ.\\
			 
			 It is notable that the main goal of the \textsc{lasso} is to select some covariates, thus the penalization is just a mean to achieve selection. But estimation of $\hat{\boldsymbol{\beta}}$ can be improved by a second estimation with \textsc{ols} based only on selected covariates \cite{SAM10088}.
		\subsection{Least Angle Regression}
		The least Angle Regression algorithm solves the \textsc{lasso} problem.
		It requires only the same order of magnitude of computational effort as \textsc{ols} applied to the full set of covariates.
		The idea is to start with all coefficients to zero and then to grow them starting with the most correlated with the response variable until another variable is equally correlated with the residual. So it is a progressive growth of the coefficient leading to reduce the residual. It finishes with the Ordinary Least Squares solution (on the right in figure \ref{plotlarstoy}). We then have a list of models with several numbers of non-zero coefficients and can choose between them with cross-validation for example.\\
		
\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/LARtheory.png}
		\caption{The geometry of Least Angle Regression}
	\end{figure}
				
\begin{figure}[h!]
	\centering
		  \includegraphics[width=350px]{figures/plotlarstoy.png}
		\caption{The \textsc{lasso} path computed by lars}\label{plotlarstoy}
	\end{figure}			
	
		 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/larcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{lar}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQElarcompl}
	\end{figure}	
	 Figure \ref{MQElarcompl} shows that strong correlations make the \textsc{mse} explode even with \textsc{lar}. Results obtained with the package {\tt lars} for R, included in {\tt CorReg}.
 	 
	 \FloatBarrier
		\subsection{Elasticnet}		% ne pas oublier de mentionner les packages existants

			Elastic net \cite{zou2005regularization} is a method developed to be a compromise between Ridge regression and \textsc{lasso} by mixing both $L_1$ and $L_2$ penalties: 
%	Given data set $(Y,X)$ and two parameters $(\lambda_1,\lamda_2)$, define artificial data set $(Y^*,X^*)$ by :
%	\begin{equation}
%		X^*_{(n+p)\times p}=(1+\lambda_2)^{-1/2}\left( X  \sqrt{\lambda_2}I \right), \ \ \  Y^*_{(n+p)}=\left( Y 0 \right)
%\end{equation}		
	%Elastic net can be writt"en:
	\begin{eqnarray}
		\boldsymbol{\hat{\beta}}&=&(1+\lambda_2) \operatorname{argmin}\left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta} \parallel_2^2 \right\rbrace \textrm{ subject to} \nonumber \\
			 & &(1-\alpha)\parallel\boldsymbol{\beta}\parallel_1+\alpha\parallel\boldsymbol{\beta}\parallel_2^2\leq t \textrm{ for some } t
	\end{eqnarray}
	where $\alpha=\frac{\lambda_2}{(\lambda_1+\lambda_2)}$. 
	
	
	\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/elasticnetcircles.png} 
			\caption{Geometric view of the Penalty for elasticnet}
		\end{figure}	
	
	%It seems to give good predictions. 
	But it is based on the grouping effect so correlated covariates get similar coefficients and are selected together whereas \textsc{lasso} will choose between one of them and will then obtain same predictions with a more parsimonious model. Once again, nothing specifically aims to reduce the correlations. Results obtained with the package {\tt elasticnet} (\cite{packageelasticnet}) for R are given in Figure \ref{MQEelasticnetcompl}.

	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/elasticnetcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{elasticnet}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEelasticnetcompl}
	\end{figure}
		
		 \FloatBarrier

		\subsection{{\sc oscar}: Octagonal Shrinkage and Clustering Algorithm for Regression }		% ne pas oublier de mentionner les packages existants

			%\cite{bondell2008simultaneous}%Oscar
			Like elasticnet, \textsc{oscar} \cite{bondell2008simultaneous} uses combination of two norms for its penalty. Here the objective is to group covariates with the same effect (by a pairwise $L_\infty$ norm) and give them exactly the same coefficient (reducing the dimension) with a simultaneous variable selection (implied by the $L_1$ norm). Thus correlations are avoided if correlated covariates are in the same cluster (not extremely flexible). A possible bias is added by the dimension reduction inherent to the coefficients clustering.
			\begin{equation}
				\hat{\boldsymbol{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \parallel\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta} \parallel^2_2 \textrm{ subject to } \sum_{j=1}^d|\beta_j|+c\sum_{j<k}\operatorname{max}(|\beta_j|,|\beta_k|) \leq \lambda		
			\end{equation}						
			Moreover \textsc{oscar} depends on two tuning parameters: $c$ and $\lambda$. For a fixed $c$ the $\lambda$ can be found by the \textsc{lar} algorithm but $c$ still has to be found "by hand" comparing final models for many values of $c$.
			
\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/oscarcircles.png} 
			\caption{Geometric view of the Penalty for {\sc oscar}}\label{oscarcircles}
		\end{figure} 
Figure 	\ref{oscarcircles} show the geometric interpretation of the penalty. It follows the same principle as the \textsc{lasso} with supplementary vertices in the four quarters to obtain equal values for the $\beta_j$. So estimator will give both zero coefficients and equal coefficients that can be grouped for interpretation and correspond to a dimension reduction. So two covariates with a similar effect may obtain the same estimated coefficient. But correlations are only implicitly taken into account and only pairwise. It lacks of an efficient algorithm  (to find $c$) and need a supplementary study to interpret the groups found.
		\subsection{Stepwise}
			
			Stepwise \cite{seber2012linear} is an algorithm to choose a subset of covariates to use in the final regression model. It is a variable selection method using \textsc{ols} for estimation. It is proposed in the R package {\tt stats} with the function {\tt step}. The main idea is to start with a first model (than can be either void or using the whole dataset or using any subset of covariates) and then to add and remove covariates step by step to improve the chosen criterion. \\
			\begin{itemize}
				\item 	Starting with a void model and having only adding steps is called Forward Selection. Covariates are added by choosing first the one that improves the most the criterion. The algorithm stops when all the covariates are in or when remaining covariates does not improve the model.
				\item Backward Elimination is the same as Forward selection but starting with the full model and removing at each step the covariates that improves the most the criterion once deleted.
				\item Bidirectional elimination is more flexible and allows to start from any model. Each step proposes to add a covariate or to delete another so it is not a hierarchical construction any more because successive models are not necessarily nested into each other.
			\end{itemize}
		A critical value can be defined to stop the algorithm when improvement becomes too small, in order to avoid over-fitting. \\
		
		Stepwise regression is subject to over-fitting and the algorithm is in trouble when confronted to correlated covariates \cite{miller2002subset} giving unstable results, especially for nested strategies, just like regression trees that are unstable because of their discrete nested nature.
\begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/stepwisecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{stepwise}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEstepwisecompl}
	\end{figure}
	Figure \ref{MQEstepwisecompl} illustrate the consequences of correlations in the dataset.
	
%			\subsection{bootstrap}
%			\cite{efron1979bootstrap,efron1994introduction}
		\FloatBarrier
		\subsection{Industrial tools}
			%Linear regression, decision trees, bayesian networks, neural network but without confidence
			%Our engineers are frequently contacted by software resellers. The concerned software are sold as non-statistical methods, based on rules (derivatives of decision trees but without underlying theory). Statistics are often described as too theoretical, without ergonomy and not compatible with the real life. 
			Our goal was also to demonstrate that statistics can provide efficient methods for real datasets, easy to use and understand.	This was a battle against correlations but also against scepticism. \\
			
			Figure \ref{Regle2D} shows the kind of results proposed by a rule-based software that is sold as a non-statistical tool. It is just a partition of the sample by binary rules (like regression trees). Some blur is added to the plot to help interpretation. The algorithm used here is somewhere between exhaustive research and decision trees. It is extremely slow (research with $d>10$ would take years) and is less efficient than regression trees. Morever, it requires to discretize the response variable to obtain ``good" and ``bad" values. The green rectangle is very far from the true green zone even for this toy example provided by the reseller.\\
			
			%But some engineers may be seduced by this kind of tool. 
			Ergonomy and quality of interpretation are stakes for us to make engineers use efficient methods instead of this kind of stuff.
	
		\begin{figure}[h!]
	\subfigure[Without blur]{
			\includegraphics[height=180px,width=245px]{figures/Regle2Dbis.png} 
	} \quad
   	\subfigure[With some blur proportional to the density of points]{
			\includegraphics[height=180px,width=240px]{figures/Regle2D.png} 
	}
	\caption{Result on a toy example provided by \textsc{FlashProcess}, similar to decision trees but less efficient and extremely slower. Colors are part of the learning set.}\label{Regle2D}
\end{figure}	

	\section{Modeling the parameters}			% ne pas oublier de mentionner les packages existants

		\subsection{CLusterwise Effect REgression ({\sc clere})}		% ne pas oublier de mentionner les packages existants

			The CLusterwise Effect REgression (\textsc{clere} \cite{yengo2012variable}) describes the $\beta_j$ no longer as fixed effect parameters but as unobserved independent random variables with $\beta_j$ following a Gaussian Mixture distribution, allowing to group them into by their component membership. \\
			
			 The idea is that if the model has a small number of groups of covariates then the mixture will have few enough components to have a number of parameters to estimate significantly lower than $d$. In such a case, it improves interpretation and ability to yeld reliable prediction with a smaller variance on $\boldsymbol{\hat{\beta}}$. A package {\tt clere} for R does exist on CRAN (\cite{packageclere}).%and is included in {\tt CorReg}. \\
			 \\
			 
			 But we have to choose the maximum number of components $g$ and have no method to choose this value. Yengo recommends to use $g=5$ in our case. It could be interpreted as the possibility to have a group of irrelevant covariates and groups with small or big values (both positives or negatives). The package is able to choose automatically the best number of components between 1 and $g$ based on a $\mbox{{\sc bic}}$ criterion but setting $g=d$ gives over-fitting.
	 Here again, it has no specific protection against or specific model for correlations. 
		\subsection{Spike and Slab}			% ne pas oublier de mentionner les packages existants

			Spike and Slab variable selection \cite{ishwaran2005spike} also relies on Gaussian mixture (the spike and the slab) hypothesis for the $\beta_j$ and gives a subset of covariates (not grouped) on which to compute \textsc{ols} but has no specific protection against correlations issues.  The $\beta_j$ are supposed to come from a mixture distribution as shown in Figure \ref{spikeslab}. It allows to have some coefficients set exactly to zero after some draws. The package {\tt spikeslab} for R is on CRAN. \\
			
\begin{figure}[h!]
	\centering
	\includegraphics[width=250px]{figures/spikeslab.png} 
	\caption{The spike and the slab}\label{spikeslab}
\end{figure}		
		
		Modeling the parameters implies to have no exact value to give to the coefficient and it is not really user-friendly, especially in our industrial context. %However, these two methods can be used for variable selection in the {\tt correg} function using the parameter {\tt select="clere"} or {\tt select="spikeslab"}.
		
		\FloatBarrier
	
		
%		\section{MCMC}
		
		
		\section{Non-linear models for regression}	
			
	\subsection{Sliced Inverse Regression}
		Sliced Inverse Regression is a semi-parametric approach that could be seen as easier to interpret than general non-parametric regression \cite{eubank1999nonparametric,hardle1990applied} that are too complex to interpret for our industrial context. \\
		
		The main idea is to compute the inverse regression $\mathbb{P}(\boldsymbol{X}|\boldsymbol{Y})$ instead of $\mathbb{P}(\boldsymbol{Y}|\boldsymbol{X})$ to perform a weighted principal component analysis, with which we can identify the effective dimension reducing directions \cite{li1991sliced}. But it requires strong hypotheses on the distribution of $\boldsymbol{X}$ (elliptic hypothesis), even if it is possible to neglect these hypotheses \cite{saracco1999regression} and see what happens. This method has been rejected because it was not sufficient in terms of ease of use (during and after estimation) for non-statisticians due to the semi-parametric aspect and the elliptic hypothesis is compromised on datasets with hundreds of covariates. It is part of the dimension reduction package {\tt dr} on CRAN (\cite{packagedr}).
		
	
	\subsection{Neural networks}	
		Neural networks \cite{fausett1994fundamentals} are a statistical model designed to mimic the brain and its neuronal organization. It seems to be really powerful but it has a predictive only goal and the model obtained cannot be interpreted easily (and cannot be interpreted at all if too complex). So it does not correspond to our needs. Interpretation remains our first goal and prediction comes only in second place.
		
	\subsection{Bayesian networks}
		
		Bayesian networks \cite{heckerman1995learning,jensen2007bayesian,friedman2000using} model the covariates and their conditional dependencies via a Directed Acyclic Graph (DAG). Such an orientation is very user-friendly because it is similar to the way we imagine causality. But it is only about conditional dependencies. Conditional dependencies allow to use information from independent covariates. The usual example is the case of wet grass in a garden. You don't remember if the sprinkler was on or off, you don't know if it has rain and these two facts are independent. Then you look at the grass in your neighbour's garden and it is not wet \dots \\
		You will deduce that your sprinkler was on. Such conditionals dependencies are used in chapter \ref{chapmiss} when confronted to missing values. \\
		
		Bayesian networks are quite good in terms of interpretation because of that graphical and oriented representation of conditional probabilities. But they suffer from great dimension (combinatory issue) and require to transform the dataset arbitrary (discretisation), that imply a loss of information and usage of a priori (that is explicitly not suitable in our industrial context). The choice of the way to discretise the dataset has a great impact on the results and nothing can help if you have no a priori on the result you want to obtain. Computation relies on a table that describes all possible combinations for each covariate. Hence it is extremely combinatory if the graph has too much edges or is not sparse enough. Moreover, you need to define the graph before computing the bayesian network and without a priori it can be challenging and time consuming.\\
		
The concept of representing dependencies with directed acyclic graph is good an we keep it in our model.		
			
	
		
	
\subsection{Classification and Regression Trees ({\sc cart})}
		%\cite{quinlan1986induction} à vérifier

	
		Classification And Regression Trees ({\sc cart}) \cite{breiman1984classification} are extremely simple to use and interpret, can work simultaneously with quantitative and qualitative covariates and are very fast to compute. They consist in recursive partitioning of the sample according to binary rules on the covariate (only one at a time) to obtain a hierarchy defined by simple rules and containing pure leaves (same value). It is followed by a pruning method to obtain leaves that are quite homogeneous and described with simple rules.\\
		
		\begin{figure}
			\centering
				\includegraphics[width=350px]{figures/arbretoy.png} 
			\caption{Regression tree obtain with the package {\tt CorReg}
			 (graphical layer on top of  the {\tt rpart} package) on the running example.}\label{arbretoy}
		\end{figure}
		{\sc cart} are implemented in the package {\tt rpart} 
		for R, on CRAN (\cite{packagerpart}). Our {\tt CorReg} 
		package offers a function to compute and plot the tree in one command with a subtitle to explain how to read the tree and global statistics on the dataset.
		But it is not convenient for linear regression problems as we see in figure \ref{arbretrivial} because a same variable will be used several times and the tree will fail to give a simple interpretation as ``$\boldsymbol{Y}$ and $\boldsymbol{X}^1$ are proportional". Trivial case : $\boldsymbol{Y}=\boldsymbol{X}^1 + \boldsymbol{\varepsilon}_Y$ where $\boldsymbol{\varepsilon}_Y \sim \mathcal{N}(0, \sigma^2_Y \boldsymbol{I}_n)$ with $\sigma_Y^2=0.5$. \\
		
		So {\sc cart} will be used as a complementary tool for datasets with both quantitative and qualitative covariates or when the dependence between $\boldsymbol{Y}$ and $\boldsymbol{X}$ is not linear. We will focus our research on linear models with only quantitative variables.
	\\
	
\begin{figure}[h!]
	\subfigure[Tree found for the trivial case]{
			\includegraphics[height=180px,width=245px]{figures/arbretrivial.png} 
	} \quad
   	\subfigure[True linear regression and splits obtained by the tree.]{
			\includegraphics[height=180px,width=240px]{figures/arbretrivialcoupure.png} 
	}
	\caption{Predictive model associated to the tree (red) and true model (black)}\label{arbretrivial}
\end{figure}

 Apart from linear models, the main issues are the lack of smoothness (prediction function with jumps) and especially instability because of the hierarchical partitioning. Modifying only one value in the dataset can impact a split and then change the range of possible splits in the resulting sub-samples so if a top split is modified the tree can be widely changed. Random Forests are a way to solve this problem and can be seen as a cross-validation method for regression trees. More details in the book from Hastie \cite{hastie2009elements}.	
	
	
		\FloatBarrier
			
	\section{Taking correlations into account}		% ne pas oublier de mentionner les packages existants
		\subsection{Principal Component Regression ({\sc pcr})}
	Principal Component Regression ({\sc pcr})\cite{jackson2005user} consists in using the axis from the Principal Component Analysis ({\sc pca}) of $\boldsymbol{X}$ instead of $\boldsymbol{X}$ itself. Then we have orthogonal covariates. The dataset $\boldsymbol{X}$ is standardized to have 0 mean and variance 1 for each of the covariates. Dimension reduction is done by keeping only the $M \leq d$ first components of the {\sc pca}. Because the axis are linear combination of the original covariates we can then express the model in terms of coefficients of the $\boldsymbol{X}^j$.\\
	\begin{eqnarray}
		\hat{\boldsymbol{\beta}}_M&=&\boldsymbol{V}_M\hat{\boldsymbol{\gamma}}_M \in \mathbb{R}^d \textrm{ where}\\
		\hat{\boldsymbol{\gamma}}_M&=&(\boldsymbol{W}_M'\boldsymbol{W}_M)^{-1}\boldsymbol{W}_M'\boldsymbol{Y} \textrm{ with} \\
		\boldsymbol{W}_M&=&\boldsymbol{X}\boldsymbol{V}_M
	\end{eqnarray}
	with $\boldsymbol{V}_M$ the submatrix of the $M$ first columns of $\boldsymbol{V}$ $d$-square matrix of the orthonormal set of right singular vectors of $\boldsymbol{X}$ defined by:
	\begin{eqnarray}
		\boldsymbol{X}&=&\boldsymbol{U}\boldsymbol{\Delta}\boldsymbol{V}'
	\end{eqnarray}
	with $\boldsymbol{\Delta}$ the $d\times d$ diagonal matrix which has on its diagonal the positive eigen values of $\boldsymbol{X}$ in descending order and $\boldsymbol{U}$ is the $n\times d$ matrix of the orthonormal set of left singular vectors of $\boldsymbol{X}$.\\
	
	Principal Component Regression requires to choose $M$ the number of axis to keep. Finally, even if dimension reduction is effective when $M<d$ each axis depends on all original covariates so it does not select any covariates and that is also a problem for interpretation. We have to choose arbitrary how to interpret each axis and how many covariates really explain each of them. So it is not really satisfying in our industrial context. Principal Component method can be seen as a truncation method whereas the ridge regression is a shrinkage method. Another problem is that principal components are constructed to explain $\boldsymbol{X}$ instead of $\boldsymbol{Y}$ even if there is no reason that relevant variables to explain $\boldsymbol{X}$ stay relevant to explain $\boldsymbol{Y}$. \\
			\subsection{Partial Least Squares Regression ({\sc pls})}

	Partial Least Square Regression ({\sc pls})\cite{abdi2003partial,geladi1986partial} also relies on a combination of the columns of  $\boldsymbol{X}$ but this combination depends on $\boldsymbol{Y}$. The dataset $\boldsymbol{X}$ is also standardized to have 0 mean and variance 1 for each of the covariates. {\sc pls} regression searches for a set
of components (called latent vectors) that performs a simultaneous decomposition of $\boldsymbol{X}$ and $\boldsymbol{Y}$ with the constraint that these components explain as much as possible of the covariance between $\boldsymbol{X}$ and $\boldsymbol{Y}$. It follows an algorithm that leads to construct successively $M$ orthogonal latent variables that are linear combination of the $\boldsymbol{X}^j$:
\begin{enumerate}
	\item Standardize $\boldsymbol{X}$ and set $\hat{\boldsymbol{Y}}^{(0)} 
% =\bar{\boldsymbol{Y}}\boldsymbol{1}
	=(\bar{\boldsymbol{Y}},\dots,\bar{\boldsymbol{Y}})$ and $\boldsymbol{R}_j^{(0)}=\boldsymbol{X}^j,j=1,\dots,d$
	\item For $m=1,\dots,d$ do:
	\begin{enumerate}
		\item $\boldsymbol{z}_m=\sum_{j=1}^d \hat{\varphi}_{mj}\boldsymbol{R}_j^{(m-1)}$, where $\hat{\varphi}_{mj}=\langle \boldsymbol{R}_j^{(m-1)},\boldsymbol{Y}\rangle$.
		\item $\hat{\theta}_m= \langle \boldsymbol{z}_m,\boldsymbol{Y} \rangle / \langle \boldsymbol{z}_m,\boldsymbol{z}_m \rangle$.
		\item $\hat{\boldsymbol{Y}}^{(m)} =\hat{\boldsymbol{Y}}^{(m-1)}+\hat{\theta}_m\boldsymbol{z}_m$.
		\item Orthogonalize each $\boldsymbol{R}_j^{(m-1)}$ with respect to $$\boldsymbol{z}_m: \boldsymbol{R}_j^{(m)}=\boldsymbol{R}_j^{(m-1)}-\left[ \langle \boldsymbol{z}_m,\boldsymbol{R}_j^{(m-1)} \rangle/ \langle \boldsymbol{z}_m,\boldsymbol{z}_m \rangle \right]\boldsymbol{z}_m, j=1,\dots,d$$.
	\end{enumerate}
	\item Output the sequence of fitted vectors $\{\hat{\boldsymbol{Y}}^{(m)}\}_1^d$. Since the $\{ \boldsymbol{z}_l\}_1^M$ are linear in $\boldsymbol{X}$, so is $\hat{\boldsymbol{Y}}^{(M)}=\boldsymbol{X}\hat{\boldsymbol{\beta}}_{PLS}(M)$ where $\hat{\boldsymbol{\beta}}_{PLS}(M)$ for a given value of $M$ can be recovered from the sequence of {\sc pls} transformations.
\end{enumerate}
where $\langle \boldsymbol{A},\boldsymbol{B} \rangle=\sum_i A_iB_i$ is the classical scalar product for two equally sized vectors $\boldsymbol{A}$ and $\boldsymbol{B}$.\\
Like the {\sc pcr}, {\sc pls} regression can give an efficient dimension reduction (with small values of $M$) but does not really select relevant covariates and interpretation requires to first interpret the latent variables generated so it is not adapted to our needs.

%When all the covariates are used it is equivalent to PCR so results tends to be similar to Principal Component Regression. PLS uses the following decomposition:
%\begin{eqnarray}
%	\boldsymbol{X}&=&\boldsymbol{T}\boldsymbol{P}' \textrm{ with } \boldsymbol{T}'\boldsymbol{T} =\boldsymbol{I}_M\\
%	\hat{\boldsymbol{Y}}&=&\boldsymbol{T}\boldsymbol{B}\boldsymbol{C}'
%\end{eqnarray}
%	where $\boldsymbol{T}$ is $n \times M$ matrix of the $M$ latent vectors, $\boldsymbol{P}$ is the $d\times M$ matrix of the loadings $\boldsymbol{B}$ is the $M\times M$ diagonal matrix of the regression weights and $\boldsymbol{C}$ the $1\times M $ weight vector (defined below), $\boldsymbol{I}_M$ is the identity matrix of size $M$
%	
%	
	
	The R package {\tt pls} on CRAN computes both Principal Component Regression and Partial Least Squares Regression (\cite{packagepls}).
		\subsection{Simultaneous Equation Model (\textsc{sem}) and Path Analysis}		% ne pas oublier de mentionner les packages existants
		Applied statistics  for non statisticians are well developed in sociology where interpretation  stakes are fare beyond prediction. Sociologists use simple models like linear regression (often with $R^2<0.2$) and describe complex situations with systems of linear regressions. Such systems are called Structural Equation Model or Simultaneaous Equation Model, better known as \textsc{sem} \cite{davidson1993estimation}. Several softwares, from the open-source GRETL \cite{CottrellLucchetti2007gretlmanual} to proprietary STATA, does implement the \textsc{sem}. The systems allow to describe which covariates have an influence on others with an orientation that users can interpret as causality \cite{pearl2000causality,pearl1998graphs}. \\
		
		\textsc{sem} are easy to understand for non-statisticians and can be resumed by Directed Acyclic Graphs (DAG) as the Bayesian networks do. But the problem is that the structure of regression between the covariates is defined {\it a priori}. \textsc{sem} are used to confirm sociological theories, not to create new theories. \\
		
		Moreover, estimation of recursive \textsc{sem}, without instrumental variables is exactly a succession of independent \textsc{ols} (confirmed with both GRETL and STATA) so the structure is only used for interpretation, not for estimation \cite{brito2006graphical}. Last but not least, there is no specific status for a response variable, each regression has the same status. We want to be able to model complex dependencies within the covariates and use this knowledge to estimate and understand a specific distinct response variable.
			%mcdonald2002principles
		\subsection{{\sc sur}: Seemingly Unrelated Regression}		% ne pas oublier de mentionner les packages existants
		Seemingly Unrelated Regression \cite{SURzellner} is an estimation method for multiples equations with correlated error terms. It does not take into account correlations between the covariates but starts to estimate the system jointly instead of independent estimations. This estimation relies on Feasible Generalized Least Squares (\textsc{fgls}) that depends on the variance-covariance matrix of the error terms. But when the error terms are independent or the subset of covariates are the same it is equivalent to successive independent \textsc{ols}. The R package {\tt systemfit} on CRAN computes {\sc sur} (\cite{packagesystemfit}). 
%			
%		\subsection{SPRING: Structured selection of Primordial Relationships IN the General linear model}		% ne pas oublier de mentionner les packages existants
%
%\textsc{SPRING} (Structured selection of Primordial Relationships IN the General linear model ) takes into account correlations between endogenous covariates but without explicit expression of the correlations and thus a reduced interpretability \cite{chiquetconf}. It is similar to SUR in that dependencies are modeled with the variance-covariance matrix of the error terms, but with sparsity constraints in this matrix.  It does not distinguish a response variable from internal structure in a dataset. Every equation in the system has a distinct response variable and the explicative covariates are the same for all equations.		
		\subsection{Selvarclust: Linear regression within covariates for clustering}		% ne pas oublier de mentionner les packages existants

			{\tt Selvarclust} is a software written in C++ modeling dependencies within a dataset			 \cite{maugis2009variable} in a Gaussian clustering context\footnote{\url{http://www.math.univ-toulouse.fr/~maugis/SelvarClustHomepage.html}}.
			The idea is to allow covariates to have different roles $(S,R,U,W)$ for the clustering:
			\begin{itemize}
				\item $S$ stands for the subset of the relevant covariates for clustering
				\item $U$ and $W$ form a partition of the complementary subset of $S$, with $U$ the subset of irrelevant covariates depending linearly from relevant covariates and $W$ the subset of covariates totally irrelevant and independent from relevant covariates
				\item $R$ is the subset of $S$ that contains the covariates explaining those in $U$, with $R\cap U=\emptyset$.
\end{itemize}			 
So we have a system of linear regression:
\begin{equation}
	\boldsymbol{X}^{U}|\boldsymbol{X}^{R};\boldsymbol{\alpha},\boldsymbol{\Omega}=a+\boldsymbol{X}^R \boldsymbol{\alpha}+ \boldsymbol{\varepsilon}
\end{equation}
where $\boldsymbol{X}$ is the $n\times d$ dataset as above, $\boldsymbol{X}^R$ and $\boldsymbol{X}^U$ are the sub-matrix of  $\boldsymbol{X}$ formed by the covariates in $R$ and $U$ respectively, $\boldsymbol{\alpha}$ is the $\operatorname{Card}(R)\times \operatorname{Card}(U)$ matrix of the regression coefficients, $a$ is the $1\times \operatorname{Card}(U)$ intercept vector and $\boldsymbol{\varepsilon}$ the $n\times \operatorname{Card}(U) $ matrix of the Gaussian error terms with zero mean and variance $\boldsymbol{\Omega}\in \mathbb{R}^{\operatorname{Card}(U)\times \operatorname{Card}(U)}$.\\

It leads to decompose the density of $\boldsymbol{X}$ in three terms $f_{clust},f_{reg},f_{indep}$ whose product will give the joint density of $\boldsymbol{X}$:
\begin{itemize}
	\item The relevant variables for clustering of $\boldsymbol{X}$ are assumed to follow a Gaussian mixture with $K$ components and a form $m$ (see \cite{biernacki2006model} for more details about the possible forms):
	\begin{equation}
		f_{clust}(\boldsymbol{X}^S|K,m,\boldsymbol{\theta})=\sum_{k=1}^K p_k \Phi (\boldsymbol{X}^S|\mu_k,\Sigma_k)
	\end{equation}
	where the parameter vector is $\boldsymbol{\theta}=(p_1,\dots,p_k,\mu_1,\dots,\mu_k,\Sigma_1,\dots,\Sigma_k)$, with $\sum_{k=1}^K p_k=1$, the proportion vector and the variance matrices satisfying the form $m$.
	\item The likelihood associated to the linear regression of $\boldsymbol{X}^U$ on $\boldsymbol{X}^R$ is then
	\begin{equation}
		f_{reg}(\boldsymbol{X}^U|r,a+\boldsymbol{X}^R\boldsymbol{\alpha},\boldsymbol{\Omega})=\prod_{i=1}^n \Phi(\boldsymbol{X}_i^U|a+\boldsymbol{X}_i^R \boldsymbol{\alpha},\boldsymbol{\Omega})
	\end{equation}
	where $r$ is the form of $\boldsymbol{\Omega}$, the variance matrix of the residual $\boldsymbol{\varepsilon}$.
	\item Variables independent of the variables that are relevant for clustering are assumed to follow a Gaussian distribution (If they were independent and following a Gaussian mixture they would then be relevant for clustering) with mean vector $\gamma$ and variance matrix $\tau$ with form $l$:
	\begin{equation}
		f_{indep}(\boldsymbol{X}^{W}|l,\gamma,\tau)=\prod_{i=1}^n \Phi (\boldsymbol{X}_i^W|\gamma,\tau)
	\end{equation}
\end{itemize}


{\tt Selvarclust} is one step beyond Simultaneous Equation Modeling with an algorithm to find the structure but:
			\begin{itemize}
				\item It is about clustering and not regression (not the same application field) so we don't have here any response variable neither method to use this structure for estimation of another regression.  And the goal is not to find the structure but to find a Gaussian clustering.
				\item It uses stepwise-like algorithm without protection against correlations \cite{raftery2006variable} even if it is known to be often unstable \cite{miller2002subset} in such a context.
			\end{itemize}
			
			In this work we propose to adapt this model for linear regression and to use it as a pretreatment on correlated covariates. We will see that, as a pretreatment, it can be used then for a wide range of statistical tools and not only linear regression.\\
			We provide a specific MCMC algorithm to find the structure between the covariates and propose two distinct models to use the structure for prediction: a marginal model and a plug-in model. Both algorithm are compared on a dataset from Maugis in section \ref{compMaugis}.	\\
			
			Aiming to realize Gaussian clustering, {\tt Selvarclust} does estimate a multivariate Gaussian mixture on $\boldsymbol{X}$ with dependencies within the covariates. Our algorithm will only aim to find a linear sub-regression structure within $\boldsymbol{X}$, relying on some additional hypotheses of independence so the two methods cannot be directly compared. The only common point is the existence of a linear sub-regression model and an algorithm that estimates it. Thus results in section \ref{compMaugis} are just informative.
	%Our first intention was to make something different from SelvarClust but our model has evolved during this thesis and finally became more comparable to SelvarClust than expected.		
			
\part{Pretreatment for correlations}
\chapter{Decorrelating covariates by a generative model}
\paragraph{Abstract:} 
We give an explicit model for correlation between covariates by a linear regression system. It helps to better understand the dataset and leads to a pretreatment by variable selection. This pretreatment allows to reduce variance of the estimator and then to distinguish irrelevant covariates from redundant covariates.
\\
\section{Introduction}
Most of the above methods do not take explicitly the correlations into account, even if the clustering methods may group the correlated covariates together.
The idea of the present thesis is that if we know explicitly the correlations, we could use this knowledge to avoid specific problem it causes. Correlations are thus new information to reduce the variance without adding any bias. 
Modeling explicitly linear correlation between variables already exists in statistics. In Gaussian model-based clustering, {\tt Selvarclust} \cite{maugis2009variable} considers that some irrelevant covariates for clustering are in linear regression with some relevant ones.
 We propose to transpose this method for linear regression.
More precisely, correlations are modeled through a system of linear sub-regressions between covariates. The set of covariates which are {\it never} at the place of a response variable in these sub-regressions is finally the greatest set of orthogonal covariates.\\

 Marginalizing over the dependent co-variables leads then to a linear regression (in relation to the initial response variable) with only orthogonal covariates. This marginalization step can be viewed also as a variable selection step but guided only by the correlations between covariates. Advantages of this approach is twofold. First, it improves interpretation through a good readability of dependency between covariates. Second, this marginal model is still a ``true'' model provided that both the initial regression model and all the sub-regressions are ``true''. As a consequence, the associated {\sc ols} will preserve an unbiased estimate but with a possibly reduced variance comparing to the {\sc ols} with the full regression model. The fact is that the variance decreases depends on the residual variances involved in the sub-regressions: The more the sub-regressions are marked, the less will be the variance of associated {\sc ols}. In fact, any other estimation method than {\sc ols} can be plugged after the marginalization step. Indeed, it can be viewed as a pretreatment against correlation which can be chained after with dimension reduction methods, without no more suffering from correlations this time.

\section{Explicit modeling of the correlations}
	Let $\boldsymbol{X}=(\boldsymbol{X}^1,\dots,\boldsymbol{X}^d)$ be a $n \times d$ matrix of observed covariates and $\boldsymbol{Y}$ be the $n \times 1$ matrix of the observed response variable. In the following, we note $\boldsymbol{X}^j$ the $j^{th}$ column of $\boldsymbol{X}$ and $\boldsymbol{X}^{J}$ where $J=\{j_1,\dots,j_k\}$ the $n\times k$ sub-matrix of $\boldsymbol{X}$ composed by the columns of $\boldsymbol{X}$ whose indices are in the set $J$. 
\\




We focus now on an original manner to solve the covariates correlation problem. The covariates number problem will be solved at a second stage by standard methods, once only decorrelated covariates will be identified. The proposed method relies on the two following hypotheses.

\begin{hyp}\label{H1}
In order to take into account the covariates correlation problem, we make the hypothesis correlation between covariates is {\it only} the consequence that some covariates {\it linearly} depend on some other covariates. More precisely, there are $d_{r}\geq 0$ such ``sub-regressions'', each sub-regression $j=1,\ldots,d_{r}$ having the covariate $\boldsymbol{X}^{J_{r}^j}$ as {\it response} variable ($J_{r}^j\in\{1,\ldots,p\}$ and $J_{r}^j\neq J_{r}^{j'}$ if $j\neq j'$) and having the $d_p^j>0$ covariates $\boldsymbol{X}^{J_{p}^j}$  as {\it predictor} variables ($J_{p}^j\subset\{1,\ldots,d\} \backslash J_{r}^j$ and $d_p^j=|J_{p}^j|$ the cardinal of $J_{p}^j$):
\begin{equation}
\boldsymbol{X}^{J_{r}^j}|\boldsymbol{X}^{J_{p}^j};\boldsymbol{\alpha}_j,\sigma^2_j=\boldsymbol{X}^{J_{p}^j}\boldsymbol{\alpha}_j+\boldsymbol{\varepsilon}_j, \label{eq:SR}
\end{equation}
where $\boldsymbol{\alpha}_j\in{\mathbb{R}^{d_r^j}}$ (${\alpha}_j^h\neq 0$ for all $j=1,\ldots,d_r$ and $h=1,\ldots,d_p^j$) and $\boldsymbol{\varepsilon}_j \sim\mathcal{N}_n(\boldsymbol{0},\sigma^2_j\boldsymbol{I})$.
\end{hyp}

\begin{hyp}\label{H2}
In addition, we make the complementary hypothesis that the response covariates and the predictor covariates are totally disjoint: for any sub-regression $j=1,\ldots,d_{r}$, $J_{p}^j\subset J_f$ where $J_{r}=\{J_{r}^1,\ldots,J_{r}^{d_r}\}$ is set of all response covariates and $J_f=\{1,\ldots,d\} \backslash J_{r}$ is the set of all {\it non} response covariates of cardinal $d_f=d-d_r=|J_f|$. We call this hypothesis the uncrossing rule.
\end{hyp}



 This second assumption allows to obtain very simple sub-regressions sequences, discarding hierarchical ones, in particular uninteresting cyclic sub-regressions. However it is not too much restrictive since any hierarchical (but non-cyclic) sequence of sub-regressions can be agglomerated into a non-hierarchical sequence of sub-regressions, even if it may implies to partially loose information through variance increase in the new non-hierarchical sub-regressions. It is made by just successively replacing endogenous covariates by their sub-regression when they are also exogenous in some other sub-regressions. 

\paragraph{Further notations} In the following, we will note also $\boldsymbol{J}_r=(J_{r}^1,\ldots,J_r^{d_r})$ the $d_r$-uple of all the response variable (to be not confused with the corresponding set $J_r$ previously defined), $\boldsymbol{J}_p=(J_{p}^1,\ldots,J_p^{d_r})$ the $d_r$-uple of all the predictors for all the sub-regressions, $\boldsymbol{d}_p=(d_p^1,\ldots,d_p^{d_{r}})$ the associated number of predictors and $\boldsymbol{S}=(\boldsymbol{J}_r,\boldsymbol{J}_p)$ the global {\it model} of all the sub-regressions. As more compact notations, we define also $\boldsymbol{X}_r=\boldsymbol{X}^{J_{r}}$ the whole set of response covariates and also $\boldsymbol{X}_f=\boldsymbol{X}^{J_{f}}$ the {\it all} other covariates, denominating now as {\it free} covariates, including those used as predictor covariates in $\boldsymbol{J}_p$. An illustration of all these notations is applied on the running example at the end of this section. The parameters are also stacked together: $\boldsymbol{\alpha}=(\boldsymbol{\alpha}_1,\ldots,\boldsymbol{\alpha}_{d_r})$ denotes the global coefficient of sub-regressions and $\boldsymbol{\sigma}^2=(\sigma^2_1,\ldots,\sigma^2_{d_r})$ denotes the corresponding global variance.

\paragraph{Remarks}
\begin{itemize}
\item Sub-regressions defined in (\ref{eq:SR}) are very easy to understand by any practitioner and, thus, will give a clear view of all the correlations present in the dataset at hand.
%\item As a consequence of Hypotheses~\ref{H1} and~\ref{H2} , ``free'' covariates $\boldsymbol{X}_f$ are {\it all} decorrelated.
\item We have considered correlations between the covariates of the main regression on $\boldsymbol{Y}$, not between the residuals. Thus $\boldsymbol{S}$ does not depend on $\boldsymbol{Y}$ and it can be estimated independently as we will see in Chapter~\ref{chapterMCMC}, even with a larger dataset (if missing values in $\boldsymbol{Y}$).
\item The model of sub-regressions $\boldsymbol{S}$ gives a system of linear regressions that can be viewed as a recursive Simultaneous Equation Model (\textsc{sem})\cite{davidson1993estimation,TIMM} or also as a Seemingly Unrelated Regression (\textsc{sur}) \cite{SURzellner} with $\boldsymbol{X}_r$ the set of endogenous covariates.
\item Each sub-regression may imply a distinct subset of explicative covariates from $\boldsymbol{X}_f$.
\item Here we suppose the $\boldsymbol{\varepsilon}_j$ independent so we estimate them by separate \textsc{ols}. But in other cases \textsc{sur} takes into account correlations between residuals and could be used to estimate the $\boldsymbol{\alpha}^j$. 
\end{itemize} 

	\fcolorbox{vertfonce}{vertclair}{\begin{minipage}{0.95\textwidth}
 
\paragraph{In the running example:}$J_f=\{1,2,4,5\}$, $d_f=4$ and $\boldsymbol{X}_f=(\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{X}^4,\boldsymbol{X}^5)$. We have $d_r=1$ response covariate $\boldsymbol{X}^3|\boldsymbol{X}^1,\boldsymbol{X}^2=\boldsymbol{X}^1+\boldsymbol{X}^2+\boldsymbol{\varepsilon}_1$ where $\boldsymbol{\varepsilon}_1\sim{\mathcal{N}_n(\boldsymbol{0},\sigma_1^2\boldsymbol{I})}$. Thus, $\boldsymbol{\alpha}_1=(1,1)'$, $\boldsymbol{J}_r=(3)$, $J_r=\{3\}$, $\boldsymbol{X}_r=(\boldsymbol{X}^3)$, $\boldsymbol{d}_p=(2)$, $\boldsymbol{J}_p=(\{1,2\})$, $\boldsymbol{X}^{J_p^1}=(\boldsymbol{X}^1,\boldsymbol{X}^2)$ and $\boldsymbol{S}=((3),(\{1,2\}))$.  
\end{minipage}}


\section{A by-product model: marginal regression with decorrelated covariates}\label{sectionmarginal}
The aim is now to use the model of linear sub-regressions $\boldsymbol{S}$ (that we assume to be known in this part) between some covariates of $\boldsymbol{X}$ to obtain a linear regression on $\boldsymbol{Y}$ relying only on uncorrelated variables $\boldsymbol{X}_f$.  The way to proceed is to marginalize the joint distribution of $\{(\boldsymbol{Y},\boldsymbol{X}_r) |\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma^2}\}$ to obtain the distribution of $\{\boldsymbol{Y} |\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma^2}\}$ depending only on uncorrelated variables $\boldsymbol{X}_f$:
\begin{equation}\label{eq:marginal}
\mathbb{P}(\boldsymbol{Y} |\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2) = \int_{{\mathbb{R}^{d_r}}}\mathbb{P}(\boldsymbol{Y}| \boldsymbol{X}_f,\boldsymbol{X}_r,\boldsymbol{S};\boldsymbol{\beta},\sigma_Y^2) \mathbb{P}(\boldsymbol{X}_r | \boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2) d\boldsymbol{X}_r.
\end{equation}
We need the following new hypothesis:

\begin{hyp}\label{H3}
We assume that all errors $\boldsymbol{\varepsilon}_Y$ and $\boldsymbol{\varepsilon}_j$ ($j=1,\ldots,d_r$) are {\it mutually independent}. It implies in particular that conditional response covariates $\{\boldsymbol{X}^{J_{r}^j}|\boldsymbol{X}^{J_{p}^j},\boldsymbol{S};\boldsymbol{\alpha}_j,\sigma^2_j\}$, with distribution defined in (\ref{eq:SR}), are {\it mutually independent}:
\begin{equation}\label{eq:H3}
\mathbb{P}(\boldsymbol{X}_r | \boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2) = \prod_{j=1}^{d_r} \mathbb{P}(\boldsymbol{X}^{J_{r}^j}|\boldsymbol{X}^{J_{p}^j},\boldsymbol{S};\boldsymbol{\alpha}_j,\sigma^2_j). 
\end{equation}
\end{hyp}

\vspace{3mm}

Noting $\boldsymbol{\beta}_r=\boldsymbol{\beta}_{J_r}$ and $\boldsymbol{\beta}_f=\boldsymbol{\beta}_{J_f}$ the regression coefficients associated respectively to the responses and to the free covariates, we can rewrite (\ref{regressionsimple}):
\begin{equation}
			\boldsymbol{Y}{|\boldsymbol{X},\boldsymbol{S}};\boldsymbol{\beta},\sigma_Y^2=\boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y. \label{eq:MainR}
\end{equation}
Combining now (\ref{eq:MainR}) with (\ref{eq:SR}), (\ref{eq:marginal}) and (\ref{eq:H3}), and also independence between each $\boldsymbol{\varepsilon}_j$ and $\boldsymbol{\varepsilon}_Y$, we obtain the following closed-form for the distribution of $\{\boldsymbol{Y} |\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2\}$:
\begin{eqnarray}
	\boldsymbol{Y}|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\alpha}_j^*)+  \sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{eq:Trueexpl} \\
	&=&\boldsymbol{X}_f\boldsymbol{\beta}_f^*+\boldsymbol{\varepsilon}_Y^*,\label{eq:modexpl}
\end{eqnarray}
where $\boldsymbol{\alpha}^*_j \in \mathbb{R}^{d_f}$ with $(\boldsymbol{\alpha}_j^*)_{J_p^j}=\boldsymbol{\alpha}_j $ and $(\boldsymbol{\alpha}_j^*)_{J_f\setminus J_p^j}=\bf 0 $. We can then define the matrix $\boldsymbol{\alpha}^* \in \mathbb{R}^{(d_f \times d_r)}$ of the coefficient of sub-regression to use more compact notation:
\begin{equation}
	\boldsymbol{X}_r|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2=\boldsymbol{X}_f\boldsymbol{\alpha}^*+\boldsymbol{\varepsilon}
\end{equation} 
Where $\boldsymbol{\varepsilon}$ is the $n\times d_r$ matrix of the $\boldsymbol{\varepsilon}^j$.\\

Consequently, we have obtained a new regression expression of $\boldsymbol{Y}$ but relying now {\it only} on uncorrelated covariates $\boldsymbol{X}_f$. This decorrelation process has also acted like a specific variable selection process because $\boldsymbol{X}_f \subseteq \boldsymbol{X}$. These two statements are expected to decrease the variance of further estimates of $\boldsymbol{\beta}$.\\

 However, the counterpart is twofold. First, this regression has a higher (or equal) residual variance than the initial one since it is now $\sigma^{2*}_Y=\sigma^2_Y+\sum_{j =1}^{d_r}\beta_{J_r^j}^2\sigma^2_j$ instead of $\sigma^2_Y$. Second, variable selection being equivalent to set $\hat{\boldsymbol{\beta}}_r=\boldsymbol{0}$, it implies possibly biased estimates of $\boldsymbol{\beta}_r$. As a conclusion, we are faced with a typical {\it bias-variance trade off}. We will illustrate it in the chapter~\ref{chaptersimulexpl}.\\


In practice, the strategy we propose is to rely estimate of  $\hat{\boldsymbol{\beta}}$ upon Equation~(\ref{eq:modexpl}). The practitioner can choose any estimate of its choice, like {\sc ols} or any variable selection procedure like {\sc lasso}. In other words, it is possible to see $(\ref{eq:modexpl})$ as a kind of {\it pretreatment} for decorrelating covariates, while assuming nothing on the subsequent estimate process.\\

%\paragraph{Remark}
%\begin{itemize}
%\item Sub-regressions defined in (\ref{eq:SR}) are very easy to understand by any practitioner and, thus, will give a clear view of all the correlations present in the dataset at hand.

%\item We have considered correlations between the covariates of the main regression on $\boldsymbol{Y}$, not between the residuals. Thus $\boldsymbol{S}$ does not depend on $\boldsymbol{Y}$ and it can be estimated independently as we will see in Chapter~\ref{chapterMCMC}, even with a larger dataset (if missing values in $\boldsymbol{Y}$).
%\item The model of sub-regressions $\boldsymbol{S}$ gives a system of linear regressions that can be viewed as a recursive Simultaneous Equation Model (\textsc{sem})\cite{davidson1993estimation,TIMM} or also as a Seemingly Unrelated Regression (\textsc{sur}) \cite{SURzellner} with $\boldsymbol{X}_r$ the set of endogenous covariates.
%\item Each sub-regression may imply a distinct subset of explicative covariates from $\boldsymbol{X}_f$.
%\item Here we suppose the $\boldsymbol{\varepsilon}_j$ independent so we estimate them by separate \textsc{ols}. But in other cases \textsc{sur} takes into account correlations between residuals and could be used to estimate the $\boldsymbol{\alpha}^j$. 
%\end{itemize} 

In the following, we will denote by {\sc CorReg} (for {\it Correlations} and {\it Regression}) the new proposed strategy.

\paragraph{Remarks:} 
\begin{itemize}
\item Identifiability of $(\boldsymbol{X},\boldsymbol{S})$ is not necessary to use a given structure but helps to find it. Moreover, uncrossing rule restricts the size of $\mathcal{S}_d$ and improves identifiability. A sufficient condition for identifiability is to have at least two regressors in each sub-regression (definition of identifiability and proof of this criterion in appendix \ref{sectionident}). 
In the followings, true $\boldsymbol{S}$ is supposed to be identifiable.
\item As a consequence of Hypotheses~\ref{H1} to~\ref{H3} , ``free'' covariates $\boldsymbol{X}_f$ are {\it all} decorrelated (see the Lemma in appendix \ref{sectionident}).
\end{itemize}
	

\fcolorbox{vertfonce}{vertclair}{\begin{minipage}{0.95\textwidth}
\paragraph{Running example:} $\boldsymbol{Y}|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2= 2\boldsymbol{X}^1+2\boldsymbol{X}^2+\boldsymbol{X}^4+\boldsymbol{X}^5+\boldsymbol{\varepsilon}_1 +\boldsymbol{\varepsilon}_Y$.
\end{minipage}}


\section{Strategy of use: pre-treatment before classical estimation/selection methods}\label{interpretation}

As a pre-treatment, the model allows usage of any method in a second time to estimate $\boldsymbol{\beta}_{f}^*$, even with variable selection methods like \textsc{lasso} or a best subset algorithm like stepwise \cite{seber2012linear}. However, we always have $\boldsymbol{\beta}^*_r=\boldsymbol{0}$.\\

After selection and estimation we will obtain a model with { \it two steps of variable selection}: the decorrelation step by marginalization (coerced selection associated to redundant information defined in $\boldsymbol{S}$) and the classical selection step, with different meanings for obtained zeros in $\hat{\boldsymbol{\beta}}^*_{f}$ (irrelevant covariates) and for $\hat{\boldsymbol{\beta}}^*_{r}=0$ (redundant information). 
 Thus we are able to distinguish the reasons that have lead to keep or remove each covariate and consistency issues do not mean interpretation issues any more. So we dodge the drawbacks of both grouping effect and variable selection.\\


The explicit structure is parsimonious and simply consists in linear regressions and thus is easily understood by non statistician, allowing them to have a better knowledge of the phenomenon inside the dataset and to take better actions. Expert knowledge can even be added to the structure, physical models for example.\\

Moreover, the uncrossing constraint (partition of $\boldsymbol{X}$) guarantee to keep a simple structure easily interpretable (no cycles and no chain-effect) and straightforward readable.\\
	
			There is no theoretical guarantee that our model is better. It's just a compromise between numerical issues caused by correlations for estimation and selection versus increased variability due to structural hypotheses. We just play on the traditional bias-variance tradeoff. 
			


	
	\section{Illustration of the tradeoff conveyed by the pre-treatment}	
	We compare the \textsc{ols} estimator on $\boldsymbol{X}$ defined in section \ref{sectionOLS} with the estimator obtained by the pre-treatment that is $\boldsymbol{X}_f$ selection.
  
For the marginal regression model defined in (\ref{eq:modexpl})
%	\begin{equation}
%		\boldsymbol{Y}_{|\boldsymbol{X}_f}= \boldsymbol{X}_f\boldsymbol{\beta}_f^*+ \boldsymbol{\varepsilon}_Y^*
%	\end{equation}			
%		So 
we have the \textsc{ols} unbiased estimator of $\boldsymbol{\beta}^*$: 
		\begin{equation}
			\hat{\boldsymbol{\beta}}_{f}^* = (\boldsymbol{X}_{f}'\boldsymbol{X}_{f})^{-1}\boldsymbol{X}_{f}'\boldsymbol{Y}  \textrm{ and }\boldsymbol{\hat\beta}_{r}^* = \boldsymbol{0}
		\end{equation}
		We see in (\ref{eq:Trueexpl}) that it gives an unbiased estimation of $\boldsymbol{Y}$ and $\boldsymbol{\beta^*}$
		but in terms of $\boldsymbol{\beta}$ this estimator could be biased:
		\begin{equation}
			\mathbb{E}(\hat{\boldsymbol{\beta}}_{f}^*)=\boldsymbol{\beta}_{f}+\sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\alpha}_j^* \textrm{ and }\mathbb{E}(\hat{\boldsymbol{\beta}}_{r}^*)=\boldsymbol{0}
		\end{equation}
		In return, its variance could be reduced compared to this one of $\hat{\boldsymbol{\beta}}$ given in (\ref{eq:varOLS}) as soon as values of $\sigma_j$ are small enough (it means strong correlations in sub-regressions) as we can see in the following expression
		\begin{equation}
			\operatorname{Var}(\hat{\boldsymbol{\beta}}_f^*)= (\sigma^2_Y+\sum_{j =1}^{d_r}\sigma^2_{j}\beta_{J_r^j}^2)(\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1} \quad \textrm{and} \quad\operatorname{Var}(\hat{\boldsymbol{\beta}}_r^*)= \boldsymbol{0}. \label{eq:varOLS*}
		\end{equation}
Indeed, no correlations between covariates $\boldsymbol{X}_f$ imply that the matrix $\boldsymbol{X}_f' \boldsymbol{X}_f$ could be sufficiently better conditioned than the matrix $\boldsymbol{X}' \boldsymbol{X}$ involved in (\ref{eq:varOLS}) to balance the added variance $\sum_{j =1}^{d_r}\sigma^2_{j}\beta_{J_r^j}^2$ in (\ref{eq:varOLS*}). This bias-variance trade off can be resumed by the Mean Squared Error (\textsc{mse}) associated to both estimates:
	\begin{eqnarray}
	\textsc{mse}(\hat{\boldsymbol{\beta}})&=&\parallel \operatorname{Bias}\parallel_2^2+\operatorname{Tr}(\operatorname{Var}(\hat{\boldsymbol{\beta}})) \\
			&=& \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}),
			 \\
			\textsc{mse}(\hat{\boldsymbol{\beta}}^*)&=& \parallel\sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\alpha}_j^* \parallel_2^2 +\parallel \boldsymbol{\beta}_r\parallel^2_2 + (\sigma^2_Y+\sum_{j =1}^{d_r}\sigma^2_{j}\beta_{J_r^j}^2 ) \operatorname{Tr}((\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1}).
	\end{eqnarray}	 
 				  
	This bias-variance tradeoff is now illustrated with the running example.
	\section{{\sc mse} comparison on the running example}\label{MSEvraiS}
In this section, all experiences have been made 100 times and we take the mean to obtain smooth curves. So we have generated 100 times $\boldsymbol{X}$ and $\boldsymbol{Y}$.\\	
	The \textsc{mse} on $\hat{\boldsymbol{Y}}$ is computed on a validation sample with 1 000 individuals.\\
%\paragraph{Theoretical MSE for OLS:}
%	 We observe the theoretical Mean Squared Error (MSE) of the estimator of both OLS and \textsc{CorReg}'s marginal  model for several values of $\sigma_1$ and $n$. Figure \ref{MQEexplOLSp5col} shows the theoretical MSE evolution with the strength of the sub-regression expressed by the standard coefficient of determination. 
%
%\begin{figure}[h!] 
%	\includegraphics[width=500px]{figures/MQEexplOLSp5col.png}\label{MQEexplOLSp5col}
%	\caption{Theoretical MSE on $\hat{\boldsymbol{\beta}}$ of OLS (plain red) and {\tt CorReg}'s marginal model (dotted blue) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$.}
%\end{figure} 
		\paragraph{Observed {\sc mse} for {\sc ols}:} It is clear in figures \ref{MSEOLSexpl} and \ref{MQEOLSYexpl} that the marginal model is more robust than \textsc{ols} on $\boldsymbol{X}$. Colored areas indicate which curve has the minimum value for faster comparison of the curves. This kind of plot will be widely use in this document. Here blue areas stand for the marginal model so our marginal model is better (in terms of \textsc{mse}) when the background is blue.
	We see in figures \ref{MSEOLSexpl} and \ref{MQEOLSYexpl} that \textsc{mse} on $\hat{\boldsymbol{Y}}_{OLS}$ give the same global results as those on $\hat{\boldsymbol{\beta}}_{OLS}$: the marginal model is better for stronger sub-regressions, smaller samples and weaker main regression. But we notice that when the \textsc{mse} on $\hat{\boldsymbol{\beta}}_{OLS}$ explodes, the \textsc{mse} on $\hat{\boldsymbol{Y}}_{OLS}$ does not grow so much. This is a good illustration of the problem generated by the correlations. The model seems to be good in prediction but coefficients are very far from the real value and interpretation can be extremely misleading.\\
	
When sub-regression get weaker ($R^2$ tends to 0) \textsc{CorReg} remains stable until extreme values (sub-regression nearly fully explained by the noise).The marginal should be particularly useful when some covariates are highly correlated, when the sample size is small or when the residual variance of $\boldsymbol{Y}$ is large. These three classical problems for \textsc{ols} make \textsc{CorReg} better. It illustrates the importance of dimension reduction when the main model has a strong noise (very usual case on real datasets where true model is not even exactly linear). \\
	
	We notice that the curve is not totally smooth for small samples ($n=15$) because of numerical approximation of the theoretical \textsc{mse}. It confirms that matricial inversion is not easy with correlated covariates even with a small number of covariates. But it is only the theoretical \textsc{mse} and we want to know what happens in the real life. \\
	
 \begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/OLSexpl.png}
	\caption{Observed \textsc{mse} on $\hat{\boldsymbol{\beta}}$ of \textsc{ols} (plain red) and {\tt CorReg}'s marginal model (dotted blue) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $d=5$ covariates.}\label{MSEOLSexpl}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/OLSYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEOLSYexpl}
	\end{figure}

	
	We also look at the observed \textsc{mse} on both $\boldsymbol{\beta}$ and $\boldsymbol{Y}$ for some of the methods depicted above.
	
 \begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/larexpl.png}
	\caption{Observed \textsc{mse} on $\hat{\boldsymbol{\beta}}$ of \textsc{lasso} with LAR on both $\boldsymbol{X}$ (red) and {\tt CorReg}'s marginal $\boldsymbol{X}^{I_f}$ (blue) for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $d=5$ covariates.}\label{MSElarexpl}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/larYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{LASSO}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQElarYexpl}
	\end{figure}
	
		\paragraph{Observed {\sc mse} for variable selection methods:}
	Figure \ref{MSElarexpl} shows that variable selection done by the \textsc{lasso} gives a biased $\hat{\boldsymbol{\beta}}$ by setting some coefficients to 0 but strong correlations makes this bias neutral for prediction (figure \ref{MQElarYexpl}). Here the \textsc{lasso} tends to propose the same model as we do  with our marginal model, but without explanation. We will see  later in section \ref{compY} that it is not sufficient in higher dimension. Elasticnet and stepwise give results quite similar to the \textsc{lasso} (figures \ref{MSEelasticnetexpl} to \ref{MQEstepwiseYexpl}). \\
%ridge
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/ridgeexpl.png}
	\caption{Observed \textsc{mse} on $\hat{\boldsymbol{\beta}}_{ridge}$ on both $\boldsymbol{X}$ (red) and {\tt CorReg}'s marginal $\boldsymbol{X}_f$ (blue) for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $d=5$ covariates.}\label{MSEridgexpl}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/ridgeYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEridgeYexpl}
	\end{figure}	
	
		\paragraph{Observed {\sc mse} for ridge regression:}	
Here again (figures \ref{MSEridgexpl} and \ref{MQEridgeYexpl}), ridge regression provides good results for this running example. But we will see later in section \ref{compY} that high dimension reduces the efficiency of the ridge regression when some covariates begin to be irrelevant or not enough relevant because ridge regression is not able to achieve variable selection. \\


 %elasticnet
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/elasticnetexpl.png}
	\caption{Observed \textsc{mse} on $\hat{\boldsymbol{\beta}}_{elasticnet}$ on both $\boldsymbol{X}$ (red) and {\tt CorReg}'s marginal $\boldsymbol{X}_f$ (blue) for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $d=5$ covariates.}\label{MSEelasticnetexpl}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/elasticnetYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{elasticnet}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEelasticnetYexpl}
	\end{figure}
  %stepwise
	\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQEreel/stepwiseexpl.png}\label{MSEstepwiseexpl}
	\caption{Observed \textsc{mse} on $\hat{\boldsymbol{\beta}}_{stepwise}$ on both $\boldsymbol{X}$ (red) and {\tt CorReg}'s marginal $\boldsymbol{X}_f$ (blue) for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. $d=5$ covariates.}
\end{figure} 
	
	 \begin{figure}
	 \centering
	  \includegraphics[width=500px]{figures/MQEreel/stepwiseYexpl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{Y}}_{stepwise}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEstepwiseYexpl}
	\end{figure}
	
 	\FloatBarrier		
			
		
\section{Connexion with graphs}\label{sectiongraph}
	We can also model $\boldsymbol{S}$ by a Directed Acyclic Graph (DAG) whose vertices are the $d$ covariates and directed edges are the links  between them described by the adjacency matrix $\boldsymbol{G}$ \cite{bondy1976graph}. This adjacency matrix is a binary $d\times d$ matrix with $\boldsymbol{G}_{i,j}=1$ if and only if $i \in J_p^j$ that is $\boldsymbol{X}^j$ is explained by $\boldsymbol{X}^i$ and can also be seen as $\boldsymbol{\alpha}^*_{i,j}\neq 0$.\\
	
	Graphical representation of $\boldsymbol{S}$ helps to understand it and can be compared to the Bayesian network representation. It helps to interpret the structure and has also been used to construct the algorithm to find $\boldsymbol{S}$ (Chapter \ref{chapterMCMC}).
	The partition of $\boldsymbol{X}$ means that the associated graph is bipartite: vertices follow a partition $(\boldsymbol{X}_r,\boldsymbol{X}_f)$ with directed edges only going from $\boldsymbol{X}_f$ to $\boldsymbol{X}_r$.
	We know (\cite{biggs1993algebraic}) as a classical result of graph theory that the power of adjacency matrices give the paths in the graph: $\boldsymbol{G}^k_{i,j}\neq 0$ means that there is at least a path of length $k$ going from $\boldsymbol{X}^i$ to $\boldsymbol{X}^j$. We note $\mathcal{S}_d$ the set of feasible structures (verifying hypothesis \ref{H2}) with $d$ covariates. Because the graph is bipartite we can deduce that $\boldsymbol{G}$ is nilpotent: $\boldsymbol{G}^2=0$. And we have the following result:\\
	
	\paragraph{Theorem:} Every binary nilpotent matrix of order 2 can be seen as an adjacency matrix of a structure that respects the uncrossing rule, and {\it vice versa}. We can deduce that the number of feasible structures with $d$ covariates is the number of $d$-sized binary nilpotent matrices of order 2. \\
	 
	\paragraph{proof:} Every binary matrix can be associated to a structure as an adjacency matrix and every structure can be described by its adjacency matrix. Thus we just have to demonstrate equivalence between uncrossing and nilpotent $\boldsymbol{G}$. If there exist a path of length 2 between some vertices $i$ and $j$ (uncrossing rule violated) then $\boldsymbol{G}^2_{i,j}\neq 0$ so the matrix is not nilpotent of order 2. So the set of nilpotent matrices is included the set of adjacency matrices associated to $\mathcal{S}_d$. If $\boldsymbol{G}$ is nilpotent then there is no path of length 2 in the graph (uncrossing rule verified). So we have the reverse inclusion and then equivalence between the set of adjacency matrices associated to $\mathcal{S}_d$ and the set of nilpotent matrices. $\square$\\
	
	We see that $\boldsymbol{G}$ completely describe $\boldsymbol{S}$ that is in fact a sparse storage of $G$.
	We decompose the structure to enumerate all the feasible structure (and thus all the binary nilpotent matrix of order 2).\\
	% 
	
	
%	\begin{eqnarray}
		%\forall \boldsymbol{S} \in \mathcal{S}_d: \mathbb{P}(\boldsymbol{S})&=& \mathbb{P}(\boldsymbol{J}_p,\boldsymbol{J}_r)=\mathbb{P}(\boldsymbol{J}_p,\boldsymbol{J}_r,d_r) \\
		%&=& \mathbb{P}(\boldsymbol{J}_p|\boldsymbol{J}_r,d_r) \times  \mathbb{P}(\boldsymbol{J}_r|d_r)\times \mathbb{P}(d_r) 
		%	\end{eqnarray}
			%If we consider equiprobability within $\mathcal{S}_d$, the

			The number of possible $\boldsymbol{J}_r$ for given values of $d$ and $d_r$ is ${d \choose d_r}=\frac{d!}{d_r!(d-d_r)!}$ (binomial coefficient).\\
	
			The number of possible $\boldsymbol{J}_p$ for given values of $d, d_r$ and $\boldsymbol{J}_r$ is $(2^{d-d_r}-1)^{d_r}$.\\
			
			Thus we have
			\begin{eqnarray}
		%\operatorname{Card}(\boldsymbol{J}_p|\boldsymbol{J}_r,d_r)&=& (2^{d-d_r}-1)^{d_r}\\
		|\mathcal{S}_d|&=&%\sum_{d_r=0}^{d-1}|\mathcal{S}_{d|d_r}|=
		 %\sum_{d_r=0}^{d-1}\frac{1}{\mathbb{P}(\boldsymbol{S}|d_r)} =
		 \sum_{d_r=0}^{d-1}{d \choose d_r}(2^{d-d_r}-1)^{d_r}
	\end{eqnarray}
	 \\
	We have then $|\mathcal{S}_2| =3 $, $|\mathcal{S}_3| =13 $ and $|\mathcal{S}_{10}| >13.26\times10^9 $ so the number of feasible structures really explodes when $d$ is growing. Next chapter shows results with $d=40$ and it corresponds to $|\mathcal{S}_{40}| >7.32\times10^{131} $ feasible structures. The function {\tt ProbaZ } of the package { \tt CorReg} gives the number of feasible structures for a given value of $d$, but {\tt R} returns $+\infty$ for $d>62$.\\

\fcolorbox{vertfonce}{vertclair}{\begin{minipage}{0.95\textwidth}

\paragraph{In the running example:} $|\mathcal{S}_5| =841 $ and the adjacency matrix is:
\begin{displaymath}G=\left(	\begin{array}{ccccc}
	0 & 0 & 1 & 0 & 0 \\ 
	0 & 0 & 1 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0
	\end{array} \right)
\end{displaymath}
\end{minipage}}
	\begin{figure}[h!]
	\centering
	\includegraphics[width=200px]{figures/graphetoy.png} 
	\caption{The bipartite graph associated to the running example. $\boldsymbol{X}_r$ is green and $\boldsymbol{X}_f$ is blue.}
	\end{figure}			
			
\chapter{Numerical results with a known structure on more complex datasets}	\label{chaptersimulexpl}
\paragraph{Abstract:} Here are the first numerical results obtained for a known structure $\boldsymbol{S}$ (but $\boldsymbol{\alpha}^* $ still has to be estimated) on simulated datasets. It illustrates efficiency of the variable selection pretreatment made by the marginal model.
		 
	
	\section{The datasets}\label{thedatasets}
		
We consider regressions on $\boldsymbol{Y}$ with $d=40$ covariates and with a $R^2$ value equal to $0.4$. Sub-regressions will have $R^2$ successively set to $(0.1,0.3,0.5,0.7,0.99)$. Variables in $\boldsymbol{X}_f$ arise from a Gaussian mixture model whose the number of components follows a Poisson's law of mean parameter equal to $5$. The coefficients of $\boldsymbol{\beta}$ and of the $\boldsymbol{\alpha}_j$'s are independently generated according to the same Poisson distribution but with a uniform random sign. All sub-regressions are of length two ($\forall j=1,\ldots,d_r, d_p^j=2$ and we have $d_r=16$ sub-regressions. The datasets are then scaled, to avoid large distortions for variances or for means due to the sub-regressions.	Different sample sizes  $n\in (30,50,100,400)$ are chosen, thus considering experiments in both situations $n<d$ and $n>d$. 
	Results are based on the true $\boldsymbol{S}$ used to generate the dataset (function {\tt mixture\_genererator} in the package {\tt CorReg}).\\
	
	When $n<d$, a frequently used method is the Moore-Penrose \cite{katsikis2008fast} generalized inverse , thus \textsc{ols} can obtain some results even with $n<d$. %(see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
	When using penalized estimators for selection, a last Ordinary Least Square step is added to improve estimation because penalisation is made to select variables but also shrinks remaining coefficients. This last step allows to keep the benefits of shrinkage (variable selection) without any impact on remaining coefficients (see \cite{SAM10088}) and is applied for both classical and marginal model.
	We compare different methods with and without {\tt CorReg} as a pretreatment. All the results are provided by the {\tt CorReg} package. Here $\boldsymbol{Y}$ depends on all the covariate and the \textsc{mse} provided were computed on a validation sample of 1 000 individuals each time. Figures will display both mean and inter-quartile intervals, coloration of the background indicates which curve is lower ({\it i.e.} better in our case). \\
	\section{Results}	
	As previously explained, we have added colored backgrounds for faster comparison of the curves. The background takes the same color as the curve that is the lowest. So we want to obtain as much blue areas as possible,meaning our marginal model is better (in terms of {\sc mse} or in terms of parsimony).
\paragraph{With {\sc ols}:} Figure \ref{MSEexplOLS} %Y_zone} to \ref{MSEexplOLSbeta_zone} 
	compares \textsc{ols} alone to \textsc{CorReg} + \textsc{ols}. 
{\sc CorReg} improves significantly the prediction power of {\sc ols} for small values of $n$ and/or heavy sub-regression structures. This advantage then shrinks when $n$ increases because the matrix to invert becomes better-conditioned and since {\sc CorReg} does not allow to retrieve that $\boldsymbol{Y}$ depends on all $\boldsymbol{X}$ because of the marginalization of some covariates implicated in the sub-regressions. It also illustrates that the model regression in $\boldsymbol{Y}$ retained by {\sc CorReg} is more parsimonious.%, provided that sub-regressions are strong enough.

\paragraph{Variable selection methods:} Figure \ref{MSEexpllar} shows that even if the \textsc{lasso} is able to select a subset of covariates and even if we have seen with \textsc{ols} that taking a subset can give better results, the \textsc{lasso} does not do so and give more complex models than our marginal model until correlations are extremely strong. We also observe that our marginal model combined with the \textsc{lasso} has varying complexities so our pretreatment by selection is just a pretreatment and not competitor against the \textsc{lasso}. Such combination improves the results in a significant way when compared to the \textsc{lasso} on the complete dataset or \textsc{ols} on the marginal model. We see that the complexity rises with $n$ but the \textsc{lasso} never keeps all the covariates even with $n=400=10\times d$ when used on the whole dataset but keep all the covariates in $\boldsymbol{X}_f$ when used on the marginal model. The main result here is that the \textsc{lasso} can be improved by pretreatment selection both with $n<d$ and $n>>d$ with strong correlations so this well known variable selection method really suffers from correlations. Elasticnet and stepwise (Figures \ref{MSEexplelasticnet} and \ref{MSEexplstepwise})gives results mostly equivalent to the \textsc{lasso} but stepwise seems to be a bit less efficient (higher \textsc{mse} values). This last point illustrates why we need a specific algorithm to find the structure $\boldsymbol{S}$ and not only variable selection by stepwise like in the method from Maugis \cite{maugis2009variable}. \\

\paragraph{Ridge regression:} Figure \ref{MSEexplridge} shows that the predictive power of ridge regression is not improved by the marginal model. Ridge regression is protected against correlations but we see that ridge regression applied on $\boldsymbol{X}_f$ (even if it is not the true model) give predictions quite similar to those from ridge regression but with less covariates. Ridge regression will only be damaged by correlations when variable selection is needed. \\

Further results are provided  with estimated $\hat{\boldsymbol{S}}$ (Section \ref{sectionsimul}) and then with real industrial datasets (Section \ref{sectionrealcase}).
	\FloatBarrier

\newpage
\subsection{{\sc ols} when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $\boldsymbol{S}$ known}
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explvraiS/MSEexplOLSY_zone.png}\label{MSEexplOLSY_zone} 
\end{tabular}		%\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explvraiS/cplexplOLS_zone.png}
		%\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplOLS_zone}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explvraiS/MSEexplOLSbeta_zone.png}
		\label{MSEexplOLSbeta_zone}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), plain red=classical (complete) model, dotted blue=marginal model}\label{MSEexplOLS}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}

%OLS
%	\begin{figure}[h!]
%	\centering
%		  \includegraphics[width=350px]{figures/explvraiS/MSEexplOLSY_zone.png}
%		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSY_zone}
%	\end{figure}
%	\begin{figure}[h!]
%	\centering
%		  \includegraphics[width=350px]{figures/explvraiS/cplexplOLS_zone.png}
%		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplOLS_zone}
%	\end{figure}
%	\begin{figure}[h!]
%	\centering
%		  \includegraphics[width=350px]{figures/explvraiS/MSEexplOLSbeta_zone.png}
%		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSbeta_zone}
%	\end{figure}
%	
%	
%	\FloatBarrier
%\newpage
%LASSO
\subsection{{\sc lasso} when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $\boldsymbol{S}$ known}
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explvraiS/MSEexpllarY_zone.png}\label{MSEexpllarY_zone} 
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explvraiS/cplexpllar_zone.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explvraiS/MSEexpllarbeta_zone.png}
		\label{MSEexpllarbeta_zone}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), plain red=classical (complete) model, dotted blue=marginal model}\label{MSEexpllar}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%elasticnet
\subsection{Elasticnet when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $\boldsymbol{S}$ known}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explvraiS/MSEexplelasticnetY_zone.png}\label{MSEexplelasticnetY_zone} 
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explvraiS/cplexplelasticnet_zone.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explvraiS/MSEexplelasticnetbeta_zone.png}
		\label{MSEexplelasticnetbeta_zone}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), plain red=classical (complete) model, dotted blue=marginal model}\label{MSEexplelasticnet}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%stepwise
\subsection{Stepwise when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $\boldsymbol{S}$ known}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explvraiS/MSEexplstepwiseY_zone.png}\label{MSEexplstepwiseY_zone} 
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explvraiS/cplexplstepwise_zone.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explvraiS/MSEexplstepwisebeta_zone.png}
		\label{MSEexplstepwisebeta_zone}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), plain red=classical (complete) model, dotted blue=marginal model}\label{MSEexplstepwise}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%ridge	
\subsection{Ridge when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$, true $\boldsymbol{S}$ known}
	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explvraiS/MSEexplridgeY_zone.png}\label{MSEexplridgeY_zone} 
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explvraiS/cplexplridge_zone.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explvraiS/MSEexplridgebeta_zone.png}
		\label{MSEexplridgebeta_zone}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), plain red=classical (complete) model, dotted blue=marginal model}\label{MSEexplridge}
\end{figure}
	\FloatBarrier
	\setcellgapes{1pt}

%	\section{Predictive efficiency}
%		Résultats comme dans l'article	mais sur vrai S	
%		modèles avec beaucoup de variables (car rapide vu que pas de MCMC).
%		Faire avec vrai modèle= modèle complet puis X1 uniquement puis X2 uniquement puis aussi Amax=15 si p largement supérieur à 100
\chapter{Estimation of the Structure of subregression by MCMC}\label{chapterMCMC}
\paragraph{Abstract:} In the previous chapters, $\boldsymbol{S}$ was supposed to be known. But in fact we have to find it so we developed an algorithm to choose the best $\boldsymbol{S}$. An MCMC algorithm is proposed to find $\boldsymbol{S}$, relying on a full generative model on $\boldsymbol{X}$. A new criterion is also introduced to choose the better $\boldsymbol{S}$ taking into account the huge number of feasible structures.
\section{Choice of model}
		% ne pas oublier de mentionner les packages existants
			\subsection{Cross validation}
				
				We want a model that would remain good for new individuals. To have an idea of the stability of a model, it is recommended to test it on a validation sample. Model parameters are estimated with a learning sample and then the model is evaluated (by its predictive \textsc{mse} for example) on a validation sample to avoid over-fitting. But it is not always possible to have a validation sample and over-fitting is a real problem. A solution is to use Cross-Validation \cite{kohavi1995study,arlot2010survey}. It consists in splitting the dataset in $k$ sub-samples ($k$-fold cross-validation) and then each of the $k$ sub-samples is successively used as validation sample for the model learnt with the $k-1$ remaining sub-samples. Each time a quality criterion  is computed (predictive \textsc{mse} or other) and then the mean of this criterion is taken as the global criterion. The global estimator is also the mean of the estimators. The two main issues are:
				\begin{itemize}
					\item How to choose $k$ the number of sub-samples?
					\item It can be time consuming as the model is estimated $k$ times.
				\end{itemize}
				If $k=n$ we call this method the ``leave-one-out" cross-validation.
				Cross-validation allows to learn the model using all individual exactly once for validation.
				Cross-validation is computed on each model we want to compare and just allows to avoid over-fitting when computing the comparison criterion.
				It is often used with the Mean Squared Error (\textsc{mse}), for example on the prediction:
				\begin{equation}
					\textsc{mse}_{\hat{Y}}=\parallel \boldsymbol{Y}-\hat{\boldsymbol{Y}} \parallel^2_2 .
				\end{equation}
			\subsection{ Bayesian Information Criterion}
			Cross-validation depends on a criterion to be used as a method to choose a model. The Mean Squared Error is not the only criterion. Probabilistic criteria can also be used when we have an hypothesis on the distribution of the studied model. Such criteria can also be used without cross-validation.
			The Bayesian Information Criterion \cite{BIChuard,schwarz1978estimating} is a widely used criterion that relies on the likelihood of the dataset knowing the model and the estimated parameters. The advantage of $\mbox{{\sc bic}}$ over simple usage of the likelihood is the penalty added to take into account the numbers of parameters to estimate (complexity is then penalized) and the number of individuals in the dataset. $\mbox{{\sc bic}}$ is consistent so it asymptotically points out the true model when $n$ grows. The Akaike Information Criterion \cite{akaike1974new} known as $\mbox{{\sc aic}}$ or the Risk Inflation Criterion \cite{foster1994risk} ($\mbox{{\sc ric}}$) can also be used, or any other criterion \cite{george1993variable} thought to be better in a given context.
			In this work we decided to start with the $\mbox{{\sc bic}}$ that is approximated by:
			\begin{equation}
				\mbox{{\sc bic}}=-2\ln(\hat{L(\boldsymbol{\theta}|\boldsymbol{X})})+|\boldsymbol{\theta}|\ln(n)
			\end{equation}
			where $L(\boldsymbol{\theta}|\boldsymbol{X})=\mathbb{P}(\boldsymbol{X}|\boldsymbol{\theta})$ is the estimated likelihood of the parameters $\boldsymbol{\theta}$ given the dataset $\boldsymbol{X}$, $|\boldsymbol{\theta}|$ is the number of free parameters to estimate and $n$ the number of individuals in the dataset. This choice comes from the popularity of $\mbox{{\sc bic}}$ and from the fact that it makes a strong penalization on the complexity and we want to obtain a model that is easy to understand so parsimony is a real stake.
			


\section{Bayesian approach}
Structural equations models are often used in social sciences and economy where a structure is supposed {\it a priori} but here we want to find it automatically, even if it remains possible to use expert knowledge to complete the structure. Graphical \textsc{lasso} \cite{friedman2008sparse} offers a method to obtain a structure based on the precision matrix (inverse of the variance-covariance matrix), setting some coefficients of the precision matrix to zero (see section \ref{sectionGlasso}). But the resulting matrix is symmetric and we need an oriented structure for $\boldsymbol{S}$ to avoid cycles.

Cross-validation is very time-consuming and thus not friendly with combinatory problems. Moreover, we need a criterion compatible with structures of different sizes ($d_r$ also has to be estimated) and not related with $\boldsymbol{Y}$ because the structure is inherent to $\boldsymbol{X}$ only. Thus it must be a global criterion. 	
Because it is about model selection, we decide to follow a Bayesian approach (\cite{raftery1995bayesian}, \cite{andrieu1999joint},\cite{chipman2001practical}).  
	

	

\section{Sub-regression structure in details}
We want to find the most probable structure $\boldsymbol{S}$ knowing the dataset, so we search for the structure that maximizes $\mathbb{P}(\boldsymbol{S}|\boldsymbol{X})$ and we have:	
	\begin{eqnarray}
	 \label{approxBIC} \mathbb{P}(\boldsymbol{S}|\boldsymbol{X})&\propto & \mathbb{P}(\boldsymbol{X}|\boldsymbol{S})\mathbb{P}(\boldsymbol{S})
	=\mathbb{P}(\boldsymbol{X}_r|\boldsymbol{X}_f,\boldsymbol{S})\mathbb{P}(\boldsymbol{X}_f|\boldsymbol{S})\mathbb{P}(\boldsymbol{S})
	\end{eqnarray}
In order to implement this paradigm, we need first to define the three probabilities which are in the right hand of the previous equation.
	\subsection{Modeling the uncorrelated covariates: a full generative approach on $P(\boldsymbol{X})$} \label{sectionfullgen}
	

\paragraph{Defining $\mathbb{P}(\boldsymbol{X}_r|\boldsymbol{X}_f,\boldsymbol{S})$} corresponds to the integrated likelihood based on $\mathbb{P}(\boldsymbol{X}_r | \boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma^2})$. It can be approximated by a {\sc bic}-like approach \cite{Sch1978}
\begin{equation}
-2\ln \mathbb{P}(\boldsymbol{X}_r|\boldsymbol{X}_f,\boldsymbol{S}) \approx -2\ln \mathbb{P}(\boldsymbol{X}_r | \boldsymbol{X}_f,\boldsymbol{S};\hat{\boldsymbol{\alpha}},\hat{\boldsymbol{\sigma}}^2) + (|{\boldsymbol{\alpha}}| + |{\boldsymbol{\sigma}}^2|) \ln(n) = \mbox{{\sc bic}}_r(\boldsymbol{S}),
\end{equation}
where $\hat{\boldsymbol{\alpha}}$ and $\hat{\boldsymbol{\sigma}}^2$ designate respectively the Maximum Likelihood Estimates ({\sc mle}) of ${\boldsymbol{\alpha}}$ and ${\boldsymbol{\sigma}}^2$, and $|\boldsymbol{\psi}|$ designates the number of free continuous parameters associated to the space of any parameter $\boldsymbol{\psi}$.

\paragraph{Defining $\mathbb{P}(\boldsymbol{X}_f|\boldsymbol{S})$} It corresponds to the integrated likelihood based on a not yet defined distribution $\mathbb{P}(\boldsymbol{X}_f|\boldsymbol{S};\boldsymbol{\theta})$ on the uncorrelated covariates $\boldsymbol{X}_f$ and parameterized by $\boldsymbol{\theta}$. In this purpose, we need the following new hypothesis.

\begin{hyp}\label{H4}
All covariates $\boldsymbol{X}^j$ with $j \in J_f$ are mutually independent and arise from the following Gaussian mixture of $k_j$ components
\begin{equation}
\mathbb{P}(\boldsymbol{X}_f^j|\boldsymbol{S};\boldsymbol{\pi}_{j},\boldsymbol{\mu}_j,\boldsymbol{\lambda}^2_j) = \sum_{h=1}^{k_j} \pi_{hj} \mathcal{N}_n(\mu_{hj} . (1,\ldots,1)',\lambda_{hj}^2\boldsymbol{I}),
\end{equation}
where $\boldsymbol{\pi}_{j}=(\pi_{1j},\ldots,\pi_{k_jj})$ is the vector of mixing proportions with $\forall 1\leq h\leq k_j , \pi_{hj}>0$ and $\sum_{h=1}^{k_j}\pi_{hj}=1$, $\boldsymbol{\mu}_j=(\mu_{1j},\ldots,\mu_{k_jj})$  is the vector of centers and $\boldsymbol{\lambda}^2_j=(\lambda^2_{1j},\ldots,\lambda^2_{k_jj})$ is the vector of variances. We stack together all the mixture parameters in $\boldsymbol{\theta}=(\boldsymbol{\pi}_{j},\boldsymbol{\mu}_j,\boldsymbol{\lambda}^2_j ; j \in J_f)$. We now have a full generative model on $\boldsymbol{X}$.

\end{hyp}
		 

\vspace{3mm}

Noting $\hat{\boldsymbol{\theta}}$ the {\sc mle} of $\boldsymbol{\theta}$, the {\sc bic} approximation can then be used again:
\begin{equation}
-2\ln \mathbb{P}(\boldsymbol{X}_f|\boldsymbol{S}) \approx -2\ln \mathbb{P}(\boldsymbol{X}_f|\boldsymbol{S};\hat{\boldsymbol{\theta}}) + |{\boldsymbol{\theta}}| \ln(n) = \mbox{{\sc bic}}_f(\boldsymbol{S}).
\end{equation}

\paragraph{Defining $\mathbb{P}(\boldsymbol{S})$} The most standard choice consists of putting a uniform distribution on the model space $\mathcal{S}_d$, this choice being noted $\mathbb{P}_U(\boldsymbol{S}) = |\mathcal{S}_d|^{-1}$, with $|\mathcal{S}_d|$ the space dimension of $\mathcal{S}_d$ as defined in section \ref{sectiongraph}.
		
	%Thus if one have some hypothesis on the distribution of some variables (exponentially distributed for example) it is possible to use it without impacting the model in other ways. %compute corresponding $\psi$ according to it. %and then improve the walk (will keep a structure only if it is really relevant).%and give it as an input of \textsc{CorReg} and then improve the efficiency of the algorithm (it will find a structure only if it is really relevant).
	
\paragraph{Remarks}\begin{itemize}
	\item Hypothesis~\ref{H4} is the keystone to define a full generative model on the whole covariates $\boldsymbol{X}$. On the one hand, the {\sc bic} criterion can be applied in this context, avoiding to use a cross-validation criterion which can be much more time-consuming. On the other hand, the great flexibility  of Gaussian mixture models \cite{mclachlan2004finite}, provided that the number of components $k_j$ has to be estimated, implies that Hypothesis~\ref{H4} is particularly weak in fact.
\item In practice, Gaussian mixture models are estimated only once for each variable $\boldsymbol{X}^j$ ($j=1,\ldots,d$). Thus, there is no combinatorial difficulty associated with them. An {\sc em} algorithm \cite{dempster1977maximum} will be used for estimating the mixture parameters and a classical {\sc bic} criterion \cite{Sch1978} will be used for selecting the different number of components $k_j$.
\end{itemize}	
	
	
	
	\subsection{Usage of the structure $\boldsymbol{S}$ for interpretation}
		
	
	Correlations can lead to serious problems in industrial context. Let's imagine a dataset with two highly correlated covariates. Estimators may give them the same coefficient (grouping effect when using elasticnet) or only keep one of them (like the \textsc{lasso} does). \\
	
	If only one is kept without explicitly pointing the correlations we will think that the deleted covariate is irrelevant even if it is not and even if it could be more relevant than the kept covariate in terms of possible actions or physical meanings.\\
	
	 On the other hand, keeping both without pointing correlations will lead to modify one of the covariates to impact the value of the response as we want and the other covariate will automatically move so we won't obtain the expected result.\\
	 
	  Choosing whereas we have to use or not the marginal model as a pretreatment is independent of the utility of the structure. Knowing explicitly the complex correlations that hold the dataset is a real stake when times come to interpret the model and to  decide actions. This is a strength of our method. We don't have to make a choice between grouping effect or variable selection because our explicit structure describes in details the complexity of the situation so we can then act knowing what we do, without having to blindly follow one of the two heuristics.

\section{Sub-regression model selection}	

%			\subsection{Bayesian criterion for quality}
%			
%			
%		Our full generative generative model allows us to compare structures with criterions like the Bayesian Information Criterion ($\mbox{{\sc bic}}$) which penalize the log-likelihood of the joint law on $\boldsymbol{X}$ according to the complexity of the structure~\cite{BIChuard}. \\
%		
%		
%			 We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion ) that choose a penalty in $\log d$ instead of $\log n$ \cite{foster1994risk} and thus gives more parsimonious models when $d$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context.
%		In the followings we use the $\mbox{{\sc bic}}$, that is more classical.
%		
%		
		\subsection{Penalization of the integrated likelihood by $\mathbb{P}(\boldsymbol{S})$} \label{compstruct}
		
	$\mathcal{S}_d$ being combinatorial, $|\mathcal{S}_d|$ is huge. It has two cumulated consequences: First, the {\it exact} probability $\mathbb{P}(\boldsymbol{S}|\boldsymbol{X})$ may be of the same order of magnitude for a large number of candidates  $\boldsymbol{S}$, including the best one; Second, the {\sc bic} {\it approximations} of this quantity may introduce additional confusion to wisely distinguish between model probabilities because the number of compared models is not taken into account \cite{massart2007concentration}. In order to limit this problem, we propose to introduce some information in $\mathbb{P}(\boldsymbol{S})$ promoting simple models through the following {\it hierarchical} uniform distribution denoted by $\mathbb{P}_H(\boldsymbol{S})$:
\begin{eqnarray}
\mathbb{P}_H(\boldsymbol{S}) & = & \mathbb{P}_H(\boldsymbol{J}_r,\boldsymbol{J}_p) \\
 & = & \mathbb{P}_H(\boldsymbol{J}_r,\boldsymbol{J}_p,d_r,\boldsymbol{d}_p) \\
 & = & \mathbb{P}_U(\boldsymbol{J}_p|\boldsymbol{d}_p,\boldsymbol{J}_r,d_r) \times \mathbb{P}_U(\boldsymbol{d}_p|\boldsymbol{J}_r,d_r) \times \mathbb{P}_U(\boldsymbol{J}_r|d_r)\times \mathbb{P}_U(d_r) \\
 & = & \left[\prod_{j=1}^{d_r} \left(\begin{array}{c} d-d_r \\ d_p^j \end{array}\right) \right]^{-1} \times \left[d-d_r\right]^{-d_r} \times \left[\left(\begin{array}{c} d \\ d_r \end{array} \right)\right]^{-1} \times [d+1]^{-1},
\end{eqnarray}
where $\left(\begin{array}{c} a \\ b \end{array}\right)$ means the number of b-element subsets of an a-element set and where all probabilities $\mathbb{P}_U(\cdot)$ denote uniform distribution on the related space at hand. $\mathbb{P}_H(\boldsymbol{S})$ gives decreasing probabilities to more complex models, provided that the following new hypothesis is verified:

\begin{hyp}\label{H5}
We set $d_r<d/2$ and also $d_p^j<d/2$ ($j=1,\ldots,d_r$).
\end{hyp}

These two thresholds are sufficiently large to be wholly realistic.

\paragraph{Final approximation of $\mathbb{P}(\boldsymbol{S}|\boldsymbol{X})$}
Merging the previous three expressions, it leads to the following two {\it global} {\sc bic} criteria, to be minimized, denoted by {\sc bic}$_U$ or {\sc bic}$_H$, depending on the choice of $\mathbb{P}_U(\boldsymbol{S})$ or $\mathbb{P}_H(\boldsymbol{S})$ respectively:
\begin{eqnarray}
\mbox{{\sc bic}}_U(\boldsymbol{S}) & = & \mbox{{\sc bic}}_r(\boldsymbol{S}) + \mbox{{\sc bic}}_f(\boldsymbol{S}) - 2\ln \mathbb{P}_U(\boldsymbol{S}) \\
\mbox{{\sc bic}}_H(\boldsymbol{S}) & = & \mbox{{\sc bic}}_r(\boldsymbol{S}) + \mbox{{\sc bic}}_f(\boldsymbol{S}) - 2\ln \mathbb{P}_H(\boldsymbol{S}).
\end{eqnarray}
In the following, we will denote by $\mbox{{\sc bic}}_*$ any of both $\mbox{{\sc bic}}_U$ and $\mbox{{\sc bic}}_H$. Numerical results in Section~\ref{compZ} will allow to compare behaviour of both criteria.

\paragraph{Remarks}
\begin{itemize}
\item As any {\sc bic} criterion, the {\sc bic}$_U$ and {\sc bic}$_H$ criteria are consistent \cite{BIChuard}.
\item Even if it favors more parsimonious models, $\mathbb{P}_H(\boldsymbol{S})$ can be also viewed as a poor informative prior on $\boldsymbol{S}$ since it is a combination of non informative priors.
\end{itemize}



%	 
%	
		\subsection{Some indicators for proximity}
		The first criterion is $\mbox{{\sc bic}}_*$ which is maximized in the MCMC. But it's value don't have any intrinsic meaning. To show how far the found structure is from the true one in terms of $\boldsymbol{S}$ we define some indicators to compare the true model $\boldsymbol{S}$ and the found one $\hat{\boldsymbol{S}}$.
			Global indicators :
			\begin{itemize}
				\item $T_r=|J_r \cap \hat{J}_r|$ (``True Response''): it corresponds to the number of estimate response covariates in $\hat{\boldsymbol{S}}$ which are {\it truly} response covariates in the true model $\boldsymbol{S}$.
				\item $W_r=|\hat{J}_r|-T_r$ (``Wrong Response''): it corresponds to the number of estimate response covariates in $\hat{\boldsymbol{S}}$ which are are {\it wrongfully} response covariates in the true model $\boldsymbol{S}$.
				\item $M_r=d_r-T_r$ (``Missing Response'') : the number of true response variables not found.
				\item $\Delta d_r=d_r-\hat{d_r}$ : the gap between the number of sub-regression in both model. The sign defines if $\hat{\boldsymbol{S}}$ is too complex or too simple compared to the true model.
				\item $\Delta compl=\sum_{j=1}^{d_r}d_p^j$ : the difference in complexity between both model.
			\end{itemize}

			
			
	\section{The walk}
		\subsection{Determinist neighbourhood}
	
We define a {\it global} neighbourhood space $\mathcal{V}(\boldsymbol{S})$ of $\boldsymbol{S}$ composed by the following four {\it specific} neighbourhood spaces $\mathcal{V}(\boldsymbol{S})=\mathcal{V}_{r+}(\boldsymbol{S})\cup\mathcal{V}_{r-}(\boldsymbol{S})\cup\mathcal{V}_{p+}(\boldsymbol{S})\cup\mathcal{V}_{p-}(\boldsymbol{S})$ described below:
	%\begin{itemize}
	\paragraph{Adding a sub-regression}: a new sub-regression with only one predictor covariate is added to $\boldsymbol{S}$
\begin{equation}
\mathcal{V}_{r+}(\boldsymbol{S}) = \left\{\tilde{\boldsymbol{S}}: \tilde{\boldsymbol{S}}\in\mathcal{S}_d, (\tilde{\boldsymbol{J}_r},\tilde{\boldsymbol{J}_p})^{1,\ldots,d_r}=(\boldsymbol{J}_r,\boldsymbol{J}_p), \tilde{J}_r^{d_r+1}\in J_f, \tilde{J}_p^{d_r+1}=\{j\}, j\in J_f \right\}.
\end{equation}	
		\paragraph{Removing a sub-regression}: a sub-regression  is removed from $\boldsymbol{S}$
\begin{equation}
\mathcal{V}_{r-}(\boldsymbol{S}) = \left\{\tilde{\boldsymbol{S}}: \tilde{\boldsymbol{S}}\in\mathcal{S}_d, (\tilde{\boldsymbol{J}_r},\tilde{\boldsymbol{J}_p})=(\boldsymbol{J}_r,\boldsymbol{J}_p)^{\{1,\ldots,d_r\}\backslash j}, j\in\{1,\ldots,d_r\} \right\}.
\end{equation}	
	\paragraph{Adding a predictor covariate}: a predictor covariate is added to one sub-regression of $\boldsymbol{S}$
\begin{equation}
\mathcal{V}_{p+}(\boldsymbol{S}) = \left\{\tilde{\boldsymbol{S}}: \tilde{\boldsymbol{S}}\in\mathcal{S}_d, \tilde{\boldsymbol{J}_r}=\boldsymbol{J}_r, \tilde{J_p}^{\{1,\dots,d_r \}\setminus \{j\}}=J_p^{\{1,\dots,d_r \}\setminus \{j\}} ,\tilde{J}_p^{j}=J_p^{j} \cup \{h\}, j\in \{1,\ldots,d_r\}, h\in J_f \setminus J_p^j\right\}.
\end{equation}
	\paragraph{Removing a predictor covariate}: a predictor covariate is removed from one sub-regression of $\boldsymbol{S}$
\begin{equation}
\mathcal{V}_{p-}(\boldsymbol{S}) = \left\{\tilde{\boldsymbol{S}}: \tilde{\boldsymbol{S}}\in\mathcal{S}_d, \tilde{\boldsymbol{J}_r}=\boldsymbol{J}_r,\tilde{J_p}^{\{1,\dots,d_r \}\setminus \{j\}}=J_p^{\{1,\dots,d_r \}\setminus \{j\}} , \tilde{J}_p^{j}=J_p^{j} \backslash \{h\}, j\in \{1,\ldots,d_r\}, h\in  J_p^j \right\}.
\end{equation}	
	%\end{itemize}	
	We just want to find the best model and this neighbourhood is determinist so it does not need to contain the current structure $\boldsymbol{S}$.
	
	
	
	\subsection{Transition probabilities}
		Once we have a neighbourhood, we have to choose a candidate for the next step.
The walk follows a time-homogeneous Markov Chain whose transition matrix $\mathcal{P}$ has $|\mathcal{S}_d|$ rows and columns (extremely wide  and sparse matrix for $d>10$ so we just compute the probabilities when we need them).
	At each step the Markov chain moves with probability:
	\begin{eqnarray}
			\forall \boldsymbol{S} \in \mathcal{S}_d, \forall \boldsymbol{S}^+\in \mathcal{V}(\boldsymbol{S}) : \mathcal{P}_{\boldsymbol{S},\boldsymbol{S}^+}=\mathbb{P}(\boldsymbol{S}^+|\mathcal{V}(\boldsymbol{S}))&=&\frac{\exp{(-\mbox{{\sc bic}}_*(\boldsymbol{S}^+))}}{\sum_{\tilde{\boldsymbol{S}}\in \mathcal{V}(\boldsymbol{S})}\exp{(-\mbox{{\sc bic}}_*(\tilde{\boldsymbol{S}}))}} 
	\end{eqnarray}
	and $\mathcal{S}_d$ is a finite state space.%la relaxation rend P non symétrique mais ne remets  pas en cause l'homogénéité	
	 
Because the walk follows a regular and thus ergodic markov chain with a finite state space, it has exactly one stationary distribution \cite{grinstead1997introduction}. %: $\pi$ and every rows of $\operatorname{lim}_{k\rightarrow \infty}\mathcal{P}^k=W$ equals $\pi$.
%	
%	
%	With $\forall S \in  \mathcal{S}$ :
%	\begin{eqnarray}		
%		0 \leq &\pi (S)& \leq 1 \nonumber \\
%		\sum_{S \in \mathcal{S}}\pi(S) &=&1 \nonumber \\
%		\pi (S) &=&\sum_{\tilde{S}\in \mathcal{S}} \pi(\tilde{S})\mathcal{P}(\tilde{S},S) \\%définition de la lois stationnaire
%	\end{eqnarray}
%	
But the walk can also be seen as a Gibbs sampler \cite{casella1992explaining} that alternate draws of $\boldsymbol{S}$ and  $\mathcal{V}(\boldsymbol{S})$ with stationary distribution $\pi \propto \operatorname{exp}(-\mbox{{\sc bic}}_*)$ on the space $\mathcal{S}_d$.
	
The output of the algorithm will be the best structure in terms of $P(\boldsymbol{S}|\boldsymbol{X})$ which weights each candidate. Practically speaking, {\tt CorReg} returns the best structure seen during the walk (even if the corresponding candidate has never been chosen). The package also give the local structure when the walk stops so user can relaunch the algorithm from the same point if he wants to go further.
The main criterion to stop the walk is a maximum number of iterations but {\tt CorReg} can also stop the walk after a given number of step on the best model found.	
	
	\subsection{Stochastic neighbourhood}
	We see that $|\mathcal{V}(\boldsymbol{S})|$ can be large so the best transition probability may not be far enough from the other to be chosen with a sufficient probability. A classical way to avoid such a problem is to reduce the neighbourhood to a random subset and to allow stationarity (to keep the walk's properties). But then we face a problem: how to choose candidates in a determinist neighbourhood made of several sub-neighbourhood? We would have to index them (time consuming) and then to choose random indexes or to choose a number of candidates to choose from each neighbourhood. One idea would be to choose only one candidate to compare to the current structure, allowing to choose first a sub-neighbourhood and then a candidate. But we observe that each candidate can be described by the way it differs from $\boldsymbol{S}$. Then we redefine a neighbourhood based on the modification of the adjacency matrix $\boldsymbol{G}$ associated to $\boldsymbol{S}$.\\
	
	For each step $(q)$, starting from $\boldsymbol{S} \in \mathcal{S}_d$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}(\boldsymbol{S})&=& \{\boldsymbol{S} \}\cup \{ \boldsymbol{S}^{(i,j)} \in \mathcal{S}_d|(i,j) \in \mathcal{A}_{(q)}\} 
	\end{eqnarray}	
	where $\mathcal{A}_{(q)}$ is a set of couples $(i,j) \in \{1,\dots,d \}^2$ with $i\neq j$ drawn at the step $(q)$ according to a strategy defined below and corresponding to the directed edge of the graph to modify (add or remove).
	And we have for  $\tilde{\boldsymbol{S}}=\boldsymbol{S}^{(i,j)}$:
	\begin{eqnarray}
		\forall (k,l)\neq (i,j), \quad	\tilde{\boldsymbol{G}}_{k,l}&=&\boldsymbol{G}_{k,l} \\
		\tilde{\boldsymbol{G}}_{i,j}&=&1-\boldsymbol{G}_{i,j} 
	\end{eqnarray}
	where $\tilde{\boldsymbol{G}}$ is the adjacency matrix associated to $\tilde{\boldsymbol{S}}$. Any strategy can be chosen for $\mathcal{A}_{(q)}$, from uniform distribution to specific heuristics.\\
	
The main advantage of such a neighbourhood is that increasing and decreasing complexities are tested at each step without arbitrary ratio. If we just look at the sub-regression system, we have to choose for each sub-regression if we add, remove or keep covariates and we also have to choose if we had or delete some sub-regressions. Adjacency matrix makes the neighbourhood extremely natural with just the modification of a value in a binary matrix. \\
Here the MCMC is not used for sampling or density estimation. We just want to find the structure with the best value of $\mbox{{\sc bic}}_*$ so it is not an evidence to allow or not stationarity. Our package {\tt CorReg} give the user the choice with stationarity,  included in the neighbourhood by default.

		\subsubsection{Strategy to draw $\mathcal{A}_{(q)}$}
			Many strategies can be imagined. 
		The only constraint on $\mathcal{A}_{(q)}$ is that $\forall (i,j) \in \mathcal{A}_{(q)}, i\neq j$.				
		We propose, for step $(q)$ to draw $j$ from $\mathcal{U}(\{1,\dots,d\})$ and then 
		\begin{equation}
			\mathcal{A}_{(q)}|j=\{ (i,j) \in  \{1,\dots,d\}^2 :i \neq j \}.
		\end{equation}
			Such a strategy can be interpreted as the uniform choice of a sub-regression to modify followed by the proposal of each possible unary change.
			Our package {\tt CorReg} let the user choose many other strategies for $\mathcal{A}_{(q)}$ like a fixed number of random couples $(i,j)$ , or the union of the $j^{th}$ line and column of $\boldsymbol{G}$. For large values of $d$, the number of candidate at each step can become critical. Each candidate requires to re-estimate the $\boldsymbol{\alpha}^j$'s for each modified sub-regression and it requires matricial inversion (estimation by \textsc{ols}) so computational cost can be high if $n$ is also large.
		\subsection{Active relaxation of constraints}
		In practice, for some of the $(i,j) \in \mathcal{A}_{(q)}$, we have $\boldsymbol{S}^{(i,j)}\notin \mathcal{S}_d$ because of the uncrossing rule. Such candidates are basically rejected so the number of candidates is not constant at each step. Moreover, complex structures reduce the size of the potential neighbourhood because of this uncrossing rule. Last but not least, even if the walk is regular, local extrema may significantly slow down the research of the optimal structure.
		Thus we propose a constraints relaxation method by a new definition of $\tilde{\boldsymbol{S}}=\boldsymbol{S}^{(i,j)}$ relying on a new definition of $\tilde{\boldsymbol{G}}$:
		\paragraph{Modification of the selected directed edge $(i,j)$ on the graph:} (Figure \ref{ident2})
	\begin{eqnarray}
		\tilde{\boldsymbol{G}}_{i,j}&=&1-\boldsymbol{G}_{i,j} \textrm{ as usual and}\\
		\forall k \neq i, l\neq j, \quad	\tilde{\boldsymbol{G}}_{k,l}&=&\boldsymbol{G}_{k,l}
\end{eqnarray}	
\paragraph{Column-wise relaxation :}		
newly predictive covariate cannot be regressed anymore (Figure \ref{ident3}):
		\begin{eqnarray}
		\forall k \in \{1,\dots,d\}\setminus \{i\}, \tilde{\boldsymbol{G}}_{k,j}&=&\boldsymbol{G}_{i,j}\boldsymbol{G}_{k,j}  \\
				\end{eqnarray}
\paragraph{Row-wise relaxation:} newly regressed covariate cannot be predictive anymore (Figure \ref{ident4}):
		\begin{eqnarray}
		\forall l \in \{1,\dots,d\}\setminus \{j\}, \tilde{\boldsymbol{G}}_{i,l}&=&\boldsymbol{G}_{i,j}\boldsymbol{G}_{i,l} \textrm{ (row-wise relaxation)}
	\end{eqnarray} \\
	
	It can be seen as forcing the modification by removing what would have made the structure not feasible. So in one step we can test a model that remove completely a sub-regression, remove the explicative role of a covariate in all sub-regressions (that was not possible with the determinist neighbourhood) and create a new pairwise sub-regression. It drastically increases the scope of the neighbourhood (Figure \ref{ident6}) and guarantee to always have the same number of candidates during the MCMC. It can be compared to simulated annealing that sometimes proposes exotic candidates to avoid local extrema, but here without any temperature to set. Here again, the neighbourhood remains natural, without arbitrary parameters to tune. \\
	
	Another advantage of the relaxation method is that it reduces complexity very quickly without having to deconstruct a sub-regression (Figure \ref{comparecomplrelax}), so it helps to have simpler models in a small amount of time (asymptotical results are the same because the chain is regular thus ergodic).\\
			
Numerical results (Section 4) illustrates the efficiency of the walk when the true model contains structures with various strength (section \ref{compZ}) and an example with a non-linear structure (Figure \ref{resnonlin}). \\

We give an example to better illustrate how it works in Figures \ref{ident1} to \ref{ident6}:
\begin{figure}
	\begin{minipage}[l]{.45\linewidth}
\includegraphics[width=200px]{figures/identifiable/identifiabilite001.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\boldsymbol{G}=\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & 0 \\ 
		0 & 0 & 0 & 1 & 1 \\ 
		0 & 0 & 0 & 0 & 1 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & 0 & 0 & 0 & 0
		\end{array} \right)$
	\end{minipage}
	\caption{We start from a structure $\boldsymbol{S}=((4,5),(\{1,2\},\{2,3\}))$ and its associated matrix $G$}\label{ident1}
\end{figure}
		
\begin{figure}
	\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=200px]{figures/identifiable/identifiabilite002.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & 0 \\ 
		0 & 0 & 0 & 1 & 1 \\ 
		0 & 0 & 0 & 0 & 1 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & \textcolor{red}{\textbf{1}}& 0 & 0 & 0
		\end{array} \right)$
	\end{minipage}
	\caption{We want to define the candidate $\tilde{\boldsymbol{S}}=\boldsymbol{S}^{(5,2)}$ and its associated matrix but the structure obtained would not be feasible (breaking the uncrossing rule). }\label{ident2}
\end{figure}		
\begin{figure}
	\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=200px]{figures/identifiable/identifiabilite003.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & \textcolor{green}{\textbf{0}} \\ 
		0 & 0 & 0 & 1 & \textcolor{green}{\textbf{0}} \\ 
		0 & 0 & 0 & 0 & \textcolor{green}{\textbf{0}} \\ 
		0 & 0 & 0 & 0 & \textcolor{green}{\textbf{0}} \\ 
		0 & \textcolor{red}{\textbf{1}}& 0 & 0 & \textcolor{green}{\textbf{0}}
		\end{array} \right)$
	\end{minipage}
	\caption{Column-wise relaxation: newly predictive covariate cannot be regressed anymore. }\label{ident3}
\end{figure}		
\begin{figure}
	\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=200px]{figures/identifiable/identifiabilite004.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\tilde{\boldsymbol{G}}=\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & 0 \\ 
		\textcolor{green}{\textbf{0}} & \textcolor{green}{\textbf{0}} & \textcolor{green}{\textbf{0}} & \textcolor{green}{\textbf{0}} & \textcolor{green}{\textbf{0}} \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & \textcolor{red}{\textbf{1}}& 0 & 0 & 0
		\end{array} \right)$
	\end{minipage}
	\caption{Row-wise relaxation: newly predictive covariate cannot be regressed anymore. }\label{ident4}
\end{figure}	
\begin{figure}
	\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=200px]{figures/identifiable/identifiabilite005.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\tilde{\boldsymbol{G}}=\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & 0 \\ 
		0 &0 & 0 & 0 & 0 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & 1& 0 & 0 & 0
		\end{array} \right)$
	\end{minipage}
	\caption{We get a feasible candidate $\tilde{\boldsymbol{S}}=((2,4),(\{5\},\{1\}))$ that does differ from $\boldsymbol{S}$ in many points. }\label{ident5}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=400px]{figures/identifiable/identifiabilite006.png} 
	\caption{All this modifications are made in only one step in the MCMC, meaning an increased scope for the neighbourhoods.}\label{ident6}
\end{figure}

\FloatBarrier				
	\section{Initialization}
		\subsection{Correlation-based initialization}
		 If we have some knowledge about some sub-regressions (physical models for example) we can add them in the found and/or initial structure. So the model is really expert-friendly.
The initial structure can be based on a first warming algorithm taking the correlations into account. Coefficients are randomly set to 1 into $\boldsymbol{G}$, according to a Bernoulli draw weighted by the absolute value of the correlations and with respect to the uncrossing constraint. Uncrossing constraint will not allow some strong correlation to be taken into account according to the ordering of the Bernoulli drawing so we can draw with a random order or by ordering by descending correlations.\\
		
		We note then that the $\mbox{{\sc bic}}_*$ associated to initial model is often worse than the $\mbox{{\sc bic}}_*$ of the void structure, so we compare several chains in Figures \ref{pourcini} and \ref{Bicini}.			
				We see that correlation-based initialization quickly beat the void structure. This can be explained by local extrema. Correlations-based initialization gives structures with a smaller "structural distance" to the true model and then the chains are less subject to local extrema.

	\begin{center}
		\begin{figure}[h!]
		\centering
		\includegraphics[width=350px]{figures/pourc_meilleur_marelaxini.png} 
		\caption{Amount of time each method is better for the 100 first steps of the MCMC. } \label{pourcini}
		\end{figure}		
	\end{center}
	
	\begin{center}
		\begin{figure}[h!]
		\centering
		\includegraphics[width=350px]{figures/typeinit.png} %mars 2013 analyse plan ini marelax
		\caption{Evolution of the $\mbox{{\sc bic}}$ (criterion to minimize in the MCMC) for each method.}\label{Bicini}
		\end{figure}
	\end{center}
	\FloatBarrier
		\subsection{Multiple intialization}	
		Local extrema are a known issue for most of optimization methods, and one would rather test multiple short chains than lose time in initialisation or long chains \cite{gilks1996markov}. 
		We also compare the results obtained with several number of chains. Figure \ref{nbini} shows the evolution of the $\mbox{{\sc bic}}$ of the best chain with a number of chains varying from 1 to 10, so the model with 10 chains contain the others and is at least as good as they are. We see that multiple initialization is efficient but the gain seems to be logarithmic in the number of tries so it is recommended to use multiple chains but not too much (time consuming). Important remark: multiple chains can be computed in parallel so it is not really time consuming.		
			
	\begin{center}
	\begin{figure}[h!]
	\centering
		\includegraphics[width=400px]{figures/courbes_BICmoyen_marelax_nbinibis.png} 
		\caption{Comparison of distinct number of correlation-based initialisations for the MCMC.}\label{nbini}
	\end{figure}
	\end{center}
		In the followings, the chain was launched with twenty initialisations each time, based on the correlation matrix.

	\section{Pruning}
		If the complexity of $\boldsymbol{S}$ is too high, pruning methods can be used. We note that, for each of the following pruning methods, the final complexity may stay the same (for example if the MCMC had a limited time to find a good model).
		\subsubsection{Variable selection}
			We can use variable selection methods like the \textsc{lasso} on $\boldsymbol{X}^{J_p^j}$ to estimate the coefficients $\boldsymbol{\alpha}_j$ and obtain some supplementary zeros. Working on $\boldsymbol{X}^{J_p^j}$ protects the \textsc{lasso} against dimension and correlations issues.
		\subsubsection{$R^2$ thresholding }
			We can also define a minimal value for the $R^2$ of the sub-regression to maintain them in the final structure. But this minimal value would be totally arbitrary and we know that it is frequent to use linear regression with real datasets that only show a $R^2$ between $0.1$ and $0.2$. It is particularly true in social sciences.
		\subsubsection{Test of hypotheses}
			Another pruning method would be to delete sub-regression that offer a F-statistic under a minimal value.
		\subsubsection{Additional cleaning steps}
			Because the walk is not exhaustive in practice (does not run enough steps), it does make sense to let the walk continue a few steps with neighbourhood containing only suppressions in the structure. Every sub-graph of a bipartite graph is bipartite thus every sub-graph can be reached. It is just an heuristic change in the strategy with:
			\begin{equation}
				\mathcal{A}_{(q)}=\{(i,J_r^j), i \in J_f, j \in \{1,\dots,d_r \}: i \in J_p^j \}.
			\end{equation}
			It is not based on any arbitrary parameter and change the result only if it finds a better structure in terms of the criterion $\mbox{{\sc bic}}_*$ used in the walk.
			For these reasons, it is our recommended pruning method. The package {\tt CorReg} allows to use this method automatically after the MCMC with the parameter {\tt clean=TRUE }.
			
	\section{Graphical { \sc lasso}}\label{sectionGlasso}
		Graphical \textsc{lasso} \cite{friedman2008sparse} \cite{witten2011new} \cite{tibshiranilasso} \cite{friedman2010applications} is set to give undirectionnal (thus symmetric) graphs by selection in the precision matrix (the inverse of the variance-covariance matrix). It does make sense for exponential family because in these cases, zeros in the precision matrix $\boldsymbol{\Sigma}^{-1}$ can be interpreted in terms of conditional independence between covariates \cite{dempster1972covariance}. But we have supposed Gaussian mixture on $\boldsymbol{X}$ and we search an oriented graph.\\
		
	However, we can still use it for initialization, for example by a Hadamard product with $\boldsymbol{G}^{(0)}$ the adjacency matrix of the initial structure. We can also try to give the graph a bipartite orientation. We first have to obtain a bipartite graph, that mean to have no even cycles. A particular case would be the minimum spanning tree \cite{graham1985history,moret1991empirical,gower1969minimum} because trees have no cycles. But it is time consuming (especially for an initialisation method) and has no theoretical properties relied to our problematic of minimizing $\mbox{{\sc bic}}_*$, so the idea was left behind after some tries.		
	
\section{CorReg}	
	The {\tt CorReg} package is now on CRAN and provides many parameters for the walk. If wanted it can return some curves associated to the walk to have an idea of what happens with distinct strategies. 		
	
We define the complexity of a structure $\boldsymbol{S}$ as the number of elements in the adjacency matrix, that is the number of links between covariates and is obtained by:
\begin{equation}
	\textrm{Complexity}(\boldsymbol{S})=\sum_{j=1}^{d_r}d_p^j.
\end{equation}	
	
		We compare some walks with each time the same dataset and the same seed for the random generator. We have $d=100$ and $n=50$.
		
		For Figures \ref{comparecomplrelax} and \ref{compareBICrelax} we start from an arbitrary structure with a complexity of $62$. We see that relaxation helps to delete these false sub-regressions and avoid to be stuck in it, improving the $\mbox{{\sc bic}}$ much faster. We also observe that final complexities are comparable. Here the MCMC was launched only once (with the totally arbitrary initial structure based on nothing), the true structure had a complexity of $120$.
\begin{center}
	\begin{figure}[h!]
	\centering
\includegraphics[width=400px]{figures/complexitycompareMCMC.png} 		
\caption{Comparison of complexity evolution with or without constraint relaxation.}\label{comparecomplrelax}
	\end{figure}
	\end{center}
			
\begin{center}
	\begin{figure}[h!]
	\centering
\includegraphics[width=400px]{figures/BICcompareMCMCrelax.png} 		
\caption{Comparison of $\mbox{{\sc bic}}$ evolution with or without constraint relaxation.}\label{compareBICrelax}
	\end{figure}
	\end{center}
			
		
	\chapter{Numerical results on simulated datasets} \label{sectionsimul}
\paragraph{Abstract:} Here are the numerical results obtained for an unknown structure $\boldsymbol{S}$ on simulated datasets. It illustrates efficiency of the MCMC algorithm used to find a relevant structure between the covariates. We start with no more information than those from real cases.
		 

	\section{Simulated datasets}	
	Now we have defined the model and the way to obtain it, we can have a look on some numerical results to see if {\tt CorReg} 	keeps its promises.
	The package has been tested on the simulated datasets from section \ref{thedatasets}.
Section \ref{compZ} shows the results obtained in terms of $\hat{\boldsymbol{S}}$. Section \ref{compY} shows the results obtained using only {\tt CorReg}, or {\tt CorReg} combined with other methods. The graph in section \ref{compY} give both mean, first and third quartiles of the chosen indicator. The \textsc{mse} on $\hat{\boldsymbol{\beta}}$ and $\hat{\boldsymbol{Y}}$ were computed on a validation sample of $1 000$ individuals. Several pattern for $\boldsymbol{Y}$ were tested to evaluate the impact of irrelevant covariates.\\

	We used the package { \tt Rmixmod} from CRAN \cite{packageRmixmod} to estimate the densities of each covariate. For each configuration, the MCMC walk was launched on $10$ initial structures with a maximum of 1 000 steps each time.
	When $n<d$, a frequently used method is the Moore-Penrose generalized inverse \cite{katsikis2008fast}, thus \textsc{ols} can obtain some results even with $n<d$. %(see tables \ref{YXlinOLS} and \ref{YX2linOLS} ).
	We compare different methods with and without {\tt CorReg} as a pretreatment. All the results are provided by the {\tt CorReg} package. Associated figures will display both mean and inter-quartile intervals.
	
		\section{Results on $\hat{\boldsymbol{S}}$}	\label{compZ}

Figure \ref{compZBIC} illustrates the impact of large samples. For $n>>d$ the MCMC found most of the truly redundant covariates and only few wrong redundant covariates. We also observe that strong correlations ($R^2\geq 0.7$) get more wrong sub-regressions for a same total number of sub-regressions. It comes from induced correlations. If two covariates are explained by the same others, they may have a strong induced pairwise correlation and if the walk tries to combine them in a single sub-regression we can have a local extremum. The walk is ergodic but in a finite number of steps it can keep such a wrong sub-regression, that's why we launch the walk several times with distinct initial structures. Such a wrong sub-regressions is not totally wrong in that it describes real correlations. So interpretation is not compromise and neither is the predictive efficiency as shown in section \ref{compY}. \\
For smaller values of $n$ we observe that the number of true sub-regressions found increases with their strength (growing $R^2$).\\
When comparing $\mbox{{\sc bic}}_U$ to $\mbox{{\sc bic}}_H$ it becomes evident that $\mbox{{\sc bic}}_H$ is less confident to keep sub-regressions (it is what it was made for). Weak sub-regressions are kept only if the sample is large enough to be confident and when the $R^2$ rises, the number of kept true sub-regressions grows quickly whereas wrong sub-regressions remain exceptional. Induced pairwise correlations give weaker sub-regressions so the walk is less attracted by them. We can then conclude that $\mbox{{\sc bic}}_H$ does achieve its main purpose that was to reduce the complexity of the structure by keeping only strong sub-regressions. In these simulated datasets, the $R^2$ were equal for each sub-regression. We can see several reasons to explain why the sub-regression are not all kept or all missing. 
\begin{itemize}
	\item The walk has only walked a finite number of steps so only a subset of all the feasible structures has been tested.
	\item Some true sub-regressions are polluted by over-fitting and the non-crossing rule can then make other true sub-regressions not compatible (the walk has to clean the previous sub-regression first).
	\item $\mbox{{\sc bic}}$ relies on the likelihood and if marginal laws are well-estimated by Mixmod, the gap between the marginal and dependent likelihood might be small and thus the walk can be slowed whereas we use a finite number of steps.
\end{itemize}

\begin{figure}[h!]
	\subfigure[With classical $\mbox{{\sc bic}}_U$ criterion]{
			\includegraphics[height=180px,width=245px]{figures/explvraiS/BIC.png} 
	} \quad
   	\subfigure[With our $\mbox{{\sc bic}}_H$ criterion]{
			\includegraphics[height=180px,width=240px]{figures/explvraiS/BICplus.png} 
	}
	\caption{Quality of the subregressions found by the MCMC. True left (plain blue) and Wrong left (dotted red) for n varying in $(30,50,100,400)$, the thicker the greater n.}\label{compZBIC}
\end{figure}


\subsection{Comparison with {\tt Selvarclust}}\label{compMaugis}
		Maugis provides results\footnote{\url{http://www.math.univ-toulouse.fr/~maugis/SelvarClustHomepage.html}} of {\tt SelvarClust} on a dataset {\it Data2.txt} containing 2 000 data points from a mixture of four Gaussian distributions $\mathcal{N}(\mu_k,\boldsymbol{I}_2)$ with $\mu_1=(-2,-2),\mu_2=(-3,-3),\mu_4=(2,2)$ and with a proportion vector $\boldsymbol{\pi}=(0.3,0.2,0.3,0.2)$. Variables $\boldsymbol{X}^1,\boldsymbol{X}^2$ corresponds to the coordinates. Eight irrelevant variables (for clustering) are appended, simulated according to $\forall 1\leq i \leq 2\; 000$:
		\begin{displaymath}
		 \boldsymbol{X}_i^{\{3,\dots , 6 \}}|\boldsymbol{X}_i^{\{1,2\}};\boldsymbol{\alpha}^*,\boldsymbol{\Omega} \sim \mathcal{N}(\boldsymbol{X}_i^{\{1,2\}}\boldsymbol{\alpha^*},\boldsymbol{\Omega})
		 \textrm{ with } \boldsymbol{\alpha}^*= \left( \begin{array}{cccc}
		0.5 & 0 & 2 & 0 \\ 
		0 & 1 & 0 & 3
		\end{array} \right) \textrm{ and } \boldsymbol{\Omega}=\operatorname{diag}(1,1,0.5,0.5)
		\end{displaymath}
		\begin{displaymath}
		 \boldsymbol{X}_i^{\{7,\dots , 10 \}}\sim \mathcal{N}(0,\boldsymbol{I}_4).
		\end{displaymath}

We compare results obtained on	$\hat{\boldsymbol{\alpha}}^*$ by both {\tt CorReg} and {\tt Selvarclust}.		
Both {\tt SelvarClust} and {\tt CorReg} add an intercept (default parameter) that is like using a constant covariate $\boldsymbol{X}^0=\boldsymbol{1}$ and then adding a first row to $\boldsymbol{\alpha}^*$ containing the intercept of the sub-regressions. This intercept is not part of the selection procedure so it will never have zero value in practice. Hence we compare the second and third lines of the matrices (an horizontal line is drawn to distinguish the intercept from the rest of the matrix).\\

{\tt SelvarClust} finds $\hat{\boldsymbol{\alpha}}^*_{Selvarclust}=$
\begin{displaymath}
	\left (\begin{array}{cccccccc}
	\textcolor{gray}{0.006052} &\textcolor{gray}{-0.025386} &\textcolor{gray}{-0.006845}&\textcolor{gray}{ -0.015952} &\textcolor{gray}{0.003420}& \textcolor{gray}{0.007839} &\textcolor{gray}{-0.047422}& \textcolor{gray}{-0.005811} \\
	\hline
\textbf{0.504791} &-0.002147& \textbf{2.007127} &-0.001010 &-0.000105 &0.022403 &0.013361 &0.000083 \\
-0.006709 &\textbf{1.000927} &0.007463&\textbf{2.997941}& -0.005955 &-0.021958 &-0.010387 &0.010765
	\end{array} \right )
\end{displaymath}		
	
	{\tt CorReg} finds:
\begin{displaymath}
	\hat{\boldsymbol{\alpha}}^*_{CorReg}=\left(
	\begin{array}{cccc}
	\textcolor{gray}{0.008698209} &\textcolor{gray}{-0.02540033} &\textcolor{gray}{-0.00978779} &\textcolor{gray}{-0.01595849} \\
	\hline
 \textbf{0.504672402}  &0&           \textbf{2.00725861} &0          \\
 0&            \textbf{1.00088836}  &0&           \textbf{2.99792339} 
	\end{array} 
	\right)
\end{displaymath}		
Both software found that $\boldsymbol{X}^1,\boldsymbol{X}^2$ are not in $\boldsymbol{J}_r=(3,\dots,6)$ but only {\tt CorReg} found the true model with $\hat{\boldsymbol{J}_r}=\boldsymbol{J}_r$ and $\hat{\boldsymbol{J}_p}=\boldsymbol{J}_p=(\{ 1\},\{ 2\},\{1 \},\{ 2\})$. 
{\tt Selvarclust} finds $\hat{\boldsymbol{S}}=((3,\dots,10),(\{1,2\},\dots,\{1,2\}))$ that is it only finds the partition $(J_r,J_f)$.
Our algorithm gives better results on $\hat{\boldsymbol{\alpha}}^*$ with more parsimonious $\hat{\boldsymbol{S}}$ (that is the true $\boldsymbol{S}$), proving that a new algorithm was needed to estimate the sub-regression structure and that the proposed MCMC is efficient.\\

{\tt CorReg} gives better results because it does not suffer from correlations like stepwise does in the algorithm from Maugis and also because the goal is not the same. \\
{\tt Selvarclust} aims to find the relevant covariates for clustering ($\boldsymbol{X}^1,\boldsymbol{X}^2$) and achieves this goal. We focus on the explicit structure of regression between the covariate instead of Gaussian clustering. Distinct goals, distinct results even if the sub-regression model is quite the same.\\
Moreover {\tt Selvarclust} does not make the same hypotheses on the dataset and allows dependencies between the regressors.
We suppose independence between the regressors and between the conditional distributions so we only estimate the univariate marginal distribution and we make it only once.
	 Then, at each step {\tt Selvarclust} has to estimate the whole distribution. Our algorithm does not allow to find complex distributions with dependencies as {\tt Selvarclust} does. Both models use a sub-regression structure and propose an algorithm that find it, but the comparison stops here. So {\tt CorReg} does not really beats {\tt Selvarclust} because hypotheses and objectives are not the same,  so the result cannot really be compared. {\tt Selvarclust} is not a competitor (it could have been but it is not the case) to {\tt CorReg } but a source of inspiration for the concept of sub-regressions within the covariates and the concept of redundant and irrelevant covariates. To obtain better results was just a necessary confirmation of that.

\subsection{Computational time}
	In terms of computation time, the most expansive thing in the MCMC is the computation of \textsc{ols} to estimate $\boldsymbol{\alpha}$ for each candidate, that is mainly successive inversion of the $(\boldsymbol{X}^{J_p^j})'\boldsymbol{X}^{J_p^j}$. \\
	Figure \ref{tempsn} illustrate the evolution of the time needed to achieve 10 times (distinct initializations) 1 000 steps on the datasets described above with the $R^2$ of the sub-regressions set to $0.7$. We see that time increases with $n$ from less than 2 seconds for $n=30$ to more than 12 seconds for $n=400$.\\	
\begin{figure}[h]
\centering
	\begin{subfigure}
	\centering
		\includegraphics[width=200px]{figures/tempsn3.png} 
	\end{subfigure}
	\begin{subfigure}
	\centering
		\includegraphics[width=200px]{figures/tempsn4.png} 
	\end{subfigure}
	\caption{Evolution of the mean time for 10 times 1 000 steps of the MCMC, 39 candidates each time (constraint relaxation). $d=40$ covariates with $d_r=16$ sub-regressions}\label{tempsn}
\end{figure}
The main impact of a change of $d$ will be the expansion of $\mathcal{S}_d$ that will require more iteration/candidates to find the true structure. But the cost of each sub-regression remains the same. Figure \ref{tempsdrelax} show the evolution of the time for $n=30$ and $d_r=0.4\times d$ sub-regressions with $R^2=0.7$ each time. $40$ candidates were tested each time.
We first observe that the increase seems slower for small values of $d$ it is because of the amount of time needed for initializations and other annex steps, when $d$ rises they become less significant and it reveals the non-linear time increase. 
For $d=1\ 000$ time raises up to 26 minutes. This time could be reduced by using sparse matrices instead of full binary matrices $\boldsymbol{G}$, it would also be more efficient with memory usage. Another way to reduce this time is to compute each initializations in parallel, dividing this time by nearly 10.
 \\
\begin{figure}[h]
\centering
	\begin{subfigure}
	\centering
		\includegraphics[width=200px]{figures/tempsd6relax.png} 
	\end{subfigure}
	\begin{subfigure}
	\centering
		\includegraphics[width=200px]{figures/tempsd7relax.png} 
	\end{subfigure}
	\caption{Evolution of the mean time for 10 times 1 000 steps of the MCMC, 40 candidates each time (constraint relaxation). $d_r=0.4\times d$ sub-regressions, $n=30$.}\label{tempsdrelax}
\end{figure}
 
 Without constraint relaxation (Figure \ref{tempsdrejet}) the number of candidates evolves at each step, reducing computational cost (but also convergence speed) and we know that only one sub-regression is changed compared to the current so we only have to compute one sub-regression (some candidates requires to compute several sub-regressions with constraint relaxation) reducing again the time of each step. Hence, rejecting candidates is a good way to achieve quickly a fixed number of step, so it could be used as a warming phase before using constraint relaxation to avoid local extrema. All the results were obtained on a laptop with intel(R) core(TM) i5 CPU with 2.4GHz and 2Go of RAM running Windows XP.
\begin{figure}[h]
\centering
	\begin{subfigure}
	\centering
		\includegraphics[width=200px]{figures/tempsd6rejet.png} 
	\end{subfigure}
	\begin{subfigure}
	\centering
		\includegraphics[width=200px]{figures/tempsd7rejet.png} 
	\end{subfigure}
	\caption{Evolution of the mean time for 10 times 1 000 steps of the MCMC, 40 candidates each time (without constraint relaxation). $d_r=0.4\times d$ sub-regressions, $n=30$.}\label{tempsdrejet}
\end{figure} 
  
\clearpage
\section{Results on prediction}\label{compY}
\subsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	 \label{explsimtout}	
We try the method with a response variable depending on all covariates to compare the results with those from section \ref{thedatasets} (same $\boldsymbol{X}$ and $\boldsymbol{Y}$). ({\tt CorReg} reduces the dimension and can't give the true model if there is a structure). %The datasets used here were those from table \ref{compZvrai}. \\

 We see that {\tt CorReg} tends to give more parsimonious models and better predictions, even if the true model is not parsimonious. We logically observe that when $n$ rises, all the models get better and the correlations cease to be a problem so the complete model starts to be better ({\tt CorReg} does not allow the true model to be chosen). The main result here is that results based on $\hat{\boldsymbol{S}}$ are still good so the MCMC is efficient enough to be useful for the study of the response variable $\boldsymbol{Y}$. Results are mostly the same than when using the true structure.\\
 
Results for \textsc{ols} (Figure \ref{MSEexplOLStout} ) are similar to those from section \ref{thedatasets} excepted for small correlations because the MCMC using $\mbox{{\sc bic}}_H$ does not find the true structure for small correlations and a void structure gives a marginal model equal to the complete one.  \\
 This phenomenon is not observed with variable selection method (Figures \ref{MSEexpllartout} to \ref{MSEexplstepwisetout} ) where covariates not deleted by the structure are deleted by the variable selection. \\
 Ridge regression results (Figure \ref{MSEexplridgetout} ) is also very similar to the previous (Figure \ref{MSEexplridge} ).\\
 
 Having simpler structure is important for interpretation so we keep the choice of using $\mbox{{\sc bic}}_H$, but users of {\tt CorReg} can use classical $\mbox{{\sc bic}}_U$ with a single boolean parameter change.

 
\FloatBarrier

\newpage
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}
	%OLS	
	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/tout/MSEexplOLSY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/tout/cplexplOLS_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/tout/MSEexplOLSbeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplOLStout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%	\begin{figure}[h!]
%	\centering
%		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplOLSY_zonetout.png}
%		\caption{Comparison of the MSE on $\hat{\boldsymbol{Y}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSY_zonetout}
%	\end{figure}
%	\begin{figure}[h!]
%	\centering
%		  \includegraphics[width=350px]{figures/explhatS/tout/cplexplOLS_zonetout.png}
%		\caption{Comparison of the complexities, red=classical (complete) model, blue=marginal model}\label{cplexplOLS_zonetout}
%	\end{figure}
%	\begin{figure}[h!]
%	\centering
%		  \includegraphics[width=350px]{figures/explhatS/tout/MSEexplOLSbeta_zonetout.png}
%		\caption{Comparison of the MSE on $\hat{\boldsymbol{\beta}}$, red=classical (complete) model, blue=marginal model}\label{MSEexplOLSbeta_zonetout}
%	\end{figure}
%	\FloatBarrier
%\newpage
%LASSO
\subsubsection{{\sc LASSO} when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/tout/MSEexpllarY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/tout/cplexpllar_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/tout/MSEexpllarbeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexpllartout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

		\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/tout/MSEexplelasticnetY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/tout/cplexplelasticnet_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/tout/MSEexplelasticnetbeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnettout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

		\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/tout/MSEexplstepwiseY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/tout/cplexplstepwise_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/tout/MSEexplstepwisebeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplstepwisetout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/tout/MSEexplridgeY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/tout/cplexplridge_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/tout/MSEexplridgebeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplridgetout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
 
 
% 
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/ridge_tout_MSE_NB.png} 
%			\caption{Comparison of the MSE between OLS and CorReg+OLS}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_tout_compl_NB.png} 
%			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
%   \end{minipage}
%\end{figure}
% 
%%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/lar_tout_MSE_NB.png} 
%			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/lar_tout_compl_NB.png} 
%			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_tout_MSE_NB.png} 
%			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_tout_compl_NB.png} 
%			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_tout_MSE_NB.png} 
%			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_tout_compl_NB.png} 
%			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
%   \end{minipage}
%\end{figure}





\clearpage
\newpage
\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_{f}}$ (best case for us)}	 \label{tableMSEsimdroite}
\FloatBarrier
 We want to see what happens in real life, when some covariates are irrelevant to describe $\boldsymbol{Y}$ according to the given dataset. We could generate $\boldsymbol{Y}$ with a random subset of $\boldsymbol{X}$ but in such a case, it would be impossible to say whether results come from sparsity or from the ratio of covariates in the subset that are in $\boldsymbol{X}_{r}$. Moreover, we will study real datasets in the next chapter so the only pattern to test here are those with some irrelevant covariates and relevant covariates only in one part of the partition on $\boldsymbol{X}$. \\
 
 	We start with $\boldsymbol{Y}$ depending only on covariates in $\boldsymbol{X_{f}}$. It is the best case for us because our marginal model is then the true model (when the true structure is found) and the complete model will need variable selection to reach the truth. Here $\boldsymbol{Y}$ depends on the 24 covariates in $\boldsymbol{X}_{f}$ with an intercept.\\
 	
 	Smaller dimension makes the coefficients easier to learn and we observe that \textsc{mse} are smaller for both model with any method compared to those from section \ref{explsimtout}.\\
 	
 	\paragraph{Ordinary Least Squares:} In figure \ref{MSEexplOLSX1} we note the global improvement of the \textsc{mse} but also a specific improvement for large values of $n$ where our marginal model resists to the complete model. It is logical because the complete model tends to reduce the coefficients associated to irrelevant covariates whereas our marginal delete them. \\
 	
 	\paragraph{Variable selection:} When looking at variable selection methods  (Figures \ref{MSEexpllarX1} to \ref{MSEexplstepwiseX1})we also have this improvement so it confirm the already observed fact that variable selection method are theoretically able to find the true model but efficiency is not really great when confronted to correlated covariates. There is no surprise here after the results for $\boldsymbol{Y}$ depending on the whole dataset $\boldsymbol{X}$. \\
 	
 	\paragraph{Ridge regression:}Ridge regression (figure \ref{MSEexplridgeX1}) is finally  improved here by our pretreatment by selection, like if we had added variable selection feature to the ridge regression. It is the method that provides the best results, but only because $\boldsymbol{Y}$ depends on all covariates in $\boldsymbol{X}_{f}$. Our pretreatment is limited in terms of variable selection.
 	
 	
 

\newpage
%OLS	
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_{f}}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X1/MSEexplOLSY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X1/cplexplOLS_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X1/MSEexplOLSbeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplOLSX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}


%LASSO
\subsubsection{{\sc lasso} when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}


	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X1/MSEexpllarY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X1/cplexpllar_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X1/MSEexpllarbeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexpllarX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}

	
	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X1/MSEexplelasticnetY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X1/cplexplelasticnet_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X1/MSEexplelasticnetbeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}

	
	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X1/MSEexplstepwiseY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X1/cplexplstepwise_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X1/MSEexplstepwisebeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplstepwiseX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%ridge
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X1/MSEexplridgeY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X1/cplexplridge_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X1/MSEexplridgebeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplridgeX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}

%\begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X1_MSE_NB.png} 
%			\caption{Comparison of the MSE between OLS and CorReg+OLS}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X1_compl_NB.png} 
%			\caption{Comparison of the complexities between OLS and CorReg+OLS} 
%   \end{minipage}
%\end{figure}
% 
%%\caption{OLS and OLS combined with constrained \textsc{CorReg}. $\boldsymbol{Y}$  depends on all variables in $\boldsymbol{X}$. \textsc{CorReg} logically wins. When $n<p$ the dataset was reduced to $p=n$ automatically by a $QR$ decomposition as the lm function of R does. Without selection, all models have min$(n,p)$ non-zero coefficients.} \label{YXlinOLS}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X1_MSE_NB.png} 
%			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X1_compl_NB.png} 
%			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X1_MSE_NB.png} 
%			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X1_compl_NB.png} 
%			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X1_MSE_NB.png} 
%			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X1_compl_NB.png} 
%			\caption{Comparison of the compexities between stepwise and CorReg+stepwise} 
%   \end{minipage}
%\end{figure}

\clearpage
	
	\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$ (worst case for us)}	 \label{tableMSEsimgauche}
We now try the method with a response depending only on variables in $\boldsymbol{X}_{r}$. The datasets used here were still the same.
Depending only on $\boldsymbol{X}_r$ implies sparsity and impossibility to obtain the true model when using the true structure.  We get unbiased models but with an increase in the variance as described in equation \ref{eq:Trueexpl}. \\
	In such a case, usage of $\mbox{{\sc bic}}_H$ instead of $\mbox{{\sc bic}}_U$ is more beneficial because each additional sub-regression deletes a covariate in our marginal model and reduces the probability to find the true model. \\
	
	\paragraph{Ordinary Least Squares:}We first look at \textsc{ols} (Figure \ref{MSEexplOLSX2}) and see that we still obtain better results for small values of $n$ or strong correlations. In real studies we will never know the true model but we can be confident that if correlations are strong or if sample is small, using our marginal model can helps whatever the true model is. This is a really encouraging result. Improvement for small correlations but $n<d$ comes from dimension reduction. When you don't have enough individual it becomes better to use a small model that does not contain the true one but only covariates correlated to the relevant one instead of trying to work with all the covariates. Let's remember that \textsc{ols} confronted to $n<d$ only delete covariates to have $n=d$ (or $d+1$ when there is an intercept). QR decomposition leads to delete the last  covariates in the dataset but in our simulations, covariates in $\boldsymbol{X}_r$ are placed randomly in the dataset so deletion by QR can be seen as random deletion. The gain implied by dimension reduction remains for $n>d$ if correlations are high enough because the matrix to invert is ill-conditioned and \textsc{ols} needs a lot of individuals to reduce the variance of the estimator. Correlations really put \textsc{ols} in trouble and our marginal model seems to be a good solution. \\
	
	\paragraph{Other methods:} Variable selection methods (Figures \ref{MSEexpllarX2} to \ref{MSEexplstepwiseX2}) still are impacted by correlations but not enough to be improved by our marginal model. Neither is the ridge regression (Figure \ref{MSEexplridgeX2}). The results confirm that this is the worst case for our marginal model.\\
	
	 Real datasets will provide $\boldsymbol{Y}$ depending on a mix of covariates from both $\boldsymbol{X}_f$ and $\boldsymbol{X}_r$ so our marginal could help even with variable selection methods or ridge regression. We also recall that the structure $\boldsymbol{S}$ is useful by itself to have a better comprehension of the dataset and help the final client to be confident in statistical tools because he sees small models that are known to be true and were found automatically by the method. Thus {\tt CorReg } also has a psychological impact on a study that should not be overlooked. Once $\hat{\boldsymbol{S}}$ is found, trying the marginal model has no cost and should be tested.


\FloatBarrier

\newpage
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}


	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X2/MSEexplOLSY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X2/cplexplOLS_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X2/MSEexplOLSbeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplOLSX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%LASSO
\subsubsection{{\sc lasso} when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X2/MSEexpllarY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X2/cplexpllar_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X2/MSEexpllarbeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexpllarX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X2/MSEexplelasticnetY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X2/cplexplelasticnet_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X2/MSEexplelasticnetbeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplelasticnetX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X2/MSEexplstepwiseY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X2/cplexplstepwise_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X2/MSEexplstepwisebeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplstepwiseX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/explhatS/X2/MSEexplridgeY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/explhatS/X2/cplexplridge_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/explhatS/X2/MSEexplridgebeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEexplridgeX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}

% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/OLS_X2_MSE_NB.png} 
%			\caption{Comparison of the MSE between OLS and CorReg+OLS}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/OLS_X2_compl_NB.png} 
%			\caption{Comparison of the compexities between OLS and CorReg+OLS} 
%   \end{minipage}
%\end{figure}
%\textsc{CorReg} is still better than OLS for strong correlations and limited values of $n$. 
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/lar_X2_MSE_NB.png} 
%			\caption{Comparison of the MSE between LASSO and CorReg+LASSO}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/lar_X2_compl_NB.png} 
%			\caption{Comparison of the compexities between LASSO and CorReg+LASSO} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/elasticnet_X2_MSE_NB.png} 
%			\caption{Comparison of the MSE between elasticnet and CorReg+elasticnet}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/elasticnet_X2_compl_NB.png} 
%			\caption{Comparison of the compexities between elasticnet and CorReg+elasticnet} 
%   \end{minipage}
%\end{figure}
%
% \begin{figure}[h!]
%	\begin{minipage}[l]{.48\linewidth}
%			\includegraphics[height=180px,width=245px]{figures/res_article/stepwise_X2_MSE_NB.png} 
%			\caption{Comparison of the MSE between stepwise and CorReg+stepwise}
%	\end{minipage} \
%   \begin{minipage}[r]{.48\linewidth}
%			\includegraphics[height=180px,width=240px]{figures/res_article/stepwise_X2_compl_NB.png} 
%			\caption{Comparison of the compexities between stepwise and CorReg+stepwise}
%   \end{minipage}
%\end{figure}
\FloatBarrier
\subsection{Robustness with non-linear case}

We have generated a non-linear structure to test the robustess of the model. $\boldsymbol{X}_f$ is a set of 6 independent Gaussian mixtures defined as previously but with random signs for the components means. $\boldsymbol{X}_r|\boldsymbol{X}_f;\boldsymbol{\alpha},\boldsymbol{\sigma}^2=\boldsymbol{X}^7|\boldsymbol{X}^{\{1,2,3\}};a,\sigma_1^2=a(\boldsymbol{X}^1)^2+\boldsymbol{X}^2+\boldsymbol{X}^3+ \boldsymbol{\varepsilon}_1$. The matrix $\boldsymbol{X}$ is then scaled before doing $$\boldsymbol{Y}|\boldsymbol{X};\sigma_Y^2=\sum_{i=1}^7\boldsymbol{X}^i+\boldsymbol{\varepsilon}_Y.$$ We let $a$ vary between $0$ and $10$ to increase progressively the non-linear part of the sub-regression. Once again, simulations has been made 100 times and the \textsc{mse} were computed with 1 000 individuals validation samples.

 \begin{figure}[h!] 
	\begin{minipage}[l]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/robust_S.png} 
			\caption{Evolution of the quality of $\hat{\boldsymbol{S}}$ when the paramater $a$ increases}\label{resnonlin}
	\end{minipage} \
   \begin{minipage}[r]{.48\linewidth}
			\includegraphics[height=180px,width=240px]{figures/robustY.png} 
			\caption{\textsc{mse} on the main regression for \textsc{ols}(thick) and \textsc{lasso} (thin) used both with (plain) or without {\tt CorReg} (dotted).}\label{MSEnonlin}
   \end{minipage}
\end{figure}
Figure \ref{MSEnonlin} illustrates the advantage of using {\tt CorReg} even with non-linear structures. Figure \ref{resnonlin} shows that the MCMC have more difficulties to find a linear structure as the non-linear part of the sub-regression increases but the model is quite robust (efficient for small values of $a$).

	\FloatBarrier	

		
		
\chapter{Numerical results on real datasets} \label{sectionrealcase}
\paragraph{Abstract:} Here are the numerical results obtained from real datasets. As in the previous chapter we have to estimate the structure. But here we don't know if the true model follows our hypotheses or not. We will see if the model does make sense in real life as we supposed previously.

	\section{Quality case study} \label{sectionexfos}
This work takes place in steel industry context, with quality oriented objective : to understand and prevent quality problems on finished product, knowing the whole process. The correlations are strong here (many parameters of the whole process without any a priori and highly correlated because of physical laws, process rules, {\it etc.}). 
		
We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item $d=205$ variables from the whole process to explain it.
		\end{itemize}
We get a training set of $n=3\;000$ products described by these $205$ variables from the industrial process and also a validation sample of $847$ products.\\

The objective here is not only to predict non-quality but to understand and then to avoid it. {\sc CorReg} provides an automatic method without any {\it a priori} and can be combined with any variable selection methods. So it allows to obtain, in a small amount of time (several hours for this dataset), some indications on the source of the problem, and to use human resources efficiently. When quality crises occur, time is extremely precious so automation is a real stake. The combinatorial aspect of the sub-regression models makes it impossible to do manually.\\

%\vspace{3mm}

To illustrate that some industrial variables are naturally highly correlated, we can measure the correlation $\rho$ between some couple of variables. For instance, the width and the weight of a steel slab gives $|\rho|=0.905$, the temperature before and after some tool gives $|\rho|=0.983$, the  roughness of both faces of the product gives $|\rho|= 0.919$ and a particular mean and a particular max gives $|\rho|=0.911$. For an overview of correlations, Figure~\ref{fig:graphCorr.quality}(a) gives an histogram of $\rho$ where we can see that, however, many other variables are not so highly correlated.\\

\textsc{CorReg} estimated a structure of $d_r=76$ sub-regressions with a mean of $\bar{\boldsymbol{d}}_p=5.17$ predictors. In the resulting uncorrelated covariate set $\boldsymbol{X}_f$ the number of values $|\rho|>0.7$ is $79.33\%$ smaller than in $\boldsymbol{X}$. Indeed, Figure~\ref{fig:graphCorr.quality}(b) displays the histogram of adjusted $R^2$ value ($R^2_{adj}$) and we can see that essentially large values of $R^2_{adj}$ are present. When we have a look at a more detailed level, we can see also that \textsc{CorReg} has been able non only to retrieve the above correlations (the width and the weight of a steel slab, {\it etc.}) but also to detect more complex structures describing physical models, like the width in function of the mean flow and the mean speed, even if the true physical model is not linear since ``width = flow / (speed * thickness)'' (here thickness is constant). Non-linear regulation models used to optimize the process were also found (but are confidential). These first results are easily understandable and meet metallurgists expertise.  Sub-regressions with small values of $R^2$ are associated with non-linear model (chemical kinetics for example).
		
\begin{figure}[h!]
\begin{center}
			\includegraphics[height=150px, width=150px]{figures/correlexfoshist.png}
			\includegraphics[height=150px,width=150px]{figures/histR2exfos.png}
\end{center}
\vspace{-5mm}
			\centerline{(a) \hspace{130px} (b)}
			\caption{Quality case study: (a) Histogram of correlations $\rho$ in $\boldsymbol{X}$, (b) histogram of the adjusted $R^2_{adj}$ for the $d_r=76$ sub-regressions.}\label{fig:graphCorr.quality}
\end{figure}  			
		
\vspace{3mm}

Note that the uncorrelated variables can be very well-modeled by parsimonious Gaussian mixtures as it is illustrated by Figure~\ref{fig:graphMixmod.quality}(a). In particular, the number of components is quite moderate as seen in Figure~\ref{fig:graphMixmod.quality}(b).
		
		\begin{figure}[h!]
\begin{center}
			\includegraphics[height=150px, width=150px]{figures/res_article/gaussianmixture_exfo.png}
			\includegraphics[height=150px,width=150px]{figures/res_article/nb_comp_X_exfo.png}
\end{center}
\vspace{-5mm}
			\centerline{(a) \hspace{130px} (b)}
			\caption{Quality case study: (a) Example of a non-Gaussian real variable easily modeled by a Gaussian mixture, (b) distribution of the number of components found for each covariate.}\label{fig:graphMixmod.quality}
\end{figure}  	


%\begin{figure}[h!]
%   \begin{minipage}[t]{.30\linewidth}
%			\includegraphics[height=150px,width=150px]{figures/correlexfoshist.png} 
%			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEexfos}
%   \end{minipage}
%   \begin{minipage}[t]{.30\linewidth}
%			\includegraphics[height=150px, width=150px]{figures/res_article/gaussianmixture_exfo.png}%{figures/histR2exfos.png} $R^2_{adj}$ of the 76 sub-regressions.
%			\caption{Example of non-Gaussian real variable easily modeled by a Gaussian mixture.}
%	\end{minipage} \hfill
%	\begin{minipage}[t]{.30\linewidth}
%			\includegraphics[height=150px,width=150px]{figures/res_article/nb_comp_X_exfo.png}%{figures/mixmod.png} 
%			\caption{Distribution of the number of components found for each covariate.}\label{graphMixmod}
%	\end{minipage} \hfill
%\end{figure}   			
%
%	
%
%		
%		\begin{figure}[h!]
%		\centering
%			\includegraphics[height=150px, width=150px]{figures/histR2exfos.png} 
%			\caption{$R^2_{adj}$ of the 76 sub-regressions.}
%\end{figure} 
%		
		
\vspace{3mm}
	
	
Table~\ref{Res_exfos} displays predictive results associated to different estimation methods with and without {\sc CorReg}. We can see that {\sc CorReg} improves the results for each method tested in terms of prediction, with generally a more parsimonious regression on $\boldsymbol{Y}$. In terms of interpretation, this regression gives a better understanding of the consequences of corrective actions on the whole process. It typically permits to determine the \textit{tuning parameters} whereas variable selection alone would point variables we can not directly act on.	So it becomes easier to take corrective actions on the process to reach the goal. The stakes are so important that even a little improvement leads to consequent benefits, and we do not even talk about the impact on the market shares that is even more important.
		

		\begin{table}[h!]
\centering
\begin{tabular}{llcc}
	\hline
	Method& Indicator& With {\sc CorReg} & Without {\sc CorReg} \\ 
	\hline \hline
	{\sc ols} & {\sc mse} & 13.30 & 14.03 \\
		& complexity& 130 & 206 \\
	\hline
	{\sc lasso} & {\sc mse} & 12.77 & 12.96 \\
		& complexity& 24 & 21 \\
	\hline
	elasticnet & {\sc mse} & \textbf{12.15} & 13.52 \\
		& complexity& 40 & 78 \\
	\hline
	ridge & {\sc mse} & 12.69 & 13.09 \\
		& complexity& 130 & 206 \\
	\hline
\end{tabular} 
\caption{Quality case study: Results obtained on a validation sample ($n=847$ individuals). In bold, the best {\sc mse} value.}\label{Res_exfos}
\end{table}


		\FloatBarrier
		\section{Production case study}\label{sectionBV}
This second example is about a phenomenon that impacts the productivity of a steel plant.
We have:
		\begin{itemize}
			\item a (confidential)  response variable,
			\item $p=145$ variables from the whole process to explain it but only $n=100$ individuals.
			\item The stakes : $20\%$ of productivity to gain on a specific product with high added value.
		\end{itemize}
		
		
		
Figure~\ref{fig:graphCorr.production}(a) shows that many variables are highly correlated. {\sc CorReg} found $d_r=55$ sub-regressions and corresponding $R_{adj}^2$ values are displayed in Figure~\ref{fig:graphCorr.production}(b). One of them seems to be weak ($R_{adj}^2=0.17$) but it corresponds in fact to a non-linear regression: It points out a link between diameter of a coil and some shape indicator. In this precise case, \textsc{CorReg} found a structure that helped to decorrelate covariates and to find the relevant part of the process to optimize. This product is made by a long process that requires several steel plants so it was necessary to point out the steel plant where the problem occurred.	
		

\begin{figure}[h!]
\begin{center}
			\includegraphics[height=150px, width=150px]{figures/histcorrelBVBI.png}
			\includegraphics[height=150px,width=150px]{figures/R2corregBVBI.png}
\end{center}
\vspace{-5mm}
			\centerline{(a) \hspace{130px} (b)}
			\caption{Production case study: (a) Histogram of correlations $\rho$ in $\boldsymbol{X}$, (b) histogram of the adjusted $R^2_{adj}$ for the $d_r=55$ sub-regressions.}\label{fig:graphCorr.production}
\end{figure}  	

\vspace{3mm}

As in the previous quality case study, we note that the uncorrelated variables can be very well-modeled by parsimonious Gaussian mixtures as it is illustrated by Figure~\ref{fig:graphMixmod.production}(a). In particular, the number of components is really moderate as seen in Figure~\ref{fig:graphMixmod.production}(b).

\vspace{3mm}

		\begin{figure}[h!]
\begin{center}
			\includegraphics[height=150px, width=150px]{figures/GMcriseBV.png}
			\includegraphics[height=150px,width=150px]{figures/nbcompBV.png}
\end{center}
\vspace{-5mm}
			\centerline{(a) \hspace{130px} (b)}
			\caption{Production case study: (a) Example of a non-Gaussian real variable easily modeled by a Gaussian mixture, (b) distribution of the number of components found for each covariate.}\label{fig:graphMixmod.production}
\end{figure}  	

		
		
%		\begin{figure}[h!]
%	\begin{minipage}[t]{.30\linewidth}
%			\includegraphics[height=150px,width=150px]{figures/nbcompBV.png}%{figures/mixmod.png} 
%			\caption{Distribution of the number of components found for each covariate.}\label{graphMixmod}
%	\end{minipage} \hfill
%	\begin{minipage}[t]{.30\linewidth}
%			\includegraphics[height=150px, width=150px]{figures/GMcriseBV.png}%{figures/histR2exfos.png} $R^2_{adj}$ of the 76 sub-regressions.
%			\caption{Another example of non-Gaussian real variable easily modeled by a Gaussian mixture.}
%	\end{minipage} \hfill
%   \begin{minipage}[t]{.30\linewidth}
%			\includegraphics[height=150px,width=150px]{figures/histcorrelBVBI.png} 
%			\caption{Histogram of correlations in $\boldsymbol{X}$.} \label{compareMSEexfos}
%   \end{minipage}
%\end{figure} 
%  		
  			


%\begin{figure}[h!]
%\centering
%	\includegraphics[height=150px, width=150px]{figures/R2corregBVBI.png} 
%			\caption{$R^2_{adj}$ of the 55 sub-regressions.}\label{R2bv}
%\end{figure}
%The response variable was binary but $n$ was too small compared to $p$ to use logistic regression so we have considered $\boldsymbol{Y}$ as a continuous variable and then made imputation by $1$ when $\hat{\boldsymbol{Y}}>0.5$ and by $0$ else.

Table~\ref{Res_prod} displays predictive results associated to different estimation methods with and without {\sc CorReg}. Note that {\sc mse} is calculated though a leave-one-out method because of the small sample size. We can again see that {\sc CorReg} globally improves the results for each method tested in terms of prediction, with always a more parsimonious regression on $\boldsymbol{Y}$.
		

\begin{table}[h!]
\centering
\begin{tabular}{llcc}
	\hline 
	Method& Indicator& With {\sc CorReg} & Without {\sc CorReg} \\ 
	\hline\hline
	{\sc ols} &  {\sc mse}& 1.95& 51 810\\
		& complexity & 91& 100 \\
	\hline 
		{\sc lasso} & {\sc mse} & {\bf 0.106} & 0.120\\
		& complexity & 27&34\\
	\hline 
		elasticnet & {\sc mse} &0.140 &0.148\\
		& complexity &10 &13\\
	\hline 
		ridge & {\sc mse} & 0.179 & 0.177\\
		& complexity &91 &146\\
	\hline 
\end{tabular} 
\caption{Production case study: Results obtained with leave-one out cross-validation ($n=100, d=145$). {\sc mse} is calculated though a leave-one-out method because of the small sample size. In bold, the best {\sc mse} value.}	\label{Res_prod}
\end{table}
The response variable was binary but $n$ was too small compared to $d$ to use logistic regression so we have considered $\boldsymbol{Y}$ as a continuous variable and then made imputation by $1$ when $\hat{\boldsymbol{Y}}>0.5$ and by $0$ else.\\

In this precise case, {\tt CorReg} found a structure that helped to decorrelate covariates in interpretation and to find the relevant part of the process to optimize. This product is made by a long process that requires several steel plants so it was necessary to point out the steel plant where the problem occurred.


\part{Extensions}	
			
		
\chapter{Using coefficients of regression and sub-regression to improve prediction}
	\paragraph{Abstract:} We have seen that eviction of redundant covariates improves the results by a good tradeoff between dimension reduction and better conditioning versus keeping all the information. But the fact is that we lost some information and we want to get it back. So we propose a plug-in model to use the redundant covariates in a second estimation step.
	\section{Introduction}
		The structure $\boldsymbol{S}$ of sub-regressions between the covariates has given us the opportunity to introduce a marginal model that in fact removes the response covariates. We have seen that it is an efficient method to decorrelate the covariates and thus to reduce the variance of the estimator, not only on simulated but also on real datasets (Chapter \ref{sectionrealcase}). But even if the sub-regressions are strong, we face a loss information that can be damageable as we have seen in Section \ref{tableMSEsimgauche}. \\
		
		We know that using all the covariates simultaneously (classical \textsc{ols}) gives bad results due to correlations. But we can use them sequentially by using the explicit decomposition of the marginal model that makes appear the coefficient $\boldsymbol{\beta}_r$ in both the noise and $\boldsymbol{\beta}_f^*$. Thus we are able to obtain by plug-in better estimates of $\boldsymbol{\beta}_r$ and $\boldsymbol{\beta}_f$ in terms of bias. Once again final result will depend on the bias-variance tradeoff and numerical results will show what to expect.
	\section{Plug-in model}
		After the estimation of the marginal model in Section \ref{sectionmarginal}, we have both $\hat{\boldsymbol{\alpha}}^*$ and $\hat{\boldsymbol{\beta}^*}$. Reminding the marginal model (that is another form of the true model)
		\begin{equation}
		\boldsymbol{Y}|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\beta},\boldsymbol{\sigma}^2,\sigma_Y^2= \boldsymbol{X}_f\underbrace{(\boldsymbol{\beta}_f+\boldsymbol{\alpha}\boldsymbol{\beta}_r)}_{\boldsymbol{\beta}^*_f}+\underbrace{\boldsymbol{\varepsilon}\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y}_{\boldsymbol{\varepsilon}_Y^*}, \label{pluginorig}
		\end{equation}
		the residual of the marginal model is decomposed as follows:
\begin{eqnarray}
	\boldsymbol{\varepsilon}_Y^*&=&\boldsymbol{\varepsilon}\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y \textrm{\ (by definition).} \label{regressnoise}
\end{eqnarray}
And because we have already estimated the structure and the marginal model we can provide some new estimators to improve estimation of $\boldsymbol{\beta}$: 
\begin{itemize}
	\item Estimation of  $\boldsymbol{\varepsilon}_Y^*$ relying on $\hat{\boldsymbol{\beta}}^*_f$:
	\begin{equation}
		\hat{\boldsymbol{\varepsilon}}_Y^*=\boldsymbol{Y}-\boldsymbol{X}_f\hat{\boldsymbol{\beta}}^*_f.
	\end{equation}
	\item Estimation of $\boldsymbol{\varepsilon}$ relying on $\hat{\boldsymbol{\alpha}}^*$:
	\begin{equation}
		\hat{\boldsymbol{\varepsilon}}=\boldsymbol{X}_r-\boldsymbol{X}_f\hat{\boldsymbol{\alpha}}^* .\label{epsilonchapeau}
	\end{equation}
\end{itemize}
These two new estimators allow us to reduce the noise of the marginal model by linear regression based on equation (\ref{regressnoise}) and to obtain an estimator of $\boldsymbol{\beta}_r$ possibly better than $\hat{\boldsymbol{\beta}}^*_r=\boldsymbol{0}$.
\begin{itemize}
	\item New estimation of $\boldsymbol{\beta}_r$ to try to reduce the bias:
	\begin{equation}
		\hat{\boldsymbol{\beta}}_r^{\varepsilon}=(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{Y}- \boldsymbol{X}_f\hat{\boldsymbol{\beta}^*_f})
	\end{equation}
\end{itemize}
We suppose that $\hat{\boldsymbol{\beta}^*_f}$ and $\hat{\boldsymbol{\alpha}}^*$ are consistent (for example using \textsc{ols}). The new estimator $\hat{\boldsymbol{\beta}}_r^{\varepsilon}$ is consistent (\textsc{ols} and Slutsky's theorem) and we have
\begin{equation}
	\mathbb{E}(\hat{\boldsymbol{\beta}}_r^{\varepsilon})=\boldsymbol{\beta}_r 
\end{equation}
and
%and the variance can be decomposed for each sub-regression. $\forall j \in \{1,\dots,d_r\}$:
%\begin{eqnarray}
%	 \hat{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{J_r^j}-\boldsymbol{X}^{J_p^j}\hat{\boldsymbol{\alpha}}_j
%\end{eqnarray}
\begin{eqnarray}
	\operatorname{Var}(\hat{\boldsymbol{\beta}}_r^{\varepsilon})&=& \operatorname{Var}[(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{Y}- \boldsymbol{X}_f\hat{\boldsymbol{\beta}^*_f})]\\
	&=&\operatorname{Var}[(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{Y}- \boldsymbol{X}_f(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{Y})] \\
	&=&\operatorname{Var}[(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{I}_n- \underbrace{\boldsymbol{X}_f(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f')}_{\boldsymbol{H}_f}\boldsymbol{Y})] \\
	&=&\sigma^2_Y(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{I}_n-\boldsymbol{H}_f)\hat{\boldsymbol{\varepsilon}}(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}
	%&=&(\sigma_Y^2(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1})(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}
\end{eqnarray}
%		\begin{equation}
%		\boldsymbol{Y}|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\beta},\boldsymbol{\sigma}^2,\sigma_Y^2= \boldsymbol{X}_f\underbrace{(\boldsymbol{\beta}_f+\boldsymbol{\alpha}\boldsymbol{\beta}_r)}_{\boldsymbol{\beta}^*_f}+\underbrace{\boldsymbol{\varepsilon}\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y}
%		\end{equation}
%		\begin{eqnarray}
%		\textrm{the true main regression }	\boldsymbol{Y}|\boldsymbol{X}_r,\boldsymbol{X}_f;\boldsymbol{\beta},\sigma_Y^2&=& \boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{\varepsilon}_Y \\
%			\textrm{the sub-regressions system }\boldsymbol{X}_r|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2&=&\boldsymbol{X}_f\boldsymbol{\alpha}^*+\boldsymbol{\varepsilon} 
%			\textrm{the marginal model} 
%%					\end{eqnarray}		 
%		we have
%		\begin{eqnarray}
%			\boldsymbol{Y}- \boldsymbol{X}_f\boldsymbol{\beta}^*_f|\boldsymbol{\varepsilon},\boldsymbol{S};\boldsymbol{\beta}_r,\sigma_Y^2 &=&\boldsymbol{\varepsilon}\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y \textrm{ where}\\
%			\boldsymbol{\varepsilon}|\boldsymbol{X},\boldsymbol{S};\boldsymbol{\alpha}&=&\boldsymbol{X}_r-\boldsymbol{X}_f\boldsymbol{\alpha}^*.
%		\end{eqnarray}		 
%		So we introduce a plug-in model
%		\begin{eqnarray}
%			\boldsymbol{Y}- \boldsymbol{X}_f\hat{\boldsymbol{\beta}^*_f}|\boldsymbol{X},\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\beta}_r,\sigma_Y^2&=&\underbrace{(\boldsymbol{X}_r-\boldsymbol{X}_f\hat{\boldsymbol{\alpha}^*})}_{\hat{\boldsymbol{\varepsilon}}}\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y 
%		\end{eqnarray}
%		That allows us to estimate $\boldsymbol{\beta}_{r}$ with a classical linear model based on previous estimations of $\boldsymbol{\beta}^*$ and $\boldsymbol{\alpha}$.
%\begin{eqnarray}
%	\hat{\boldsymbol{\varepsilon}}&=&\boldsymbol{X}_r-\boldsymbol{X}_f\hat{\boldsymbol{\alpha}}^* \label{epsilonchapeau} \\
%	\hat{\boldsymbol{\beta}}_r&=&(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{Y}- \boldsymbol{X}_f\hat{\boldsymbol{\beta}^*_f})
%\end{eqnarray}		
%		
%		
		Then we can estimate $\boldsymbol{Y} $ based on (\ref{pluginorig}) with an estimation of the part of the residuals coming from the marginalization:
%		\begin{equation}
%			\boldsymbol{Y}|\boldsymbol{X}_f,\hat{\boldsymbol{\varepsilon}},\boldsymbol{S};\hat{\boldsymbol{\beta}}^*_f,\boldsymbol{\beta}_r,\boldsymbol{\sigma}_Y^2= \boldsymbol{X}_f\hat{\boldsymbol{\beta}}^*_f + \hat{\boldsymbol{\varepsilon}}\boldsymbol{\beta}_{r}+\boldsymbol{\varepsilon}_Y 
%		\end{equation}
\begin{equation}
	\hat{\boldsymbol{Y}}_{plug-in}=\boldsymbol{X}_f\hat{\boldsymbol{\beta}}^*_f + \hat{\boldsymbol{\varepsilon}}\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}.
\end{equation}
This an asymptotically unbiased estimator of $\boldsymbol{Y}$ because the marginal model is unbiased and all the estimators are consistent. 
		We can even improve estimation of $\boldsymbol{\beta}_f$ by doing an identification step.	We have $\boldsymbol{\beta}^*_f=\boldsymbol{\beta}_f+\boldsymbol{\alpha}^*\boldsymbol{\beta}_r $.
		\begin{itemize}
			\item New estimation of $\boldsymbol{\beta}_f $:
			\begin{equation}
			\hat{\boldsymbol{\beta}}_f^{\varepsilon}=\hat{\boldsymbol{\beta}}^*_f-\hat{\boldsymbol{\alpha}}^*\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}.
			\end{equation}
		\end{itemize}
Properties of $\hat{\boldsymbol{\beta}}_f^{\varepsilon}$ are obtained as above and give:
	\begin{eqnarray}
		\mathbb{E}(\hat{\boldsymbol{\beta}}_f^{\varepsilon})&=&\boldsymbol{\beta}_f \textrm{ (still unbiased)}\\
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_f^{\varepsilon})&=&\operatorname{Var}[\hat{\boldsymbol{\beta}}^*_f-\hat{\boldsymbol{\alpha}}^*\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}] \\
		&=&\sigma_Y^2[(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}+\hat{\boldsymbol{\alpha}}^*(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{I}_n-\boldsymbol{H}_f)\hat{\boldsymbol{\varepsilon}}(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}(\hat{\boldsymbol{\alpha}}^*)']
	\end{eqnarray}

Variance of $\hat{\boldsymbol{\beta}^{\varepsilon}}$ has supplementary  terms but relies on smaller and better conditioned matrices than classical \textsc{ols} applied on $\boldsymbol{X}$ so we hope that correlations are strong enough to make the difference.	Compared to \ref{eq:varOLS*} we have a new bias-variance tradeoff to study with numerical results.\\
	
	Figure \ref{MQE2} points out the target of this plug-in model: the cases with enough correlations to have problem when using $\boldsymbol{X}$	but not enough correlations to have truly redundant covariates and to be able to remove some of them (marginal model) without significant information loss. We see that our new model is efficient.
\begin{figure}[h!]
	\includegraphics[width=500px]{figures/MQE_toutOLSp5col.png}
	\caption{\textsc{mse} on $\hat{\boldsymbol{\beta}}$ of \textsc{ols} (plain red) and {\tt CorReg} marginal (blue dashed) and {\tt CorReg} plug-in (green dotted) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. Results obtained on the running example with $d=5$ covariates.}\label{MQE2}
\end{figure}	
			
		
	\section{Interpretation and latent variables}
			$\hat{\boldsymbol{\beta}}_{r}$ can be interpreted as the proper effect of $\boldsymbol{X}_r$ on $\boldsymbol{Y}$ in that it is the effect of the part of $\boldsymbol{X}_r$ that is independent from other covariates. Then if $\boldsymbol{X}_r$ is correlated to $\boldsymbol{Y}$ only through its correlation with $\boldsymbol{X}_f$ this sequential estimation will point it out and give a parsimonious model ($\hat{\boldsymbol{\beta}}_r=0$) but the real stake is greater. We can see $\boldsymbol{\varepsilon}$ as a latent variable instead of the noise of a sub-regression. This latent variable is known to be independent of $\boldsymbol{X}_f$ and dependent on $\boldsymbol{X}_r$ so we can appreciate its meaning and we also know its value by equation (\ref{epsilonchapeau}). Thus, the plug-in model can reveal some kinds of latent variables.\\
			
			Both marginal and plug-in model are easy to compute then we can use for example the marginal model for interpretation (more parsimonious) and the plug-in model for prediction. But we will see in the numerical results (Section \ref{resnumpred}) that it is not always the better choice because even if plug-in estimator can always be consistent (each covariate can be used) contrary to the marginal model, cumulated variances are a real problem and the marginal model is often better in prediction.
			
			
	

	\section{Consistency of the {\sc lasso}}\label{consistency}
		Consistency issues of the {\sc lasso} are well known and Zhao \cite{Zhao2006MSC} gives a very simple example to illustrate it.
		We have taken the same example to show how our method is better to find the true relevant covariates.
		Here $d=3$ and $n=1\;000$.\\
		We define $\boldsymbol{X}^1,\boldsymbol{X}^2, \boldsymbol{\varepsilon}_Y, \boldsymbol{\varepsilon}_{1} i.i.d. \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}_n)$ and then \\
		\begin{eqnarray}
		\boldsymbol{X}^3|\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{S};\sigma_1^2&=&\frac{2}{3}\boldsymbol{X}^1+\frac{2}{3}\boldsymbol{X}^2+\frac{1}{3}\boldsymbol{\varepsilon}_1 \textrm{ and} \\
		\boldsymbol{Y}|\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{S};\sigma_Y^2&=&2\boldsymbol{X}^1+3\boldsymbol{X}^2+\boldsymbol{\varepsilon}_Y.
		\end{eqnarray}
		
		
		We compare consistencies of complete, marginal and full plug-in model with {\sc lasso} (and LAR) for selection.
		It happens on some tries that our MCMC algorithm don't find the true structure but a permuted one so we both look at the results obtained with the true $\boldsymbol{S}=((3),(\{1,2\}))$ (but $\hat{\boldsymbol{\alpha}}$ is used) and with the structure found by the Markov chain after a few seconds.
		
		True $\boldsymbol{S}$ was found $987$ times on $1\;000$ tries.\\% (model is not identifiable because $\boldsymbol{X}^j$ are all Gaussian).
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical {\sc lasso} & {\tt CorReg} + marginal {\sc lasso}& {\tt CorReg} + full plug-in {\sc lasso}\\ 
		\hline 
		True $\boldsymbol{S}$ &  1.006784 & \textbf{1.005468} & \textbf{1.006093} \\ 
		\hline 
		$\hat{\boldsymbol{S}}$ & 1.006784 & 1.028154 & \textbf{1.006350} \\ 
		\hline 
		\end{tabular} 
		\caption{\textsc{mse} observed on a validation sample (1 000 individuals)}
		\end{table}

		We observe as we hoped that our marginal model is better when using true $\boldsymbol{S}$ (coercing real zeros) and that marginal with $\hat{\boldsymbol{S}}$ is penalised (coercing wrong coefficients to be zeros when true $\boldsymbol{S}$ is not found).
		But the main point is that the plug-in model stays better than the classical one with the true $\boldsymbol{S}$ and corrects enough the marginal model to be better than the classical {\sc lasso} when using $\hat{\boldsymbol{S}}$. Improvements here are very small but regular even on 1 000 tries.\\
		
		When we look at the consistency (Table \ref{testidentifiableG}):
		\begin{table}[h!]	
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical {\sc lasso} & {\tt CorReg}  marginal + {\sc lasso}& {\tt CorReg}  full plug-in  + {\sc lasso} \\ 
		\hline 
		True $\boldsymbol{S}$ &  0 & \textbf{1000} & \textbf{820} \\ 
		\hline 
		$\hat{\boldsymbol{S}}$ & 0 & \textbf{987} & \textbf{809} \\ 
		\hline 
		\end{tabular} 
		\caption{Number of consistent models found ($\boldsymbol{Y}$ depending on $\boldsymbol{X}^1,\boldsymbol{X}^2$ and only them) on $1\;000$ tries}\label{testidentifiableG}
		\end{table}				
		it is clear that the plug-in model significantly improves the consistency of the model estimated on $\boldsymbol{S}$ when compared to the classical {\sc lasso}. Both models have the choice to keep or not each covariate but only the plug-in model find sometimes (and in most of the cases) the true set of relevant covariates. Model using the true structure can't be improved because the marginal is already consistent so the plug-in is worse or equal to the marginal one in terms of consistency. Classical {\sc lasso} is never consistent on this example but we don't only improve this situation, we give consistent models in most of the cases.\\
	%	$299$ times on $1000$ tries, the plug-in model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		
%		We also made the same experiment but with $\boldsymbol{X}^1,\boldsymbol{X}^2$ (and consequently $\boldsymbol{X}^3$) following Gaussian mixtures %(to improve identifiability) 
%		randomly generated by our {\tt CorReg} package for R. 
%		True $\boldsymbol{S}$ is now found $714$ times on $1\;000$ tries (Table \ref{testidentifiableGM}) . So it confirms that Gaussian mixture models are easier to identify.
%		
%		
%		\begin{table}[h!]
%		\centering
%		\begin{tabular}{|c|c|c|c|}
%		\hline 
%		 & Classical LASSO & {\tt CorReg} marginal + LASSO& {\tt CorReg} full plug-in + LASSO \\ 
%%		\hline 
%%		True $S$ &  1.571029 & \textbf{1.569559} & \textbf{1.570801} \\ 
%     	\hline 
%		$\hat{S}$ & 1.005402 & 1.465768 & \textbf{1.005066} \\ 
%		\hline 
%		\end{tabular} 
%		\caption{MSE observed on a validation sample (1000 individuals)}
%		\end{table}
%
%		And when we look at the consistency :
%		\begin{table}[h!]
%		\centering	
%		\begin{tabular}{|c|c|c|c|}
%		\hline 
%		 & Classical LASSO & {\tt CorReg} marginal + LASSO& {\tt CorReg} full plug-in  + LASSO \\ 
%%		\hline 
%%		True $S$ &  0 & 1000 & 789 \\ 
%		\hline 
%		$\hat{S}$ & 0 & 714 & \textbf{608} \\ 
%		\hline 
%		\end{tabular} 
%		\caption{Number of consistent model found ($Y$ depending on $X_1,X_2$ and only them) on $1000$ tries}\label{testidentifiableGM}
%		\end{table}				
				
		
	%	$299$ times on $1000$ tries, the predictive model using $\hat{S}$ is better than classical LASSO in terms of MSE \underline{and} consistent (classical LASSO is never consistent).
		
\FloatBarrier
	\section{Numerical results} \label{resnumpred}
		We test the plug-in model with datasets generated the same way as for section \ref{compY}.\\
		
\subsection{$\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}	
We first try the method with a response depending on all covariates. (The marginal model reduces the dimension and can't give the true model if there is a structure).

\paragraph{Ordinary Least Squares} We observe for \textsc{ols} (Figure \ref{MSEpredOLStout}) that the plug-in model gives results similar in efficiency to the marginal model, but remains better than the complete model for smaller  correlations even for $n=400$. We also observe that we can found a model with more than $n$ coefficients when each estimation step computes less than $n$ coefficients. It means that  we estimate more coefficients than the classical \textsc{ols} and keep a smaller variance so the plug-in model can also be an alternative to the complete model. It is interesting to see that \textsc{ols} combined with the plug-in model is a sort of sequential estimation that allows to estimate more than $n$ coefficients.
\paragraph{Other methods:} Combined with variable selection methods (Figures \ref{MSEpredlartout} to \ref{MSEpredstepwisetout}) it does converge to the complete model results for large values of $n$ so it improves the marginal model for weak correlations (it is what it was built for) has no significant interest compared to the complete model. Ridge regression (Figure \ref{MSEpredridgetout}) leads to the same conclusion. 

 

\newpage
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/tout/MSEpredOLSY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/tout/cplpredOLS_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/tout/MSEpredOLSbeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredOLStout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}

%OLS	
	
	
\newpage
\subsubsection{{\sc lasso} when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

%LASSO
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/tout/MSEpredlarY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/tout/cplpredlar_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/tout/MSEpredlarbeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredlartout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

%elasticnet
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/tout/MSEpredelasticnetY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/tout/cplpredelasticnet_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/tout/MSEpredelasticnetbeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredelasticnettout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/tout/MSEpredstepwiseY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/tout/cplpredstepwise_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/tout/MSEpredstepwisebeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredstepwisetout}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends on all variables in $\boldsymbol{X}$}

\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/tout/MSEpredridgeY_zonetout.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/tout/cplpredridge_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/tout/MSEpredridgebeta_zonetout.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredridgetout}
\end{figure}
	\FloatBarrier


\subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_{f}}$ }
We look then at the case where $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$ \label{tableMSEsimdroitepred}.
\paragraph{Ordinary Least Squares:} Figure \ref{MSEpredOLSX1} shows that the plug-in model stays better than \textsc{ols} even when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_{f}}$. These good results come from the sequential estimation with the first part of the parameters estimated with a well-conditioned matrix. Hence the variance is reduced even without  

\paragraph{Other methods:} Variable selection methods (Figures \ref{MSEpredlarX1} to \ref{MSEpredstepwiseX1})are not improved by the plug-in model and the marginal model remains the best. But ridge regression (Figure \ref{MSEpredridgeX1}) is much similar to \textsc{ols} and then the plug-in model gives good results, even if the marginal model is sufficient.


	 \subsection{$\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_{r}}$ }
 \label{tableMSEsimgauchepred}
We then try the method with a response depending only on variables in $\boldsymbol{X}_r$. 
Depending only on $\boldsymbol{X}_r$ implies sparsity and impossibility for the marginal model to obtain the true model when using the true structure, so we hope to see an improvement with the plug-in method. This case is the reason why we have developed the plug-in model. \\

\paragraph{Better but not sufficient:} Concerning \textsc{ols} (Figure \ref{MSEpredOLSX2}) we note that the plug-in model improves results of the marginal model for large values of $n$. This is still the case with variable selection methods (Figures \ref{MSEpredlarX2} to \ref{MSEpredstepwiseX2}) even if it is not sufficient to improve the complete model. This phenomenon is still observed with the ridge regression (Figure \ref{MSEpredridgeX2}) with more efficiency but the complete model stays better.\\

	The plug-in model sounds good when described theoretically and figure \ref{MQE2} makes us hope to obtain good results with it. But the reality is that the plug-in model (by definition) relies on a first estimation (the marginal model) thus it does reach the true model but asymptotically, and with a slow convergence speed. \\
	
	Moreover, if $\hat{\boldsymbol{S}}$ is not exactly the true $\boldsymbol{S}$ but the partition is good, the marginal model is not impacted whereas the plug-in model uses both $\hat{\boldsymbol{\beta}}^*_{f}$ and $\hat{\boldsymbol{\alpha}}$ so it is does not rely only on one estimator. $\hat{\boldsymbol{\alpha}}$ is another source of bias for finite values of $n$ and it depends itself on $\hat{\boldsymbol{S}}$. Ordinary Least Squares really are in great trouble when confronted to correlated datasets so the plug-in model improves \textsc{ols} anyway but other methods are a bit less sensitive to correlations so it is difficult to improve them with a plug-in model relying on so many estimators. However, like the marginal model, the plug-in model has a small computational cost compared to the estimation of $\boldsymbol{S}$. So we recommend to compute both complete, marginal and plug-in model and to compare the results in a second time.

 
 
\FloatBarrier

\newpage
	\setcellgapes{1pt}
%OLS	
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}

\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X1/MSEpredOLSY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X1/cplpredOLS_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X1/MSEpredOLSbeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredOLSX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%LASSO
\subsubsection{{\sc lasso} when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}

	
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X1/MSEpredlarY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X1/cplpredlar_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X1/MSEpredlarbeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredlarX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}

	
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X1/MSEpredelasticnetY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X1/cplpredelasticnet_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X1/MSEpredelasticnetbeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredelasticnetX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}

	
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X1/MSEpredstepwiseY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X1/cplpredstepwise_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X1/MSEpredstepwisebeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredstepwiseX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_f}$}

\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X1/MSEpredridgeY_zoneX1.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X1/cplpredridge_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X1/MSEpredridgebeta_zoneX1.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredridgeX1}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}

%OLS	
\subsubsection{Ordinary Least Squares when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

	
\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X2/MSEpredOLSY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X2/cplpredOLS_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X2/MSEpredOLSbeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredOLSX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%LASSO
\subsubsection{{\sc lasso} when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X2/MSEpredlarY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X2/cplpredlar_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X2/MSEpredlarbeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredlarX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%elasticnet
\subsubsection{Elasticnet when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X2/MSEpredelasticnetY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X2/cplpredelasticnet_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X2/MSEpredelasticnetbeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredelasticnetX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%stepwise
\subsubsection{Stepwise when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

	\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X2/MSEpredstepwiseY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X2/cplpredstepwise_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X2/MSEpredstepwisebeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredstepwiseX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
%ridge	
\subsubsection{Ridge regression when $\boldsymbol{Y}$ depends only on covariates in $\boldsymbol{X_r}$}

\begin{figure}[h!]
\centering
\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	\setcellgapes{0pt}
	(a) & \includegraphics[width=450px]{figures/predhatS/X2/MSEpredridgeY_zoneX2.png}
\end{tabular}		
	\end{subfigure}
	\begin{subfigure}
	\centering
	\begin{tabular}[c]{m{5px} m{450px}}
	(b) &  \includegraphics[width=450px]{figures/predhatS/X2/cplpredridge_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}
	\centering
		 \begin{tabular}[c]{m{5px} m{450px}}
	(c) &  \includegraphics[width=450px]{figures/predhatS/X2/MSEpredridgebeta_zoneX2.png}
		\end{tabular}
	\end{subfigure}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{Y}}$ (a), complexities (b) and \textsc{mse} on $\hat{\boldsymbol{\beta}}$ (c), red=classical (complete) model, blue=marginal model}\label{MSEpredridgeX2}
\end{figure}
	\FloatBarrier
\newpage
	\setcellgapes{1pt}
	
\chapter{Missing values} \label{chapmiss}
	\paragraph{Abstract:} The full generative model defined on $\boldsymbol{X}$ with $\boldsymbol{S}$ gives the conditional distribution of missing values. Thus we are able to make multiple imputations based on the sub-regression structure. The full generative model also helps to manage missing values in the MCMC with the ability to compute $\mbox{{\sc bic}}_*$ only on the observed values. 
	
	 \section{State of the art}
	Real datasets often have missing values and it is a very recurrent issue in industry. 
			Here we suppose that missing values are Missing Completely At Random (MCAR) but other missing-data mechanisms do exist for missing values. For example, Missing values can depend on the observed values and then we say that they are Missing At Random (MAR), Missing value can also simply be not missing at random.
	 Many methods do exist to manage such problems in a regression context \cite{little1992regression}:
	 \begin{enumerate}
	\item Complete-Case analysis is a listwise deletion of missing values that may lead to no results if no individual is complete.  	
	\item Available-Case analysis methods use the largest sets of available cases for estimating individual parameters so it does not remove any additional value. But individual estimation of the parameters leads to bad results when the covariates are highly correlated \cite{haitovsky1968missing}.
	\item Imputation method are various, from imputation by the mean to conditional imputation based on $\boldsymbol{X}$ or based on $\boldsymbol{X}$ and $\boldsymbol{Y}$. These methods are often used with weighted least-squares to minimize the influence of imputed values.
	\item Methods based on a full generative model, using the joint distribution to estimate the regression parameters or to make multiple imputations.
	\end{enumerate} 
%Détailler les différents pattern de valeurs manquantes
%	 
%Detailler Les 6 types de méthodes	 
%	 
	We have a full generative model on $\boldsymbol{X}$ with explicit dependencies within the covariates. So when a value is missing, we know its distribution but in addition, we know its conditional distribution based on observed values for the same individual. Thus we are able to make imputation and to describe the missing values with their conditional distribution. This is a positive side-effect of the explicit generative model on $\boldsymbol{X}$.\\
		
	%This chapter is a perspective of this thesis that we have already started to work on, but missing values is a wide subject and we only propose some results.
	
	
	In the followings we note $x_{i,j}$ the $i^{th}$ individual of the $j^{th}$ covariate and $\boldsymbol{M}$ the $n\times d$ binary matrix indicating whether a value is missing or not: $\boldsymbol{M}_{i,j}=1$ if $x_{i,j}$ is missing, 0 else. 
	We define $\boldsymbol{X}_M=(\boldsymbol{X}_{1,M},\dots,\boldsymbol{X}_{n,M})$ the $n$-uple of the vectors $\boldsymbol{X}_{i,M}=(x_{i,j})_{\substack{j \in \{1,\dots,d \} \\ \boldsymbol{M}_{i,j}=1}}$ of the missing values for individual $i$, $\boldsymbol{X}_O=(\boldsymbol{X}_{1,O},\dots,\boldsymbol{X}_{n,O})$ the $n$-uple of the vectors $\boldsymbol{X}_{i,O}=\left(x_{i,j}\right)_{\substack{j \in \{1,\dots,d \} \\ \boldsymbol{M}_{i,j}=0}}$ of the observed values for individual $i$, $\boldsymbol{d}_M=(d_{1,M},\dots,d_{n,M})$ where the $d_{i,M}=|\boldsymbol{X}_{i,M} |$ are  the number of missing values for individual $i$ and $d_M=\sum_{i=1}^nd_{i,M}$ the total number of missing values in $\boldsymbol{X}$. We also define the three vectors $\boldsymbol{X}_{i,O}^{J_r}=\left(x_{i,j}\right)_{\substack{j \in J_r \\ \boldsymbol{M}_{i,j}=0}} $, $\boldsymbol{X}_{i,O}^{J_f}=\left(x_{i,j}\right)_{\substack{j \in J_f \\ \boldsymbol{M}_{i,j}=0}} $, and $\boldsymbol{X}_{i,O}^{J_p^j}=\left(x_{i,l}\right)_{\substack{l \in J_p^j \\ \boldsymbol{M}_{i,l}=0}} $ to simplify the notations (that are a bit heavy) in the followings.\\
	$\Theta=\{\boldsymbol{\mu}_X,\boldsymbol{\Sigma}_X \}$ stands for the parameters of the Gaussian mixture followed by $\boldsymbol{X}$.
	$\bar{\boldsymbol{\alpha}}$ is the $d\times d$ matrix of the sub-regression coefficients with $\alpha_{i,j}$ the coefficients associated to $\boldsymbol{X}^i$ in the sub-regression explaining $\boldsymbol{X}^j$.\\  
	
	Maugis  has developed another software \cite{maugis2012selvarclustmv} that manage missing values: {\tt SelvarclustMV}. It relies on the assumption of Missing At Random (MAR) values and on the more constraining hypothesis that regressor covariates are completely observed (with our notations: $\forall j \in \{1,\dots,d_r \}, \forall k \in J_p^j, \forall 1\leq i \leq n, \boldsymbol{M}_{i,k}=0$). The model (Partition, sub-regression parameters and Gaussian mixtures parameters) is then estimated with an Expectation-Maximization algorithm using all the observed values (even incomplete individuals) without any imputation. Numerical results shows that {\tt SelvarclustMV} is highly competitive compared to imputation methods.\\
 Here again the algorithm used in {\tt SelvarclustMV} stands in a Gaussian clustering context and is not exactly adapted to our situation. Estimation of $\boldsymbol{S}$ and $\boldsymbol{\alpha}^*$ was inconclusive (see section \ref{compMaugis}) without missing values so the situation would not be improved with missing values.
 Moreover, we do not want to make any assumption on the position of the missing values. Thus we investigate the possibility to estimate $\bar{\boldsymbol{\alpha}}$ by Maximum Likelihood as {\tt SelvarclustMV} does but without supposing that any covariate is fully observed.\\

\section{Estimation of $\boldsymbol{S}$ with missing values}

\subsection{Marginal likelihood}
The first thing we do with $\boldsymbol{X}$ in the \textsc{CorReg} process is to estimate $\boldsymbol{S}$. It is done by comparison of the \textsc{bic} that rely on the likelihood. Because covariates in $\boldsymbol{X}_f$ are orthogonal, complete-case estimation is equivalent to global estimation on the observed values, so we just use {\tt Rmixmod} for each covariate on the observed values to obtain the observed likelihood of each marginal covariate. These likelihood are computed only once before the MCMC starts and are then used when needed to compute the \textsc{bic} of a given candidate.\\

	 During the MCMC, for each candidate we have to compute the likelihood of the candidate, depending on $\boldsymbol{\alpha}$ the matrix of the sub-regression coefficients. Each sub-regression is supposed to be parsimonious and might be estimated by a Complete-Case method if the number of missing values is not too high, or any other estimator. We will see later how to use the generative model and maximum likelihood instead. The first challenge is to compute the observed likelihood and then the $\textsc{bic}_*$. \\
	 
We start with the complete likelihood of $\boldsymbol{X}$:
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,\boldsymbol{S};\boldsymbol{X})&=& \prod_{i=1}^n f(\boldsymbol{X}_i)= \prod_{i=1}^n\left[f(\boldsymbol{X}_i^{J_r}|\boldsymbol{X}_i^{J_f};\boldsymbol{\alpha},\Theta,\boldsymbol{S})f(\boldsymbol{X}_i^{J_f};\boldsymbol{\alpha},\Theta,\boldsymbol{S}) \right] \\
	&=&\prod_{i=1}^n\left[\prod_{j \in J_r}f(x_{i,j}|\boldsymbol{X}_i^{J_f};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\prod_{j \notin J_r} f(x_{i,j};\boldsymbol{\alpha},\Theta,\boldsymbol{S}) \right] \\
	&=&\prod_{i=1}^n\left[\prod_{j =1}^{d_r}f(x_{i,J_r^j}|\boldsymbol{X}_i^{J_p^j};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\prod_{j \notin J_r} f(x_{i,j};\boldsymbol{\alpha},\Theta,\boldsymbol{S}) \right] \\
%	&=&\prod_{i=1}^n\left[\prod_{\substack{j \in I_r \\ \boldsymbol{M}_{i,j}=1}}P(x_{i,j}|\boldsymbol{X}_i^{I_f^j})\prod_{\substack{j \in I_r \\ \boldsymbol{M}_{i,j}=0}}P(x_{i,j}|\boldsymbol{X}_i^{I_f^j})
%			\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=1}} P(x_{i,j})\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}} P(x_{i,j}) \right] \\
\end{eqnarray}
To compute the $\textsc{bic}_*$ we use then the log-likelihood:
\begin{eqnarray}
	\mathcal{L}(\boldsymbol{\alpha},\Theta,\boldsymbol{S};\boldsymbol{X})&=&\sum_{i=1}^n\left[\sum_{j =1}^{d_r}\log \left(f(x_{i,J_r^j}|\boldsymbol{X}_i^{J_p^j};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\right)+\sum_{j \notin J_r} \log \left(f(x_{i,j};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\right) \right]. \label{loglikmiss}
\end{eqnarray}
	 When missing values occurs, we restrict the likelihood to the known values by integration on $\boldsymbol{X}_M$.
%		We have 
%	\begin{equation}
%		g(\boldsymbol{X}|\Theta)=\int_{\boldsymbol{X}_M}f(\boldsymbol{X}|\Theta)d \boldsymbol{X} \label{integralmiss}
%	\end{equation}
%For the covariates in $\boldsymbol{X}_f$, we use the density estimated  ($e.g.$ a Gaussian Mixture model estimated by \textsc{Mixmod}) or given as hypothesis. All individuals are supposed $iid$ so $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \notin I_r $: 
%				 \begin{equation}
%				 	g(x_{i,j}|\Theta)=f(x_{i,j}|\Theta)=\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k}) 
%				 \end{equation} with $k_j,\pi_{j,k}, \mu_{j,k}$ and $\Sigma_{j,k}$ estimated by Mixmod (for example). 
%\\				 		
				 		
%		Then we have
%		\begin{eqnarray}
%			g(\boldsymbol{X}|\Theta)&=& g(\boldsymbol{X}_r|\boldsymbol{X}_f,\Theta)g(\boldsymbol{X}_f|\Theta) \\
%			&=&\prod_{i=1}^n\left[ g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}} g(x_{i,j}|\Theta) \right]\\
%			&=&\prod_{i=1}^n\left[ g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)
%							\prod_{\substack{j \notin I_r \\ \boldsymbol{M}_{i,j}=0}}\sum_{k=1}^{k_j}\pi_{j,k}\Phi(x_{i,j}|\mu_{j,k},\Sigma_{j,k}) \right] \label{decomplikelimiss}
%		\end{eqnarray}
%		reminding that covariates in $\boldsymbol{X}_f$ are orthogonal. \\
%		
%	 Residuals of the sub-regressions are orthogonal but missing values can make the residuals dependent. We have to decompose more precisely $g(\boldsymbol{X}_i^{I_r} |\boldsymbol{X}_i^{I_f},\Theta)$. To have a better view on the dependencies implied, we first write the marginal distributions. \\

We know that $\boldsymbol{X}$ is a Gaussian mixture ({\it iid} individuals, vectors of orthogonal Gaussian mixtures $\boldsymbol{X}_f$ and linear combinations of these Gaussian mixtures and some Gaussian for $\boldsymbol{X}_r$) with $K$ the number of its components.
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,\boldsymbol{S};\boldsymbol{X}_O)&=&\int_{\boldsymbol{X}_M}L(\boldsymbol{\alpha},\Theta,\boldsymbol{S};\boldsymbol{X})d\boldsymbol{X}_M 
	=\int_{\mathbb{R}^{d_M}}\sum_{k=1}^K \pi_k \Phi_k(\boldsymbol{X};\boldsymbol{\alpha},\Theta,\boldsymbol{S})d\boldsymbol{X}_M \\
	&=&\sum_{k=1}^K \pi_k \int_{\mathbb{R}^{d_M}}\Phi_k(\boldsymbol{X};\boldsymbol{\alpha},\Theta,\boldsymbol{S})d\boldsymbol{X}_M \\
	&=&\sum_{k=1}^K \pi_k \int_{\mathbb{R}^{d_M}}\prod_{i=1}^n\Phi_k(\boldsymbol{X}_i;\boldsymbol{\alpha},\Theta,\boldsymbol{S})d\boldsymbol{X}_M \\
	&=&\sum_{k=1}^K \pi_k \prod_{i=1}^n\int_{\mathbb{R}^{d_{i,M}}}\Phi_k(\boldsymbol{X}_i;\boldsymbol{\alpha},\Theta,\boldsymbol{S})d\boldsymbol{X}_{i,M} \\
	&=&\sum_{k=1}^K \pi_k \prod_{i=1}^n\Phi_k(\boldsymbol{X}_{i,O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\\
	&=&\sum_{k=1}^K \pi_k \Phi_k(\boldsymbol{X}_{O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})=f(\boldsymbol{X}_{O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})
\end{eqnarray}






% $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \notin I_r $:
%	 \begin{equation}
%	 \int_{\boldsymbol{X}_M}f(x_{i,j}|\boldsymbol{\alpha},\Theta,S) d\boldsymbol{X}=1
%	 \end{equation}	
To compute this likelihood, we will use the decomposition
\begin{eqnarray}
	L(\boldsymbol{\alpha},\Theta,\boldsymbol{S};\boldsymbol{X}_O)&=&f(\boldsymbol{X}_{O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})=\prod_{i=1}^nf(\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})f(\boldsymbol{X}^{J_f}_{i,O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\ \  \\
	&=&\prod_{i=1}^nf(\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\prod_{\substack{j \in J_f \\ \boldsymbol{M}_{i,j}=0}}f(x_{i,j};\boldsymbol{\alpha},\Theta,\boldsymbol{S})
\end{eqnarray}

with	  $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}= 0 \textrm{ and } j \notin J_r $:
	 \begin{equation}
	 f(x_{i,j};\boldsymbol{\alpha},\Theta,\boldsymbol{S})=\sum_{k=1}^{K_j}\pi_{j,k}\Phi_k(x_{i,j};\mu_{j,k},\Sigma_{j,k}) \label{likmissdroite}
	 \end{equation} with $K_j,\pi_{j,k}, \mu_{j,k}$, $\Sigma_{j,k}$ and the likelihood estimated once (for example by {\tt Rmixmod} \cite{packageRmixmod}) before the MCMC starts. 
%	 $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}\neq 0 \textrm{ and } j \in I_r $:
%	 \begin{equation}
%	 \int_{\boldsymbol{X}_M}f(x_{i,j}|\boldsymbol{X}_i^{I_f^j},\boldsymbol{\alpha},\Theta,S) d\boldsymbol{X}=1
%	 \end{equation}
\\
	 And, $\forall (i,j) \textrm{ with } \boldsymbol{M}_{i,j}= 0 \textrm{ and } j \in \{1,\dots,d_r \} $:
		\begin{eqnarray}
 f(x_{i,J_r^j}|\boldsymbol{X}_{i,O}^{J_p^j};\boldsymbol{\alpha},\Theta,\boldsymbol{S})&=& \sum_{k=1}^{K_{iJ_r^j}}\pi_{iJ_r^j,k}\Phi(x_{i,J_r^j};\mu_{iJ_r^j,k},\Sigma_{iJ_r^j,k}) \textrm{ where }  \label{Missingdensity}\\
				\boldsymbol{\pi}_{iJ_r^j} &=& \bigotimes_{\substack{l \in J_p^j \\ \boldsymbol{M}_{i,l}=1 } } \boldsymbol{\pi}_l \textrm{ and  }K_{iJ_r^j}=|\boldsymbol{\pi}_{iJ_r^j}| ,\\
				\boldsymbol{\mu}_{iJ_r^j}&=& \sum_{\substack{l \in J_p^j \\ \boldsymbol{M}_{i,l}=0  }}\bar{\boldsymbol{\alpha}}_{l,J_r^j}x_{i,l} + \bigoplus_{\substack{l \in J_p^j \\ \boldsymbol{M}_{i,l}=1  }} \bar{\boldsymbol{\alpha}}_{l,J_r^j} \boldsymbol{\mu}_l \\
				\boldsymbol{\Sigma}_{iJ_r^j} &=& \sigma_{J_r^j}^2 + \bigoplus_{\substack{l \in J_p^j \\ \boldsymbol{M}_{i,l}=1 }}\bar{\boldsymbol{\alpha}}_{l,J_r^j}^2 \boldsymbol{\Sigma}_l		
		\end{eqnarray}	
		
	Where $\bigotimes$ is the Kronecker product and and $\bigoplus$ is the Kronecker sum with $$A\bigoplus B= A\bigotimes\boldsymbol{I}_b+\boldsymbol{I}_a\bigotimes B$$ where $A$ and $B$ are two squares matrices of order $a$ and $b$ respectively and $\boldsymbol{I}_a,\boldsymbol{I}_b$ are the two associated identity matrices.
		This could be easily used for imputation of the missing values in $\boldsymbol{X}_r$ knowing the parameters $\boldsymbol{\alpha}, \Theta$ and $\boldsymbol{S}$. \\
		
		We note that we obtain a Gaussian when there is no missing value in $J_p^j$.
		But we see that	$f(\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})$ is not the product of the $f(x_{i,J_r^j}|\boldsymbol{X}_{i,O}^{J_p^j};\boldsymbol{\alpha},\Theta,\boldsymbol{S}) $	if a same missing value occurs in distinct sub-regressions. Thus if each sub-regression is a distinct connex component then we can use (\ref{Missingdensity}) and we have
		\begin{equation}
		L(\boldsymbol{\alpha},\Theta,\boldsymbol{S};\boldsymbol{X}_O)=\prod_{i=1}^n\prod_{\substack{j =1 \\ \boldsymbol{M}_{i,J_r^j}=0}}^{d_r}f(x_{i,J_r^j}|\boldsymbol{X}^{J_p^j}_{i,O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\prod_{\substack{j \in I_f \\ \boldsymbol{M}_{i,j}=0}}f(x_{i,j};\boldsymbol{\alpha},\Theta,\boldsymbol{S}) \label{simplemisslik}
\end{equation}		
	In such a case, computation is fast and easy. It is the case supposed in {\tt SelvarclustMV}.\\

 
		But for the general case we need to manage the dependencies implied by missing values in common covariates in the $J_p^j$.
%		Because $\forall 1\leq i \leq n, \boldsymbol{X}_i$ is a Gaussian Mixture, $\boldsymbol{X}_{i,O}$ and then $(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}_{i,O}^{I_f})$ are Gaussian mixtures too. \\
		%We note $f(\boldsymbol{X})=\sum_{k=1}^K\pi_k \mathcal{N}(\boldsymbol{\mu}_{X,k};\boldsymbol{\Sigma}_{X,k})$.
		
\begin{eqnarray}
L(\boldsymbol{\alpha},\Theta,\boldsymbol{S};\boldsymbol{X}_O)&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\\
&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O}^{J_r}|\boldsymbol{X}_{i,O}^{J_f};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\Phi_k(\boldsymbol{X}_{i,O}^{J_f};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\\
&=&\prod_{i=1}^n\sum_{k=1}^{K} \pi_{k} \Phi_k(\boldsymbol{X}_{i,O}^{J_r}|\boldsymbol{X}_{i,O}^{J_f};\boldsymbol{\alpha},\Theta,\boldsymbol{S})\prod_{\substack{j \in J_f \\ \boldsymbol{M}_{i,j}=0}}\Phi_k(x_{i,j};\mu_{j,k},\Sigma_{j,k}) \label{liklihoodmissglobal}
\end{eqnarray}		

%définintion des objets génériques
where	
\begin{eqnarray}
	\boldsymbol{\pi}=(\pi_1,\dots,\pi_K)&=&\bigotimes_{\substack{j \in J_f }} \boldsymbol{\pi}_j \textrm{ (Kronecker product)}\\
	K&=& |\boldsymbol{\pi}|% \\	
\end{eqnarray}
		with $ \boldsymbol{\pi}_j, \mu_{j,k},\Sigma_{j,k}$ are estimated once before the MCMC starts (by Mixmod for example).
		We note 
		\begin{eqnarray}
		\boldsymbol{\mu}_{\boldsymbol{X}^{J_f}}&=&  \prod_{\substack{j \in J_f}}\boldsymbol{\mu}_{j} \textrm{ (Cartesian product) and } \\	
	\boldsymbol{\sigma}_{\boldsymbol{X}^{J_f}}&=&\prod_{\substack{j \in J_f}}\boldsymbol{\sigma}_{j} \textrm{ (Cartesian product). }
		\end{eqnarray}
		Then we can describe the parameters as following:
		$\forall 1\leq i \leq n, \forall 1\leq k \leq K$ we have the conditional distribution of the sub-regressions
\begin{eqnarray}
		\Phi_k(\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O};\boldsymbol{\alpha},\Theta,\boldsymbol{S})&=&\Phi_k(\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O};\boldsymbol{\mu}_{\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O},k},\boldsymbol{\Sigma}_{\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O},k})
		\end{eqnarray}
				%P(\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O};\boldsymbol{\alpha},\Theta,S)&=&\Phi_k(\boldsymbol{X}^{I_r}_{i,O};\boldsymbol{\mu}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k},\boldsymbol{\Sigma}_{\boldsymbol{X}^{I_r}_{i,O}|\boldsymbol{X}^{I_f}_{i,O},k})\\
				where the mean vector is
				\begin{equation}
		\boldsymbol{\mu}_{\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O},k}= 
				\boldsymbol{\mu}_{\boldsymbol{X}^{J_r}_{i,O},k}+
				\boldsymbol{\Sigma}_{X_{i,O}^{J_r},X_{i,O}^{J_f},k}(\boldsymbol{\Sigma}_{X_{i,O}^{J_f},X_{i,O}^{J_f},k})^{-1}
				( (\boldsymbol{X}_{i,O}^{J_f})'-\boldsymbol{\mu}_{X^{J_f}_{i,O},k}) 
				\end{equation}
				and the variance is
						\begin{equation}
\boldsymbol{\Sigma}_{\boldsymbol{X}^{J_r}_{i,O}|\boldsymbol{X}^{J_f}_{i,O},k}=\boldsymbol{\Sigma}_{X_{i,O}^{J_r},X_{i,O}^{J_r},k}-\boldsymbol{\Sigma}_{X_{i,O}^{J_r},X_{i,O}^{J_f},k}
		(\boldsymbol{\Sigma}_{X_{i,O}^{J_f},X_{i,O}^{J_f},k})^{-1} \boldsymbol{\Sigma}_{X_{i,O}^{J_f},X_{i,O}^{J_r},k}.
		\end{equation}
		We can then describe each part of these formulas for each $1\leq i \leq n$ and $1\leq k \leq K$:\\
		The mean vector of observed regressed covariates: $\forall j \in \{1,\dots,d_r\}\textrm{ with }\boldsymbol{M}_{i,J_r^j}=0:$
 \begin{eqnarray}
		  \boldsymbol{\mu}_{X_{i}^{J_r^j},k}&=&\sum_{l \in J_p^j}\bar{\boldsymbol{\alpha}}_{l,J_r^j}\mu_{l,k} 
\end{eqnarray}		
The variance of observed regressed covariates: $\forall j \in \{1,\dots,d_r\} \textrm{ with } \boldsymbol{M}_{i,J_r^j}=0$ 
	\begin{equation}
		\operatorname{Var}_{k}(x_{i,J_r^j})=\sigma_{j}^2+ \sum_{l \in J_p^j} \bar{\boldsymbol{\alpha}}_{l,J_r^j}^2\sigma_{\boldsymbol{X}^l,k}^2
	\end{equation}			
	The variance of observed regressors: $\forall j \in J_f \textrm{ with } \boldsymbol{M}_{i,j}=0$
\begin{equation}
	\operatorname{Var}_{k}(x_{i,j})=\sigma_{X^j,k}^2
\end{equation}	
The covariance between two regressed covariates with common regressors: \\	
	$\forall j_1 \in \{1,\dots,d_r\}, j_2 \in \{1,\dots,d_r\},J_p^{j_1}\cap J_p^{j_2}\neq \emptyset \textrm{ with } \boldsymbol{M}_{i,J_r^{j_1}}=\boldsymbol{M}_{i,J_r^{j_2}}=0$
	\begin{equation}
		\operatorname{Cov}_{k}(x_{i,J_r^{j_1}},x_{i,J_r^{j_2}})%=\sum_{l\in J_p^{j_1}\cap J_p^{j_2}}\bar{\boldsymbol{\alpha}}_{l,J_r^{j_1}}\bar{\boldsymbol{\alpha}}_{l,J_r^{j_2}}	\operatorname{Var}_{k}(x_{i,l})
		 =\sum_{l\in J_p^{j_1}\cap J_p^{j_2}}\bar{\boldsymbol{\alpha}}_{l,J_r^{j_1}}\bar{\boldsymbol{\alpha}}_{l,J_r^{j_2}}\sigma_{\boldsymbol{X}^l,k}^2
	\end{equation}
	The covariance between two regressed covariates without any common regressors:\\
	$\forall j_1 \in \{1,\dots,d_r\}, j_2 \in \{1,\dots,d_r\},J_p^{j_1}\cap J_p^{j_2}= \emptyset \textrm{ with } \boldsymbol{M}_{i,J_r^{j_1}}=\boldsymbol{M}_{i,J_r^{j_2}}=0$
	\begin{equation}
	\operatorname{Cov}_{k}(x_{i,J_r^{j_1}},x_{i,J_r^{j_2}})=0
\end{equation}
The covariance between two regressors is always zero:\\
	$\forall j_1 \in J_f, j_2 \in J_f \textrm{ with } \boldsymbol{M}_{i,j_1}=\boldsymbol{M}_{i,j_2}=0$
	\begin{equation}
	\operatorname{Cov}_{k}(x_{i,j_1},x_{i,j_2})=0
\end{equation}
The covariance between a regressed covariate and one of its regressors: \\
	$\forall j_1 \in \{1,\dots,d_r\}, j_2 \in J_p^{j_1} \textrm{ with } \boldsymbol{M}_{i,J_r^{j_1}}=\boldsymbol{M}_{i,j_2}=0$
	\begin{equation}
	\operatorname{Cov}_{k}(x_{i,J_r^{j_1}},x_{i,j_2})= \bar{\boldsymbol{\alpha}}_{j_2,J_r^{j_1}}\sigma_{X^{j_2},k}^2
\end{equation}
The covariance between a regressed covariate and covariate neither regressing it nor regressed is zero by hypothesis \ref{H1}: 
$\forall j_1 \in \{1,\dots,d_r\}, j_2 \notin J_p^{j_1}\cup J_r \textrm{ with } \boldsymbol{M}_{i,J_r^{j_1}}=\boldsymbol{M}_{i,j_2}=0$
	\begin{equation}
	\operatorname{Cov}_{k}(x_{i,J_r^{j_1}},x_{i,j_2})= 0
\end{equation}
Then we are able to maximize this likelihood to find $\boldsymbol{\alpha}$.
	\subsection{Likelihood computation optimized}
	The main problem with the likelihood in its global form (\ref{liklihoodmissglobal}) is that the number of components explodes so we can't use it in practice. But in many cases, it can be simplified.
We see that the $0$ in the variance-covariance matrix does not depend on the component $k$ so the structure of sparsity of $\boldsymbol{\Sigma}$ can be stored and used back in each iteration for a given structure $\boldsymbol{S}$ to reduce computing time.  Another strictly technical tip would be to use sparse matrix storage to avoid null value storage and useless zero multiplications.\\

		Moreover, we can look if there are missing values shared by several sub-regression. 
		Connex component detection could be done to reduce the dimension down to strictly dependent covariates and use equation (\ref{Missingdensity}) elsewhere.
		We just need to compute the  row-sums of the adjacency matrix $\boldsymbol{G}$ or to search for redundancy in $J_f$ and then if there is no redundancy or if $\forall j$ redundant we have $\sum_{i=1}^n\boldsymbol{M}_{i,j}=0$ then we can use the simplified form of the likelihood given in (\ref{simplemisslik}). For faster computation we can stock the vector of covariates that have missing values.
		So the true value of the likelihood can be computed efficiently in most of cases but in the MCMC, it remains the possibility to evaluate a structure with explosive likelihood expression when combined with the missing values and we need to compute the likelihood for a great number of candidates. Then it is possible to use directly the simplified form of the likelihood, that can be seen as an approximation of the likelihood, not taking into account some of the dependencies but it would offer no guarantee in terms of efficiency for the MCMC. %Numerical results on simulated datasets will show if this approximation is effective.

%		In first approximation we can suppose independence between the sub-regression:
%		\begin{equation}
%		\forall (j,j') \in I_r \times I_r, g(x_{i,j}| \boldsymbol{X}_{i}^{I_f},\Theta) \perp g(x_{i,j'}| \boldsymbol{X}_{i}^{I_f},\Theta)
%\end{equation}		 
%then we have the complete expression of the likelihood with \ref{decomplikelimiss} and \ref{Missingdensity}.
% Such approximation can be costless according to the position of the missing values ({\it e.g.} if they are all in $\boldsymbol{X}^{I_r}$). It is closer to the real model than the orthogonal hypothesis made by classical imputation by the mean. Moreover, sub-regressions are used only locally and errors don't cumulate whereas the true general decomposition combine many sub-regressions with cumulated noise of approximation. Thus, a general model would be better asymptotically but may not be efficient with finite dataset if the structure is complex. This first approximation is a good candidate to compare to the naive model (not taking into account the structure of sub-regression but making imputations by the mean for each covariate individually). 
%		
%		However, we write the real generalized expression for the log-likelihood.
%		Let $\mathcal{I}_r$ be a permutation of $I_r$ (arbitrary chosen, so the package will use identity). We define the general decomposition:
%		\begin{eqnarray}
%			g(\boldsymbol{X}^{I_r}|\boldsymbol{X}^{I_f},\Theta)&=& \prod_{i=1}^n \left[g(x_{i,\mathcal{I}_r(p_r)}|\boldsymbol{X}_i^{I_f},\Theta)\prod_{j=1}^{p_r-1}g(x_{i,\mathcal{I}_r(j)}|\boldsymbol{X}_{i}^{\mathcal{I}_r(j+1)},\dots ,\boldsymbol{X}_{i}^{\mathcal{I}_r(p_r)}, \boldsymbol{X}_i^{I_f},\Theta)\right]
%		\end{eqnarray}
%		where we don't know the expression of $ g(x_{i,\mathcal{I}_r(j)}|\boldsymbol{X}_{i}^{\mathcal{I}_r(j+1)},\dots ,\boldsymbol{X}_{i}^{\mathcal{I}_r(p_r)}, \boldsymbol{X}_i^{I_f},\Theta)$ so the previous approximation stands still. 
%			
%	
%To estimate $\boldsymbol{\alpha}$ we use an EM algorithm. We start with an arbitrary value $\boldsymbol{\alpha}^{(0)}$, then:
%For the iteration $h$ of the algorithm at the E step we want 
%\begin{equation}
%	E_{\boldsymbol{X}_{\boldsymbol{M}}|\boldsymbol{X}_O,\boldsymbol{\alpha},\Theta,S}\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha}^{(h)},S,\Theta)\right]
%\end{equation}
%So we get $\boldsymbol{X}_M^{(h)}$ the  imputation for $\boldsymbol{X}_M$ and then the M step simply is
%\begin{equation}
%	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}E\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha},S,\Theta) \right]
%\end{equation}
%and we can use the same method as the one for classical case without missing values (OLS, SUR, {\it etc.}).
%		And we continue until convergence ($\parallel \boldsymbol{\alpha}^{(h+1)} - \boldsymbol{\alpha}^{(h)}\parallel < tol $ where $tol$ is the tolerance.\\
%		
%Imputation in $\boldsymbol{X}^{I_r}$ is made according to (\ref{Missingdensity}) and we have orthogonality in $\boldsymbol{X}^{I_f}$:
%\begin{displaymath}
%\forall j \notin I_r, P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f}\setminus \boldsymbol{X}^j)=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r})
%\end{displaymath}
%Moreover, $\forall j \notin I_r, \forall l \in I_r $ with $j \notin I_f^l, \boldsymbol{X}^j\perp \boldsymbol{X}^j$ so
%\begin{displaymath}
%\forall j \notin I_r, P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r},\boldsymbol{X}^{I_f}\setminus \boldsymbol{X}^j)=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r})=P(\boldsymbol{X}^j|\boldsymbol{X}^{I_r^j})
%\end{displaymath}
%where $I_r^j=\{i \in I_r| j \in I_f^j \}=\{i \in I_r|\alpha_{j,i}\neq 0 \}$ So we will make imputations (for E step only) according to $P(\boldsymbol{X}^j|\boldsymbol{X}_O^{I_r^j})$
%		\\
%$\forall 1 \leq i \leq n$	
%To use formulas on conditional distribution for Gaussian multivariate distribution we first write $P(\boldsymbol{X}_i^j,\boldsymbol{X}_i^{I_r^j})$ which is a Gaussian mixture with $K_{ij}$ components.
%\\
%$\forall j \notin I_r, \forall (l_1,l_2) \in I_r^j, P(x_{i,l_1}|x_{i,j},x_{i,l_2})=P(x_{i,l_1}|x_{i,j})$
%\begin{eqnarray}
%	P(x_{i,j},\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\alpha})&=& P(x_{i,j})\prod_{\substack{ l \in I_r^j \\M_{i,l=0} } }P(x_{i,l}|x_{i,j}) \\
%	&=&\sum_{k =1}^{ K_{ij}} \pi_{ij,k} \phi(x_{i,j},\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\mu}_{ij,k},\boldsymbol{\Sigma}_{ij,k}) \textrm{ where } \\
%	\boldsymbol{\pi}_{ij}&=&\boldsymbol{\pi}_j\otimes \left[ \bigotimes_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 } } \boldsymbol{\pi}_{ijl} \right]=\boldsymbol{\pi}_j\otimes\bigotimes_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 } } \left(\bigotimes_{\substack{h \in I_f^l \\ h \neq j} }\boldsymbol{\pi}_h \right)
%	 \textrm{ and  }K_{ij}=|\boldsymbol{\pi}_{ij}| ,\\
%	\boldsymbol{\mu}_{ij}&=&\boldsymbol{\mu}_j \times \left[\prod_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 }}\boldsymbol{\mu}_{ijl} \right]
%		=\boldsymbol{\mu}_j \times \prod_{\substack{l \in I_r^j \\ \boldsymbol{M}_{i,l}=0 }}\left[\alpha_{j,l}x_{i,j}+\bigoplus_{\substack{h \in I_f^l}}\alpha_{h,l}\boldsymbol{\mu}_{h} \right] \\	
%		\boldsymbol{\Sigma}_{ij}&& \textrm{is the associated variance-covariance matrix}
%\end{eqnarray}
%		Cartesian product and power in the expression of the mean.
%		
%
%Then we have 
%\begin{eqnarray}
%	P(x_{i,j}|\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\alpha})&= &\sum_{k=1}^{K_{ij}}\pi_{ij,k}\phi(x_{i,j}|\boldsymbol{X}_{i,O}^{I_r^j},\boldsymbol{\mu}_{ij,k},\boldsymbol{\Sigma}_{ij,k}) \\
%	&=&\sum_{k=1}^{K_{ij}}\pi_{ij,k}\phi \left(x_{i,j},\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}},\Sigma_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}\right) \textrm{ with} \\
%	\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}&=& \mu_{j,k} + \operatorname{cov}(x_{i,j,k},\boldsymbol{X}_{i,O,k}^{I_r^j})\operatorname{var}(\boldsymbol{X}_{i,O,k}^{I_r^j})^{-1}(\boldsymbol{X}_{i,O}^{I_r^j}-\boldsymbol{\mu}_{\boldsymbol{X}_{i,O}^{I_r^j},k})\textrm{ and} \\
%	\Sigma_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}&=&\operatorname{var}(x_{i,j,k})-\operatorname{cov}(x_{i,j},\boldsymbol{X}_{i,O,k}^{I_r^j})\operatorname{var}(\boldsymbol{X}_{i,O,k}^{I_r^j})^{-1}\operatorname{cov}(\boldsymbol{X}_{i,O,k}^{I_r^j},x_{i,j,k})
%\end{eqnarray}
% 
%But we do not need to compute the variance because we only want $E_{\boldsymbol{X}_{\boldsymbol{M}}|\boldsymbol{X}_O,\boldsymbol{\alpha},\Theta,S}\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha}^{(h)},S,\Theta)\right]$ so the mean is sufficient, we impute  
%\begin{equation}
%	\hat{x}_{i,j}=\frac{1}{K_{ij}}\sum_{k=1}^{K_{ij}}\pi_{ij,k}\mu_{ij,k|\boldsymbol{X}_{i,O}^{I_r^j}}
%\end{equation}
%
%We have 
%\begin{eqnarray}
%	\forall l \in I_r^j,\forall 1\leq k \leq K_{ij}, \operatorname{cov}(x_{i,j,k},\boldsymbol{X}_{i,O,k}^{l})&=&\operatorname{cov}(x_{i,j,k},\sum_{h\in I_f^l}x_{i,h}\alpha_{h,l}+\varepsilon_{i,l})
%	=\alpha_{j,l}\sigma_{j,k}^2 \\
%	\forall l \in I_r^j,\forall 1\leq k \leq K_{ij}, \operatorname{var}(x_{i,l,k})&=&\sigma_l^2+\sum_{h \in I_f^l}\alpha_{h,l}^2\sigma_{h,k}^2 
%	\end{eqnarray}
%	$\forall l_1\neq l_2 \in I_r^j,\forall 1\leq k \leq K_{ij},$
%	\begin{eqnarray}
%	 \operatorname{cov}(x_{i,l_1,k},x_{i,l_2,k})&=&\operatorname{cov}(\sum_{h\in I_f^{l_1}}x_{i,h,k}\alpha_{h,l_1}+\varepsilon_{i,l_1},\sum_{h\in I_f^{l_2}}x_{i,h,k}\alpha_{h,l_2}+\varepsilon_{i,l_2})\\
%	&=& \operatorname{cov}(\sum_{h\in I_f^{l_1}}x_{i,h,k}\alpha_{h,l_1},\sum_{h\in I_f^{l_2}}x_{i,h,k}\alpha_{h,l_2}) \\
%	&=& \sum_{h \in I_f^{l_1}\cap I_f^{l_2}}\sigma_{h,k}^2\alpha_{h,l_1}\alpha_{h,l_2}
%\end{eqnarray}
%
%
	\subsection{Weighted penalty for $\mbox{{\sc bic}}$}
			Now we have defined the way to compute the likelihood, other questions remain: how to define the number of parameters in the structure? How to take into account missingness (structures relying on highly missing covariates should be penalized)?
			We have seen that for a same covariate $\boldsymbol{X}^j$ with $ j \in J_r$, the number of parameters is not the same for each individual depending whether or not $\boldsymbol{M}_{i,j}=0$. We can compute the likelihood of each individual separately but the penalty (for $\mbox{{\sc bic}}_*$) cannot be added at the individual level (because $\log(1)=0$ so it would be annihilated). \\
			
			To penalize models that suppose dependencies based only on a few individuals, we propose to use the mean of the complexities obtained for a given covariate.
			\begin{equation}
			c_j=\frac{1}{n}\sum_{i=1}^nc_{i,j}
\end{equation}						where $c_{i,j}$ is the number of parameters to estimate in $\mathbb{P}(x_{i,j}|\boldsymbol{X}_i\setminus \boldsymbol{X}_i^j)$.
			\begin{eqnarray}
		-2\log \mathbb{P}(\boldsymbol{X}|\boldsymbol{S})&\approx & \mbox{{\sc bic}}=-2\mathcal{L}(\boldsymbol{X},\boldsymbol{S},\boldsymbol{\Theta})+|\boldsymbol{\Theta}|\log(n) \\
		&=& -2\mathcal{L}(\boldsymbol{X},\boldsymbol{S},\boldsymbol{\Theta})+(\sum_{j=1}^dc_j)\log(n)
	\end{eqnarray}
			 Thus if a structure is only touched by one missing value the penalty will be smaller than if the same structure has more missing values implied.
			Another way would be to use ${\sc ric}$ (see \cite{foster1994risk}) so the penalty does not depend on $n$ but is associated with $\log(d)$. %Or to make a compromise and penalize by $\frac{k_i\log(d)}{\log(n)}$.
		
%	We can also imagine to use other criterions, like the $RIC$ (Risk Inflation Criterion \cite{foster1994risk}) that choose a penalty in $\log p$ instead of $\log n$ and thus gives more parsimonious models when $p$ is larger than $n$ (high dimension) or any other criterion \cite{george1993variable} thought to be better in a given context. 
%	
			
%			In fact we have the same number of parameters to estimate with or without missing values but the problem is that some of the parameters are estimated based only on a portion of the individuals so each parameter has a different weight.
%			The penalty in $k\log(n)$ stands for $k$ parameters each $n-estimated$ so our proposed penalty term is an intuitive way to use a weighted penalty.
%	

	\section{EM Algorithm to estimate $\boldsymbol{\alpha}$ with a given $\boldsymbol{S}$}
			Sometimes the maximization of a likelihood is too complex to be solved analytically. In such a case, we can use Expectation-Maximization algorithms \cite{mclachlan2007algorithm} that maximize a likelihood by the optimization of its parameters and estimation of latent variables $Z$. This kind of algorithm allows to manage missing values \cite{dempster1977maximum}. \\
			
EM is an alternate optimization of the parameters $\boldsymbol{\theta}$ and the latent variables $Z$ with two steps at each iteration $(q)$:
\begin{itemize}
	\item The Expectation step in which the expected value of the loglikelihood is computed for the current estimate of the parameters $\boldsymbol{\theta}^{(q)}$ . It finds the best value of $Z$ given $\boldsymbol{\theta}^{(q)}$.
	\begin{equation}
		Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(q)})=E_{Z|\boldsymbol{X},\boldsymbol{\theta}^{(q)}} [\log L(\boldsymbol{\theta};\boldsymbol{X},Z)]
	\end{equation}
	\item The Maximization step that maximizes:
	\begin{equation}
		\boldsymbol{\theta}^{(q+1)}=\arg \max_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(q)})
	\end{equation}
\end{itemize}
	So it alternates between estimation of $Z$ and $\boldsymbol{\theta}$ until convergence. This algorithm can be used to choose the best mixture model associated to the distribution of an observed variable for example.		
	It requires to set initial values for $\boldsymbol{\theta}^{(0)}$ and faces local extrema problems so it is recommended to make multiple initializations and run of the algorithm.\\
			
			Some variants were developed like the Stochastic EM \cite{diebolt1996stochastic,celeux1986algorithme} or the Classification EM \cite{celeux1992classification}.
			
			
\subsection{Stochastic EM}
	The integrated likelihood depicted above depends on $\boldsymbol{\alpha}$ which was formerly estimated by \textsc{ols} when there was no missing values. But when missing values occurs in a sub-regression we need another solution.\\
	
	We use a  Stochastic Expectation Maximization (\textsc{sem}) algorithm \cite{celeux1986algorithme} to estimate $\boldsymbol{\alpha}$ because missing values do not allow to use \textsc{ols} and  the log-likelihood (\ref{loglikmiss}) is not linear so a simple Expectation-Maximization (EM) would be difficult to compute.\\
	
	Another method would be to estimate the $\boldsymbol{\alpha}_j$ with \textsc{ols} applied on sub-matrix of $(\boldsymbol{X}^{J_r^j},\boldsymbol{X}^{J_p^j})$ without missing values (complete-case method). Small sub-regressions may increase the probability to find such sub-matrices. Moreover, small sub-regression have only few parameters and can be estimated even with only a small number of individuals.
		
	
	\paragraph{initialization:} We start with some imputation (for example by the mean) for each missing value (done only once for the MCMC). $\boldsymbol{\alpha}^{(0)}$ can be initialized by complete-case method	(sparse structure) or using imputed values in $\boldsymbol{X}$ and then \textsc{ols}.
	At iteration $(h)$,
	\paragraph{SE step:}
		We generate the missing values according to $P(\boldsymbol{X}_M|\boldsymbol{X}_O; \boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{S})$, that is stochastic imputation.
	\paragraph{M step:}
		We estimate 
		\begin{equation}
	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}E\left[\mathcal{L}(\boldsymbol{X}|\boldsymbol{\alpha},\boldsymbol{S},\Theta) \right]
\end{equation}
and we can use the same method as the one for classical case without missing values (\textsc{ols}, \textsc{sur}, {\it etc.}).\\

		We continue until convergence ($\parallel \boldsymbol{\alpha}^{(h+1)} - \boldsymbol{\alpha}^{(h)}\parallel < tol $ where $tol$ is the tolerance). Then we make $m$ iterations and take $\hat{\boldsymbol{\alpha}}$ as the mean of these $m$ last iterations.\\
		
	The stochastic imputation made at the SE step is computed by Gibbs sampling.

	\subsection{Stochastic imputation by Gibbs sampling}
		Gibbs sampling \cite{casella1992explaining} is a special case of Markov Chain Monte Carlo algorithm \cite{gilks1996markov,chib1995understanding,roberts2001optimal} that allows to sample from a complex $d-$multivariate distribution when direct sampling is difficult. It is a randomized algorithm so each run may give distinct results. It generates a Markov Chain that follows the desired distribution with nearby draws. It starts from an initial value $\boldsymbol{X}^{(0)}$ and then for each iteration $(q)$ and successively each variable $x_j^{(q+1)}$ to draw, it draws from $\mathbb{P}(x_j|x_1^{(q+1)}, \dots,x_{j-1}^{(q+1)},x_{j+1}^{(q)},\dots,x_d^{(q)})$ using the most recent drawn values each time. \\
		
		Gibbs sampling method can be used to generate the missing values at the SE step. We note $Z$ the set of the $Z_{i,j}$ indicating the component from which $x_{i,j}$ (the $i^{th}$ individual of the $j^{th}$ covariate) is generated.
		\paragraph{Initialisation:}  %For example to the first component (such an initialisation does not depend on $K$). 
		$\boldsymbol{X}_M$ are imputed by the marginal means of drawn independently from the univariate distribution estimated for the MCMC (by {\tt Rmixmod} for example). all the $Z_{i,j}$ are randomly set based on the $t_{i,j,k}$ as explained below (we just need those of $\boldsymbol{X}_f$). 
		\paragraph{Iteration:} At each iteration of the Gibbs sampler we first make successive imputation on the missing values: \\
			Regressed covariates: $\forall 1\leq i \leq n, \forall j \in \{1,\dots,d_r \}, \boldsymbol{M}_{i,J_r^j}=1  $:  $x_{i,J_r^j}$ is generated according to 
			\begin{eqnarray}
			\mathbb{P}(x_{i,J_r^j}|\boldsymbol{X}_{i}\setminus x_{i,J_r^j},Z;\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{S})&=&
			\mathbb{P}(x_{i,J_r^j}|\boldsymbol{X}_{i}^{J_p^j};\boldsymbol{\alpha}^{(h)}_j,\Theta,{S})\\
			%&=&\mathbb{P}(x_{i,J_r^j}|\boldsymbol{X}_{i}\setminus x_{i,J_r^j},z_{i, J_r^j};\boldsymbol{\alpha}^{(h)},\Theta,{S})
		%	\mathbb{P}(x_{i,j}|\boldsymbol{X}_{i,O},\boldsymbol{X}_{i,\bar{M}_{i,j}};\alpha^{(h)},\Theta,S) \\
			%&=&\mathbb{P}(x_{i,j}|\boldsymbol{X}_i^{I_f^j};\boldsymbol{\alpha}^{(h)},\Theta,S)
			&=&\Phi(\boldsymbol{X}_i^{J_p^j}\boldsymbol{\alpha}^{(h)}_{j};\sigma_j^2 ) \textrm{ Gaussian}
			\end{eqnarray}		
			And for regressors:
			%We have $\mathbb{P}(\boldsymbol{X}|Z)=\mathcal{N}(\boldsymbol{\mu}_{|Z},\boldsymbol{\Sigma}_{|Z})$. \\
			$\forall (i,j) \in \{1,\dots,n \}\times J_f,\boldsymbol{M}_{i,j}=1$:  $x_{i,j}$ is generated according (using Schur complement and Gaussian conditional distribution) to 
			\begin{eqnarray}
			\mathbb{P}(x_{i,j}|\boldsymbol{X}_{i}\setminus x_{i,j},Z;\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{S})&=&\mathbb{P}(x_{i,j}|\boldsymbol{X}_{i,\bar{j}},Z;\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{S})			
			\end{eqnarray}			
			\begin{eqnarray}
			=\Phi(\mu_{j|Z_{i,j}} + \boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}^{-1}_{X_{\bar{ij}},X_{\bar{ij}}|Z_i}(X_{\bar{ij}}-\boldsymbol{\mu}_{X_{\bar{ij}}|Z_i}) ;  \sigma_{j|Z_{i,j}}^2-\boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}^{-1}_{X_{\bar{ij}},X_{\bar{ij}}|Z_i}\boldsymbol{\Sigma}_{j,X_{\bar{ij}}|Z_i}')
			\end{eqnarray}		
			Where all the values needed here were described above for the likelihood computation with $\boldsymbol{X}_{\bar{ij}}=\boldsymbol{X}_i\setminus x_{i,j}$ and $Z_i=\{Z_{i,j}:j=1,\dots,d \}$.\\
			
%With $\forall j \in I_r$ 
%\begin{equation}
%	\operatorname{var}_{|Z_{i}}(x_{i,j})=\sigma_{j|Z_{i,j}}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{l|Z_i,l}^2=\sigma_{j}^2+ \sum_{l \in I_f^j} \alpha_{l,j}^2\sigma_{l|Z_i,l}^2
%\end{equation}			
%	$\forall j \notin I_r $
%\begin{equation}
%	\operatorname{var}_{|Z_{i}}(x_{i,j})=\sigma_{j|Z_{i,j}}^2
%\end{equation}			
%	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}\neq \emptyset $
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=\sum_{k\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{k,j_1}\alpha_{k,j_2}\operatorname{var}_{|Z_i}(x_{k|Z_i,k}) =\sum_{k\in I_f^{j_1}\cap I_f^{j_2}}\alpha_{k,j_1}\alpha_{k,j_2}\sigma_{k|Z_{i,k}}^2
%\end{equation}
%	$\forall j_1 \in I_r, j_2 \in I_r,I_f^{j_1}\cap I_f^{j_2}= \emptyset $
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=0
%\end{equation}
%	$\forall j_1 \in I_f, j_2 \in I_f$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})=0
%\end{equation}
%	$\forall j_1 \in I_r, j_2 \in I_f^{j_1}$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})= \alpha_{j_2,j_1}\sigma^2_{j_2|Z_{i,j_2}}
%\end{equation}
%$\forall j_1 \in I_r, j_2 \notin I_f^{j_1}\cup I_r$
%	\begin{equation}
%	\operatorname{cov}_{|Z_{i}}(x_{i,j_1},x_{i,j_2})= 0
%\end{equation}
%We see that the $0$ in the variance-covariance matrix does not depend on $Z$ so the structure of sparsity of $\boldsymbol{\Sigma}$ can be stored and used back in each iteration for a given structure $S$ to reduce computing time.

	Then, $\forall 1\leq i \leq n, \forall j \in J_f$ we draw new values for $Z_{i,j}$ according to
	\begin{eqnarray}
		\mathbb{P}(Z_{i,j}|\boldsymbol{X},Z\setminus Z_{i,j};\Theta,\boldsymbol{\alpha},\boldsymbol{S})&=&P(Z_{i,j}|\boldsymbol{X}_i,Z\setminus Z_{i,j};\Theta,\boldsymbol{\alpha},\boldsymbol{S})=\mathcal{M}(t_{i,j,1},\dots ,t_{i,j,K_j})\ \  \\
		\textrm{where } t_{i,j,k}&=&\frac{\pi_{j,k}\Phi(x_{i,j};\mu_{j,k},\sigma_{j,k}^2)}{\sum_{l=1}^{K_j}\pi_{j,l}\Phi(x_{i,j};\mu_{j,l},\sigma_{j,l}^2) }
	\end{eqnarray}
		
	
	
	We see that $Z_{i,j}$ are not used if there is no missing values in $\boldsymbol{X}_i$ and others are not all needed so we can also optimize computation time by  computing only the $Z_{i,j}$ that are needed in the Gibbs.
	For the last iteration of the Gibbs, in the last iteration of the Stochastic EM, we do not need to draw $Z$.	\\
	
	Instead of using long chain for each Gibbs, we can use small chains because Stochastic EM iteration will simulate longer chains so it remains efficient with a smaller computation cost.
	Computation cost will be the main purpose here because we need an iterative algorithm (Gibbs sampler) at each iteration of another iterative algorithm (Stochastic EM) for each candidate of the MCMC.
	So alternative method should be preferred for large datasets with many missing values and only a small amount of time.\\
	
	Because $K$ can be very large we search a way to estimate the likelihood.
	We can use a Gibbs algorithm to estimate the likelihood:
	\begin{eqnarray}
	\mathbb{P}(\boldsymbol{X}_O;\Theta, \boldsymbol{S}, \boldsymbol{\alpha})&=& 
		\sum_{Z\in \mathcal{Z}}\int_{\boldsymbol{X}_M}\frac{\mathbb{P}(\boldsymbol{X}_M,Z,\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},\boldsymbol{S})}{\mathbb{P}(\boldsymbol{X}_M,Z|\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},\boldsymbol{S})}\mathbb{P}(\boldsymbol{X}_M,Z|\boldsymbol{X}_O;\Theta,\boldsymbol{\alpha},\boldsymbol{S}) dX \\
		&\approx &\frac{1}{Q} \sum_{q=1}^Q\frac{\mathbb{P}(\boldsymbol{X}_M^{(q)},\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha},\boldsymbol{S})}{\mathbb{P}(\boldsymbol{X}_M^{(q)}|\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha},\boldsymbol{S})} \textrm{ by the law of large numbers } \ \ \ \ \ 
	\end{eqnarray}		
	where $Q$ is the number of iterations of the Gibbs sampler.
	But to be faster, we use the previous Gibbs algorithm with:
	\begin{eqnarray}
	\mathbb{P}(\boldsymbol{X}_O;\Theta, \boldsymbol{S}, \boldsymbol{\alpha})&\approx & \frac{1}{Q} \sum_{q=1}^Q\frac{\mathbb{P}(\boldsymbol{X}_M^{(q)},\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha}^{(q)},\boldsymbol{S})}{\mathbb{P}(\boldsymbol{X}_M^{(q)}|\boldsymbol{X}_O,Z^{(q)};\Theta,\boldsymbol{\alpha}^{(q)},\boldsymbol{S})}
	\end{eqnarray}	
		
%\subsection{Alternative imputation}
%	For a given value of $\boldsymbol{\alpha}$, an alternative to the SE step described above would be to make imputation directly by the sub-regressions 
%	
%	$\forall x_{i,j} \in \boldsymbol{X}_M $ :
%	\\
%if $j\in J_r$, we can use Equation(\ref{Missingdensity}) as an approximation of $E[x_{i,j}|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,\boldsymbol{S}]$. Using only the sub-regression explaining $x_{i,j}$ but not the potential other sub-regression that could have add some information.
%	  
%	$\forall j \in J_f$, we note $r_{i,j}=\{l \in J_r|\bar{ \boldsymbol{\alpha}}_{j,l}\neq 0, \boldsymbol{M}_{i,j}=0 \}$ the set of observed covariates for individual $i$ that are explained by $x_{i,j}$ according to $\boldsymbol{S}$.
%	\\
%	If $j\in J_f$ and $M_{i,j}=1$ we can do:
%	\begin{eqnarray}
%	E[x_{i,j}|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,\boldsymbol{S}]%&=&\frac{1}{|r_{i,j}|}\sum_{k \in r_{i,j}}E_{|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,\boldsymbol{S}}\left[\frac{1}{\bar{\boldsymbol{\alpha}}_{j,k}}\left(x_{i,k}-\varepsilon_{k}(i)-\sum_{l \in J_p^k} x_{i,l}\bar{\boldsymbol{\alpha}}_{l,k}\right)\right] \ \ \ \\
%	&=& \frac{1}{|r_{i,j}|}\sum_{k \in r_{i,j}}E_{|\boldsymbol{\alpha}^{(h)},\Theta,\boldsymbol{X}_O,S}\left[\frac{1}{\bar{\boldsymbol{\alpha}}_{j,k}}\left(x_{i,k}- \sum_{l \in J_p^k} x_{i,l}\bar{\boldsymbol{\alpha}}_{l,k}\right)\right]
%	\end{eqnarray}
%	that is the mean of the expectations of the inverse sub-regressions implying $x_{i,j}$ with value in $\boldsymbol{X}^{J_r}_i$ not missing.
%


%		\subsection{Estimation of the coefficients in each regression}
%			Estimating the $\boldsymbol{\alpha}_j$  with missing values is just estimating independent regressions with missing values. We have seen in equation (\ref{Missingdensity}) that we know the expression of this density for a given the $\boldsymbol{\alpha}_j$. So it's just about maximizing the likelihood of this density on the $\boldsymbol{\alpha}_j$. This can be done with an Expectation-Maximization (EM) algorithm \cite{dempster1977maximum} or one of its extensions \cite{mclachlan2007algorithm}.
%			
%step E: ($\Theta$ stands for the parameters of the gaussian mixtures for the marginal distributions, estimated once by Mixmod):
%\begin{equation}
%	\boldsymbol{X}^{(h)}=E[\boldsymbol{X}|\boldsymbol{X}_{O},\boldsymbol{\alpha}^{(h)},\boldsymbol{\varepsilon},\Theta,S]
%\end{equation}			
%	
%	step M:	
%\begin{equation}
%	\boldsymbol{\alpha}^{(h+1)}=\operatorname{argmax}_{\boldsymbol{\alpha}}(\mathcal{L}(\boldsymbol{X}^{(h)},\boldsymbol{\alpha},\boldsymbol{\varepsilon},\Theta,S)) \textrm{ by OLS}
%\end{equation}	
%	
%		
%			But estimation of the $\boldsymbol{\alpha}_j$ is the most critical part of the MCMC in terms of computational time so it could be a bad idea to put there another iterative algorithm. 
%			Alternatives does exist :
%			\begin{itemize}
%				\item Because sub-regression are supposed to be parsimonious, we could imagine to estimate each column of $\boldsymbol{\alpha}$ with full sub-matrices of $\boldsymbol{X}_f$. When relying on too much missing values, $\hat{\boldsymbol{\alpha}}$ would be a bad candidate and then penalized directly by the likelihood (and it could be a good thing). Computational cost would be reduced significantly.
%				\item To estimate the $\boldsymbol{\alpha}_j$ (and not for the global likelihood) we could use data imputation (by the mean) and then obtain a full matrix but still ignoring missing values when estimating the likelihood. Imputation only concerns the estimation of the sub-regression coefficients and because null coefficients in sub-regression are coerced at each step, imputation only concerns a few covariates each time.
%			\end{itemize}
%			
%			
%			 $\forall j \in I_r$, estimation of $\boldsymbol{\alpha}^j$ only depends on individuals not missing in $\boldsymbol{X}^j$ (individuals are {\it iid}).
%			 So we work with a restriction of $\boldsymbol{X}$ for each $\boldsymbol{\alpha}^j$. Thus in this section, to simplify the notation, we will consider no missing values in $\boldsymbol{X}_r$ but in fact we work with restrictions.
%			
%			The EM algorithm can be written here: we start with some $\Theta^{(0)}=(\boldsymbol{\alpha},\boldsymbol{\varepsilon}) $ initial value for $\Theta$. The $\pi_{ij,k}$ are estimated once for each covariate (for example by Mixmod) and stay the same during the EM algorithm.
%			Naive E step : estimation of 
%			\begin{equation}
%				\boldsymbol{X}^{(h)}=E(\boldsymbol{X}|\boldsymbol{X}_{\bar M},\Theta^{(h)}) \textrm{ so it simply is}
%			\end{equation}
%			$\forall (i,j), \boldsymbol{M}_{i,j}=1, j\neq I_r$, 			
%			\begin{equation}
%				x^{(h)}_{i,j}=E(x_{i,j}|\boldsymbol{X}_{\bar M},\Theta^{(h)}) =\sum_{k=1}^{k_{ij}}\pi_{ij,k}\mu_{ij,k}^{(h)} \label{Estep}
%			\end{equation}
%			where, $\forall j \in I_f, k_{ij}=k_j, \pi_{ij,k}=\pi_{j,k}, \mu_{ij,k}=\mu_{j,k}$ \\
%			M-step : we determine $\Theta^{(h+1)}$ as the solution of the equation
%			\begin{equation}
%				E(\boldsymbol{X}|\Theta)=\boldsymbol{X}^{(h)} \textrm{ done by OLS}
%			\end{equation}
%			So the M step is just computing linear regressions on the filled dataset.
%			
%			
%		real E step : individuals are $iid$ so we just look at the expression for one individual, and use it for all
%		$\forall 1\leq n \leq n , \forall j \notin I_r$, we note $\bar{\boldsymbol{X}}_{i,j}=(\boldsymbol{X}_{\bar M}\cap \boldsymbol{X}_{i} \setminus \boldsymbol{X}^j)$ 
%			\begin{eqnarray}
%				P(\boldsymbol{X}_{fi}^M,\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M} | \Theta)&=&
%					P(\boldsymbol{X}_{fi}^M|\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M}|\Theta) \\
%				&=&P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M}|\Theta) \\
%				P(\boldsymbol{X}_{fi}^M|\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M},\Theta)&=&\frac{P(\boldsymbol{X}_{ri}^M|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M}|\Theta)}{P(\boldsymbol{X}_{fi}^{\bar M},\boldsymbol{X}_{ri}^{\bar M}|\Theta)} \\
%				&=&\frac{P(\boldsymbol{X}_{ri}^{O}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M}|\Theta)P(\boldsymbol{X}_{fi}^{\bar M}|\Theta)
%				}{
%				P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{\bar M}|\Theta)
%				} \\
%				&=&\frac{P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{M},\boldsymbol{X}_{fi}^{\bar M},\Theta)P(\boldsymbol{X}_{fi}^{M}|\Theta)}{P(\boldsymbol{X}_{ri}^{\bar M}|\boldsymbol{X}_{fi}^{\bar M},\Theta)}
%			\end{eqnarray}
%			
%			No imputation for missing left. Imputations for missing right are just used to obtain $\hat{\boldsymbol{\alpha}}$ but not when computing the $BIC$ or $BIC_+$.
%			
			
	\section{Missing values in the main regression}
		The easier way to manage missing values in the main regression would be to draw missing values and then use classical methods. Imputation could be done with the Stochastic EM described above, with the possibility to repeat the estimation of the coefficients of regression a few times (with distinct imputations) and then take the mean. We should for example try multiple draw and {\sc lasso} for variable selection like variable selection by random forest. One great advantage of multiple imputation procedures is that it gives an idea of the precision of the imputations with the variance of these imputed values among the multiple draws. So we know whether it is reliable or not. \\
		
		
				
%Another (faster but not optimal) way would be to estimating missing values in $\boldsymbol{X}_r$only use the structure for $\boldsymbol{X}_r$ and use the distribution given by {\tt RMixmod} for $\boldsymbol{X}_f$ along the MCMC. The full SEM would then be used only once with the final structure to make imputation in $\boldsymbol{X}$ before using variable selection methods like the LASSO.

		
		But another way would be to consider classical estimation methods as likelihood optimizer and then adapt them to the integrated likelihood of our model. Thus we can imagine to use {\sc lasso} without imputation. But the choice of the penalty using the LAR algorithm need also to adapt the LAR that is based on correlations that are computed on vectors with distinct number of individuals (due to missing values). So it requires more work but could be a good perspective.
	\section{Numerical results on simulated datastets}
		\subsection{Estimation of the sub-regression coefficients}
			We take datasets from the experiments in part I and then we compare the \textsc{mse} obtained on $\boldsymbol{\alpha}$ with our Stochastic EM to those obtain by classical \textsc{ols} after imputation of the missing values by the marginal empirical means. Here $d=40$ and $n=30$, missing values position are generated randomly for each of the 100 datasets to obtain $10 \%$ of missing values each time. Thus we have 120 missing values and none of the datasets contain a full individual without missing values.
Both methods were tested with the true structure $\boldsymbol{S}$. Initial value of $\boldsymbol{\alpha}$ for the Stochastic EM was the result of the method using imputation by the empirical mean. Only 10 iterations for the Stochastic EM after 2 warming steps with only 1 iteration for the Gibbs at each step.

\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/MSEalphaSEM.png} 
	\caption{\textsc{mse} on $\boldsymbol{\alpha}$ is significantly lower and and with smaller variance with our Stochastic EM than with imputation by the mean.}\label{MSEalphaSEM}
\end{figure}

	We see (Figure \ref{MSEalphaSEM}) that our Stochastic EM is nearly 13 times more efficient in mean that estimation based on imputation by the mean. Our results are extremely good because each sub-regression is true and we have 30 individuals (even if missing values kind of reduce this number) to estimate 3 coefficients only each time. Although, using imputed values lead to learn a true regression with a factually incorrect dataset. Thus we should prefer to work without imputing the missing values but using the full generative model and the dependencies it implies. Imputation will always introduce some noise.

		
			\subsection{Multiple imputation}
			We have then imputed missing values in $\boldsymbol{X}_r$  by using the corresponding sub-regressions after $\boldsymbol{\alpha}$ has been estimated  by the Stochastic EM.
			Missing values in $\boldsymbol{X}_f$ are estimated by the mean of 50 Gibbs iterations after the Stochastic EM and 2 warming steps of the Gibbs. Figure \ref{MSEXsubreg} shows the significative gain in \textsc{mse} produced by our method.
			\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/MSEXsubreg.png} 
	\caption{\textsc{mse} on $\hat{\boldsymbol{X}}$ is significantly lower when using our Stochastic EM than with imputation by the mean.}\label{MSEXsubreg}
\end{figure}
\FloatBarrier
		\subsection{Results on the main regression}
		We use the previously imputed $\hat{\boldsymbol{X}}$ to estimate $\boldsymbol{Y}$ with $\boldsymbol{\beta}=(1,\dots,1)$ and $\sigma_Y=10$.
			\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/MSESEMYtoymean.png} 
	\caption{\textsc{mse} on $\hat{\boldsymbol{Y}}$ are lower when using our Stochastic EM (blue) than with imputation by the mean (red) for the three model (complete, marginal and plug-in) using \textsc{ols} or \textsc{lasso}}\label{MSESEMYtoymean}
\end{figure}	

			\begin{figure}[h!]
	\centering
	\includegraphics[width=350px]{figures/MSESEMYtoysd.png} 
	\caption{our Stochastic EM (blue) provides more robust results than imputation by the mean (red) but the variances are still too wide.}\label{MSESEMYtoysd}
\end{figure}		
We obtain on a validation sample of 1000 individuals a predictive \textsc{mse} smaller in mean with our method (Figure \ref{MSESEMYtoymean}). But the variances are too important to really conclude (Figure \ref{MSESEMYtoysd}). We can say that imputation by Stochastic EM is more robust, but the Gibbs do not give satisfying results at the moment. Maybe the increase of the number of steps allowed by a code optimization would help to improve these results. For now, we can just say that our generative model significantly improves estimation of $\boldsymbol{\alpha}$ and makes possible to find $\boldsymbol{S}$ based on a dataset with missing values.\\

		One big advantage with our regression model is that it does not depend on the response variable $\boldsymbol{Y}$ so the structure can be learnt independently. Thus we can imagine to obtain big samples to learn the structure without being annoyed by the missing values. Then when a response variable is chosen, we can keep the same $\boldsymbol{S}$ and use previously computed values of $\boldsymbol{\alpha}$ as initial value for the Stochastic EM. 

	\section{Missing values on real datasets}	
		To be able to evaluate the results on a real dataset, we have deleted some values in the production dataset from section \ref{sectionBV} to obtain $10\%$ of missing values. Figure \ref{missingBV} shows the pattern of the missing values (MCAR). It confirms that $10\%$ of missing values is sufficient to have no complete line or column in the dataset. 
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=350px]{figures/missingBV.png} 
		\caption{Graphical representation of the dataset with $10\%$ of missing values}\label{missingBV}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=350px]{figures/MSEXmissBV.png} 
		\caption{\textsc{mse} on $\hat{\boldsymbol{X}}$ is $1.32$ times lower in mean when using our method (blue) than with imputation by the mean (red).}\label{MSEXmissBV}
	\end{figure}
	We see in figure \ref{MSEXmissBV} that our Stochastic EM gives a smaller \textsc{mse} with a smaller variance than imputation by the mean. There is still a lot of work to do in the field of missing values and we have only walked the firsts steps of this huge perspective, but these first results are encouraging and make us feel like this orientation is worthy to be followed.


\chapter{Conclusion and perspectives}
	\section{Conclusion}
		It is well-known that no model is the better in every situation. Here we propose two additional models (marginal and plug-in) but the best idea is to compare the full, marginal and plug-in and then choose the best for the study concerned. Our goal was not to replace any model but to enlarge the scope of statisticians in the real life. It is important to note that our model can be useful for interpretation even if the full model is chosen for interpretation, because we explicitly describe the correlations between the covariates. Moreover, it is only a pretreatment so it could easily be used with future statistical tools. \\
		
		Our model is easy to understand and to use. Usage of linear regression to model the correlations definitely separates us from "black boxes" so users are confident in what they do. The well-known and trivial sub-regression found comfort users in that if a structure does exist, {\tt CorReg} will find it so when a new sub-regression, or a new main regression is given they are more likely to look further and try it. The automated aspect shows the power of statistics without a priori so users begin to understand that statistics are not only descriptive or predictive but based on {\it a priori} models. This method seems to have a positive impact on the way users looks at the statistics (according to them). \\
		
			It is good to see that sequential methods (plug-in model) and automation can produce good results. Probabilistic models are efficient even without human expertise and let the experts improve the results by adding their expertise in the model (coercing some sub-regression for example). So we hope that statistics will continue to be a central tool for engineers.
		
		
	\section{Perspectives}
		\subsection{Non-linear regression}
			Polynomial regression, {\it etc.} might be improved by a method like this. Logistic regression \cite{hosmer2000applied} is in fact a linear regression with a post-treatment on $\boldsymbol{Y}$ so it is clear that we would obtain same phenomenon. It is an easy generalization of \textsc{CorReg}.
		\subsection{Pretreatment not only for regression}
			Classification and Regression Tree, and any other method could also benefit from the variable selection pretreatment implied by our marginal model.
		\subsection{Improved programming}
			Even if it is written in C++, the algorithm could be optimized by a better usage of sparse matrices, memory usage optimization, and other small things that could reduce computational cost to be faster and allow to work with larger datasets (already works with thousands of covariates).
		\subsection{Missing values in classical methods}
			The full generative approach could be used to manage missing values without imputation for many classical methods.
			It can notably be used for clustering and not only in response variable prediction context.
			Missing values were just introduced here and represent a consequent perspective.
		\subsection{Interpretation improvements}
			Ergonomy of the software should be improved to better fit industrial needs. This work is in progress and further work will be provided just after the redaction of this thesis to get closer to this goal.
		\subsubsection{Regression mixture models}
			When the population studied can be decomposed in several classes of individuals, correlations between the covariates can depend on these classes. One perspective would be to search mixture sub-regressions \cite{de1989mixtures} with some parameters and sub-regressions common to several classes and some other with distinct values.
		\subsubsection{Using $\boldsymbol{Y}$ to estimate $S$}
			Like PLS regression, we could imagine to estimate $S$ using also $\boldsymbol{Y}$ to find a structure that will give the best results when applied on $\boldsymbol{Y}$. But it would probably give distinct structures according to the chosen model (marginal or plug-in). It would also require a criterion that could be the $BIC$ of the combination of both $S$ and the chosen regression on $\boldsymbol{Y}$ or a {\sc MSE}  but with added computational cost if the LASSO (for example) has to be computed for each candidate structure in the MCMC.
\cleardoublepage

\addcontentsline{toc}{chapter}{References}
\bibliographystyle{apalike}
%\bibliographystyle{plain}
%\nocite{*}
\bibliography{biblio}
%\addcontentsline{toc}{chapter}{Appendices}
%\appendix
%	\chapter{Graphs and CorReg}
%		\section{Matricial notations}
%		\section{Properties}
%		Toute matrice binaire est associable à une matrice d'adjacence d'un graphe orienté (DAG)
%		
%		Matrice nilpotente = carré nul
%		
%		carré d'une matrice d'adjacence => chemins de longueur 2
%		
%		donc matrices binaires nilpotentes = graphes orientés sans chemins de longueur 2 donc celui qui reçoit n'emet pas donc graphe bi-partie
%		
%		dénombrement : liste systématique des possibilités (par décompostion comme dans la hiérarchie)		
%		
%	\chapter{Mixture models}
%		\section{Linear combination}
%			
%		\section{Industrial examples}
\part{Appendices}
\begin{appendices}
	\chapter{Identifiability}\label{sectionident}
		\section{Definition}
		We call identifiability:
\begin{eqnarray}
	\nexists (\boldsymbol{S},\tilde{\boldsymbol{S}}) \in \mathcal{S}_d\times \mathcal{S}_d \textrm{ with } \boldsymbol{S} \neq \tilde{\boldsymbol{S}} \textrm{ and }
	\mathbb{P}(\boldsymbol{X};\boldsymbol{S})=\mathbb{P}(\boldsymbol{X};\tilde{\boldsymbol{S}})
 \end{eqnarray}
 To avoid label-switching consideration, we suppose here (without loss of generality) that $\boldsymbol{J}_r$ is ordered by ascending order of the labels of the covariates.
	Hence identifiability is paired with the hypotheses we made on $ \mathcal{S}_d$. It is not sufficient to find a structure of linear sub-regression, the structure also has to verify hypotheses \ref{H1} to \ref{H3} (uncrossing rule + dependencies exhaustively described by the structure and then independence between the conditional response covariates). As a consequence, the covariance between two covariates is not null if and only if these covariates are linked by some sub-regressions.
 
		\section{Sufficient condition for identifiability}\label{suffcondident}

\paragraph{Identifability criterion:} The model $\boldsymbol{S}$ is identifiable if
			\begin{equation}
				\forall j \in \{1,\dots,d_r\},  d_p^j>1\label{identcondition}.
			\end{equation}
That is to have at least two regressors in each sub-regression.\\

To prove the sufficiency of this condition for identifiability we rely on the following lemma.

\paragraph{Lemma:} With $\boldsymbol{X}$ and $\boldsymbol{S}$ following hypotheses \ref{H1} to \ref{H3}, covariance between two distinct covariates does differ from $0$ in only two cases:
\begin{enumerate}
	\item One of the two variables is a regressor of the other in a sub-regression	
	\begin{eqnarray}
		j \in \{1,\dots,d_r\}, i \in J_p^j \textrm{ then } \operatorname{cov}(\boldsymbol{X}^{i},\boldsymbol{X}^{J_r^j})\neq 0
	\end{eqnarray}
	\item Both variables are regressed by a common covariate in their respective sub-regression:
	\begin{eqnarray}
		\exists k \in J_f, \exists (i,j)\in \{1,\dots,d_r\}\times \{1,\dots,d_r\} \textrm{ with } i\neq j,&&  k\in J_p^i \textrm{ and } k\in J_p^j \\
		\textrm{ then } \operatorname{cov}(\boldsymbol{X}^{J_r^i},\boldsymbol{X}^{J_r^j})&\neq &0 \nonumber
	\end{eqnarray}
\end{enumerate}

\paragraph{proof of the lemma:}
The two cases lead immediately to non-zero covariance so we just look at other combinations of covariates.
\begin{itemize}
	\item if $\exists (i,j)\in \{1,\dots,d_r\}\times \{1,\dots,d_r\}, \operatorname{cov}(\boldsymbol{X}^{J_r^i},\boldsymbol{X}^{J_r^j})\neq 0$ then hypothesis \ref{H2} (uncrossing rule) guarantee that the two covariates are not in a same sub-regression so the covariance must come from the noises of the sub-regression but hypothesis \ref{H3} say that they are independent. The only remaining case is then the second case of the lemma: common covariate in the sub-regressions.
	\item if $(i,j)\in J_f\times J_f$ then  $\operatorname{cov}(\boldsymbol{X}^i,\boldsymbol{X}^j)=0 $ because covariates in $\boldsymbol{X}_f$ are orthogonal (by hypotheses \ref{H1} and \ref{H2}).
	\item if $j \in \{1,\dots,d_r\}, i \in J_f$ and $\operatorname{cov}(\boldsymbol{X}^{J_r^j},\boldsymbol{X}^i)\neq 0$ then $i\in J_p^j$ by hypotheses \ref{H1} and \ref{H3} and equation \ref{eq:H3}.
\end{itemize}
$\square$

\paragraph{proof of the identifiability criterion:} We suppose that \ref{identcondition} is verified and the model is not identifiable:
\begin{eqnarray}
	\exists (\boldsymbol{S},\tilde{\boldsymbol{S}}) \in \mathcal{S}_d\times \mathcal{S}_d \textrm{ with } \boldsymbol{S} \neq \tilde{\boldsymbol{S}} \textrm{ and }
	\mathbb{P}(\boldsymbol{X};\boldsymbol{S})=\mathbb{P}(\boldsymbol{X};\tilde{\boldsymbol{S}})
 \end{eqnarray}
 $\tilde{\boldsymbol{S}}=(\tilde{\boldsymbol{J}}_r,\tilde{\boldsymbol{J}}_p)$ contains $\tilde{d}_r$ sub-regressions and is characterized by $\tilde{\boldsymbol{J}}_r=(\tilde{J}_r^1,\dots,\tilde{J}_r^{\tilde{d}_r}),\tilde{\boldsymbol{J}}_p=(\tilde{J}_p^1,\dots,\tilde{J}_p^{\tilde{d}_r})$.\\
Because $\boldsymbol{S}\neq \tilde{\boldsymbol{S}} $ we have $\boldsymbol{J}_r\neq \tilde{\boldsymbol{J}}_r$ or $\boldsymbol{J}_p\neq \tilde{\boldsymbol{J}}_p$.
\begin{itemize}
\item If $\boldsymbol{J}_r = \tilde{\boldsymbol{J}}_r$ and $\boldsymbol{S}\neq \tilde{\boldsymbol{S}}$ then one sub-regression as a predictor that stands only in one of the two structures. We suppose (without loss of generality) that $\exists j \in \{1,\dots,d_r\}$ for which
$\exists i \in J_p^j$ with $i \notin \tilde{J}_p^j$ so $\operatorname{cov}_{S}(\boldsymbol{X}^{J_r^j},\boldsymbol{X}^{i})\neq 0$ and $\operatorname{cov}_{\tilde{S}}(\boldsymbol{X}^{J_r^j},\boldsymbol{X}^{i})= 0$ (from the lemma) so the two structure don't give the same joint distribution, leading to a contradiction.
\item If $\boldsymbol{J}_r\neq \tilde{\boldsymbol{J}}_r$ then one of the two models has a sub-regression that is not in the other. We suppose (without loss of generality) that
 $\exists J_r^j \in J_r$ with $J_r^j \notin \tilde{J}_r$ then $J_r^j\in \tilde{J_f}$ (recall $J_f=\{1,\dots,d \}\setminus J_r$). We note that $J_r^j \in J_r$ means $\exists k_1\neq k_2, \{k_1,k_2 \}\subset J_p^j \subset J_f$. Then $\operatorname{cov}_{\boldsymbol{S}}(\boldsymbol{X}^{J_r^j},\boldsymbol{X}^{k_1})\neq 0$ and $\operatorname{cov}_{\boldsymbol{S}}(\boldsymbol{X}^{J_r^j},\boldsymbol{X}^{k_2})\neq 0 $ so by the lemma $k_1$ and $k_2$ are responses variables in $\tilde{\boldsymbol{S}}$: $\exists (l_1,l_2) \in \{1,\dots,\tilde{d}_r \}\times \{1,\dots,\tilde{d}_r \}, \tilde{J}_r^{l_1}=k_1,\tilde{J}_r^{l_2}=k_2$ and $J_r^j$ is a regressor of $k_1$ and $k_2$: $J_r^j \in J_p^{l_1}, J_r^j \in J_p^{l_2}$ thus $\operatorname{cov}_{\tilde{\boldsymbol{S}}}(\boldsymbol{X}^{k_1},\boldsymbol{X}^{k_2})\neq 0$ that is not possible because $ \{k_1,k_2 \}\subset J_p^j \subset J_f$ and covariates in $\boldsymbol{X}^{J_f}$ are orthogonal by hypotheses.
\end{itemize}

Finally, condition \ref{identcondition} is sufficient for identifiability of $\boldsymbol{S}$.
  	$\square$		
	
\paragraph{Remark:} Because sub-regressions with at least two regressors are identifiable, 
the only non-identifiable sub-regressions could be those with only one regressor, leading only to pairwise correlations that can be seen directly in the correlation matrix. Such sub-regression can be permuted without any impact on interpretation so such trivial sub-regression are not a problem even if they may occur with real datasets. One more thing: exact sub-regression with at least two sub-regressors are identifiable with our hypotheses. 	
	
			
%			\subsection{With Gaussian mixtures by comparison with SelvarClust} \label{preuveident}
%	The model presented above relies on a discrete structure $\boldsymbol{S}$ between the covariates. But to find it we need identifiability property to insure that we will asymptotically find the true model. Identifiability of the structure is asked in following terms: Is it possible to find another structure $\tilde{\boldsymbol{S}}$ of linear regression between the covariates leading to the same joint distribution and marginal distributions? 
%	
%		If there are exact sub-regressions ($\exists j\in \{1,\dots,d_r\}, \sigma^2_j=0$), the structure won't be identifiable. But it only means that several structure will have the same likelihood and they will have the same interpretation. So it's not really a problem. Moreover, when an exact sub-regression is found, we can delete one of the implied variables without any loss of information and the structure will define a list of variable from which to delete. {\tt CorReg} prints a warning to point out exact regressions when found.
%	In the followings we suppose $\forall j\in \{1,\dots,d_r\}, \sigma^2_j\neq 0$, then $\boldsymbol{X}_f'\boldsymbol{X}_f$ and $\boldsymbol{X}'\boldsymbol{X}$ are of full rank (but the later is ill-conditioned for small values of $\sigma^2_j$).
%	\\
%	
%Our full generative model is a $d$-sized Gaussian mixture model of $K$ distinct components and 
%%	\begin{equation}
%%	f(\boldsymbol{X}|K,S)=f(\boldsymbol{X}_f|K,S)f(\boldsymbol{X}_r|\boldsymbol{X}_f,S)
%%	\end{equation}	
%%	
%	can be seen as a $\mathbf{SR}$ model defined by Maugis \cite{maugis2009variable}. In this section, $S$ will denote the set of variable as in the paper from Maugis and we call Gaussian mixtures the Gaussian mixtures with at least two distinct components. The equivalence with Maugis's model is defined by:
%	$\boldsymbol{X}_r=\boldsymbol{y}^{S^c}$ and $\boldsymbol{X}_f=\boldsymbol{y}^R$. We have supposed independence between variables in $\boldsymbol{X}_f$ so the identifiability theorem from Maugis tells that our model is identifiable if variables in $\boldsymbol{X}_f$ are Gaussian mixtures.% (what we supposed in section \ref{sectionfullgen}).
%	\\
%	
%	
%%First, we observe that if each variable in $\boldsymbol{X}_r$ is a Gaussian mixture, then there must be at least one Gaussian mixture on the right of each sub-regression. 
%We define $\boldsymbol{X}^G \subsetneq \boldsymbol{X}_f$ containing Gaussian variables and we note the Gaussian mixtures $\boldsymbol{X}^{G^c}\neq \emptyset$ its complement in $\boldsymbol{X}_f$.
%We suppose that variables in $\boldsymbol{X}_r$ are all Gaussian mixtures. It implies that $\forall j\in \{1,\dots,d_r\},\exists i \in J_p^j $ so that $\boldsymbol{X}^i \subset \boldsymbol{X}^{G^c} $ since any linear combination of Gaussian variable would only give a Gaussian (so each sub-regression contain at least one Gaussian mixture as a regressor).
%\\
%	We have $\boldsymbol{X}_r=\boldsymbol{X}_f\boldsymbol{\alpha}^* + \boldsymbol{\varepsilon}$. 
%	
%		%\\We note $\Theta$ the parameter of the model.We want to know if $P(\boldsymbol{X}|S,\Theta)=P(\boldsymbol{X}|\tilde{S},\tilde{\Theta})$ does imply $(S,\Theta)=(\tilde{S},\tilde{\Theta})$.
%The theorem from Maugis guarantee that a sub-regression between Gaussian mixtures is identifiable in terms of which one is regressed by others.
%		\begin{eqnarray}
%%			\forall j \in I_r, \boldsymbol{X}^j_{|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_j^G+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c}+ \boldsymbol{\varepsilon}_j \\
%%			\boldsymbol{X}^j_{|\boldsymbol{X}^{G^c}}&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_j^{G^c} + \tilde{\boldsymbol{\varepsilon}}_j \textrm{ is identifiable where} \\
%%			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_j^{G}+ \boldsymbol{\varepsilon}_j \textrm{ is Gaussian. 
%		 \boldsymbol{X}_{r}|\boldsymbol{X}^G,\boldsymbol{X}^{G^c}&=& \boldsymbol{X}^G\boldsymbol{\alpha}_G^*+\boldsymbol{X}^{G^c}\boldsymbol{\alpha}_{G^c}^*+ \boldsymbol{\varepsilon} \\
%			%\boldsymbol{X}_{r|\boldsymbol{X}^{G^c}}
%			&=& \boldsymbol{X}^{G^c}\boldsymbol{\alpha}_{G^c}^* + \tilde{\boldsymbol{\varepsilon}} \textrm{ which is identifiable with} \\
%			\tilde{\boldsymbol{\varepsilon}}_j&=&\boldsymbol{X}^{G}\boldsymbol{\alpha}_{j,G}^*+ \boldsymbol{\varepsilon}_j \textrm{ that is Gaussian.}  
%		\end{eqnarray}
%		So a sufficient condition for identifiability is to have at least one Gaussian mixture in each sub-regression.	
%		It implies then that: $\forall j \in \{1,\dots,d_r \}, \boldsymbol{X}^{J_r^j} $ is a Gaussian mixture and $\exists i \in J_p^j,\boldsymbol{X}^{i} \subset \boldsymbol{X}^G $.  \\
%		
	%Tables \ref{testidentifiableG}	and \ref{testidentifiableGM} illustrates this property.


	\newpage
\chapter{CorReg: the concept}	
	
		The {\tt CorReg} package is already downloadable on CRAN under CeCILL Licensing. This package permits to generate datasets according to our generative model, to estimate the structure (C++ code) of regression within a given dataset and to estimate both explicative and predictive model with many regression tools (\textsc{ols},stepwise,\textsc{lasso},elasticnet,clere,spike and slab, adaptive \textsc{lasso} and every models in the {\tt lars} package). So every simulation presented above can be done with {\tt CorReg}.
	{\tt CorReg} also provides tools to interpret found structures and visualize the dataset (missing values and correlations). \\%More informations can be found on the website www.correg.org which is dedicated to \textsc{CorReg}. 
	The objective of {\tt CorReg} is also to bring recent statistical tools to engineers. Thus it will be made available in Microsoft Excel by the end of the year 2014, probably using Basic Excel R Toolkit(BERT\footnote{https://github.com/StructuredDataLLC/Basic-Excel-R-Toolkit}). \\
	
	It also provides some small scripts put in functions to obtain graphical representations and basic statistics with legends for non-statistician with only one command line (or macro button in Excel).\\
	 One example of graphical tool is the {\tt matplot\_zone } function that allows to compare several curves according to a given function (input parameter) and was widely used to compare the \textsc{mse} and complexities in this document. Another example is the {\tt recursive\_tree} function to plot classification and regression trees with basic statistics and legend but also to successively compute trees removing some correlated covariates or covariates that cannot be changed in the process to see if they are replaced by others more useful (this recursive aspect has given its name to the function).\\
	 
	More features will be added as statistics will continue to be taught to engineers at ArcelorMittal Dunkerque to provide ergonomic and powerful statistical tools to non-statisticians. 	
	\end{appendices}
\end{document}
