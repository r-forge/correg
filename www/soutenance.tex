\documentclass[11pt]{beamer}
\usetheme{Malmoe}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
  %\usepackage{color}
  \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage[english]{babel}
\usepackage{listings} 
\usepackage[toc,page]{appendix}
\usepackage{array}%,multirow,makecell}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{graphicx}
%\usepackage{framed}
%\usepackage[tikz]{bclogo}
\usepackage{hyperref}
\usepackage{url}
\usepackage{lmodern}
 % \usepackage[usenames,dvipsnames,svgnames,table]{xcolor}%[usenames,dvipsnames,svgnames,table]

\let\oldtabular=\tabular
\def\tabular{\small\oldtabular}
\setcounter{tocdepth}{1}
%  \usetheme{Warsaw}
 \graphicspath{{figures/}}
  \author{Clément Théry, Christophe Biernacki, Gaétan Loridant}\institute{ArcelorMittal Dunkerque, Université de Lille 1,équipe M$\Theta$dal Inria}
\title{CorReg}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
%\date{} 
%\subject{} 

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\tableofcontents
\end{frame}

\section{Context}
	\subsection{Industrial context}
		\begin{frame}
				  \begin{enumerate}
				\item Steel industry databases.
				\item Goal: To understand and prevent quality problems on finished product, knowing the whole process, \underline{without a priori}.
			\end{enumerate}
	        \begin{center}
	          \begin{tabular}{ccc}
	         \includegraphics[width=70px,height=70px]{liquid.jpg} & \includegraphics[width=70px,height=70px]{Brame1.jpg} & \includegraphics[width=70px,height=70px]{Brame.jpg} \\
	          	\includegraphics[width=70px,height=70px]{ecras_moy.jpg} & \includegraphics[width=70px,height=70px]{tcc2.jpg} & \includegraphics[width=70px,height=70px]{bobines.jpg}
	          \end{tabular}
	        \end{center}
		\end{frame}
	\subsection{Statistical context}
		\begin{frame}{Regression}
			\begin{equation}
				\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\label{regressionsimple}
			\end{equation}
			where 	$\boldsymbol{\varepsilon}\sim \mathcal{N}(0,\sigma_Y^2\boldsymbol{I}_n)$	
		\end{frame}
		
		\begin{frame}{OLS}
		\begin{figure}[h!]
	\centering
	\includegraphics[width=220px]{figures/OLS_geometric_interpretation.png}
	\caption{Multiple linear regression with Ordinary Least Squares seen as a projection on the $d-$dimensional hyperplane spanned by the regressors $\boldsymbol{X}$. Public domain image.} \label{geomOLS}
	\end{figure}
		\end{frame}
		
		\begin{frame}{OLS}
		$\boldsymbol{\beta}$ can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{ols}), that is the unbiased maximum likelihood estimator \cite{saporta2006probabilites,dodge2004analyse}: %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}_{OLS}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaOLS}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}. \label{eq:varOLS}
	\end{equation}
	 In fact it is the Best Linear Unbiased Estimator (BLUE).
	 The theoretical \textsc{mse} is given by
	\begin{equation}
	\textsc{mse}(\hat{\boldsymbol{\beta}}_{OLS})= \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}). \nonumber 
	\end{equation}
		\end{frame}
		
\begin{frame}{Running example}

%$d=5$ with four independent scaled Gaussian 
$\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{X}^4,\boldsymbol{X}^5  \sim \mathcal{N}(0,1)$  and 
$\boldsymbol{X}^3=\boldsymbol{X}^1+\boldsymbol{X}^2+\boldsymbol{\varepsilon}_1$ where 
$\boldsymbol{\varepsilon}_1\sim{\mathcal{N}(\boldsymbol{0},\sigma_1^2\boldsymbol{I}_n)}$. \\
Two {\it scenarii} for $\boldsymbol{Y}$:\\
 $\boldsymbol{\beta}=(1,1,1,1,1)'$ and $\sigma_Y \in \{10,20\}$.
 \\
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_1$ gets smaller. $R^2$ stands for the coefficient of determination which is here:
	\begin{equation}\label{defR2}
	R^2=1-\frac{\operatorname{Var}(\boldsymbol{\varepsilon}_1)}{\operatorname{Var}(\boldsymbol{X}^3)}
	\end{equation}

\end{frame}	
	
	\begin{frame}
	\begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/OLScompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates (running example). } \label{MQEOLScompl}
	\end{figure}	
\end{frame}		
		\begin{frame}{Ridge Regression}
		\cite{hoerl1970ridge,marquardt1975ridge} proposes a possibly biased estimator for $\boldsymbol{\beta}$ that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \eta \textrm{ with } \eta>0
	\end{equation}
	But this penalty is not guided by correlations. 
	The solution of the ridge regression is given by
	\begin{equation}
		 \hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} -\lambda\boldsymbol{I}_n\right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaridge}
	\end{equation}
 Methods do exist to automatically choose a good value for $\lambda$ \cite{cule2013ridge,er2013systematic} and a R package called {\tt ridge} is on {\sc cran} \cite{packageridge}.
		\end{frame}
		
		\begin{frame}{Ridge Regression}
 \begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/ridgecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEridgecompl}
	\end{figure}
			\end{frame}
		
		\begin{frame}{LASSO}
		The Least Absolute Shrinkage and Selection Operator ({\sc lasso}, \cite{tibshirani1996regression} and \cite{tibshiranilasso}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$ instead of the $L_2$ penalty of the ridge regression:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0 . \nonumber 
		\end{equation}	
		
		
		
		\end{frame}
		\begin{frame}{LASSO}
		\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/lasso.png} 
			\caption{Geometric view of the Penalty for the \textsc{lasso} (left) compared to ridge regression (right) as shown in the book from Hastie \cite{hastie2009elements}} \label{lassogeom}
		\end{figure}
		Figure shows the contour of error (red) and constraint function (blue).
		  The axis stands for the regression coefficients.
		\end{frame}
		
	

\begin{frame}
 \begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/larcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{lar}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQElarcompl}
	\end{figure}	
	{\tt lars } package on {\sc cran} (\cite{packagelars}).
\end{frame}
		
		
		
		\begin{frame}{ SEM }
		 Modélisation de la structure mais à la main et aucun impact sur l'estimation
		 \end{frame}
		 
		\begin{frame}{ Selvarclust }
			Semble très bien mais n'aboutit pas vers la régression donc on le prolonge en CorReg
		\end{frame}
\section{Proposed Models}
	\begin{frame}{Hypothesis 1}
	There are $d_{r}\geq 0$ ``sub-regressions'', each sub-regression $j=1,\ldots,d_{r}$ having the covariate $\boldsymbol{X}^{J_{r}^j}$ as {\it response} variable ($J_{r}^j\in\{1,\ldots,p\}$ and $J_{r}^j\neq J_{r}^{j'}$ if $j\neq j'$) and having the $d_p^j>0$ covariates $\boldsymbol{X}^{J_{p}^j}$  as {\it predictor} variables ($J_{p}^j\subset\{1,\ldots,d\} \backslash J_{r}^j$ and $d_p^j=|J_{p}^j|$ the cardinal of $J_{p}^j$):
\begin{equation}
\boldsymbol{X}^{J_{r}^j}%|\boldsymbol{X}^{J_{p}^j};\boldsymbol{\alpha}_j,\sigma^2_j
=\boldsymbol{X}^{J_{p}^j}\boldsymbol{\alpha}_j+\boldsymbol{\varepsilon}_j, \label{eq:SR}
\end{equation}
where $\boldsymbol{\alpha}_j\in{\mathbb{R}^{d_r^j}}$ (${\alpha}_j^h\neq 0$ for all $j=1,\ldots,d_r$ and $h=1,\ldots,d_p^j$) and $\boldsymbol{\varepsilon}_j \sim\mathcal{N}_n(\boldsymbol{0},\sigma^2_j\boldsymbol{I})$.
	\end{frame}
	\begin{frame}{Hypothesis 2}
	the response covariates and the predictor covariates are totally disjoint: for any sub-regression $j=1,\ldots,d_{r}$, $J_{p}^j\subset J_f$ where $J_{r}=\{J_{r}^1,\ldots,J_{r}^{d_r}\}$ is set of all response covariates and $J_f=\{1,\ldots,d\} \backslash J_{r}$ is the set of all {\it non} response covariates of cardinal $d_f=d-d_r=|J_f|$. We call this hypothesis the \underline{uncrossing rule}. Then:
	\begin{equation}
			\boldsymbol{Y} %{|\boldsymbol{X},\boldsymbol{S}};\boldsymbol{\beta},\sigma_Y^2
			=\boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y. \label{eq:MainR}
\end{equation}
	\end{frame}
	\begin{frame}{Hypotheses 3}
	We assume that all errors $\boldsymbol{\varepsilon}_Y$ and $\boldsymbol{\varepsilon}_j$ ($j=1,\ldots,d_r$) are {\it mutually independent}. It implies in particular that conditional response covariates $\{\boldsymbol{X}^{J_{r}^j}|\boldsymbol{X}^{J_{p}^j},\boldsymbol{S};\boldsymbol{\alpha}_j,\sigma^2_j\}$ are {\it mutually independent}:
\begin{equation}\label{eq:H3}
\mathbb{P}(\boldsymbol{X}_r | \boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2) = \prod_{j=1}^{d_r} \mathbb{P}(\boldsymbol{X}^{J_{r}^j}|\boldsymbol{X}^{J_{p}^j},\boldsymbol{S};\boldsymbol{\alpha}_j,\sigma^2_j). 
\end{equation}
	\end{frame}
	\subsection{Marginal model}
		\begin{frame}{Marginal model}
		 We obtain  for the distribution of $\{\boldsymbol{Y} |\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2\}$:
\begin{eqnarray}
	\boldsymbol{Y}%|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2
	&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\alpha}_j^*)+  \sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{eq:Trueexpl} \\
	&=&\boldsymbol{X}_f\boldsymbol{\beta}_f^*+\boldsymbol{\varepsilon}_Y^*,\label{eq:modexpl}
\end{eqnarray}
where $\boldsymbol{\alpha}^*_j \in \mathbb{R}^{d_f}$ with $(\boldsymbol{\alpha}_j^*)_{J_p^j}=\boldsymbol{\alpha}_j $ and $(\boldsymbol{\alpha}_j^*)_{J_f\setminus J_p^j}=\bf 0 $. We define $\boldsymbol{\alpha}^* \in \mathbb{R}^{(d_f \times d_r)}$ to use more compact notations:
\begin{eqnarray}
	\boldsymbol{X}_r %|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2
	&=&\boldsymbol{X}_f\boldsymbol{\alpha}^*+\boldsymbol{\varepsilon} \nonumber \\
	\boldsymbol{Y}%|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2
	&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \boldsymbol{\alpha}^*\boldsymbol{\beta_{r}})+ \boldsymbol{\varepsilon}\boldsymbol{\beta_{r}}+\boldsymbol{\varepsilon}_Y \label{eq:Trueexpl2}
\end{eqnarray} 
Where $\boldsymbol{\varepsilon}$ is the $n\times d_r$ matrix whose columns are the $\boldsymbol{\varepsilon}_j$, the noises of the sub-regressions.
		\end{frame}
	\subsection{Plug-in model}
		\begin{frame}{Plug-in model}
%\fcolorbox{jaunefonce}{jauneclair}{\begin{minipage}{1\textwidth}
\begin{equation}
	\boldsymbol{\varepsilon}_Y^*%|\boldsymbol{\varepsilon};\boldsymbol{\beta}_r,\sigma_Y^2
	=\boldsymbol{\varepsilon}\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y. \label{eq:regplugin} 
\end{equation}
%\end{minipage}}
Then the Best Linear Unbiased Estimator ({\sc BLUE}) for $\boldsymbol{\beta}_r$ is given ({\sc mle} estimator) by:
\begin{equation}
  \hat{\boldsymbol{\beta}_r}=(\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon})^{-1}\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}_Y^* . \label{olsplugin}
\end{equation}
    And we have the following estimators:
	\begin{eqnarray}
		\hat{\boldsymbol{\varepsilon}}&=& \boldsymbol{X}_r-\boldsymbol{X}_f\hat{\boldsymbol{\alpha}}^* \nonumber \textrm{ and} \\
		\hat{\boldsymbol{\varepsilon}}_Y^*&=&\boldsymbol{Y}-\boldsymbol{X}_f\hat{\boldsymbol{\beta}}_f^* \nonumber
	\end{eqnarray}
	that we can use by plug-in.
	\end{frame}
	\begin{frame}{Plug-in model}
	\begin{eqnarray}
		\hat{\boldsymbol{\beta}}_r^{\varepsilon}&=&(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}}_Y^*  \nonumber 
	\end{eqnarray}
that depends on all covariates in $\boldsymbol{X}$ and relies on the estimated coefficients of sub-regressions $\hat{\boldsymbol{\alpha}}^*$ and on the estimate $\hat{\boldsymbol{\beta}}_f^*$ of the coefficients in the marginal model.
Then we can estimate $\boldsymbol{Y}$ by:
\begin{eqnarray}
	\hat{\boldsymbol{Y}}_{plug-in}&=&\boldsymbol{X}_f\hat{\boldsymbol{\beta}}^*_f + \hat{\boldsymbol{\varepsilon}}\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}. \label{hatYplugin} 
\end{eqnarray}
We can improve estimation of $\boldsymbol{\beta}_f$ (in terms of bias) by doing an additional identification step.	We know that $\boldsymbol{\beta}^*_f=\boldsymbol{\beta}_f+\boldsymbol{\alpha}^*\boldsymbol{\beta}_r $ so we naturally define the following estimator:
			\begin{equation}
			\hat{\boldsymbol{\beta}}_f^{\varepsilon}=\hat{\boldsymbol{\beta}}^*_f-\hat{\boldsymbol{\alpha}}^*\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}. \nonumber 
			\end{equation}
		\end{frame}
		
		\begin{frame}{Marginal properties}
		biased
		\end{frame}
		\begin{frame}{Plug-in properties}
		asymptotically unbiased
		\end{frame}
		\begin{frame}
		\begin{figure}[h!]
\centering
	\includegraphics[width=280px]{figures/MQE_toutOLSp5col.png}
	\caption{\textsc{mse} on $\hat{\boldsymbol{\beta}}$ of \textsc{ols} (plain red) and {\tt CorReg} marginal (blue dashed) and {\tt CorReg} plug-in (green dotted) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. Results obtained on the running example with $d=5$ covariates.}\label{MQE2}
\end{figure}
		\end{frame}
		
\begin{frame}{Lasso Consistency}
Consistency issues of the {\sc lasso} are well known and Zhao \cite{Zhao2006MSC} gives a very simple example to illustrate it.
		We have taken the same example to show how our method is better to find the true relevant covariates.
		Here $d=3$ and $n=1\;000$.\\
		We define $\boldsymbol{X}^1,\boldsymbol{X}^2, \boldsymbol{\varepsilon}_Y, \boldsymbol{\varepsilon}_{1} \quad i.i.d. \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}_n)$ and then \\
		\begin{eqnarray}
		\boldsymbol{X}^3%|\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{S};\sigma_1^2
		&=&\frac{2}{3}\boldsymbol{X}^1+\frac{2}{3}\boldsymbol{X}^2+\frac{1}{3}\boldsymbol{\varepsilon}_1 \textrm{ and}  \nonumber \\
		\boldsymbol{Y}%|\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{S};\sigma_Y^2
		&=&2\boldsymbol{X}^1+3\boldsymbol{X}^2+\boldsymbol{\varepsilon}_Y. \nonumber 
		\end{eqnarray}
\end{frame}		
	\begin{frame}{Lasso Consistency}
True $\boldsymbol{S}$ was found $991$ times on $1\;000$ tries.% (model is not identifiable because $\boldsymbol{X}^j$ are all Gaussian).
		\begin{table}[h!]
		\resizebox{\linewidth}{!}{%
		 \begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical {\sc lasso} & {\tt CorReg} marginal + {\sc lasso}& {\tt CorReg} full plug-in + {\sc lasso}\\ 
		\hline 
		True $\boldsymbol{S}$ &  1.003303 (0.046) & \textbf{1.002273} (0.046)& \textbf{1.002812} (0.046)\\ 
		\hline 
		$\hat{\boldsymbol{S}}$ & 1.003303 (0.046)& 1.017622 (0.17)& \textbf{1.002812} (0.046)\\ 
		\hline 
		\end{tabular} 
		}
		\caption{\textsc{mse} observed on a validation sample (1 000 individuals) and their standard deviation (between brackets).}\label{MSEconsistlasso}
		\end{table}

We look at the consistency  that is the real stake:
		\begin{table}[h!]	
		\resizebox{\linewidth}{!}{%
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical {\sc lasso} & {\tt CorReg}  marginal + {\sc lasso}& {\tt CorReg}  full plug-in  + {\sc lasso} \\ 
		\hline 
		True $\boldsymbol{S}$ &  0 & \textbf{1000} & \textbf{835} \\ 
		\hline 
		$\hat{\boldsymbol{S}}$ & 0 & \textbf{991} & \textbf{829} \\ 
		\hline 
		\end{tabular} 
		}
		\caption{Number of consistent models found on $1\;000$ tries.}\label{testidentifiableG}
		\end{table}	
\end{frame}		
		
\section{Structure estimation}
	\begin{frame}{•}
	
	\end{frame}

\section{Results}
	\subsection{Simulation results}
		\begin{frame}{•}
		
		\end{frame}
	\subsection{Industrial results}
		\begin{frame}{•}
		
		\end{frame}
\section{Missing values}
	\begin{frame}{•}
	Modèle génératif complet avec dépendances
	\end{frame}
	\begin{frame}{•}
	Explosion des mélanges 
	\end{frame}
	\begin{frame}{•}
	SEM avec Gibbs 
	\end{frame}
	\begin{frame}{•}
	Bic pondéré 
	\end{frame}
	\begin{frame}{•}
	Résultats pourris 
	\end{frame}
\section{Tools}
	\begin{frame}{•}
		Excel, fonctions graphiques, arbres de décision
	\end{frame}

\begin{frame}
\bibliographystyle{apalike}
%\bibliographystyle{plain}
%\nocite{*}
\bibliography{biblio}
\end{frame}

\end{document}