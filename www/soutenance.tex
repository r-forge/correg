\documentclass[11pt]{beamer}
\usetheme{Malmoe}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
  %\usepackage{color}
  \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage[english]{babel}
\usepackage{listings} 
\usepackage[toc,page]{appendix}
\usepackage{array}%,multirow,makecell}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{graphicx}
%\usepackage{framed}
%\usepackage[tikz]{bclogo}
\usepackage{hyperref}
\usepackage{url}
\usepackage{lmodern}
 % \usepackage[usenames,dvipsnames,svgnames,table]{xcolor}%[usenames,dvipsnames,svgnames,table]

\let\oldtabular=\tabular
\def\tabular{\small\oldtabular}
\setcounter{tocdepth}{1}
%  \usetheme{Warsaw}
 \graphicspath{{figures/}}
  \author{Clément Théry, Christophe Biernacki, Gaétan Loridant}\institute{ArcelorMittal Dunkerque, Université de Lille 1,équipe M$\Theta$dal Inria}
\title{CorReg}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
%\date{} 
%\subject{} 

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\tableofcontents
\end{frame}

\section{Context}
	\subsection{Industrial context}
		\begin{frame}
				  \begin{enumerate}
				\item Steel industry databases.
				\item Goal: To understand and prevent quality problems on finished product, knowing the whole process, \underline{without a priori}.
			\end{enumerate}
	        \begin{center}
	          \begin{tabular}{ccc}
	         \includegraphics[width=70px,height=70px]{liquid.jpg} & \includegraphics[width=70px,height=70px]{Brame1.jpg} & \includegraphics[width=70px,height=70px]{Brame.jpg} \\
	          	\includegraphics[width=70px,height=70px]{ecras_moy.jpg} & \includegraphics[width=70px,height=70px]{tcc2.jpg} & \includegraphics[width=70px,height=70px]{bobines.jpg}
	          \end{tabular}
	        \end{center}
		\end{frame}
	\subsection{Statistical context}
		\begin{frame}{Regression}
			\begin{equation}
				\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\label{regressionsimple}
			\end{equation}
			where 	$\boldsymbol{\varepsilon}\sim \mathcal{N}(0,\sigma_Y^2\boldsymbol{I}_n)$	
		\end{frame}
		
		\begin{frame}{OLS}
		\begin{figure}[h!]
	\centering
	\includegraphics[width=220px]{figures/OLS_geometric_interpretation.png}
	\caption{Multiple linear regression with Ordinary Least Squares seen as a projection on the $d-$dimensional hyperplane spanned by the regressors $\boldsymbol{X}$. Public domain image.} \label{geomOLS}
	\end{figure}
		\end{frame}
		
		\begin{frame}{OLS}
		$\boldsymbol{\beta}$ can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{ols}), that is the unbiased maximum likelihood estimator \cite{saporta2006probabilites,dodge2004analyse}: %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}_{OLS}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaOLS}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}. \label{eq:varOLS}
	\end{equation}
	 In fact it is the Best Linear Unbiased Estimator (BLUE).
	 The theoretical \textsc{mse} is given by
	\begin{equation}
	\textsc{mse}(\hat{\boldsymbol{\beta}}_{OLS})= \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}). \nonumber 
	\end{equation}
		\end{frame}
		
\begin{frame}{Running example}

%$d=5$ with four independent scaled Gaussian 
$\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{X}^4,\boldsymbol{X}^5  \sim \mathcal{N}(0,1)$  and 
$\boldsymbol{X}^3=\boldsymbol{X}^1+\boldsymbol{X}^2+\boldsymbol{\varepsilon}_1$ where 
$\boldsymbol{\varepsilon}_1\sim{\mathcal{N}(\boldsymbol{0},\sigma_1^2\boldsymbol{I}_n)}$. \\
Two {\it scenarii} for $\boldsymbol{Y}$:\\
 $\boldsymbol{\beta}=(1,1,1,1,1)'$ and $\sigma_Y \in \{10,20\}$.
 \\
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_1$ gets smaller. $R^2$ stands for the coefficient of determination which is here:
	\begin{equation}\label{defR2}
	R^2=1-\frac{\operatorname{Var}(\boldsymbol{\varepsilon}_1)}{\operatorname{Var}(\boldsymbol{X}^3)}
	\end{equation}

\end{frame}	
	
	\begin{frame}
	\begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/OLScompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates (running example). } \label{MQEOLScompl}
	\end{figure}	
\end{frame}		
		\begin{frame}{Ridge Regression}
		\cite{hoerl1970ridge,marquardt1975ridge} proposes a possibly biased estimator for $\boldsymbol{\beta}$ that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \eta \textrm{ with } \eta>0
	\end{equation}
	But this penalty is not guided by correlations. 
	The solution of the ridge regression is given by
	\begin{equation}
		 \hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} -\lambda\boldsymbol{I}_n\right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaridge}
	\end{equation}
 Methods do exist to automatically choose a good value for $\lambda$ \cite{cule2013ridge,er2013systematic} and a R package called {\tt ridge} is on {\sc cran} \cite{packageridge}.
		\end{frame}
		
		\begin{frame}{Ridge Regression}
 \begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/ridgecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEridgecompl}
	\end{figure}
			\end{frame}
		
		\begin{frame}{LASSO}
		The Least Absolute Shrinkage and Selection Operator ({\sc lasso}, \cite{tibshirani1996regression} and \cite{tibshiranilasso}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$ instead of the $L_2$ penalty of the ridge regression:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0 . \nonumber 
		\end{equation}	
		
		
		
		\end{frame}
		\begin{frame}{LASSO}
		\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/lasso.png} 
			\caption{Geometric view of the Penalty for the \textsc{lasso} (left) compared to ridge regression (right) as shown in the book from Hastie \cite{hastie2009elements}} \label{lassogeom}
		\end{figure}
		Figure shows the contour of error (red) and constraint function (blue).
		  The axis stands for the regression coefficients.
		\end{frame}
		
\begin{frame}

		Here again we have to choose a value for $\lambda$.
	 The Least Angle Regression (\textsc{LAR} \cite{efron2004least}) algorithm offers a very efficient way to obtain the whole \textsc{lasso} path.  It can be used through the {\tt lars } package on {\sc cran} (\cite{packagelars}).
\end{frame}		

\begin{frame}
 \begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/larcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{lar}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQElarcompl}
	\end{figure}	
\end{frame}
		
		
		
		\begin{frame}{ SEM }
		 Modélisation de la structure mais à la main et aucun impact sur l'estimation
		 \end{frame}
		 
		\begin{frame}{ Selvarclust }
			Semble très bien mais n'aboutit pas vers la régression donc on le prolonge en CorReg
		\end{frame}
\section{Proposed Models}
	\begin{frame}{•}
	
	\end{frame}
	\subsection{Marginal model}
		\begin{frame}{•}
		
		\end{frame}
	\subsection{Plug-in model}
		\begin{frame}{•}
		
		\end{frame}
\section{Structure estimation}
	\begin{frame}{•}
	
	\end{frame}

\section{Results}
	\subsection{Simulation results}
		\begin{frame}{•}
		
		\end{frame}
	\subsection{Industrial results}
		\begin{frame}{•}
		
		\end{frame}
\section{Missing values}
	\begin{frame}{•}
	Modèle génératif complet avec dépendances
	\end{frame}
	\begin{frame}{•}
	Explosion des mélanges 
	\end{frame}
	\begin{frame}{•}
	SEM avec Gibbs 
	\end{frame}
	\begin{frame}{•}
	Bic pondéré 
	\end{frame}
	\begin{frame}{•}
	Résultats pourris 
	\end{frame}
\section{Tools}
	\begin{frame}{•}
		Excel, fonctions graphiques, arbres de décision
	\end{frame}

\begin{frame}
\bibliographystyle{apalike}
%\bibliographystyle{plain}
%\nocite{*}
\bibliography{biblio}
\end{frame}

\end{document}