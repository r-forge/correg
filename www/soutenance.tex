\documentclass[11pt]{beamer}
\usetheme{Malmoe}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
  %\usepackage{color}
  \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage[english]{babel}
\usepackage{listings} 
\usepackage[toc,page]{appendix}
\usepackage{array}%,multirow,makecell}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{graphicx}
%\usepackage{framed}
%\usepackage[tikz]{bclogo}
\usepackage{hyperref}
\usepackage{url}
\usepackage{lmodern}
 % \usepackage[usenames,dvipsnames,svgnames,table]{xcolor}%[usenames,dvipsnames,svgnames,table]

\let\oldtabular=\tabular
\def\tabular{\small\oldtabular}
\setcounter{tocdepth}{1}
%  \usetheme{Warsaw}
 \graphicspath{{figures/}}
  \author{Clément Théry, Christophe Biernacki, Gaétan Loridant}\institute{ArcelorMittal Dunkerque, Université de Lille 1,équipe M$\Theta$dal Inria}
\title{CorReg}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
%\date{} 
%\subject{} 

%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}
%%%% fin macro %%%%

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\tableofcontents
\end{frame}

\section{Context}
	\subsection{Industrial context}
		\begin{frame}
				  \begin{enumerate}
				\item Steel industry databases.
				\item Goal: To understand and prevent quality problems on finished product, knowing the whole process, \underline{without a priori}.
			\end{enumerate}
	        \begin{center}
	          \begin{tabular}{ccc}
	         \includegraphics[width=70px,height=70px]{liquid.jpg} & \includegraphics[width=70px,height=70px]{Brame1.jpg} & \includegraphics[width=70px,height=70px]{Brame.jpg} \\
	          	\includegraphics[width=70px,height=70px]{ecras_moy.jpg} & \includegraphics[width=70px,height=70px]{tcc2.jpg} & \includegraphics[width=70px,height=70px]{bobines.jpg}
	          \end{tabular}
	        \end{center}
		\end{frame}
	\subsection{Statistical context}
		\begin{frame}{Linear Regression}
			\begin{equation}
				\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\label{regressionsimple}
			\end{equation}
			where 	$\boldsymbol{\varepsilon}\sim \mathcal{N}(0,\sigma_Y^2\boldsymbol{I}_n)$	
		\end{frame}
		
		
		
		\begin{frame}{MLE}
		$\boldsymbol{\beta}$ can be estimated by $\hat{\boldsymbol{\beta}}$ with Ordinary Least Squares (\textsc{ols}), that is the unbiased maximum likelihood estimator \cite{saporta2006probabilites,dodge2004analyse}: %As shown in section \ref{sectionOLS}, 
	\begin{equation}
		\boldsymbol{\hat{\beta}}_{OLS}=\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaOLS}
	\end{equation}
	with variance matrix
	\begin{equation}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_{OLS})=\sigma_Y^2\left(\boldsymbol{X}'\boldsymbol{X} \right) ^{-1}. \label{eq:varOLS}
	\end{equation}
	 In fact it is the Best Linear Unbiased Estimator (BLUE).
	 The theoretical \textsc{mse} is given by
	\begin{equation}
	\textsc{mse}(\hat{\boldsymbol{\beta}}_{OLS})= \sigma_Y^2 \operatorname{Tr}((\boldsymbol{X}'\boldsymbol{X})^{-1}). \nonumber 
	\end{equation}
		\end{frame}
		\begin{frame}{OLS}
		\begin{figure}[h!]
	\centering
	\includegraphics[width=220px]{figures/OLS_geometric_interpretation.png}
	\caption{Multiple linear regression with Ordinary Least Squares seen as a projection on the $d-$dimensional hyperplane spanned by the regressors $\boldsymbol{X}$. Public domain image.} \label{geomOLS}
	\end{figure}
		\end{frame}
\begin{frame}{Running example}

%$d=5$ with four independent scaled Gaussian 
$\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{X}^4,\boldsymbol{X}^5  \sim \mathcal{N}(0,1)$  and 
$\boldsymbol{X}^3=\boldsymbol{X}^1+\boldsymbol{X}^2+\boldsymbol{\varepsilon}_1$ where 
$\boldsymbol{\varepsilon}_1\sim{\mathcal{N}(\boldsymbol{0},\sigma_1^2\boldsymbol{I}_n)}$. \\
Two {\it scenarii} for $\boldsymbol{Y}$:\\
 $\boldsymbol{\beta}=(1,1,1,1,1)'$ and $\sigma_Y \in \{10,20\}$.
 \\
It is clear that $\boldsymbol{X}'\boldsymbol{X}$ will become more ill-conditioned as $\sigma_1$ gets smaller. $R^2$ stands for the coefficient of determination which is here:
	\begin{equation}\label{defR2}
	R^2=1-\frac{\operatorname{Var}(\boldsymbol{\varepsilon}_1)}{\operatorname{Var}(\boldsymbol{X}^3)}
	\end{equation}

\end{frame}	
	
	\begin{frame}
	\begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/OLScompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{OLS}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates (running example). } \label{MQEOLScompl}
	\end{figure}	
\end{frame}		
		\begin{frame}{Ridge Regression}
		\cite{hoerl1970ridge,marquardt1975ridge} proposes a possibly biased estimator for $\boldsymbol{\beta}$ that can be written in terms of a parametric $L_2$ penalty:
	\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin}_{\boldsymbol{\beta}} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel \boldsymbol{\beta} \parallel_2^2\leq \eta \textrm{ with } \eta>0
	\end{equation}
	But this penalty is not guided by correlations. 
	The solution of the ridge regression is given by
	\begin{equation}
		 \hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}'\boldsymbol{X} -\lambda\boldsymbol{I}_n\right) ^{-1}\boldsymbol{X}'\boldsymbol{Y}\label{betaridge}
	\end{equation}
 Methods do exist to automatically choose a good value for $\lambda$ \cite{cule2013ridge,er2013systematic} and a R package called {\tt ridge} is on {\sc cran} \cite{packageridge}.
		\end{frame}
		
		\begin{frame}{Ridge Regression}
 \begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/ridgecompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{ridge}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQEridgecompl}
	\end{figure}
			\end{frame}
		
		\begin{frame}{LASSO}
		The Least Absolute Shrinkage and Selection Operator ({\sc lasso}, \cite{tibshirani1996regression} and \cite{tibshiranilasso}) consists in a shrinkage of the regression coefficients based on a $\lambda$ parametric $L_1$ penalty to obtain zeros in $\hat{\boldsymbol{\beta}}$ instead of the $L_2$ penalty of the ridge regression:
		\begin{equation}
		 \boldsymbol{\hat{\beta}}=\operatorname{argmin} \left\lbrace \parallel \boldsymbol{Y}-\boldsymbol{X\beta}\parallel_2^2 \right\rbrace \textrm{ subject to } \parallel\boldsymbol{\beta} \parallel_1\leq \lambda \textrm{ with } \lambda>0 . \nonumber 
		\end{equation}	
		
		
		
		\end{frame}
		\begin{frame}{LASSO}
		\begin{figure}[h!]
			\centering
			\includegraphics[width=150px]{figures/lasso.png} 
			\caption{Geometric view of the Penalty for the \textsc{lasso} (left) compared to ridge regression (right) as shown in the book from Hastie \cite{hastie2009elements}} \label{lassogeom}
		\end{figure}
		Figure shows the contour of error (red) and constraint function (blue).
		  The axis stands for the regression coefficients.
		\end{frame}
		
	

\begin{frame}
 \begin{figure}
	 \centering
	  \includegraphics[width=250px]{figures/MQEreel/larcompl.png}
	  \caption{Evolution of observed Mean Squared error on $\hat{\boldsymbol{\beta}}_{lar}$ with the strength of the correlations for various sample sizes and strength of regression. $d=5$ covariates. } \label{MQElarcompl}
	\end{figure}	
	{\tt lars } package on {\sc cran} (\cite{packagelars}).
\end{frame}
		
		
		
		\begin{frame}{Simultaneous Equation Modeling }
		 \begin{eqnarray}
		 	\boldsymbol{X}_{n,d}&=&\boldsymbol{X}_{n,d}\boldsymbol{\beta}_{d,d} + \boldsymbol{\varepsilon}_{n,d} \nonumber
		 \end{eqnarray}
		 No real "response variable", no real estimators for recursive {\sc sem}, no real variable selection (structure finder)
		 \end{frame}
		 
		\begin{frame}{ {\tt Selvarclust} }
			\begin{equation}
	\boldsymbol{X}^{U}%|\boldsymbol{X}^{R};\boldsymbol{\alpha},\boldsymbol{\Omega}
	=a+\boldsymbol{X}^R \boldsymbol{\alpha}+ \boldsymbol{\varepsilon}  \nonumber 
\end{equation}
	\begin{itemize}
	\item Structure based on stepwise \cite{raftery2006variable,miller2002subset}
	\item Made only for clustering purpose.
\end{itemize}	  
	 
		\end{frame}
\section{Proposed Models}
	\begin{frame}{Hypothesis 1}
	There are $d_{r}\geq 0$ ``sub-regressions'', each sub-regression $j=1,\ldots,d_{r}$ having the covariate $\boldsymbol{X}^{J_{r}^j}$ as {\it response} variable ($J_{r}^j\in\{1,\ldots,p\}$ and $J_{r}^j\neq J_{r}^{j'}$ if $j\neq j'$) and having the $d_p^j>0$ covariates $\boldsymbol{X}^{J_{p}^j}$  as {\it predictor} variables ($J_{p}^j\subset\{1,\ldots,d\} \backslash J_{r}^j$ and $d_p^j=|J_{p}^j|$ the cardinal of $J_{p}^j$):
\begin{equation}
\boldsymbol{X}^{J_{r}^j}%|\boldsymbol{X}^{J_{p}^j};\boldsymbol{\alpha}_j,\sigma^2_j
=\boldsymbol{X}^{J_{p}^j}\boldsymbol{\alpha}_j+\boldsymbol{\varepsilon}_j, \label{eq:SR}
\end{equation}
where $\boldsymbol{\alpha}_j\in{\mathbb{R}^{d_r^j}}$ (${\alpha}_j^h\neq 0$ for all $j=1,\ldots,d_r$ and $h=1,\ldots,d_p^j$) and $\boldsymbol{\varepsilon}_j \sim\mathcal{N}_n(\boldsymbol{0},\sigma^2_j\boldsymbol{I})$.
	\end{frame}
	\begin{frame}{Hypothesis 2}
	The response covariates and the predictor covariates are totally disjoint: for any sub-regression $j=1,\ldots,d_{r}$, $J_{p}^j\subset J_f$ where $J_{r}=\{J_{r}^1,\ldots,J_{r}^{d_r}\}$ is set of all response covariates and $J_f=\{1,\ldots,d\} \backslash J_{r}$ is the set of all {\it non} response covariates of cardinal $d_f=d-d_r=|J_f|$. We call this hypothesis the \underline{uncrossing rule}. Then:
	\begin{equation}
			\boldsymbol{Y} %{|\boldsymbol{X},\boldsymbol{S}};\boldsymbol{\beta},\sigma_Y^2
			=\boldsymbol{X}_f\boldsymbol{\beta}_f+\boldsymbol{X}_r\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y. \label{eq:MainR}
\end{equation}
	\end{frame}
	
	\begin{frame}{Hypotheses 3}
	We assume that all errors $\boldsymbol{\varepsilon}_Y$ and $\boldsymbol{\varepsilon}_j$ ($j=1,\ldots,d_r$) are {\it mutually independent}. It implies in particular that conditional response covariates $\{\boldsymbol{X}^{J_{r}^j}|\boldsymbol{X}^{J_{p}^j},\boldsymbol{S};\boldsymbol{\alpha}_j,\sigma^2_j\}$ are {\it mutually independent}:
\begin{equation}\label{eq:H3}
\mathbb{P}(\boldsymbol{X}_r | \boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2) = \prod_{j=1}^{d_r} \mathbb{P}(\boldsymbol{X}^{J_{r}^j}|\boldsymbol{X}^{J_{p}^j},\boldsymbol{S};\boldsymbol{\alpha}_j,\sigma^2_j). 
\end{equation}
	\end{frame}
	\subsection{Marginal model}
		\begin{frame}{Marginal model}
		 We obtain  for the distribution of $\{\boldsymbol{Y} |\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2\}$:
\begin{eqnarray}
	\boldsymbol{Y}%|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2
	&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\alpha}_j^*)+  \sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\varepsilon}_j+\boldsymbol{\varepsilon}_Y \label{eq:Trueexpl} \\
	&=&\boldsymbol{X}_f\boldsymbol{\beta}_f^*+\boldsymbol{\varepsilon}_Y^*,\label{eq:modexpl}
\end{eqnarray}
where $\boldsymbol{\alpha}^*_j \in \mathbb{R}^{d_f}$ with $(\boldsymbol{\alpha}_j^*)_{J_p^j}=\boldsymbol{\alpha}_j $ and $(\boldsymbol{\alpha}_j^*)_{J_f\setminus J_p^j}=\bf 0 $. We define $\boldsymbol{\alpha}^* \in \mathbb{R}^{(d_f \times d_r)}$ to use more compact notations:
\begin{eqnarray}
	\boldsymbol{X}_r %|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\alpha},\boldsymbol{\sigma}^2
	&=&\boldsymbol{X}_f\boldsymbol{\alpha}^*+\boldsymbol{\varepsilon} \nonumber \\
	\boldsymbol{Y}%|\boldsymbol{X}_f,\boldsymbol{S};\boldsymbol{\beta},\boldsymbol{\alpha},\sigma_Y^2,\boldsymbol{\sigma}^2
	&=&\boldsymbol{X}_f (\boldsymbol{\beta}_f+ \boldsymbol{\alpha}^*\boldsymbol{\beta_{r}})+ \boldsymbol{\varepsilon}\boldsymbol{\beta_{r}}+\boldsymbol{\varepsilon}_Y \label{eq:Trueexpl2}
\end{eqnarray} 
Where $\boldsymbol{\varepsilon}$ is the $n\times d_r$ matrix whose columns are the $\boldsymbol{\varepsilon}_j$, the noises of the sub-regressions.
		\end{frame}
	\subsection{Plug-in model}
		\begin{frame}{Plug-in model}
%\fcolorbox{jaunefonce}{jauneclair}{\begin{minipage}{1\textwidth}
\begin{equation}
	\boldsymbol{\varepsilon}_Y^*%|\boldsymbol{\varepsilon};\boldsymbol{\beta}_r,\sigma_Y^2
	=\boldsymbol{\varepsilon}\boldsymbol{\beta}_r+\boldsymbol{\varepsilon}_Y. \label{eq:regplugin} 
\end{equation}
%\end{minipage}}
Then the Best Linear Unbiased Estimator ({\sc BLUE}) for $\boldsymbol{\beta}_r$ is given ({\sc mle} estimator) by:
\begin{equation}
  \hat{\boldsymbol{\beta}_r}=(\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon})^{-1}\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}_Y^* . \label{olsplugin}
\end{equation}
    And we have the following estimators:
	\begin{eqnarray}
		\hat{\boldsymbol{\varepsilon}}&=& \boldsymbol{X}_r-\boldsymbol{X}_f\hat{\boldsymbol{\alpha}}^* \nonumber \textrm{ and} \\
		\hat{\boldsymbol{\varepsilon}}_Y^*&=&\boldsymbol{Y}-\boldsymbol{X}_f\hat{\boldsymbol{\beta}}_f^* \nonumber
	\end{eqnarray}
	that we can use by plug-in.
	\end{frame}
	\begin{frame}{Plug-in model}
	\begin{eqnarray}
		\hat{\boldsymbol{\beta}}_r^{\varepsilon}&=&(\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}})^{-1}\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}}_Y^*  \nonumber 
	\end{eqnarray}
that depends on all covariates in $\boldsymbol{X}$ and relies on the estimated coefficients of sub-regressions $\hat{\boldsymbol{\alpha}}^*$ and on the estimate $\hat{\boldsymbol{\beta}}_f^*$ of the coefficients in the marginal model.
Then we can estimate $\boldsymbol{Y}$ by:
\begin{eqnarray}
	\hat{\boldsymbol{Y}}_{plug-in}&=&\boldsymbol{X}_f\hat{\boldsymbol{\beta}}^*_f + \hat{\boldsymbol{\varepsilon}}\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}. \label{hatYplugin} 
\end{eqnarray}
We can improve estimation of $\boldsymbol{\beta}_f$ (in terms of bias) by doing an additional identification step.	We know that $\boldsymbol{\beta}^*_f=\boldsymbol{\beta}_f+\boldsymbol{\alpha}^*\boldsymbol{\beta}_r $ so we naturally define the following estimator:
			\begin{equation}
			\hat{\boldsymbol{\beta}}_f^{\varepsilon}=\hat{\boldsymbol{\beta}}^*_f-\hat{\boldsymbol{\alpha}}^*\hat{\boldsymbol{\beta}}_{r}^{\varepsilon}. \nonumber 
			\end{equation}
		\end{frame}
		
		\begin{frame}{Marginal model's properties}
				\begin{equation}
			\mathbb{E}(\hat{\boldsymbol{\beta}}_{f}^*)=\boldsymbol{\beta}_{f}+\sum_{j =1}^{d_r}\beta_{J_r^j}\boldsymbol{\alpha}_j^* \textrm{ and }\mathbb{E}(\hat{\boldsymbol{\beta}}_{r}^*)=\boldsymbol{0}  \nonumber 
		\end{equation}
		
		\begin{equation}
			\operatorname{Var}(\hat{\boldsymbol{\beta}}_f^*)= (\sigma^2_Y+\sum_{j =1}^{d_r}\sigma^2_{j}\beta_{J_r^j}^2)(\boldsymbol{X}_f' \boldsymbol{X}_f)^{-1} \quad \textrm{and} \quad\operatorname{Var}(\hat{\boldsymbol{\beta}}_r^*)= \boldsymbol{0}. \label{eq:varOLS*}
		\end{equation}
		\end{frame}
		
\begin{frame}{Plug-in model's properties}
		Asymptotically unbiased (consistent estimators + continuous mapping theorem)
\begin{eqnarray}
	\operatorname{Var}(\hat{\boldsymbol{\beta}}_r^{\varepsilon})
	&=& \sigma^2_Y\big[\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}}[\hat{\boldsymbol{\varepsilon}}'(\boldsymbol{I}_n-\boldsymbol{H}_f)\hat{\boldsymbol{\varepsilon}}]^{-1}\hat{\boldsymbol{\varepsilon}}'\hat{\boldsymbol{\varepsilon}}\big]^{-1} \nonumber \\
	&=&\sigma^2_Y\big[(\boldsymbol{X}_r^{\varepsilon})'\boldsymbol{X}_r^{\varepsilon}\big]^{-1}
\nonumber
\end{eqnarray}

\begin{eqnarray}
		\operatorname{Var}(\hat{\boldsymbol{\beta}}_f^{\varepsilon})
		&=&\sigma_Y^2\big[(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}+\hat{\boldsymbol{\alpha}}^*\big[(\boldsymbol{X}_r^{\varepsilon})'\boldsymbol{X}_r^{\varepsilon}\big]^{-1}(\hat{\boldsymbol{\alpha}}^*)'\big].\nonumber \label{varpluginf}
	\end{eqnarray}		
		
\end{frame}



\begin{frame}
	\fontsize{8}{8}\selectfont{\begin{eqnarray}
	&&\operatorname{Var}_{ols}(\hat{\boldsymbol{\beta}}_{f})= \nonumber \\ 
	&& \sigma_Y^2\big[ 
(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}+(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{X}_r
[\boldsymbol{X}_r'\boldsymbol{X}_r-\boldsymbol{X}_r'\boldsymbol{X}_f(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{X}_r]^{-1}
\boldsymbol{X}_r'\boldsymbol{X}_f(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\big] , \nonumber 
%	\nonumber ,\\
%	\operatorname{Var}_{ols}(\hat{\boldsymbol{\beta}}_{r})= 
%	\nonumber .
\end{eqnarray}}
\begin{equation}
\qquad = \sigma_Y^2\big[ 
(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}+\underbrace{(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{X}_r}_{\hat{\boldsymbol{\alpha}}^*_{ols}}
[ \boldsymbol{X}_r'(\boldsymbol{I}_n-\boldsymbol{H}_f)\boldsymbol{X}_r]^{-1}
\underbrace{\boldsymbol{X}_r'\boldsymbol{X}_f(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}}_{(\hat{\boldsymbol{\alpha}}^*_{ols})'}\big] \nonumber
\end{equation}
\begin{eqnarray}
	\operatorname{Var}_{ols}(\hat{\boldsymbol{\beta}}_{r})&=&
	\sigma_Y^2\big[ \boldsymbol{X}_r'\boldsymbol{X}_r-\boldsymbol{X}_r'\boldsymbol{X}_f(\boldsymbol{X}_f'\boldsymbol{X}_f)^{-1}\boldsymbol{X}_f'\boldsymbol{X}_r\big]^{-1} \\ \nonumber
	&=&\sigma_Y^2\big[ \boldsymbol{X}_r'(\boldsymbol{I}_n-\boldsymbol{H}_f)\boldsymbol{X}_r\big]^{-1}. \nonumber
\end{eqnarray}
\end{frame}		

		\begin{frame}
		\begin{figure}[h!]
\centering
	\includegraphics[width=280px]{figures/MQE_toutOLSp5col.png}
	\caption{\textsc{mse} on $\hat{\boldsymbol{\beta}}$ of \textsc{ols} (plain red) and {\tt CorReg} marginal (blue dashed) and {\tt CorReg} plug-in (green dotted) estimators for varying $R^2$ of the sub-regression, $n$ and $\sigma_Y$. Results obtained on the running example with $d=5$ covariates.}\label{MQE2}
\end{figure}
		\end{frame}
		
\begin{frame}{Lasso Consistency}
Consistency issues of the {\sc lasso} are well known and Zhao \cite{Zhao2006MSC} gives a very simple example to illustrate it.
		We have taken the same example to show how our method is better to find the true relevant covariates.
		Here $d=3$ and $n=1\;000$.\\
		We define $\boldsymbol{X}^1,\boldsymbol{X}^2, \boldsymbol{\varepsilon}_Y, \boldsymbol{\varepsilon}_{1} \quad i.i.d. \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}_n)$ and then \\
		\begin{eqnarray}
		\boldsymbol{X}^3%|\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{S};\sigma_1^2
		&=&\frac{2}{3}\boldsymbol{X}^1+\frac{2}{3}\boldsymbol{X}^2+\frac{1}{3}\boldsymbol{\varepsilon}_1 \textrm{ and}  \nonumber \\
		\boldsymbol{Y}%|\boldsymbol{X}^1,\boldsymbol{X}^2,\boldsymbol{S};\sigma_Y^2
		&=&2\boldsymbol{X}^1+3\boldsymbol{X}^2+\boldsymbol{\varepsilon}_Y. \nonumber 
		\end{eqnarray}
\end{frame}		
	\begin{frame}{Lasso Consistency}
True $\boldsymbol{S}$ was found $991$ times on $1\;000$ tries.% (model is not identifiable because $\boldsymbol{X}^j$ are all Gaussian).
We look at the consistency  that is the real stake:
		\begin{table}[h!]	
		\resizebox{\linewidth}{!}{%
		\begin{tabular}{|c|c|c|c|}
		\hline 
		 & Classical {\sc lasso} & {\tt CorReg}  marginal + {\sc lasso}& {\tt CorReg}  full plug-in  + {\sc lasso} \\ 
		\hline 
		True $\boldsymbol{S}$ &  0 & \textbf{1000} & \textbf{835} \\ 
		\hline 
		$\hat{\boldsymbol{S}}$ & 0 & \textbf{991} & \textbf{829} \\ 
		\hline 
		\end{tabular} 
		}
		\caption{Number of consistent models found on $1\;000$ tries.}\label{testidentifiableG}
		\end{table}	
\end{frame}		
		
\section{Structure estimation}
	\begin{frame}
	\fontsize{8}{8}\selectfont{
	\begin{displaymath}G=\left(	\begin{array}{ccccc}
	0 & 0 & 1 & 0 & 0 \\ 
	0 & 0 & 1 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 0 & 0
	\end{array} \right)
\end{displaymath}
}
	\begin{figure}[h!]
	\centering
	\includegraphics[width=200px]{figures/graphetoy.png} 
	\caption{The bipartite graph associated to the running example. $\boldsymbol{X}_r$ is green and $\boldsymbol{X}_f$ is blue.}
	\end{figure}
	\end{frame}
	
\begin{frame}{Hypothesis 4: Full generative model}

All covariates $\boldsymbol{X}^j$ with $j \in J_f$ are mutually independent and arise from the following Gaussian mixture of $K_j$ components
\begin{equation}
\forall 1\leq i \leq n, \mathbb{P}(x_{i,j}|\boldsymbol{S};\boldsymbol{\pi}_{j},\boldsymbol{\mu}_j,\boldsymbol{\Sigma}_j) = \sum_{h=1}^{K_j} \pi_{j,h} \Phi(x_{i,j};\mu_{j,h} ,\Sigma_{j,h}), \nonumber
\end{equation}
where $\boldsymbol{\pi}_{j}=(\pi_{j,1},\ldots,\pi_{j,K_j})$ is the vector of mixing proportions with $\forall 1\leq h\leq k_j , \pi_{j,h}>0$ and $\sum_{h=1}^{K_j}\pi_{j,h}=1$, $\boldsymbol{\mu}_j=(\mu_{j,1},\ldots,\mu_{j,K_j})$  is the vector of centres and $\boldsymbol{\Sigma}_j=(\Sigma_{j,1},\ldots,\Sigma_{j,K_j})$ is the vector of variances and $\Phi$ is the Gaussian density function. \\We stack together all these mixture parameters in $\boldsymbol{\theta}=(\boldsymbol{\pi}_{j},\boldsymbol{\mu}_j,\boldsymbol{\Sigma}_j ; j \in J_f)$. We now have a full generative model on $\boldsymbol{X}$.
\end{frame}

\begin{frame}{Penalized BIC}
we propose to introduce some information in $\mathbb{P}(\boldsymbol{S})$ promoting simple models through the following {\it hierarchical} uniform distribution denoted by $\mathbb{P}_H(\boldsymbol{S})$:
\fontsize{10}{10}\selectfont{\begin{eqnarray}
\mathbb{P}_H(\boldsymbol{S}) & = & \mathbb{P}_H(\boldsymbol{J}_r,\boldsymbol{J}_p) \nonumber \\
 & = & \mathbb{P}_H(\boldsymbol{J}_r,\boldsymbol{J}_p,d_r,\boldsymbol{d}_p)  \nonumber \\
 & = & \mathbb{P}_U(\boldsymbol{J}_p|\boldsymbol{d}_p,\boldsymbol{J}_r,d_r) \times \mathbb{P}_U(\boldsymbol{d}_p|\boldsymbol{J}_r,d_r) \times \mathbb{P}_U(\boldsymbol{J}_r|d_r)\times \mathbb{P}_U(d_r)  \nonumber \\
 & = & \left[\prod_{j=1}^{d_r} \left(\begin{array}{c} d-d_r \\ d_p^j \end{array}\right) \right]^{-1} \times \left[d-d_r\right]^{-d_r} \times \left[\left(\begin{array}{c} d \\ d_r \end{array} \right)\right]^{-1} \times [d+1]^{-1} \nonumber 
\end{eqnarray}
}
\end{frame}
\subsection{Markov chain}
\begin{frame}
		At each step the Markov chain moves with probability:
	\begin{eqnarray}
			\forall \boldsymbol{S} \in \mathcal{S}_d, \forall \boldsymbol{S}^+\in \mathcal{V}(\boldsymbol{S}): %\mathcal{P}_{\boldsymbol{S},\boldsymbol{S}^+}=
			\mathbb{P}(\boldsymbol{S}^+|\mathcal{V}(\boldsymbol{S}))&=&\frac{\exp{(-\mbox{{\sc bic}}_*(\boldsymbol{S}^+))}}{\sum_{\tilde{\boldsymbol{S}}\in \mathcal{V}(\boldsymbol{S})}\exp{(-\mbox{{\sc bic}}_*(\tilde{\boldsymbol{S}}))}} \nonumber
	\end{eqnarray}
	where $\mathcal{V}(\boldsymbol{S})$ is a neighbourhood and $\mathcal{S}_d$ is the set of feasible structures within $d$ variables.
\end{frame}

\begin{frame}
	For each step $(q)$, starting from $\boldsymbol{S} \in \mathcal{S}_d$ we define a neighbourhood:
		\begin{eqnarray}
		\mathcal{V}(\boldsymbol{S})&=& \{\boldsymbol{S} \}\cup \{ \boldsymbol{S}^{(i,j)} \in \mathcal{S}_d|(i,j) \in \mathcal{A}_{(q)}\} \nonumber 
	\end{eqnarray}	
	where $\mathcal{A}_{(q)}$ is a set of couples $(i,j) \in \{1,\dots,d \}^2$ with $i\neq j$ drawn at the step $(q)$ according to a strategy defined below and corresponding to the directed edge of the graph to modify (add or remove).
	And we have for  $\tilde{\boldsymbol{S}}=\boldsymbol{S}^{(i,j)}$:
	\begin{eqnarray}
		\forall (k,l)\neq (i,j), \quad	\tilde{\boldsymbol{G}}_{k,l}&=&\boldsymbol{G}_{k,l} \nonumber  \\
		\tilde{\boldsymbol{G}}_{i,j}&=&1-\boldsymbol{G}_{i,j}  \nonumber 
	\end{eqnarray}
	where $\tilde{\boldsymbol{G}}$ is the adjacency matrix associated to $\tilde{\boldsymbol{S}}$. Any strategy can be chosen for $\mathcal{A}_{(q)}$, from uniform distribution to specific heuristics.
\end{frame}
\subsection{Constraint Relaxation}
\begin{frame}
New definition of $\tilde{\boldsymbol{G}}$:
		Modification of the selected directed edge $(i,j)$ on the graph:
	\begin{eqnarray}
		\tilde{\boldsymbol{G}}_{i,j}&=&1-\boldsymbol{G}_{i,j} \textrm{ as usual and} \nonumber \\
		\forall k \neq i, l\neq j, \quad	\tilde{\boldsymbol{G}}_{k,l}&=&\boldsymbol{G}_{k,l} \nonumber 
\end{eqnarray}	
Column-wise relaxation :		
newly predictive covariate cannot be regressed anymore:
		\begin{eqnarray}
		\forall k \in \{1,\dots,d\}\setminus \{i\}, \tilde{\boldsymbol{G}}_{k,j}&=&\boldsymbol{G}_{i,j}\boldsymbol{G}_{k,j}  \nonumber  \\
				\end{eqnarray}
Row-wise relaxation: newly regressed covariate cannot be predictive anymore:
		\begin{eqnarray}
		\forall l \in \{1,\dots,d\}\setminus \{j\}, \tilde{\boldsymbol{G}}_{i,l}&=&\boldsymbol{G}_{i,j}\boldsymbol{G}_{i,l} \textrm{ (row-wise relaxation)} \nonumber 
	\end{eqnarray} 
\end{frame}

\begin{frame}
\begin{figure}
	\begin{minipage}[l]{.45\linewidth}
\includegraphics[width=.90\linewidth]{figures/identifiable/identifiabilite001.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\boldsymbol{G}=\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & 0 \\ 
		0 & 0 & 0 & 1 & 1 \\ 
		0 & 0 & 0 & 0 & 1 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & 0 & 0 & 0 & 0
		\end{array} \right)$
	\end{minipage}
	\caption{We start from a structure $\boldsymbol{S}=((4,5),(\{1,2\},\{2,3\}))$ and its associated matrix $G$}\label{ident1}
\end{figure}
\end{frame}
\begin{frame}		
\begin{figure}
	\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=.90\linewidth]{figures/identifiable/identifiabilite002.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & 0 \\ 
		0 & 0 & 0 & 1 & 1 \\ 
		0 & 0 & 0 & 0 & 1 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & \textcolor{red}{\textbf{1}}& 0 & 0 & 0
		\end{array} \right)$
	\end{minipage}
	\caption{We want to define the candidate $\tilde{\boldsymbol{S}}=\boldsymbol{S}^{(5,2)}$ and its associated matrix but the structure obtained would not be feasible (breaking the uncrossing rule). }\label{ident2}
\end{figure}
\end{frame}
\begin{frame}		
\begin{figure}
	\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=.90\linewidth]{figures/identifiable/identifiabilite003.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & \textcolor{green}{\textbf{0}} \\ 
		0 & 0 & 0 & 1 & \textcolor{green}{\textbf{0}} \\ 
		0 & 0 & 0 & 0 & \textcolor{green}{\textbf{0}} \\ 
		0 & 0 & 0 & 0 & \textcolor{green}{\textbf{0}} \\ 
		0 & \textcolor{red}{\textbf{1}}& 0 & 0 & \textcolor{green}{\textbf{0}}
		\end{array} \right)$
	\end{minipage}
	\caption{Column-wise relaxation: newly predictive covariate cannot be regressed anymore. }\label{ident3}
\end{figure}
\end{frame}
\begin{frame}		
\begin{figure}
	\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=.90\linewidth]{figures/identifiable/identifiabilite004.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\tilde{\boldsymbol{G}}=\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & 0 \\ 
		\textcolor{green}{\textbf{0}} & \textcolor{green}{\textbf{0}} & \textcolor{green}{\textbf{0}} & \textcolor{green}{\textbf{0}} & \textcolor{green}{\textbf{0}} \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & \textcolor{red}{\textbf{1}}& 0 & 0 & 0
		\end{array} \right)$
	\end{minipage}
	\caption{Row-wise relaxation: newly predictive covariate cannot be regressed anymore. }\label{ident4}
\end{figure}	
\end{frame}
\begin{frame}
\begin{figure}
	\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=.90\linewidth]{figures/identifiable/identifiabilite005.png} 
	\end{minipage}
	\begin{minipage}[c]{.45\linewidth}
		$\tilde{\boldsymbol{G}}=\left( \begin{array}{ccccc}
		0 & 0 & 0 & 1 & 0 \\ 
		0 &0 & 0 & 0 & 0 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & 0 & 0 & 0 & 0 \\ 
		0 & 1& 0 & 0 & 0
		\end{array} \right)$
	\end{minipage}
	\caption{We get a feasible candidate $\tilde{\boldsymbol{S}}=((2,4),(\{5\},\{1\}))$ that does differ from $\boldsymbol{S}$ in many points. }\label{ident5}
\end{figure}
\end{frame}
\begin{frame}
\begin{figure}
\centering
\includegraphics[width=250px]{figures/identifiable/identifiabilite006.png} 
	\caption{All this modifications are made in only one step in the MCMC, meaning an increased scope for the neighbourhoods.}\label{ident6}
\end{figure}
\end{frame}

\subsection{Pruning}
\begin{frame}{Pruning}
	Every sub-graph of a bipartite graph is bipartite thus every sub-graph can be reached. We propose an heuristic change in the strategy with:
			\begin{equation}
				\mathcal{A}_{(q)}=\{(i,J_r^j), i \in J_f, j \in \{1,\dots,d_r \}: i \in J_p^j \}. \nonumber 
			\end{equation}
\end{frame}
\subsection{Initializations}
\begin{frame}
	\begin{center}
		\begin{figure}[h!]
		\centering
		\includegraphics[width=250px]{figures/typeinit.png} %mars 2013 analyse plan ini marelax
		\caption{Evolution of the $\mbox{{\sc bic}}$ (criterion to minimize in the MCMC) for each method.}\label{Bicini}
		\end{figure}
	\end{center}
\end{frame}

\begin{frame}
	\begin{center}
	\begin{figure}[h!]
	\centering
		\includegraphics[width=250px]{figures/courbes_BICmoyen_marelax_nbinibis.png} 
		\caption{Comparison of distinct number of correlation-based initializations for the MCMC. Dark blue=1, red=10.}\label{nbini}
	\end{figure}
	\end{center}
		In the following, the chain was launched with twenty initializations each time, based on the correlation matrix.
\end{frame}

\begin{frame}
\begin{center}
	\begin{figure}[h!]
	\centering
\includegraphics[width=300px]{figures/complexitycompareMCMC.png} 		
\caption{Comparison of complexity evolution with or without constraint relaxation.}\label{comparecomplrelax}
	\end{figure}
	\end{center}
	\end{frame}

\begin{frame}	
\begin{center}
	\begin{figure}[h!]
	\centering
\includegraphics[width=300px]{figures/BICcompareMCMCrelax.png} 		
\caption{Comparison of $\mbox{{\sc bic}}$ evolution with or without constraint relaxation.}\label{compareBICrelax}
	\end{figure}
	\end{center}
\end{frame}

\section{Results}
	\subsection{Simulation results}
		
		\begin{frame}
			\begin{figure}[h!]
	 \includegraphics[width=250px]{figures/predhatS/tout/MSEpredOLSbeta_zonetout.png}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{\beta}}$, plain red=classical (complete) model, dashed blue=marginal model, dotted green=plug-in model. {\sc ols} estimators}
\end{figure}
		\end{frame}
				\begin{frame}
			\begin{figure}[h!]
	 \includegraphics[width=250px]{figures/predhatS/tout/MSEpredlarbeta_zonetout.png}
	\caption{Comparison of the \textsc{mse} on $\hat{\boldsymbol{\beta}}$, plain red=classical (complete) model, dashed blue=marginal model, dotted green=plug-in model. {\sc lasso} estimators}
\end{figure}
		\end{frame}
		\subsection{Non-linear cases}
		
\begin{frame}
We define $%\boldsymbol{X}_r|\boldsymbol{X}_f;\boldsymbol{\alpha},\boldsymbol{\sigma}^2=
\boldsymbol{X}^7%|\boldsymbol{X}^{\{1,2,3\}};a,\sigma_1^2
=a(\boldsymbol{X}^1)^2+\boldsymbol{X}^2+\boldsymbol{X}^3+ \boldsymbol{\varepsilon}_1$. The matrix $\boldsymbol{X}$ is then scaled before doing $$\boldsymbol{Y}%|\boldsymbol{X};\sigma_Y^2
=\sum_{i=1}^7\boldsymbol{X}^i+\boldsymbol{\varepsilon}_Y.$$ We let $a$ vary between $0$ and $10$ to increase progressively the non-linear part of the sub-regression. Once again, simulations has been made 100 times and the \textsc{mse} were computed with 1 000 individuals validation samples.
\end{frame}		
		
		\begin{frame}
\begin{figure}[h!] 
	\subfigure[Evolution of the quality of $\hat{\boldsymbol{S}}$ when the paramater $a$ increases]{
			\includegraphics[height=180px,width=230px]{figures/robust_S.png} 
			\label{resnonlin}
	} 
   \subfigure[\textsc{mse} on the main regression for \textsc{ols}(thick) and \textsc{lasso} (thin) used both with (plain) or without {\tt CorReg} (dotted).]{
			\includegraphics[height=180px,width=230px]{figures/robustY.png} \label{MSEnonlin}
   }
   \caption{Non-linear case analysis.}
\end{figure}		\end{frame}

		\begin{frame}
\begin{figure}[h!]  
   \subfigure[\textsc{mse} on the main regression for \textsc{ols}(thick) and \textsc{lasso} (thin) used both with (plain) or without {\tt CorReg} (dotted).]{
			\includegraphics[height=180px,width=230px]{figures/robustY.png} \label{MSEnonlin}
   }
\end{figure}		\end{frame}

	\subsection{Industrial results}
		\begin{frame}
		We have :
		\begin{itemize}
			\item a quality parameter (confidential) as response variable,
			\item $d=205$ variables from the whole process to explain it.
		\end{itemize}
We get a training set of $n=3\;000$ products described by these $205$ variables from the industrial process and also a validation sample of $847$ products.\\

The objective here is not only to predict non-quality but to understand and then to avoid it.
		\end{frame}
		
	\begin{frame}
	\begin{figure}[h!]
\begin{center}
			\includegraphics[height=150px, width=150px]{figures/res_article/gaussianmixture_exfo.png}
			\includegraphics[height=150px, width=150px]{figures/res_article/nb_comp_X_exfo.png}
\end{center}		
			\caption{Quality case study: (left) Example of a non-Gaussian real variable easily modeled by a Gaussian mixture, (right) distribution of the number of components found for each covariate.}\label{fig:graphMixmod.quality}
\end{figure}  	
	\end{frame}		
		
		\begin{frame}
		\begin{figure}[h!]
\begin{center}
			\includegraphics[width=220px, height=80px]{figures/histR2exfos.png}
\end{center}
			\caption{Quality case study: histogram of the adjusted $R^2_{adj}$ for the $d_r=76$ sub-regressions.}
\end{figure}
		 The width and the weight of a steel slab gives $|\rho|=0.905$, the temperature before and after some tool gives $|\rho|=0.983$, the  roughness of both faces of the product gives $|\rho|= 0.919$ and a particular mean and a particular max gives $|\rho|=0.911$.
		\end{frame}
		
		
\begin{frame}
		\begin{table}[h!]
\centering
\begin{tabular}{llcc}
	\hline
	Method& Indicator& {\tt CorReg}'s reduced model)& complete model \\ 
	\hline \hline
	{\sc ols} & {\sc mse} & 13.30 & 14.03 \\
		& complexity& 130 & 206 \\
	\hline
	{\sc lasso} & {\sc mse} & 12.77 & 12.96 \\
		& complexity& 24 & 21 \\
	\hline
	elasticnet & {\sc mse} & \textbf{12.15} & 13.52 \\
		& complexity& 40 & 78 \\
	\hline
	ridge & {\sc mse} & 12.69 & 13.09 \\
		& complexity& 130 & 206 \\
	\hline
\end{tabular} 
\caption{Quality case study: Results obtained on a validation sample ($n=847$ individuals). In bold, the best {\sc mse} value.}\label{Res_exfos}
\end{table}
\end{frame}		
		

		
\section{Missing values}
	\begin{frame}{Hope}
		Full generative model + explicit dependencies = missing values management
	\end{frame}
	\begin{frame}
	The number of component $k$ can be huge (combinations of all the components of the covariates in $\boldsymbol{X}_f$). In fact we have 
	\begin{equation}
		K=\prod_{j\in J_f}K_j  \nonumber 
	\end{equation}
	where $K_j$ is the number of components of the Gaussian mixture followed by $\boldsymbol{X}_j$ as defined in Hypothesis 4. It is clear that $K$ can really explode even if some components may be identical (but it happens with zero probability). For instance, if $\boldsymbol{X}$ contains only 10 independent Gaussian mixtures and with only 2 components each, then $\boldsymbol{X}$ will have up to $K=2^{10}=$ 1 024 components. And if these mixtures have 3 components each, then it rises up to $K=$ 59 049 components.
	\end{frame}
	
	\begin{frame}{Problem}
	\begin{itemize}
		\item Too much components
		\item The likelihood may not be linear
	\end{itemize}
	EM is not possible in practice.
	\end{frame}
	\begin{frame}{Stochastic EM}
		It is possible to simplify the problem by using Stochastic EM.\\
		But the number of components stays a problem.
	\end{frame}
\begin{frame}{Solution}
	Gibbs Sampling for stochastic imputation in the Stochastic EM
\end{frame}	
	
	\begin{frame}
 We define
			\begin{equation}
			c_j=\frac{1}{n}\sum_{i=1}^nc_{i,j},  \nonumber 
\end{equation}						where $c_{i,j}$ is the number of parameters to estimate in $\mathbb{P}(x_{i,j}|\boldsymbol{X}_i\setminus \boldsymbol{X}_i^j)$. If all predictors $\boldsymbol{X}^{J_p^j}_i$ are observed then we have to estimate $d_p^j$ coefficients and an intercept and the variance of the residual so  $c_{i,j}=d_p^j+2$. But for each missing predictor, we have to estimate the parameters of its distribution (Gaussian mixture with proportion, mean and variance to estimate for each component) so we have $$c_{i,j}=d_p^j+2 + \sum_{\substack{l \in J_p^j \\ M_{i,l}=1}} (3 K_l-1)$$  (the sum of the proportions is 1 so one estimation is useless for each mixture).
	\end{frame}
	\begin{frame}{Results}
	Very slow and too much variability
	\end{frame}
\section{Tools}
\begin{frame}
\begin{figure}
\centering
\includegraphics[width=280px]{figures/captureexcel.PNG} 
	\caption{Screenshot of the Graphical User Interface of {\tt CorReg} in Excel.}\label{captureexcel}
\end{figure}
\end{frame}

	\begin{frame}
		\begin{figure}
			\centering
				\includegraphics[width=250px]{figures/arbretoy.png} 
			\caption{Regression tree obtain with the package {\tt CorReg}
			 (graphical layer on top of  the {\tt rpart} package) on the running example.}\label{arbretoy}
		\end{figure}
		arbres de décision, showdata, Conan, {\it etc.}
	\end{frame}

\begin{frame}
\bibliographystyle{apalike}
%\bibliographystyle{plain}
%\nocite{*}
\bibliography{biblio}
\end{frame}
\begin{frame}
\begin{center}
\huge Merci pour ce moment\dots
\end{center} 
\end{frame}
\end{document}